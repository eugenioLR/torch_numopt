{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch_numopt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import *\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.f1 = nn.Linear(input_size, 10, device=device)\n",
    "        self.f2 = nn.Linear(10, 20, device=device)\n",
    "        self.f3 = nn.Linear(20, 20, device=device)\n",
    "        self.f4 = nn.Linear(20, 10, device=device)\n",
    "        self.f5 = nn.Linear(10, 1, device=device)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        # self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.f1(x))\n",
    "        x = self.activation(self.f2(x))\n",
    "        x = self.activation(self.f3(x))\n",
    "        x = self.activation(self.f4(x))\n",
    "        x = self.f5(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([800, 10]) torch.Size([800, 1])\n",
      "torch.Size([200, 10]) torch.Size([200, 1])\n"
     ]
    }
   ],
   "source": [
    "# X, y = load_diabetes(return_X_y=True, scaled=False)\n",
    "# X, y = make_regression(n_samples=1000, n_features=100)\n",
    "X, y = make_friedman1(n_samples=1000, noise=1e-2)\n",
    "\n",
    "X_scaler = MinMaxScaler()\n",
    "X = X_scaler.fit_transform(X)\n",
    "X = torch.Tensor(X).to(device)\n",
    "\n",
    "y_scaler = MinMaxScaler()\n",
    "y = y_scaler.fit_transform(y.reshape((-1, 1)))\n",
    "y = torch.Tensor(y).to(device)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "torch_data = TensorDataset(X_train, y_train)\n",
    "data_loader = DataLoader(torch_data, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.43445923924446106\n",
      "epoch:  1, loss: 0.2384142130613327\n",
      "epoch:  2, loss: 0.13895511627197266\n",
      "epoch:  3, loss: 0.08712276816368103\n",
      "epoch:  4, loss: 0.060338135808706284\n",
      "epoch:  5, loss: 0.04675840958952904\n",
      "epoch:  6, loss: 0.04001617804169655\n",
      "epoch:  7, loss: 0.03673045337200165\n",
      "epoch:  8, loss: 0.03515221178531647\n",
      "epoch:  9, loss: 0.034401919692754745\n",
      "epoch:  10, loss: 0.03404721990227699\n",
      "epoch:  11, loss: 0.03387952968478203\n",
      "epoch:  12, loss: 0.0337994359433651\n",
      "epoch:  13, loss: 0.033760275691747665\n",
      "epoch:  14, loss: 0.03374011814594269\n",
      "epoch:  15, loss: 0.03372875228524208\n",
      "epoch:  16, loss: 0.03371612727642059\n",
      "epoch:  17, loss: 0.03369523212313652\n",
      "epoch:  18, loss: 0.03368362784385681\n",
      "epoch:  19, loss: 0.03367273882031441\n",
      "epoch:  20, loss: 0.033651143312454224\n",
      "epoch:  21, loss: 0.03363918140530586\n",
      "epoch:  22, loss: 0.0336291529238224\n",
      "epoch:  23, loss: 0.033606477081775665\n",
      "epoch:  24, loss: 0.033594027161598206\n",
      "epoch:  25, loss: 0.033584319055080414\n",
      "epoch:  26, loss: 0.03356083109974861\n",
      "epoch:  27, loss: 0.03354795649647713\n",
      "epoch:  28, loss: 0.03353951871395111\n",
      "epoch:  29, loss: 0.03351474180817604\n",
      "epoch:  30, loss: 0.03350130468606949\n",
      "epoch:  31, loss: 0.033492933958768845\n",
      "epoch:  32, loss: 0.03346691653132439\n",
      "epoch:  33, loss: 0.03345266357064247\n",
      "epoch:  34, loss: 0.033442672342061996\n",
      "epoch:  35, loss: 0.03341483697295189\n",
      "epoch:  36, loss: 0.03339975327253342\n",
      "epoch:  37, loss: 0.033389654010534286\n",
      "epoch:  38, loss: 0.03336035832762718\n",
      "epoch:  39, loss: 0.03334440663456917\n",
      "epoch:  40, loss: 0.03333498165011406\n",
      "epoch:  41, loss: 0.03330382704734802\n",
      "epoch:  42, loss: 0.03328719362616539\n",
      "epoch:  43, loss: 0.033281244337558746\n",
      "epoch:  44, loss: 0.033248528838157654\n",
      "epoch:  45, loss: 0.03323116898536682\n",
      "epoch:  46, loss: 0.03322834521532059\n",
      "epoch:  47, loss: 0.03319304436445236\n",
      "epoch:  48, loss: 0.03317462280392647\n",
      "epoch:  49, loss: 0.03317352756857872\n",
      "epoch:  50, loss: 0.033136047422885895\n",
      "epoch:  51, loss: 0.03311648964881897\n",
      "epoch:  52, loss: 0.03310493379831314\n",
      "epoch:  53, loss: 0.033076539635658264\n",
      "epoch:  54, loss: 0.03305518627166748\n",
      "epoch:  55, loss: 0.03304268792271614\n",
      "epoch:  56, loss: 0.03301239758729935\n",
      "epoch:  57, loss: 0.032989464700222015\n",
      "epoch:  58, loss: 0.032976120710372925\n",
      "epoch:  59, loss: 0.03294629231095314\n",
      "epoch:  60, loss: 0.03292151540517807\n",
      "epoch:  61, loss: 0.032907385379076004\n",
      "epoch:  62, loss: 0.03287952020764351\n",
      "epoch:  63, loss: 0.03285295516252518\n",
      "epoch:  64, loss: 0.032837990671396255\n",
      "epoch:  65, loss: 0.03281227499246597\n",
      "epoch:  66, loss: 0.03278306499123573\n",
      "epoch:  67, loss: 0.03276696056127548\n",
      "epoch:  68, loss: 0.032741762697696686\n",
      "epoch:  69, loss: 0.03271009027957916\n",
      "epoch:  70, loss: 0.032692648470401764\n",
      "epoch:  71, loss: 0.0326683409512043\n",
      "epoch:  72, loss: 0.03263304382562637\n",
      "epoch:  73, loss: 0.032614052295684814\n",
      "epoch:  74, loss: 0.0325908288359642\n",
      "epoch:  75, loss: 0.03255227208137512\n",
      "epoch:  76, loss: 0.03253163769841194\n",
      "epoch:  77, loss: 0.03251105919480324\n",
      "epoch:  78, loss: 0.03246759995818138\n",
      "epoch:  79, loss: 0.03244490548968315\n",
      "epoch:  80, loss: 0.032425712794065475\n",
      "epoch:  81, loss: 0.032377682626247406\n",
      "epoch:  82, loss: 0.03235268220305443\n",
      "epoch:  83, loss: 0.03233746811747551\n",
      "epoch:  84, loss: 0.032282765954732895\n",
      "epoch:  85, loss: 0.03225509822368622\n",
      "epoch:  86, loss: 0.03224283829331398\n",
      "epoch:  87, loss: 0.03218211978673935\n",
      "epoch:  88, loss: 0.03215142339468002\n",
      "epoch:  89, loss: 0.032145388424396515\n",
      "epoch:  90, loss: 0.03207515552639961\n",
      "epoch:  91, loss: 0.032040856778621674\n",
      "epoch:  92, loss: 0.032040536403656006\n",
      "epoch:  93, loss: 0.0319618321955204\n",
      "epoch:  94, loss: 0.03192342445254326\n",
      "epoch:  95, loss: 0.03190154954791069\n",
      "epoch:  96, loss: 0.03184265270829201\n",
      "epoch:  97, loss: 0.03179847449064255\n",
      "epoch:  98, loss: 0.03177415207028389\n",
      "epoch:  99, loss: 0.03171445056796074\n",
      "epoch:  100, loss: 0.03166481480002403\n",
      "epoch:  101, loss: 0.03163770213723183\n",
      "epoch:  102, loss: 0.03157979995012283\n",
      "epoch:  103, loss: 0.03152177482843399\n",
      "epoch:  104, loss: 0.03149127587676048\n",
      "epoch:  105, loss: 0.03143402561545372\n",
      "epoch:  106, loss: 0.03136814385652542\n",
      "epoch:  107, loss: 0.03133384510874748\n",
      "epoch:  108, loss: 0.03128178417682648\n",
      "epoch:  109, loss: 0.031203618273139\n",
      "epoch:  110, loss: 0.031164806336164474\n",
      "epoch:  111, loss: 0.03111671470105648\n",
      "epoch:  112, loss: 0.03102760948240757\n",
      "epoch:  113, loss: 0.030983662232756615\n",
      "epoch:  114, loss: 0.03094632364809513\n",
      "epoch:  115, loss: 0.030839011073112488\n",
      "epoch:  116, loss: 0.030788850039243698\n",
      "epoch:  117, loss: 0.030758429318666458\n",
      "epoch:  118, loss: 0.030635908246040344\n",
      "epoch:  119, loss: 0.03057876043021679\n",
      "epoch:  120, loss: 0.03056839294731617\n",
      "epoch:  121, loss: 0.03041825257241726\n",
      "epoch:  122, loss: 0.03035251796245575\n",
      "epoch:  123, loss: 0.030316151678562164\n",
      "epoch:  124, loss: 0.030182745307683945\n",
      "epoch:  125, loss: 0.030107803642749786\n",
      "epoch:  126, loss: 0.030067071318626404\n",
      "epoch:  127, loss: 0.02993341162800789\n",
      "epoch:  128, loss: 0.029843689873814583\n",
      "epoch:  129, loss: 0.02979760617017746\n",
      "epoch:  130, loss: 0.029659349471330643\n",
      "epoch:  131, loss: 0.02955634333193302\n",
      "epoch:  132, loss: 0.029504315927624702\n",
      "epoch:  133, loss: 0.029368840157985687\n",
      "epoch:  134, loss: 0.02924448810517788\n",
      "epoch:  135, loss: 0.029185710474848747\n",
      "epoch:  136, loss: 0.02904665470123291\n",
      "epoch:  137, loss: 0.028905920684337616\n",
      "epoch:  138, loss: 0.02884005382657051\n",
      "epoch:  139, loss: 0.0287098977714777\n",
      "epoch:  140, loss: 0.02853906899690628\n",
      "epoch:  141, loss: 0.02846495620906353\n",
      "epoch:  142, loss: 0.028327934443950653\n",
      "epoch:  143, loss: 0.02813807874917984\n",
      "epoch:  144, loss: 0.02805560640990734\n",
      "epoch:  145, loss: 0.027925245463848114\n",
      "epoch:  146, loss: 0.02769917994737625\n",
      "epoch:  147, loss: 0.027608761563897133\n",
      "epoch:  148, loss: 0.02746208757162094\n",
      "epoch:  149, loss: 0.0272195003926754\n",
      "epoch:  150, loss: 0.02712106518447399\n",
      "epoch:  151, loss: 0.026980286464095116\n",
      "epoch:  152, loss: 0.026695499196648598\n",
      "epoch:  153, loss: 0.026590261608362198\n",
      "epoch:  154, loss: 0.026407403871417046\n",
      "epoch:  155, loss: 0.026121964678168297\n",
      "epoch:  156, loss: 0.02601226605474949\n",
      "epoch:  157, loss: 0.025817008689045906\n",
      "epoch:  158, loss: 0.025497116148471832\n",
      "epoch:  159, loss: 0.02538525126874447\n",
      "epoch:  160, loss: 0.02511214092373848\n",
      "epoch:  161, loss: 0.024815944954752922\n",
      "epoch:  162, loss: 0.02470455877482891\n",
      "epoch:  163, loss: 0.024382218718528748\n",
      "epoch:  164, loss: 0.02407846972346306\n",
      "epoch:  165, loss: 0.023970648646354675\n",
      "epoch:  166, loss: 0.023535406216979027\n",
      "epoch:  167, loss: 0.023282142356038094\n",
      "epoch:  168, loss: 0.023180121555924416\n",
      "epoch:  169, loss: 0.022654982283711433\n",
      "epoch:  170, loss: 0.02243315987288952\n",
      "epoch:  171, loss: 0.022367611527442932\n",
      "epoch:  172, loss: 0.021698202937841415\n",
      "epoch:  173, loss: 0.021533938124775887\n",
      "epoch:  174, loss: 0.021173255518078804\n",
      "epoch:  175, loss: 0.02071325108408928\n",
      "epoch:  176, loss: 0.02059072256088257\n",
      "epoch:  177, loss: 0.019948415458202362\n",
      "epoch:  178, loss: 0.019706744700670242\n",
      "epoch:  179, loss: 0.019498005509376526\n",
      "epoch:  180, loss: 0.018819177523255348\n",
      "epoch:  181, loss: 0.01869049109518528\n",
      "epoch:  182, loss: 0.01798265427350998\n",
      "epoch:  183, loss: 0.017757588997483253\n",
      "epoch:  184, loss: 0.017328830435872078\n",
      "epoch:  185, loss: 0.016827570274472237\n",
      "epoch:  186, loss: 0.016724904999136925\n",
      "epoch:  187, loss: 0.01636044681072235\n",
      "epoch:  188, loss: 0.010226422920823097\n",
      "epoch:  189, loss: 0.010071715340018272\n",
      "epoch:  190, loss: 0.010045398026704788\n",
      "epoch:  191, loss: 0.009884326718747616\n",
      "epoch:  192, loss: 0.009835133329033852\n",
      "epoch:  193, loss: 0.009835063479840755\n",
      "epoch:  194, loss: 0.00965893641114235\n",
      "epoch:  195, loss: 0.009637381881475449\n",
      "epoch:  196, loss: 0.009538345038890839\n",
      "epoch:  197, loss: 0.00948382169008255\n",
      "epoch:  198, loss: 0.009469036012887955\n",
      "epoch:  199, loss: 0.009356390684843063\n",
      "epoch:  200, loss: 0.009333784691989422\n",
      "epoch:  201, loss: 0.009298790246248245\n",
      "epoch:  202, loss: 0.00921802781522274\n",
      "epoch:  203, loss: 0.009204838424921036\n",
      "epoch:  204, loss: 0.009139744564890862\n",
      "epoch:  205, loss: 0.009105700068175793\n",
      "epoch:  206, loss: 0.009096149355173111\n",
      "epoch:  207, loss: 0.009032580070197582\n",
      "epoch:  208, loss: 0.009014999493956566\n",
      "epoch:  209, loss: 0.009008008986711502\n",
      "epoch:  210, loss: 0.00895156804472208\n",
      "epoch:  211, loss: 0.008941730484366417\n",
      "epoch:  212, loss: 0.008919467218220234\n",
      "epoch:  213, loss: 0.008886593393981457\n",
      "epoch:  214, loss: 0.008880226872861385\n",
      "epoch:  215, loss: 0.00885012000799179\n",
      "epoch:  216, loss: 0.008832428604364395\n",
      "epoch:  217, loss: 0.008827700279653072\n",
      "epoch:  218, loss: 0.008796889334917068\n",
      "epoch:  219, loss: 0.008787261322140694\n",
      "epoch:  220, loss: 0.008783530443906784\n",
      "epoch:  221, loss: 0.008754901587963104\n",
      "epoch:  222, loss: 0.008749009110033512\n",
      "epoch:  223, loss: 0.008746062405407429\n",
      "epoch:  224, loss: 0.00872184056788683\n",
      "epoch:  225, loss: 0.008717309683561325\n",
      "epoch:  226, loss: 0.008714365772902966\n",
      "epoch:  227, loss: 0.008693033829331398\n",
      "epoch:  228, loss: 0.008689742535352707\n",
      "epoch:  229, loss: 0.008679880760610104\n",
      "epoch:  230, loss: 0.008668133988976479\n",
      "epoch:  231, loss: 0.008665628731250763\n",
      "epoch:  232, loss: 0.008651944808661938\n",
      "epoch:  233, loss: 0.008645709604024887\n",
      "epoch:  234, loss: 0.008643746376037598\n",
      "epoch:  235, loss: 0.008628997951745987\n",
      "epoch:  236, loss: 0.00862565916031599\n",
      "epoch:  237, loss: 0.00862399023026228\n",
      "epoch:  238, loss: 0.008615362457931042\n",
      "epoch:  239, loss: 0.00850570760667324\n",
      "epoch:  240, loss: 0.008498392067849636\n",
      "epoch:  241, loss: 0.008497167378664017\n",
      "epoch:  242, loss: 0.00849145371466875\n",
      "epoch:  243, loss: 0.00848781131207943\n",
      "epoch:  244, loss: 0.00848681852221489\n",
      "epoch:  245, loss: 0.008479614742100239\n",
      "epoch:  246, loss: 0.008477714844048023\n",
      "epoch:  247, loss: 0.008476884104311466\n",
      "epoch:  248, loss: 0.008466767147183418\n",
      "epoch:  249, loss: 0.008407308720052242\n",
      "epoch:  250, loss: 0.008403172716498375\n",
      "epoch:  251, loss: 0.008402206003665924\n",
      "epoch:  252, loss: 0.008395099081099033\n",
      "epoch:  253, loss: 0.00839329045265913\n",
      "epoch:  254, loss: 0.008392451331019402\n",
      "epoch:  255, loss: 0.008385716937482357\n",
      "epoch:  256, loss: 0.00838459562510252\n",
      "epoch:  257, loss: 0.00838230550289154\n",
      "epoch:  258, loss: 0.008377987891435623\n",
      "epoch:  259, loss: 0.00837715994566679\n",
      "epoch:  260, loss: 0.008372986689209938\n",
      "epoch:  261, loss: 0.008370860479772091\n",
      "epoch:  262, loss: 0.008370227180421352\n",
      "epoch:  263, loss: 0.008366466499865055\n",
      "epoch:  264, loss: 0.008364388719201088\n",
      "epoch:  265, loss: 0.008363722823560238\n",
      "epoch:  266, loss: 0.008358700200915337\n",
      "epoch:  267, loss: 0.008357476443052292\n",
      "epoch:  268, loss: 0.008356893435120583\n",
      "epoch:  269, loss: 0.008350511081516743\n",
      "epoch:  270, loss: 0.008306603878736496\n",
      "epoch:  271, loss: 0.008303526788949966\n",
      "epoch:  272, loss: 0.008302832022309303\n",
      "epoch:  273, loss: 0.008297771215438843\n",
      "epoch:  274, loss: 0.008296441286802292\n",
      "epoch:  275, loss: 0.008295813575387001\n",
      "epoch:  276, loss: 0.008290579542517662\n",
      "epoch:  277, loss: 0.008289550431072712\n",
      "epoch:  278, loss: 0.008287664502859116\n",
      "epoch:  279, loss: 0.008283372968435287\n",
      "epoch:  280, loss: 0.008282539434731007\n",
      "epoch:  281, loss: 0.00827819388359785\n",
      "epoch:  282, loss: 0.008275976404547691\n",
      "epoch:  283, loss: 0.00827527604997158\n",
      "epoch:  284, loss: 0.008270160295069218\n",
      "epoch:  285, loss: 0.008268840610980988\n",
      "epoch:  286, loss: 0.008268223144114017\n",
      "epoch:  287, loss: 0.008264836855232716\n",
      "epoch:  288, loss: 0.008215484209358692\n",
      "epoch:  289, loss: 0.00821202713996172\n",
      "epoch:  290, loss: 0.008211242966353893\n",
      "epoch:  291, loss: 0.008206702768802643\n",
      "epoch:  292, loss: 0.008204858750104904\n",
      "epoch:  293, loss: 0.008204241283237934\n",
      "epoch:  294, loss: 0.008199479430913925\n",
      "epoch:  295, loss: 0.00819843914359808\n",
      "epoch:  296, loss: 0.008197697810828686\n",
      "epoch:  297, loss: 0.008192869834601879\n",
      "epoch:  298, loss: 0.00819207914173603\n",
      "epoch:  299, loss: 0.008187569677829742\n",
      "epoch:  300, loss: 0.008185910992324352\n",
      "epoch:  301, loss: 0.008185313083231449\n",
      "epoch:  302, loss: 0.008180756121873856\n",
      "epoch:  303, loss: 0.00817963294684887\n",
      "epoch:  304, loss: 0.00817908626049757\n",
      "epoch:  305, loss: 0.008162791840732098\n",
      "epoch:  306, loss: 0.008131208829581738\n",
      "epoch:  307, loss: 0.00812892708927393\n",
      "epoch:  308, loss: 0.008128267712891102\n",
      "epoch:  309, loss: 0.008123879320919514\n",
      "epoch:  310, loss: 0.008122362196445465\n",
      "epoch:  311, loss: 0.008121771737933159\n",
      "epoch:  312, loss: 0.008117214776575565\n",
      "epoch:  313, loss: 0.008116197772324085\n",
      "epoch:  314, loss: 0.008116123266518116\n",
      "epoch:  315, loss: 0.008111068978905678\n",
      "epoch:  316, loss: 0.008110287599265575\n",
      "epoch:  317, loss: 0.008108235895633698\n",
      "epoch:  318, loss: 0.008105113171041012\n",
      "epoch:  319, loss: 0.008104469627141953\n",
      "epoch:  320, loss: 0.008100971579551697\n",
      "epoch:  321, loss: 0.008099282160401344\n",
      "epoch:  322, loss: 0.00809868611395359\n",
      "epoch:  323, loss: 0.008093992248177528\n",
      "epoch:  324, loss: 0.00809285044670105\n",
      "epoch:  325, loss: 0.008092213422060013\n",
      "epoch:  326, loss: 0.008087429217994213\n",
      "epoch:  327, loss: 0.008086390793323517\n",
      "epoch:  328, loss: 0.00808617752045393\n",
      "epoch:  329, loss: 0.008081330917775631\n",
      "epoch:  330, loss: 0.008080562576651573\n",
      "epoch:  331, loss: 0.008079825900495052\n",
      "epoch:  332, loss: 0.008075668476521969\n",
      "epoch:  333, loss: 0.008074925281107426\n",
      "epoch:  334, loss: 0.008073024451732635\n",
      "epoch:  335, loss: 0.008069930598139763\n",
      "epoch:  336, loss: 0.00806925818324089\n",
      "epoch:  337, loss: 0.008066545240581036\n",
      "epoch:  338, loss: 0.008064163848757744\n",
      "epoch:  339, loss: 0.00806355383247137\n",
      "epoch:  340, loss: 0.008060332387685776\n",
      "epoch:  341, loss: 0.008058500476181507\n",
      "epoch:  342, loss: 0.00805791188031435\n",
      "epoch:  343, loss: 0.008054601959884167\n",
      "epoch:  344, loss: 0.008052912540733814\n",
      "epoch:  345, loss: 0.008052336983382702\n",
      "epoch:  346, loss: 0.008049678057432175\n",
      "epoch:  347, loss: 0.008047320879995823\n",
      "epoch:  348, loss: 0.008046691305935383\n",
      "epoch:  349, loss: 0.008044800721108913\n",
      "epoch:  350, loss: 0.008041749708354473\n",
      "epoch:  351, loss: 0.008041081950068474\n",
      "epoch:  352, loss: 0.008038898929953575\n",
      "epoch:  353, loss: 0.008036132901906967\n",
      "epoch:  354, loss: 0.008035455830395222\n",
      "epoch:  355, loss: 0.008033362217247486\n",
      "epoch:  356, loss: 0.008030455559492111\n",
      "epoch:  357, loss: 0.008029773831367493\n",
      "epoch:  358, loss: 0.00802969466894865\n",
      "epoch:  359, loss: 0.008024957962334156\n",
      "epoch:  360, loss: 0.008024183101952076\n",
      "epoch:  361, loss: 0.00802366528660059\n",
      "epoch:  362, loss: 0.008019411005079746\n",
      "epoch:  363, loss: 0.008018586784601212\n",
      "epoch:  364, loss: 0.008018087595701218\n",
      "epoch:  365, loss: 0.008012913167476654\n",
      "epoch:  366, loss: 0.00797363743185997\n",
      "epoch:  367, loss: 0.007970262318849564\n",
      "epoch:  368, loss: 0.007969429716467857\n",
      "epoch:  369, loss: 0.007965967990458012\n",
      "epoch:  370, loss: 0.007962951436638832\n",
      "epoch:  371, loss: 0.00796217005699873\n",
      "epoch:  372, loss: 0.00795825570821762\n",
      "epoch:  373, loss: 0.007956067100167274\n",
      "epoch:  374, loss: 0.007955368608236313\n",
      "epoch:  375, loss: 0.007952354848384857\n",
      "epoch:  376, loss: 0.007949674502015114\n",
      "epoch:  377, loss: 0.007948929443955421\n",
      "epoch:  378, loss: 0.007945980876684189\n",
      "epoch:  379, loss: 0.007943333126604557\n",
      "epoch:  380, loss: 0.00794258527457714\n",
      "epoch:  381, loss: 0.007939607836306095\n",
      "epoch:  382, loss: 0.007936934009194374\n",
      "epoch:  383, loss: 0.00793624110519886\n",
      "epoch:  384, loss: 0.00793342012912035\n",
      "epoch:  385, loss: 0.007930799387395382\n",
      "epoch:  386, loss: 0.007930103689432144\n",
      "epoch:  387, loss: 0.007930051535367966\n",
      "epoch:  388, loss: 0.007924878038465977\n",
      "epoch:  389, loss: 0.007923990488052368\n",
      "epoch:  390, loss: 0.007923114113509655\n",
      "epoch:  391, loss: 0.00791865959763527\n",
      "epoch:  392, loss: 0.007917849346995354\n",
      "epoch:  393, loss: 0.007917416281998158\n",
      "epoch:  394, loss: 0.00791251752525568\n",
      "epoch:  395, loss: 0.007911651395261288\n",
      "epoch:  396, loss: 0.007911075837910175\n",
      "epoch:  397, loss: 0.007907135412096977\n",
      "epoch:  398, loss: 0.007905516773462296\n",
      "epoch:  399, loss: 0.007904881611466408\n",
      "epoch:  400, loss: 0.00790129043161869\n",
      "epoch:  401, loss: 0.007899392396211624\n",
      "epoch:  402, loss: 0.007898720912635326\n",
      "epoch:  403, loss: 0.0078961830586195\n",
      "epoch:  404, loss: 0.007893264293670654\n",
      "epoch:  405, loss: 0.00789250060915947\n",
      "epoch:  406, loss: 0.007890348322689533\n",
      "epoch:  407, loss: 0.007887067273259163\n",
      "epoch:  408, loss: 0.007886244915425777\n",
      "epoch:  409, loss: 0.0078846774995327\n",
      "epoch:  410, loss: 0.00788086373358965\n",
      "epoch:  411, loss: 0.007880094461143017\n",
      "epoch:  412, loss: 0.007877860218286514\n",
      "epoch:  413, loss: 0.007874625734984875\n",
      "epoch:  414, loss: 0.00787379126995802\n",
      "epoch:  415, loss: 0.007872719317674637\n",
      "epoch:  416, loss: 0.007868235930800438\n",
      "epoch:  417, loss: 0.00786731205880642\n",
      "epoch:  418, loss: 0.007865656167268753\n",
      "epoch:  419, loss: 0.007861676625907421\n",
      "epoch:  420, loss: 0.007860760204494\n",
      "epoch:  421, loss: 0.007859272882342339\n",
      "epoch:  422, loss: 0.007855075411498547\n",
      "epoch:  423, loss: 0.007854210212826729\n",
      "epoch:  424, loss: 0.007852356880903244\n",
      "epoch:  425, loss: 0.007848606444895267\n",
      "epoch:  426, loss: 0.007847748696804047\n",
      "epoch:  427, loss: 0.007847203873097897\n",
      "epoch:  428, loss: 0.007842316292226315\n",
      "epoch:  429, loss: 0.007841354236006737\n",
      "epoch:  430, loss: 0.007840769365429878\n",
      "epoch:  431, loss: 0.007835879921913147\n",
      "epoch:  432, loss: 0.00783484522253275\n",
      "epoch:  433, loss: 0.00783425010740757\n",
      "epoch:  434, loss: 0.007829821668565273\n",
      "epoch:  435, loss: 0.007828336209058762\n",
      "epoch:  436, loss: 0.007827699184417725\n",
      "epoch:  437, loss: 0.007824132218956947\n",
      "epoch:  438, loss: 0.007821845822036266\n",
      "epoch:  439, loss: 0.007821137085556984\n",
      "epoch:  440, loss: 0.007817555218935013\n",
      "epoch:  441, loss: 0.007815242744982243\n",
      "epoch:  442, loss: 0.007814545184373856\n",
      "epoch:  443, loss: 0.007811061106622219\n",
      "epoch:  444, loss: 0.007808685768395662\n",
      "epoch:  445, loss: 0.007807956542819738\n",
      "epoch:  446, loss: 0.007805177476257086\n",
      "epoch:  447, loss: 0.0078021869994699955\n",
      "epoch:  448, loss: 0.007801409810781479\n",
      "epoch:  449, loss: 0.0077988626435399055\n",
      "epoch:  450, loss: 0.0077956742607057095\n",
      "epoch:  451, loss: 0.00779488030821085\n",
      "epoch:  452, loss: 0.0077943261712789536\n",
      "epoch:  453, loss: 0.007789303082972765\n",
      "epoch:  454, loss: 0.007788375020027161\n",
      "epoch:  455, loss: 0.007787834852933884\n",
      "epoch:  456, loss: 0.007782884407788515\n",
      "epoch:  457, loss: 0.007781916297972202\n",
      "epoch:  458, loss: 0.007781332824379206\n",
      "epoch:  459, loss: 0.007777145132422447\n",
      "epoch:  460, loss: 0.007775524631142616\n",
      "epoch:  461, loss: 0.007774876896291971\n",
      "epoch:  462, loss: 0.00777103565633297\n",
      "epoch:  463, loss: 0.007769069634377956\n",
      "epoch:  464, loss: 0.007768386043608189\n",
      "epoch:  465, loss: 0.00776517391204834\n",
      "epoch:  466, loss: 0.007762636989355087\n",
      "epoch:  467, loss: 0.007761897053569555\n",
      "epoch:  468, loss: 0.007759005296975374\n",
      "epoch:  469, loss: 0.007756058592349291\n",
      "epoch:  470, loss: 0.007755294907838106\n",
      "epoch:  471, loss: 0.007752504199743271\n",
      "epoch:  472, loss: 0.007749409880489111\n",
      "epoch:  473, loss: 0.007748633623123169\n",
      "epoch:  474, loss: 0.007747488562017679\n",
      "epoch:  475, loss: 0.007742947433143854\n",
      "epoch:  476, loss: 0.007742034737020731\n",
      "epoch:  477, loss: 0.0077414424158632755\n",
      "epoch:  478, loss: 0.007737013511359692\n",
      "epoch:  479, loss: 0.007735484279692173\n",
      "epoch:  480, loss: 0.00773485004901886\n",
      "epoch:  481, loss: 0.007731182035058737\n",
      "epoch:  482, loss: 0.00772892776876688\n",
      "epoch:  483, loss: 0.007728211116045713\n",
      "epoch:  484, loss: 0.007724473252892494\n",
      "epoch:  485, loss: 0.00772230327129364\n",
      "epoch:  486, loss: 0.007721593603491783\n",
      "epoch:  487, loss: 0.007718883920460939\n",
      "epoch:  488, loss: 0.007715850602835417\n",
      "epoch:  489, loss: 0.007715065032243729\n",
      "epoch:  490, loss: 0.0077143157832324505\n",
      "epoch:  491, loss: 0.007709580007940531\n",
      "epoch:  492, loss: 0.007708634715527296\n",
      "epoch:  493, loss: 0.007708043325692415\n",
      "epoch:  494, loss: 0.007703906390815973\n",
      "epoch:  495, loss: 0.007702277973294258\n",
      "epoch:  496, loss: 0.0077016414143145084\n",
      "epoch:  497, loss: 0.007697848603129387\n",
      "epoch:  498, loss: 0.007695998530834913\n",
      "epoch:  499, loss: 0.007695329375565052\n",
      "epoch:  500, loss: 0.00769225787371397\n",
      "epoch:  501, loss: 0.007689796853810549\n",
      "epoch:  502, loss: 0.00768905458971858\n",
      "epoch:  503, loss: 0.00768890418112278\n",
      "epoch:  504, loss: 0.0076838284730911255\n",
      "epoch:  505, loss: 0.007682885508984327\n",
      "epoch:  506, loss: 0.007682324852794409\n",
      "epoch:  507, loss: 0.007677717600017786\n",
      "epoch:  508, loss: 0.007676603272557259\n",
      "epoch:  509, loss: 0.007675997447222471\n",
      "epoch:  510, loss: 0.007671480067074299\n",
      "epoch:  511, loss: 0.007670092396438122\n",
      "epoch:  512, loss: 0.007669476792216301\n",
      "epoch:  513, loss: 0.007664999458938837\n",
      "epoch:  514, loss: 0.007663605269044638\n",
      "epoch:  515, loss: 0.007662979885935783\n",
      "epoch:  516, loss: 0.007659830618649721\n",
      "epoch:  517, loss: 0.007657249458134174\n",
      "epoch:  518, loss: 0.007656493224203587\n",
      "epoch:  519, loss: 0.007654260378330946\n",
      "epoch:  520, loss: 0.007650773040950298\n",
      "epoch:  521, loss: 0.007649932987987995\n",
      "epoch:  522, loss: 0.0076493457891047\n",
      "epoch:  523, loss: 0.007645637262612581\n",
      "epoch:  524, loss: 0.007643578574061394\n",
      "epoch:  525, loss: 0.007642893586307764\n",
      "epoch:  526, loss: 0.007640091702342033\n",
      "epoch:  527, loss: 0.0076371775940060616\n",
      "epoch:  528, loss: 0.0076363408006727695\n",
      "epoch:  529, loss: 0.007634652312844992\n",
      "epoch:  530, loss: 0.007630772422999144\n",
      "epoch:  531, loss: 0.00762981828302145\n",
      "epoch:  532, loss: 0.007629470434039831\n",
      "epoch:  533, loss: 0.007624308578670025\n",
      "epoch:  534, loss: 0.007623198442161083\n",
      "epoch:  535, loss: 0.007622567471116781\n",
      "epoch:  536, loss: 0.0076178754679858685\n",
      "epoch:  537, loss: 0.007616491988301277\n",
      "epoch:  538, loss: 0.0076158433221280575\n",
      "epoch:  539, loss: 0.00761254271492362\n",
      "epoch:  540, loss: 0.00760988611727953\n",
      "epoch:  541, loss: 0.007609102874994278\n",
      "epoch:  542, loss: 0.007608343381434679\n",
      "epoch:  543, loss: 0.007603440899401903\n",
      "epoch:  544, loss: 0.007602427154779434\n",
      "epoch:  545, loss: 0.0076018148101866245\n",
      "epoch:  546, loss: 0.007596871815621853\n",
      "epoch:  547, loss: 0.007595738861709833\n",
      "epoch:  548, loss: 0.00759510975331068\n",
      "epoch:  549, loss: 0.007590798195451498\n",
      "epoch:  550, loss: 0.007589157670736313\n",
      "epoch:  551, loss: 0.0075884414836764336\n",
      "epoch:  552, loss: 0.007585992105305195\n",
      "epoch:  553, loss: 0.0075826821848750114\n",
      "epoch:  554, loss: 0.0075818211771547794\n",
      "epoch:  555, loss: 0.007580633275210857\n",
      "epoch:  556, loss: 0.007576155476272106\n",
      "epoch:  557, loss: 0.0075751300901174545\n",
      "epoch:  558, loss: 0.007574521470814943\n",
      "epoch:  559, loss: 0.007570307236164808\n",
      "epoch:  560, loss: 0.007568540517240763\n",
      "epoch:  561, loss: 0.00756785087287426\n",
      "epoch:  562, loss: 0.007564874365925789\n",
      "epoch:  563, loss: 0.007561965379863977\n",
      "epoch:  564, loss: 0.0075611514039337635\n",
      "epoch:  565, loss: 0.007560921832919121\n",
      "epoch:  566, loss: 0.007555491756647825\n",
      "epoch:  567, loss: 0.007554453797638416\n",
      "epoch:  568, loss: 0.007553813047707081\n",
      "epoch:  569, loss: 0.007549460511654615\n",
      "epoch:  570, loss: 0.007547763176262379\n",
      "epoch:  571, loss: 0.007547050714492798\n",
      "epoch:  572, loss: 0.0075445896945893764\n",
      "epoch:  573, loss: 0.00754115404561162\n",
      "epoch:  574, loss: 0.0075402893126010895\n",
      "epoch:  575, loss: 0.007539685349911451\n",
      "epoch:  576, loss: 0.007535250391811132\n",
      "epoch:  577, loss: 0.007533599156886339\n",
      "epoch:  578, loss: 0.007532911840826273\n",
      "epoch:  579, loss: 0.007530770264565945\n",
      "epoch:  580, loss: 0.007527089677751064\n",
      "epoch:  581, loss: 0.007526202127337456\n",
      "epoch:  582, loss: 0.007526150438934565\n",
      "epoch:  583, loss: 0.0075206514447927475\n",
      "epoch:  584, loss: 0.007519556209445\n",
      "epoch:  585, loss: 0.0075189219787716866\n",
      "epoch:  586, loss: 0.0075147091411054134\n",
      "epoch:  587, loss: 0.0075129736214876175\n",
      "epoch:  588, loss: 0.007512247655540705\n",
      "epoch:  589, loss: 0.007510191760957241\n",
      "epoch:  590, loss: 0.007506493479013443\n",
      "epoch:  591, loss: 0.007505547255277634\n",
      "epoch:  592, loss: 0.007504946086555719\n",
      "epoch:  593, loss: 0.00750030716881156\n",
      "epoch:  594, loss: 0.007498883176594973\n",
      "epoch:  595, loss: 0.0074982112273573875\n",
      "epoch:  596, loss: 0.007494530640542507\n",
      "epoch:  597, loss: 0.007492241449654102\n",
      "epoch:  598, loss: 0.007491475436836481\n",
      "epoch:  599, loss: 0.0074908374808728695\n",
      "epoch:  600, loss: 0.00748581625521183\n",
      "epoch:  601, loss: 0.007484824862331152\n",
      "epoch:  602, loss: 0.007484201341867447\n",
      "epoch:  603, loss: 0.007479206193238497\n",
      "epoch:  604, loss: 0.007478035520762205\n",
      "epoch:  605, loss: 0.007477393839508295\n",
      "epoch:  606, loss: 0.00747270043939352\n",
      "epoch:  607, loss: 0.0074712675996124744\n",
      "epoch:  608, loss: 0.007470561657100916\n",
      "epoch:  609, loss: 0.007467877119779587\n",
      "epoch:  610, loss: 0.0074645453132689\n",
      "epoch:  611, loss: 0.007463616784662008\n",
      "epoch:  612, loss: 0.007462966255843639\n",
      "epoch:  613, loss: 0.007457784377038479\n",
      "epoch:  614, loss: 0.007456402759999037\n",
      "epoch:  615, loss: 0.007455723360180855\n",
      "epoch:  616, loss: 0.007451836485415697\n",
      "epoch:  617, loss: 0.007449271157383919\n",
      "epoch:  618, loss: 0.007448449730873108\n",
      "epoch:  619, loss: 0.007447971496731043\n",
      "epoch:  620, loss: 0.007442470174282789\n",
      "epoch:  621, loss: 0.007441249676048756\n",
      "epoch:  622, loss: 0.007440566550940275\n",
      "epoch:  623, loss: 0.007435558829456568\n",
      "epoch:  624, loss: 0.007433965336531401\n",
      "epoch:  625, loss: 0.0074332416988909245\n",
      "epoch:  626, loss: 0.007429712917655706\n",
      "epoch:  627, loss: 0.007426931988447905\n",
      "epoch:  628, loss: 0.007426074706017971\n",
      "epoch:  629, loss: 0.007423596456646919\n",
      "epoch:  630, loss: 0.0074195098131895065\n",
      "epoch:  631, loss: 0.007418469060212374\n",
      "epoch:  632, loss: 0.007417797576636076\n",
      "epoch:  633, loss: 0.007412659469991922\n",
      "epoch:  634, loss: 0.007411067374050617\n",
      "epoch:  635, loss: 0.007410277612507343\n",
      "epoch:  636, loss: 0.00740406196564436\n",
      "epoch:  637, loss: 0.0074026621878147125\n",
      "epoch:  638, loss: 0.007401889655739069\n",
      "epoch:  639, loss: 0.0073970286175608635\n",
      "epoch:  640, loss: 0.007394617423415184\n",
      "epoch:  641, loss: 0.007393709383904934\n",
      "epoch:  642, loss: 0.0073915510438382626\n",
      "epoch:  643, loss: 0.007386765442788601\n",
      "epoch:  644, loss: 0.007385601289570332\n",
      "epoch:  645, loss: 0.007384864147752523\n",
      "epoch:  646, loss: 0.0073797148652374744\n",
      "epoch:  647, loss: 0.007377716712653637\n",
      "epoch:  648, loss: 0.0073768929578363895\n",
      "epoch:  649, loss: 0.007373990956693888\n",
      "epoch:  650, loss: 0.007369927130639553\n",
      "epoch:  651, loss: 0.0073688458651304245\n",
      "epoch:  652, loss: 0.007368119433522224\n",
      "epoch:  653, loss: 0.007363326381891966\n",
      "epoch:  654, loss: 0.007360987830907106\n",
      "epoch:  655, loss: 0.007360060699284077\n",
      "epoch:  656, loss: 0.007357683032751083\n",
      "epoch:  657, loss: 0.007352475542575121\n",
      "epoch:  658, loss: 0.0073510874062776566\n",
      "epoch:  659, loss: 0.007350187283009291\n",
      "epoch:  660, loss: 0.007343541365116835\n",
      "epoch:  661, loss: 0.0073415543884038925\n",
      "epoch:  662, loss: 0.007340665441006422\n",
      "epoch:  663, loss: 0.00733809033408761\n",
      "epoch:  664, loss: 0.007333578076213598\n",
      "epoch:  665, loss: 0.007332358509302139\n",
      "epoch:  666, loss: 0.007331622764468193\n",
      "epoch:  667, loss: 0.007326988503336906\n",
      "epoch:  668, loss: 0.007324662990868092\n",
      "epoch:  669, loss: 0.00732374656945467\n",
      "epoch:  670, loss: 0.007321865763515234\n",
      "epoch:  671, loss: 0.007317111361771822\n",
      "epoch:  672, loss: 0.007315902505069971\n",
      "epoch:  673, loss: 0.007315169554203749\n",
      "epoch:  674, loss: 0.007311602123081684\n",
      "epoch:  675, loss: 0.007308224216103554\n",
      "epoch:  676, loss: 0.007307186257094145\n",
      "epoch:  677, loss: 0.0073064472526311874\n",
      "epoch:  678, loss: 0.007300750818103552\n",
      "epoch:  679, loss: 0.007299000397324562\n",
      "epoch:  680, loss: 0.007298151031136513\n",
      "epoch:  681, loss: 0.007297304924577475\n",
      "epoch:  682, loss: 0.007291287649422884\n",
      "epoch:  683, loss: 0.0072899674996733665\n",
      "epoch:  684, loss: 0.00728917121887207\n",
      "epoch:  685, loss: 0.007285231724381447\n",
      "epoch:  686, loss: 0.007282026577740908\n",
      "epoch:  687, loss: 0.00728097977116704\n",
      "epoch:  688, loss: 0.007280243560671806\n",
      "epoch:  689, loss: 0.007274429779499769\n",
      "epoch:  690, loss: 0.007272778544574976\n",
      "epoch:  691, loss: 0.007271955721080303\n",
      "epoch:  692, loss: 0.007269629277288914\n",
      "epoch:  693, loss: 0.0072649503126740456\n",
      "epoch:  694, loss: 0.007263694889843464\n",
      "epoch:  695, loss: 0.00726292422041297\n",
      "epoch:  696, loss: 0.0072578322142362595\n",
      "epoch:  697, loss: 0.007255537901073694\n",
      "epoch:  698, loss: 0.0072546168230473995\n",
      "epoch:  699, loss: 0.007253894116729498\n",
      "epoch:  700, loss: 0.007248066831380129\n",
      "epoch:  701, loss: 0.007246502209454775\n",
      "epoch:  702, loss: 0.007245689630508423\n",
      "epoch:  703, loss: 0.007244325242936611\n",
      "epoch:  704, loss: 0.007238916587084532\n",
      "epoch:  705, loss: 0.007237559650093317\n",
      "epoch:  706, loss: 0.007236772682517767\n",
      "epoch:  707, loss: 0.007234042976051569\n",
      "epoch:  708, loss: 0.007229882292449474\n",
      "epoch:  709, loss: 0.00722868787124753\n",
      "epoch:  710, loss: 0.007227929309010506\n",
      "epoch:  711, loss: 0.007223970256745815\n",
      "epoch:  712, loss: 0.007220837287604809\n",
      "epoch:  713, loss: 0.007219797000288963\n",
      "epoch:  714, loss: 0.007219049148261547\n",
      "epoch:  715, loss: 0.007214710116386414\n",
      "epoch:  716, loss: 0.0072118788957595825\n",
      "epoch:  717, loss: 0.007210841868072748\n",
      "epoch:  718, loss: 0.007210074458271265\n",
      "epoch:  719, loss: 0.007205209229141474\n",
      "epoch:  720, loss: 0.0072026033885777\n",
      "epoch:  721, loss: 0.007201599422842264\n",
      "epoch:  722, loss: 0.0072008478455245495\n",
      "epoch:  723, loss: 0.007194987963885069\n",
      "epoch:  724, loss: 0.007193294819444418\n",
      "epoch:  725, loss: 0.007192430552095175\n",
      "epoch:  726, loss: 0.007191751152276993\n",
      "epoch:  727, loss: 0.007185746915638447\n",
      "epoch:  728, loss: 0.007184240035712719\n",
      "epoch:  729, loss: 0.007183419074863195\n",
      "epoch:  730, loss: 0.0071796514093875885\n",
      "epoch:  731, loss: 0.007176220882683992\n",
      "epoch:  732, loss: 0.007175095845013857\n",
      "epoch:  733, loss: 0.0071743084117770195\n",
      "epoch:  734, loss: 0.0071693663485348225\n",
      "epoch:  735, loss: 0.00716689508408308\n",
      "epoch:  736, loss: 0.007165910210460424\n",
      "epoch:  737, loss: 0.007165160030126572\n",
      "epoch:  738, loss: 0.007159752771258354\n",
      "epoch:  739, loss: 0.00715776439756155\n",
      "epoch:  740, loss: 0.007156848907470703\n",
      "epoch:  741, loss: 0.007156111765652895\n",
      "epoch:  742, loss: 0.00715071614831686\n",
      "epoch:  743, loss: 0.00714876689016819\n",
      "epoch:  744, loss: 0.007147893775254488\n",
      "epoch:  745, loss: 0.0071471636183559895\n",
      "epoch:  746, loss: 0.007142225280404091\n",
      "epoch:  747, loss: 0.007139948196709156\n",
      "epoch:  748, loss: 0.0071390243247151375\n",
      "epoch:  749, loss: 0.007138281594961882\n",
      "epoch:  750, loss: 0.0071332682855427265\n",
      "epoch:  751, loss: 0.007131049409508705\n",
      "epoch:  752, loss: 0.007130141369998455\n",
      "epoch:  753, loss: 0.00712940376251936\n",
      "epoch:  754, loss: 0.007124192547053099\n",
      "epoch:  755, loss: 0.0071220602840185165\n",
      "epoch:  756, loss: 0.007121141999959946\n",
      "epoch:  757, loss: 0.007119951769709587\n",
      "epoch:  758, loss: 0.00711436290293932\n",
      "epoch:  759, loss: 0.007112910971045494\n",
      "epoch:  760, loss: 0.0071121035143733025\n",
      "epoch:  761, loss: 0.007108510937541723\n",
      "epoch:  762, loss: 0.007104893215000629\n",
      "epoch:  763, loss: 0.007103749550879002\n",
      "epoch:  764, loss: 0.0071029746904969215\n",
      "epoch:  765, loss: 0.007098848931491375\n",
      "epoch:  766, loss: 0.007095737848430872\n",
      "epoch:  767, loss: 0.0070946537889540195\n",
      "epoch:  768, loss: 0.007093894295394421\n",
      "epoch:  769, loss: 0.007088799029588699\n",
      "epoch:  770, loss: 0.007086559664458036\n",
      "epoch:  771, loss: 0.007085587829351425\n",
      "epoch:  772, loss: 0.007084845099598169\n",
      "epoch:  773, loss: 0.007079709321260452\n",
      "epoch:  774, loss: 0.0070774792693555355\n",
      "epoch:  775, loss: 0.0070765260607004166\n",
      "epoch:  776, loss: 0.007075783796608448\n",
      "epoch:  777, loss: 0.00706995464861393\n",
      "epoch:  778, loss: 0.007068207487463951\n",
      "epoch:  779, loss: 0.0070673138834536076\n",
      "epoch:  780, loss: 0.007065409328788519\n",
      "epoch:  781, loss: 0.007060140371322632\n",
      "epoch:  782, loss: 0.007058657240122557\n",
      "epoch:  783, loss: 0.007057775277644396\n",
      "epoch:  784, loss: 0.007054135203361511\n",
      "epoch:  785, loss: 0.007050238084048033\n",
      "epoch:  786, loss: 0.007048956584185362\n",
      "epoch:  787, loss: 0.0070481435395777225\n",
      "epoch:  788, loss: 0.007043709047138691\n",
      "epoch:  789, loss: 0.007040722295641899\n",
      "epoch:  790, loss: 0.007039596326649189\n",
      "epoch:  791, loss: 0.007038809824734926\n",
      "epoch:  792, loss: 0.007034361828118563\n",
      "epoch:  793, loss: 0.007031402550637722\n",
      "epoch:  794, loss: 0.007030308712273836\n",
      "epoch:  795, loss: 0.007029518485069275\n",
      "epoch:  796, loss: 0.007025169674307108\n",
      "epoch:  797, loss: 0.007022204343229532\n",
      "epoch:  798, loss: 0.007021093275398016\n",
      "epoch:  799, loss: 0.00702030910179019\n",
      "epoch:  800, loss: 0.007015990559011698\n",
      "epoch:  801, loss: 0.0070130228996276855\n",
      "epoch:  802, loss: 0.007011914160102606\n",
      "epoch:  803, loss: 0.007011136040091515\n",
      "epoch:  804, loss: 0.007006896194070578\n",
      "epoch:  805, loss: 0.007003923878073692\n",
      "epoch:  806, loss: 0.00700279651209712\n",
      "epoch:  807, loss: 0.007002023048698902\n",
      "epoch:  808, loss: 0.006997407879680395\n",
      "epoch:  809, loss: 0.006994711700826883\n",
      "epoch:  810, loss: 0.006993666756898165\n",
      "epoch:  811, loss: 0.0069929007440805435\n",
      "epoch:  812, loss: 0.006988571956753731\n",
      "epoch:  813, loss: 0.006985710468143225\n",
      "epoch:  814, loss: 0.006984613370150328\n",
      "epoch:  815, loss: 0.006983847822993994\n",
      "epoch:  816, loss: 0.006979708559811115\n",
      "epoch:  817, loss: 0.006976740434765816\n",
      "epoch:  818, loss: 0.006975596770644188\n",
      "epoch:  819, loss: 0.006974820513278246\n",
      "epoch:  820, loss: 0.006971044931560755\n",
      "epoch:  821, loss: 0.006967735942453146\n",
      "epoch:  822, loss: 0.006966592278331518\n",
      "epoch:  823, loss: 0.0069657983258366585\n",
      "epoch:  824, loss: 0.006962117739021778\n",
      "epoch:  825, loss: 0.006958748213946819\n",
      "epoch:  826, loss: 0.006957520730793476\n",
      "epoch:  827, loss: 0.006956722587347031\n",
      "epoch:  828, loss: 0.006954227574169636\n",
      "epoch:  829, loss: 0.006949913688004017\n",
      "epoch:  830, loss: 0.006948478985577822\n",
      "epoch:  831, loss: 0.006947632879018784\n",
      "epoch:  832, loss: 0.0069466703571379185\n",
      "epoch:  833, loss: 0.006941103842109442\n",
      "epoch:  834, loss: 0.006939436309039593\n",
      "epoch:  835, loss: 0.00693853572010994\n",
      "epoch:  836, loss: 0.006937801837921143\n",
      "epoch:  837, loss: 0.00693259434774518\n",
      "epoch:  838, loss: 0.006930493749678135\n",
      "epoch:  839, loss: 0.006929533090442419\n",
      "epoch:  840, loss: 0.0069287847727537155\n",
      "epoch:  841, loss: 0.006924025248736143\n",
      "epoch:  842, loss: 0.006921566091477871\n",
      "epoch:  843, loss: 0.006920527666807175\n",
      "epoch:  844, loss: 0.006919762119650841\n",
      "epoch:  845, loss: 0.006915646605193615\n",
      "epoch:  846, loss: 0.00691272271797061\n",
      "epoch:  847, loss: 0.006911603733897209\n",
      "epoch:  848, loss: 0.006910810247063637\n",
      "epoch:  849, loss: 0.006909881252795458\n",
      "epoch:  850, loss: 0.0069044893607497215\n",
      "epoch:  851, loss: 0.006902878172695637\n",
      "epoch:  852, loss: 0.006901976652443409\n",
      "epoch:  853, loss: 0.006901247892528772\n",
      "epoch:  854, loss: 0.00689665088430047\n",
      "epoch:  855, loss: 0.006894175428897142\n",
      "epoch:  856, loss: 0.00689316913485527\n",
      "epoch:  857, loss: 0.006892387289553881\n",
      "epoch:  858, loss: 0.0068916864693164825\n",
      "epoch:  859, loss: 0.006887723226100206\n",
      "epoch:  860, loss: 0.006884787231683731\n",
      "epoch:  861, loss: 0.006883644033223391\n",
      "epoch:  862, loss: 0.00688285194337368\n",
      "epoch:  863, loss: 0.006881228648126125\n",
      "epoch:  864, loss: 0.006876449100673199\n",
      "epoch:  865, loss: 0.006874916609376669\n",
      "epoch:  866, loss: 0.006874029524624348\n",
      "epoch:  867, loss: 0.006873311009258032\n",
      "epoch:  868, loss: 0.006868643686175346\n",
      "epoch:  869, loss: 0.006866337265819311\n",
      "epoch:  870, loss: 0.006865296512842178\n",
      "epoch:  871, loss: 0.006864540744572878\n",
      "epoch:  872, loss: 0.006863836199045181\n",
      "epoch:  873, loss: 0.0068590762093663216\n",
      "epoch:  874, loss: 0.006856860127300024\n",
      "epoch:  875, loss: 0.006855813320726156\n",
      "epoch:  876, loss: 0.0068550799041986465\n",
      "epoch:  877, loss: 0.006853369530290365\n",
      "epoch:  878, loss: 0.006848533172160387\n",
      "epoch:  879, loss: 0.006847049109637737\n",
      "epoch:  880, loss: 0.006846178323030472\n",
      "epoch:  881, loss: 0.006845442578196526\n",
      "epoch:  882, loss: 0.0068404944613575935\n",
      "epoch:  883, loss: 0.006838299799710512\n",
      "epoch:  884, loss: 0.006837274879217148\n",
      "epoch:  885, loss: 0.006836518179625273\n",
      "epoch:  886, loss: 0.006834459491074085\n",
      "epoch:  887, loss: 0.006830023601651192\n",
      "epoch:  888, loss: 0.00682852603495121\n",
      "epoch:  889, loss: 0.006827615667134523\n",
      "epoch:  890, loss: 0.006826881784945726\n",
      "epoch:  891, loss: 0.0068221925757825375\n",
      "epoch:  892, loss: 0.0068198335357010365\n",
      "epoch:  893, loss: 0.006818805821239948\n",
      "epoch:  894, loss: 0.006818024907261133\n",
      "epoch:  895, loss: 0.0068153743632137775\n",
      "epoch:  896, loss: 0.0068112704902887344\n",
      "epoch:  897, loss: 0.006809908896684647\n",
      "epoch:  898, loss: 0.006809012498706579\n",
      "epoch:  899, loss: 0.006808261852711439\n",
      "epoch:  900, loss: 0.00680332537740469\n",
      "epoch:  901, loss: 0.0068010129034519196\n",
      "epoch:  902, loss: 0.006799948401749134\n",
      "epoch:  903, loss: 0.0067991409450769424\n",
      "epoch:  904, loss: 0.006795940920710564\n",
      "epoch:  905, loss: 0.0067922682501375675\n",
      "epoch:  906, loss: 0.006790845189243555\n",
      "epoch:  907, loss: 0.006789960898458958\n",
      "epoch:  908, loss: 0.00678918557241559\n",
      "epoch:  909, loss: 0.006784322205930948\n",
      "epoch:  910, loss: 0.006781891454011202\n",
      "epoch:  911, loss: 0.0067807650193572044\n",
      "epoch:  912, loss: 0.0067799328826367855\n",
      "epoch:  913, loss: 0.006779178977012634\n",
      "epoch:  914, loss: 0.006774510722607374\n",
      "epoch:  915, loss: 0.00677195331081748\n",
      "epoch:  916, loss: 0.006770783569663763\n",
      "epoch:  917, loss: 0.006769933737814426\n",
      "epoch:  918, loss: 0.006769879255443811\n",
      "epoch:  919, loss: 0.006763575132936239\n",
      "epoch:  920, loss: 0.006761586293578148\n",
      "epoch:  921, loss: 0.006760533899068832\n",
      "epoch:  922, loss: 0.006759703624993563\n",
      "epoch:  923, loss: 0.006758140400052071\n",
      "epoch:  924, loss: 0.006752744782716036\n",
      "epoch:  925, loss: 0.006750984583050013\n",
      "epoch:  926, loss: 0.006749932188540697\n",
      "epoch:  927, loss: 0.006749116349965334\n",
      "epoch:  928, loss: 0.0067483363673090935\n",
      "epoch:  929, loss: 0.006745454855263233\n",
      "epoch:  930, loss: 0.0067413258366286755\n",
      "epoch:  931, loss: 0.006739827338606119\n",
      "epoch:  932, loss: 0.006738889962434769\n",
      "epoch:  933, loss: 0.0067381118424236774\n",
      "epoch:  934, loss: 0.006735536269843578\n",
      "epoch:  935, loss: 0.006731310393661261\n",
      "epoch:  936, loss: 0.00672979885712266\n",
      "epoch:  937, loss: 0.006728835869580507\n",
      "epoch:  938, loss: 0.006728059146553278\n",
      "epoch:  939, loss: 0.006725956220179796\n",
      "epoch:  940, loss: 0.006721498444676399\n",
      "epoch:  941, loss: 0.0067198872566223145\n",
      "epoch:  942, loss: 0.006718925666064024\n",
      "epoch:  943, loss: 0.006718146614730358\n",
      "epoch:  944, loss: 0.006716574542224407\n",
      "epoch:  945, loss: 0.006711677182465792\n",
      "epoch:  946, loss: 0.0067099761217832565\n",
      "epoch:  947, loss: 0.006708973087370396\n",
      "epoch:  948, loss: 0.0067081814631819725\n",
      "epoch:  949, loss: 0.0067054834216833115\n",
      "epoch:  950, loss: 0.006701461039483547\n",
      "epoch:  951, loss: 0.006699929945170879\n",
      "epoch:  952, loss: 0.006699014455080032\n",
      "epoch:  953, loss: 0.00669824006035924\n",
      "epoch:  954, loss: 0.006695281248539686\n",
      "epoch:  955, loss: 0.006691462825983763\n",
      "epoch:  956, loss: 0.006690021138638258\n",
      "epoch:  957, loss: 0.006689088419079781\n",
      "epoch:  958, loss: 0.006688316352665424\n",
      "epoch:  959, loss: 0.006684120744466782\n",
      "epoch:  960, loss: 0.006681225728243589\n",
      "epoch:  961, loss: 0.00667994748800993\n",
      "epoch:  962, loss: 0.006679054349660873\n",
      "epoch:  963, loss: 0.006678276229649782\n",
      "epoch:  964, loss: 0.006673320662230253\n",
      "epoch:  965, loss: 0.006670830305665731\n",
      "epoch:  966, loss: 0.0066696410067379475\n",
      "epoch:  967, loss: 0.006668791174888611\n",
      "epoch:  968, loss: 0.006668017711490393\n",
      "epoch:  969, loss: 0.006662847939878702\n",
      "epoch:  970, loss: 0.006660504266619682\n",
      "epoch:  971, loss: 0.006659318692982197\n",
      "epoch:  972, loss: 0.006658467464148998\n",
      "epoch:  973, loss: 0.006657697726041079\n",
      "epoch:  974, loss: 0.0066522941924631596\n",
      "epoch:  975, loss: 0.006650056689977646\n",
      "epoch:  976, loss: 0.006648923270404339\n",
      "epoch:  977, loss: 0.006648088805377483\n",
      "epoch:  978, loss: 0.006647306494414806\n",
      "epoch:  979, loss: 0.006641661748290062\n",
      "epoch:  980, loss: 0.006639546249061823\n",
      "epoch:  981, loss: 0.006638396997004747\n",
      "epoch:  982, loss: 0.006637522950768471\n",
      "epoch:  983, loss: 0.006637400481849909\n",
      "epoch:  984, loss: 0.00663113035261631\n",
      "epoch:  985, loss: 0.006628966890275478\n",
      "epoch:  986, loss: 0.006627808790653944\n",
      "epoch:  987, loss: 0.006626935210078955\n",
      "epoch:  988, loss: 0.0066261328756809235\n",
      "epoch:  989, loss: 0.00662168487906456\n",
      "epoch:  990, loss: 0.006618719547986984\n",
      "epoch:  991, loss: 0.006617334671318531\n",
      "epoch:  992, loss: 0.006616406608372927\n",
      "epoch:  993, loss: 0.006615594029426575\n",
      "epoch:  994, loss: 0.006613518111407757\n",
      "epoch:  995, loss: 0.006608782801777124\n",
      "epoch:  996, loss: 0.006606957875192165\n",
      "epoch:  997, loss: 0.006605901289731264\n",
      "epoch:  998, loss: 0.006605061236768961\n",
      "epoch:  999, loss: 0.00660362234339118\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size=X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "# opt = torch_numopt.ConjugateGradient(model.parameters(), lr=1e-4, model=model, c1=1e-4, tau=0.1, line_search_method=\"const\")\n",
    "opt = torch_numopt.ConjugateGradient(model.parameters(), lr=10, model=model, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"armijo\", cg_method=\"PR\")\n",
    "# opt = torch_numopt.ConjugateGradient(model.parameters(), lr=10, model=model, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"wolfe\")\n",
    "# opt = torch_numopt.ConjugateGradient(model.parameters(), lr=10, model=model, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"strong-wolfe\")\n",
    "# opt = torch_numopt.ConjugateGradient(model.parameters(), lr=10, model=model, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"goldstein\")\n",
    "\n",
    "\n",
    "all_loss = {}\n",
    "for epoch in range(1000):\n",
    "    print(\"epoch: \", epoch, end=\"\")\n",
    "    all_loss[epoch + 1] = 0\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y, loss_fn)\n",
    "\n",
    "        all_loss[epoch + 1] += loss\n",
    "    all_loss[epoch + 1] /= len(data_loader)\n",
    "    print(\", loss: {}\".format(all_loss[epoch + 1].detach().numpy().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics: R2 = 0.7557700806203207\n",
      "Test metrics:  R2 = 0.702852068839014\n"
     ]
    }
   ],
   "source": [
    "pred_train = model.forward(X_train).detach()\n",
    "pred_test = model.forward(X_test).detach()\n",
    "print(f\"Train metrics: R2 = {r2_score(pred_train, y_train)}\")\n",
    "print(f\"Test metrics:  R2 = {r2_score(pred_test, y_test)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
