{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch_numopt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import *\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.f1 = nn.Linear(input_size, 10, device=device)\n",
    "        self.f2 = nn.Linear(10, 20, device=device)\n",
    "        self.f3 = nn.Linear(20, 20, device=device)\n",
    "        self.f4 = nn.Linear(20, 10, device=device)\n",
    "        self.f5 = nn.Linear(10, 1, device=device)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        # self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.f1(x))\n",
    "        x = self.activation(self.f2(x))\n",
    "        x = self.activation(self.f3(x))\n",
    "        x = self.activation(self.f4(x))\n",
    "        x = self.f5(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([800, 10]) torch.Size([800, 1])\n",
      "torch.Size([200, 10]) torch.Size([200, 1])\n"
     ]
    }
   ],
   "source": [
    "# X, y = load_diabetes(return_X_y=True, scaled=False)\n",
    "# X, y = make_regression(n_samples=1000, n_features=100)\n",
    "X, y = make_friedman1(n_samples=1000, noise=1e-2)\n",
    "\n",
    "X_scaler = MinMaxScaler()\n",
    "X = X_scaler.fit_transform(X)\n",
    "X = torch.Tensor(X).to(device)\n",
    "\n",
    "y_scaler = MinMaxScaler()\n",
    "y = y_scaler.fit_transform(y.reshape((-1, 1)))\n",
    "y = torch.Tensor(y).to(device)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "torch_data = TensorDataset(X_train, y_train)\n",
    "data_loader = DataLoader(torch_data, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.09446775168180466\n",
      "epoch:  1, loss: 0.06652283668518066\n",
      "epoch:  2, loss: 0.05295877903699875\n",
      "epoch:  3, loss: 0.04589714854955673\n",
      "epoch:  4, loss: 0.04201127961277962\n",
      "epoch:  5, loss: 0.039848800748586655\n",
      "epoch:  6, loss: 0.03863472118973732\n",
      "epoch:  7, loss: 0.03794476389884949\n",
      "epoch:  8, loss: 0.03754350543022156\n",
      "epoch:  9, loss: 0.037298452109098434\n",
      "epoch:  10, loss: 0.03713229298591614\n",
      "epoch:  11, loss: 0.03688928112387657\n",
      "epoch:  12, loss: 0.03675800561904907\n",
      "epoch:  13, loss: 0.0366847887635231\n",
      "epoch:  14, loss: 0.03664105758070946\n",
      "epoch:  15, loss: 0.036586277186870575\n",
      "epoch:  16, loss: 0.03651970252394676\n",
      "epoch:  17, loss: 0.03647948428988457\n",
      "epoch:  18, loss: 0.036419447511434555\n",
      "epoch:  19, loss: 0.036356303840875626\n",
      "epoch:  20, loss: 0.03631788119673729\n",
      "epoch:  21, loss: 0.03624971956014633\n",
      "epoch:  22, loss: 0.036192089319229126\n",
      "epoch:  23, loss: 0.03617352619767189\n",
      "epoch:  24, loss: 0.03607888147234917\n",
      "epoch:  25, loss: 0.03602500259876251\n",
      "epoch:  26, loss: 0.0359933003783226\n",
      "epoch:  27, loss: 0.03590558469295502\n",
      "epoch:  28, loss: 0.035853587090969086\n",
      "epoch:  29, loss: 0.03581283241510391\n",
      "epoch:  30, loss: 0.03572491183876991\n",
      "epoch:  31, loss: 0.03567430377006531\n",
      "epoch:  32, loss: 0.03562290221452713\n",
      "epoch:  33, loss: 0.03553807735443115\n",
      "epoch:  34, loss: 0.03548765555024147\n",
      "epoch:  35, loss: 0.03542373701930046\n",
      "epoch:  36, loss: 0.035340502858161926\n",
      "epoch:  37, loss: 0.03529065474867821\n",
      "epoch:  38, loss: 0.03521560877561569\n",
      "epoch:  39, loss: 0.03513253852725029\n",
      "epoch:  40, loss: 0.03508247062563896\n",
      "epoch:  41, loss: 0.034994978457689285\n",
      "epoch:  42, loss: 0.034908927977085114\n",
      "epoch:  43, loss: 0.034857865422964096\n",
      "epoch:  44, loss: 0.03475791588425636\n",
      "epoch:  45, loss: 0.034671682864427567\n",
      "epoch:  46, loss: 0.03461848199367523\n",
      "epoch:  47, loss: 0.03451497107744217\n",
      "epoch:  48, loss: 0.03441530838608742\n",
      "epoch:  49, loss: 0.03435911238193512\n",
      "epoch:  50, loss: 0.034234706312417984\n",
      "epoch:  51, loss: 0.034138135612010956\n",
      "epoch:  52, loss: 0.03407875448465347\n",
      "epoch:  53, loss: 0.033952150493860245\n",
      "epoch:  54, loss: 0.03383979573845863\n",
      "epoch:  55, loss: 0.03377361595630646\n",
      "epoch:  56, loss: 0.03362342715263367\n",
      "epoch:  57, loss: 0.03350885584950447\n",
      "epoch:  58, loss: 0.03344151750206947\n",
      "epoch:  59, loss: 0.033283937722444534\n",
      "epoch:  60, loss: 0.03315236046910286\n",
      "epoch:  61, loss: 0.03307616338133812\n",
      "epoch:  62, loss: 0.032913874834775925\n",
      "epoch:  63, loss: 0.032764729112386703\n",
      "epoch:  64, loss: 0.03267855569720268\n",
      "epoch:  65, loss: 0.03250783681869507\n",
      "epoch:  66, loss: 0.032334137707948685\n",
      "epoch:  67, loss: 0.03223804384469986\n",
      "epoch:  68, loss: 0.032079607248306274\n",
      "epoch:  69, loss: 0.03187097609043121\n",
      "epoch:  70, loss: 0.0317576564848423\n",
      "epoch:  71, loss: 0.03161616623401642\n",
      "epoch:  72, loss: 0.03135748207569122\n",
      "epoch:  73, loss: 0.031224999576807022\n",
      "epoch:  74, loss: 0.031122947111725807\n",
      "epoch:  75, loss: 0.03079748898744583\n",
      "epoch:  76, loss: 0.03063567727804184\n",
      "epoch:  77, loss: 0.030594244599342346\n",
      "epoch:  78, loss: 0.030178498476743698\n",
      "epoch:  79, loss: 0.02998797595500946\n",
      "epoch:  80, loss: 0.02987724542617798\n",
      "epoch:  81, loss: 0.029509883373975754\n",
      "epoch:  82, loss: 0.02926544100046158\n",
      "epoch:  83, loss: 0.0291372649371624\n",
      "epoch:  84, loss: 0.028762657195329666\n",
      "epoch:  85, loss: 0.028472885489463806\n",
      "epoch:  86, loss: 0.028323836624622345\n",
      "epoch:  87, loss: 0.02797253429889679\n",
      "epoch:  88, loss: 0.027593202888965607\n",
      "epoch:  89, loss: 0.027418101206421852\n",
      "epoch:  90, loss: 0.027099991217255592\n",
      "epoch:  91, loss: 0.026623675599694252\n",
      "epoch:  92, loss: 0.026422569528222084\n",
      "epoch:  93, loss: 0.02607049234211445\n",
      "epoch:  94, loss: 0.02553695999085903\n",
      "epoch:  95, loss: 0.02531757578253746\n",
      "epoch:  96, loss: 0.024973688647150993\n",
      "epoch:  97, loss: 0.0243612602353096\n",
      "epoch:  98, loss: 0.024118315428495407\n",
      "epoch:  99, loss: 0.023717788979411125\n",
      "epoch:  100, loss: 0.023077988997101784\n",
      "epoch:  101, loss: 0.022842418402433395\n",
      "epoch:  102, loss: 0.022487735375761986\n",
      "epoch:  103, loss: 0.02176184020936489\n",
      "epoch:  104, loss: 0.021509001031517982\n",
      "epoch:  105, loss: 0.021169263869524002\n",
      "epoch:  106, loss: 0.02037152834236622\n",
      "epoch:  107, loss: 0.020129945129156113\n",
      "epoch:  108, loss: 0.019649038091301918\n",
      "epoch:  109, loss: 0.018952548503875732\n",
      "epoch:  110, loss: 0.01874765008687973\n",
      "epoch:  111, loss: 0.01807507313787937\n",
      "epoch:  112, loss: 0.017553536221385002\n",
      "epoch:  113, loss: 0.017391858622431755\n",
      "epoch:  114, loss: 0.0165313258767128\n",
      "epoch:  115, loss: 0.01622668467462063\n",
      "epoch:  116, loss: 0.016030890867114067\n",
      "epoch:  117, loss: 0.015173734165728092\n",
      "epoch:  118, loss: 0.015024175867438316\n",
      "epoch:  119, loss: 0.014305869117379189\n",
      "epoch:  120, loss: 0.014059602282941341\n",
      "epoch:  121, loss: 0.013770661316812038\n",
      "epoch:  122, loss: 0.013221613131463528\n",
      "epoch:  123, loss: 0.01313407439738512\n",
      "epoch:  124, loss: 0.0125221386551857\n",
      "epoch:  125, loss: 0.012425151653587818\n",
      "epoch:  126, loss: 0.011990334838628769\n",
      "epoch:  127, loss: 0.011833522468805313\n",
      "epoch:  128, loss: 0.011495825834572315\n",
      "epoch:  129, loss: 0.011356993578374386\n",
      "epoch:  130, loss: 0.011122012510895729\n",
      "epoch:  131, loss: 0.010967690497636795\n",
      "epoch:  132, loss: 0.010708554647862911\n",
      "epoch:  133, loss: 0.01065252535045147\n",
      "epoch:  134, loss: 0.010273342952132225\n",
      "epoch:  135, loss: 0.009417041204869747\n",
      "epoch:  136, loss: 0.009371795691549778\n",
      "epoch:  137, loss: 0.009287364780902863\n",
      "epoch:  138, loss: 0.00920665543526411\n",
      "epoch:  139, loss: 0.009194782935082912\n",
      "epoch:  140, loss: 0.009109907783567905\n",
      "epoch:  141, loss: 0.009095910005271435\n",
      "epoch:  142, loss: 0.00907148327678442\n",
      "epoch:  143, loss: 0.009031559340655804\n",
      "epoch:  144, loss: 0.009026378393173218\n",
      "epoch:  145, loss: 0.009004942141473293\n",
      "epoch:  146, loss: 0.00879879854619503\n",
      "epoch:  147, loss: 0.008788924664258957\n",
      "epoch:  148, loss: 0.008748937398195267\n",
      "epoch:  149, loss: 0.00873640924692154\n",
      "epoch:  150, loss: 0.008732913061976433\n",
      "epoch:  151, loss: 0.008707226254045963\n",
      "epoch:  152, loss: 0.008703886531293392\n",
      "epoch:  153, loss: 0.008690111339092255\n",
      "epoch:  154, loss: 0.008684998378157616\n",
      "epoch:  155, loss: 0.008684305474162102\n",
      "epoch:  156, loss: 0.008669743314385414\n",
      "epoch:  157, loss: 0.008668040856719017\n",
      "epoch:  158, loss: 0.008656907826662064\n",
      "epoch:  159, loss: 0.008654248900711536\n",
      "epoch:  160, loss: 0.008647629991173744\n",
      "epoch:  161, loss: 0.008641240186989307\n",
      "epoch:  162, loss: 0.00863991491496563\n",
      "epoch:  163, loss: 0.00858776830136776\n",
      "epoch:  164, loss: 0.008537895046174526\n",
      "epoch:  165, loss: 0.008534829132258892\n",
      "epoch:  166, loss: 0.008533783257007599\n",
      "epoch:  167, loss: 0.008513594046235085\n",
      "epoch:  168, loss: 0.00844329223036766\n",
      "epoch:  169, loss: 0.00843844749033451\n",
      "epoch:  170, loss: 0.008436915464699268\n",
      "epoch:  171, loss: 0.008424734696745872\n",
      "epoch:  172, loss: 0.008422358892858028\n",
      "epoch:  173, loss: 0.008418437093496323\n",
      "epoch:  174, loss: 0.008407806046307087\n",
      "epoch:  175, loss: 0.008405860513448715\n",
      "epoch:  176, loss: 0.00839607510715723\n",
      "epoch:  177, loss: 0.008390776813030243\n",
      "epoch:  178, loss: 0.008389129303395748\n",
      "epoch:  179, loss: 0.008377430960536003\n",
      "epoch:  180, loss: 0.008374481461942196\n",
      "epoch:  181, loss: 0.008373015560209751\n",
      "epoch:  182, loss: 0.008361968211829662\n",
      "epoch:  183, loss: 0.008358086459338665\n",
      "epoch:  184, loss: 0.008356516249477863\n",
      "epoch:  185, loss: 0.008346360176801682\n",
      "epoch:  186, loss: 0.008342084474861622\n",
      "epoch:  187, loss: 0.008340556174516678\n",
      "epoch:  188, loss: 0.008332481607794762\n",
      "epoch:  189, loss: 0.008328243158757687\n",
      "epoch:  190, loss: 0.008326796814799309\n",
      "epoch:  191, loss: 0.008323031477630138\n",
      "epoch:  192, loss: 0.008315151557326317\n",
      "epoch:  193, loss: 0.008313261903822422\n",
      "epoch:  194, loss: 0.008312064222991467\n",
      "epoch:  195, loss: 0.008305524475872517\n",
      "epoch:  196, loss: 0.008300065994262695\n",
      "epoch:  197, loss: 0.008298431523144245\n",
      "epoch:  198, loss: 0.008291656151413918\n",
      "epoch:  199, loss: 0.008286476135253906\n",
      "epoch:  200, loss: 0.00828479789197445\n",
      "epoch:  201, loss: 0.008279059082269669\n",
      "epoch:  202, loss: 0.008273032493889332\n",
      "epoch:  203, loss: 0.008271362632513046\n",
      "epoch:  204, loss: 0.008268246427178383\n",
      "epoch:  205, loss: 0.008259708061814308\n",
      "epoch:  206, loss: 0.008257532492280006\n",
      "epoch:  207, loss: 0.008256218396127224\n",
      "epoch:  208, loss: 0.008248108439147472\n",
      "epoch:  209, loss: 0.008243213407695293\n",
      "epoch:  210, loss: 0.008241561241447926\n",
      "epoch:  211, loss: 0.008240285329520702\n",
      "epoch:  212, loss: 0.008234578184783459\n",
      "epoch:  213, loss: 0.008227702230215073\n",
      "epoch:  214, loss: 0.008226050063967705\n",
      "epoch:  215, loss: 0.008224722929298878\n",
      "epoch:  216, loss: 0.008217938244342804\n",
      "epoch:  217, loss: 0.008212928660213947\n",
      "epoch:  218, loss: 0.008211475796997547\n",
      "epoch:  219, loss: 0.008205979131162167\n",
      "epoch:  220, loss: 0.00820065662264824\n",
      "epoch:  221, loss: 0.008199156261980534\n",
      "epoch:  222, loss: 0.008195356465876102\n",
      "epoch:  223, loss: 0.008188297972083092\n",
      "epoch:  224, loss: 0.00818671565502882\n",
      "epoch:  225, loss: 0.008183612488210201\n",
      "epoch:  226, loss: 0.0081761060282588\n",
      "epoch:  227, loss: 0.00817443523555994\n",
      "epoch:  228, loss: 0.008172511123120785\n",
      "epoch:  229, loss: 0.008163838647305965\n",
      "epoch:  230, loss: 0.008162161335349083\n",
      "epoch:  231, loss: 0.008159431628882885\n",
      "epoch:  232, loss: 0.00815157312899828\n",
      "epoch:  233, loss: 0.00814987625926733\n",
      "epoch:  234, loss: 0.008148781955242157\n",
      "epoch:  235, loss: 0.008140084333717823\n",
      "epoch:  236, loss: 0.008137494325637817\n",
      "epoch:  237, loss: 0.008136309683322906\n",
      "epoch:  238, loss: 0.008128831163048744\n",
      "epoch:  239, loss: 0.00812510121613741\n",
      "epoch:  240, loss: 0.008123738691210747\n",
      "epoch:  241, loss: 0.008118625730276108\n",
      "epoch:  242, loss: 0.008112527430057526\n",
      "epoch:  243, loss: 0.0081108920276165\n",
      "epoch:  244, loss: 0.008109954185783863\n",
      "epoch:  245, loss: 0.008099805563688278\n",
      "epoch:  246, loss: 0.008097922429442406\n",
      "epoch:  247, loss: 0.008096751756966114\n",
      "epoch:  248, loss: 0.008087921887636185\n",
      "epoch:  249, loss: 0.008085043169558048\n",
      "epoch:  250, loss: 0.008083791472017765\n",
      "epoch:  251, loss: 0.008076668716967106\n",
      "epoch:  252, loss: 0.008072531782090664\n",
      "epoch:  253, loss: 0.008071132004261017\n",
      "epoch:  254, loss: 0.00806952454149723\n",
      "epoch:  255, loss: 0.008060519583523273\n",
      "epoch:  256, loss: 0.008058687672019005\n",
      "epoch:  257, loss: 0.008057563565671444\n",
      "epoch:  258, loss: 0.00804927945137024\n",
      "epoch:  259, loss: 0.008046500384807587\n",
      "epoch:  260, loss: 0.00804534275084734\n",
      "epoch:  261, loss: 0.008040441200137138\n",
      "epoch:  262, loss: 0.008034787140786648\n",
      "epoch:  263, loss: 0.008033236488699913\n",
      "epoch:  264, loss: 0.00803214032202959\n",
      "epoch:  265, loss: 0.008024736307561398\n",
      "epoch:  266, loss: 0.008021132089197636\n",
      "epoch:  267, loss: 0.008019775152206421\n",
      "epoch:  268, loss: 0.00801894348114729\n",
      "epoch:  269, loss: 0.008009146898984909\n",
      "epoch:  270, loss: 0.008007250726222992\n",
      "epoch:  271, loss: 0.008006086573004723\n",
      "epoch:  272, loss: 0.007998083718121052\n",
      "epoch:  273, loss: 0.007994570769369602\n",
      "epoch:  274, loss: 0.007993237115442753\n",
      "epoch:  275, loss: 0.007987594231963158\n",
      "epoch:  276, loss: 0.007981675677001476\n",
      "epoch:  277, loss: 0.00797999370843172\n",
      "epoch:  278, loss: 0.007978769950568676\n",
      "epoch:  279, loss: 0.007970181293785572\n",
      "epoch:  280, loss: 0.00796687975525856\n",
      "epoch:  281, loss: 0.007965397089719772\n",
      "epoch:  282, loss: 0.00796419195830822\n",
      "epoch:  283, loss: 0.007955309003591537\n",
      "epoch:  284, loss: 0.007951965555548668\n",
      "epoch:  285, loss: 0.007950497791171074\n",
      "epoch:  286, loss: 0.007945992052555084\n",
      "epoch:  287, loss: 0.00793861411511898\n",
      "epoch:  288, loss: 0.00793677568435669\n",
      "epoch:  289, loss: 0.007935475558042526\n",
      "epoch:  290, loss: 0.007925374433398247\n",
      "epoch:  291, loss: 0.007922764867544174\n",
      "epoch:  292, loss: 0.00792142003774643\n",
      "epoch:  293, loss: 0.007914621382951736\n",
      "epoch:  294, loss: 0.007909067906439304\n",
      "epoch:  295, loss: 0.00790721271187067\n",
      "epoch:  296, loss: 0.007905926555395126\n",
      "epoch:  297, loss: 0.007904675789177418\n",
      "epoch:  298, loss: 0.007899710908532143\n",
      "epoch:  299, loss: 0.007892388850450516\n",
      "epoch:  300, loss: 0.007890445180237293\n",
      "epoch:  301, loss: 0.007889078930020332\n",
      "epoch:  302, loss: 0.007883485406637192\n",
      "epoch:  303, loss: 0.007876387797296047\n",
      "epoch:  304, loss: 0.00787428766489029\n",
      "epoch:  305, loss: 0.0078728711232543\n",
      "epoch:  306, loss: 0.007868676446378231\n",
      "epoch:  307, loss: 0.007859963923692703\n",
      "epoch:  308, loss: 0.007857671007514\n",
      "epoch:  309, loss: 0.007856234908103943\n",
      "epoch:  310, loss: 0.00785001926124096\n",
      "epoch:  311, loss: 0.007843109779059887\n",
      "epoch:  312, loss: 0.007840977050364017\n",
      "epoch:  313, loss: 0.007839547470211983\n",
      "epoch:  314, loss: 0.007829527370631695\n",
      "epoch:  315, loss: 0.00782568659633398\n",
      "epoch:  316, loss: 0.007823904044926167\n",
      "epoch:  317, loss: 0.007822362706065178\n",
      "epoch:  318, loss: 0.007810679264366627\n",
      "epoch:  319, loss: 0.0078079355880618095\n",
      "epoch:  320, loss: 0.007806194480508566\n",
      "epoch:  321, loss: 0.007804579567164183\n",
      "epoch:  322, loss: 0.007792719639837742\n",
      "epoch:  323, loss: 0.007789830677211285\n",
      "epoch:  324, loss: 0.00778816407546401\n",
      "epoch:  325, loss: 0.007780466228723526\n",
      "epoch:  326, loss: 0.007773643359541893\n",
      "epoch:  327, loss: 0.0077715241350233555\n",
      "epoch:  328, loss: 0.007769981399178505\n",
      "epoch:  329, loss: 0.007762485183775425\n",
      "epoch:  330, loss: 0.007755182217806578\n",
      "epoch:  331, loss: 0.007752861827611923\n",
      "epoch:  332, loss: 0.007751211524009705\n",
      "epoch:  333, loss: 0.0077411760576069355\n",
      "epoch:  334, loss: 0.007735435850918293\n",
      "epoch:  335, loss: 0.007733330130577087\n",
      "epoch:  336, loss: 0.00773168820887804\n",
      "epoch:  337, loss: 0.007721682079136372\n",
      "epoch:  338, loss: 0.007715683430433273\n",
      "epoch:  339, loss: 0.00771344406530261\n",
      "epoch:  340, loss: 0.007711737882345915\n",
      "epoch:  341, loss: 0.007700017653405666\n",
      "epoch:  342, loss: 0.007695196196436882\n",
      "epoch:  343, loss: 0.007693189196288586\n",
      "epoch:  344, loss: 0.007691525388509035\n",
      "epoch:  345, loss: 0.0076784901320934296\n",
      "epoch:  346, loss: 0.0076743620447814465\n",
      "epoch:  347, loss: 0.00767236715182662\n",
      "epoch:  348, loss: 0.007666552439332008\n",
      "epoch:  349, loss: 0.007655673194676638\n",
      "epoch:  350, loss: 0.007652984466403723\n",
      "epoch:  351, loss: 0.007651160471141338\n",
      "epoch:  352, loss: 0.0076393564231693745\n",
      "epoch:  353, loss: 0.0076336972415447235\n",
      "epoch:  354, loss: 0.007631347049027681\n",
      "epoch:  355, loss: 0.007629561237990856\n",
      "epoch:  356, loss: 0.007615662179887295\n",
      "epoch:  357, loss: 0.007611591834574938\n",
      "epoch:  358, loss: 0.007609514985233545\n",
      "epoch:  359, loss: 0.0076048667542636395\n",
      "epoch:  360, loss: 0.007593004032969475\n",
      "epoch:  361, loss: 0.007589846383780241\n",
      "epoch:  362, loss: 0.007587945554405451\n",
      "epoch:  363, loss: 0.007579748053103685\n",
      "epoch:  364, loss: 0.007571326568722725\n",
      "epoch:  365, loss: 0.0075687142089009285\n",
      "epoch:  366, loss: 0.007566917687654495\n",
      "epoch:  367, loss: 0.007556711323559284\n",
      "epoch:  368, loss: 0.007550067268311977\n",
      "epoch:  369, loss: 0.007547730579972267\n",
      "epoch:  370, loss: 0.0075460160151124\n",
      "epoch:  371, loss: 0.007532957941293716\n",
      "epoch:  372, loss: 0.007528756745159626\n",
      "epoch:  373, loss: 0.007526722736656666\n",
      "epoch:  374, loss: 0.007526211906224489\n",
      "epoch:  375, loss: 0.007511052303016186\n",
      "epoch:  376, loss: 0.007507601287215948\n",
      "epoch:  377, loss: 0.007505680434405804\n",
      "epoch:  378, loss: 0.007502202410250902\n",
      "epoch:  379, loss: 0.007489543408155441\n",
      "epoch:  380, loss: 0.007486496586352587\n",
      "epoch:  381, loss: 0.0074846865609288216\n",
      "epoch:  382, loss: 0.007478333078324795\n",
      "epoch:  383, loss: 0.007468170486390591\n",
      "epoch:  384, loss: 0.0074654133059084415\n",
      "epoch:  385, loss: 0.007463634479790926\n",
      "epoch:  386, loss: 0.007450544275343418\n",
      "epoch:  387, loss: 0.0074462550692260265\n",
      "epoch:  388, loss: 0.007444124668836594\n",
      "epoch:  389, loss: 0.007442401722073555\n",
      "epoch:  390, loss: 0.007429015822708607\n",
      "epoch:  391, loss: 0.007424830924719572\n",
      "epoch:  392, loss: 0.007422760594636202\n",
      "epoch:  393, loss: 0.007420296780765057\n",
      "epoch:  394, loss: 0.007407140918076038\n",
      "epoch:  395, loss: 0.007403343915939331\n",
      "epoch:  396, loss: 0.007401364389806986\n",
      "epoch:  397, loss: 0.007396577391773462\n",
      "epoch:  398, loss: 0.007385039236396551\n",
      "epoch:  399, loss: 0.007381717674434185\n",
      "epoch:  400, loss: 0.007379736751317978\n",
      "epoch:  401, loss: 0.007374927401542664\n",
      "epoch:  402, loss: 0.007363373413681984\n",
      "epoch:  403, loss: 0.0073600043542683125\n",
      "epoch:  404, loss: 0.007357986178249121\n",
      "epoch:  405, loss: 0.007351988926529884\n",
      "epoch:  406, loss: 0.007341397926211357\n",
      "epoch:  407, loss: 0.007338082417845726\n",
      "epoch:  408, loss: 0.007336065173149109\n",
      "epoch:  409, loss: 0.007330581545829773\n",
      "epoch:  410, loss: 0.007319377735257149\n",
      "epoch:  411, loss: 0.007315935101360083\n",
      "epoch:  412, loss: 0.00731385312974453\n",
      "epoch:  413, loss: 0.007312190718948841\n",
      "epoch:  414, loss: 0.007297884207218885\n",
      "epoch:  415, loss: 0.007293699774891138\n",
      "epoch:  416, loss: 0.007291458081454039\n",
      "epoch:  417, loss: 0.007289620582014322\n",
      "epoch:  418, loss: 0.007275909185409546\n",
      "epoch:  419, loss: 0.007271208334714174\n",
      "epoch:  420, loss: 0.007268831599503756\n",
      "epoch:  421, loss: 0.007266960572451353\n",
      "epoch:  422, loss: 0.007254722528159618\n",
      "epoch:  423, loss: 0.007248743902891874\n",
      "epoch:  424, loss: 0.007246123626828194\n",
      "epoch:  425, loss: 0.00724418880417943\n",
      "epoch:  426, loss: 0.007234096992760897\n",
      "epoch:  427, loss: 0.007225862704217434\n",
      "epoch:  428, loss: 0.007223176769912243\n",
      "epoch:  429, loss: 0.0072212074883282185\n",
      "epoch:  430, loss: 0.007210497744381428\n",
      "epoch:  431, loss: 0.00720251677557826\n",
      "epoch:  432, loss: 0.00719978054985404\n",
      "epoch:  433, loss: 0.007197774481028318\n",
      "epoch:  434, loss: 0.0071876500733196735\n",
      "epoch:  435, loss: 0.007178911939263344\n",
      "epoch:  436, loss: 0.007175980601459742\n",
      "epoch:  437, loss: 0.007173900026828051\n",
      "epoch:  438, loss: 0.007165421266108751\n",
      "epoch:  439, loss: 0.00715520279482007\n",
      "epoch:  440, loss: 0.007152060978114605\n",
      "epoch:  441, loss: 0.007149958983063698\n",
      "epoch:  442, loss: 0.0071439980529248714\n",
      "epoch:  443, loss: 0.0071320366114377975\n",
      "epoch:  444, loss: 0.007128441706299782\n",
      "epoch:  445, loss: 0.007126228883862495\n",
      "epoch:  446, loss: 0.007125476375222206\n",
      "epoch:  447, loss: 0.007109323050826788\n",
      "epoch:  448, loss: 0.007104774937033653\n",
      "epoch:  449, loss: 0.007102351635694504\n",
      "epoch:  450, loss: 0.007100376300513744\n",
      "epoch:  451, loss: 0.007088105659931898\n",
      "epoch:  452, loss: 0.007081508170813322\n",
      "epoch:  453, loss: 0.0070784566923975945\n",
      "epoch:  454, loss: 0.007076325826346874\n",
      "epoch:  455, loss: 0.007073936518281698\n",
      "epoch:  456, loss: 0.007059781346470118\n",
      "epoch:  457, loss: 0.007054463494569063\n",
      "epoch:  458, loss: 0.007052118889987469\n",
      "epoch:  459, loss: 0.007050135638564825\n",
      "epoch:  460, loss: 0.0070379870012402534\n",
      "epoch:  461, loss: 0.007030609995126724\n",
      "epoch:  462, loss: 0.007027920335531235\n",
      "epoch:  463, loss: 0.007025786209851503\n",
      "epoch:  464, loss: 0.007019862066954374\n",
      "epoch:  465, loss: 0.007007565349340439\n",
      "epoch:  466, loss: 0.007003694772720337\n",
      "epoch:  467, loss: 0.007001240272074938\n",
      "epoch:  468, loss: 0.006999203935265541\n",
      "epoch:  469, loss: 0.006987418979406357\n",
      "epoch:  470, loss: 0.0069800736382603645\n",
      "epoch:  471, loss: 0.006976489443331957\n",
      "epoch:  472, loss: 0.006974362302571535\n",
      "epoch:  473, loss: 0.006972386036068201\n",
      "epoch:  474, loss: 0.006961337756365538\n",
      "epoch:  475, loss: 0.006952883210033178\n",
      "epoch:  476, loss: 0.006949772126972675\n",
      "epoch:  477, loss: 0.006947525776922703\n",
      "epoch:  478, loss: 0.0069455550983548164\n",
      "epoch:  479, loss: 0.006937404163181782\n",
      "epoch:  480, loss: 0.006927698850631714\n",
      "epoch:  481, loss: 0.0069232541136443615\n",
      "epoch:  482, loss: 0.00692091416567564\n",
      "epoch:  483, loss: 0.006918894127011299\n",
      "epoch:  484, loss: 0.00691224867478013\n",
      "epoch:  485, loss: 0.006900610402226448\n",
      "epoch:  486, loss: 0.0068965633399784565\n",
      "epoch:  487, loss: 0.006894032936543226\n",
      "epoch:  488, loss: 0.0068931132555007935\n",
      "epoch:  489, loss: 0.006877399515360594\n",
      "epoch:  490, loss: 0.006872294005006552\n",
      "epoch:  491, loss: 0.006869521923363209\n",
      "epoch:  492, loss: 0.006867458112537861\n",
      "epoch:  493, loss: 0.006862908136099577\n",
      "epoch:  494, loss: 0.006731918081641197\n",
      "epoch:  495, loss: 0.006687423214316368\n",
      "epoch:  496, loss: 0.00667189434170723\n",
      "epoch:  497, loss: 0.006664630025625229\n",
      "epoch:  498, loss: 0.006660897750407457\n",
      "epoch:  499, loss: 0.006657959893345833\n",
      "epoch:  500, loss: 0.006655523553490639\n",
      "epoch:  501, loss: 0.006653155200183392\n",
      "epoch:  502, loss: 0.006646748166531324\n",
      "epoch:  503, loss: 0.006634042132645845\n",
      "epoch:  504, loss: 0.006627924274653196\n",
      "epoch:  505, loss: 0.006624505389481783\n",
      "epoch:  506, loss: 0.006621776148676872\n",
      "epoch:  507, loss: 0.006619404070079327\n",
      "epoch:  508, loss: 0.006615077145397663\n",
      "epoch:  509, loss: 0.006601899396628141\n",
      "epoch:  510, loss: 0.006595683749765158\n",
      "epoch:  511, loss: 0.0065919868648052216\n",
      "epoch:  512, loss: 0.006589111872017384\n",
      "epoch:  513, loss: 0.006586709059774876\n",
      "epoch:  514, loss: 0.006584459450095892\n",
      "epoch:  515, loss: 0.006570169236510992\n",
      "epoch:  516, loss: 0.006563256960362196\n",
      "epoch:  517, loss: 0.006559409666806459\n",
      "epoch:  518, loss: 0.006556438747793436\n",
      "epoch:  519, loss: 0.006553990300744772\n",
      "epoch:  520, loss: 0.006551721133291721\n",
      "epoch:  521, loss: 0.00653962092474103\n",
      "epoch:  522, loss: 0.006531413644552231\n",
      "epoch:  523, loss: 0.006527211517095566\n",
      "epoch:  524, loss: 0.006524098105728626\n",
      "epoch:  525, loss: 0.006521439179778099\n",
      "epoch:  526, loss: 0.006519094575196505\n",
      "epoch:  527, loss: 0.006518564652651548\n",
      "epoch:  528, loss: 0.006503684911876917\n",
      "epoch:  529, loss: 0.00649771885946393\n",
      "epoch:  530, loss: 0.006492942571640015\n",
      "epoch:  531, loss: 0.006489809602499008\n",
      "epoch:  532, loss: 0.006487163715064526\n",
      "epoch:  533, loss: 0.00648482795804739\n",
      "epoch:  534, loss: 0.00648260535672307\n",
      "epoch:  535, loss: 0.006480372045189142\n",
      "epoch:  536, loss: 0.00647822255268693\n",
      "epoch:  537, loss: 0.006476076785475016\n",
      "epoch:  538, loss: 0.006471650209277868\n",
      "epoch:  539, loss: 0.00646067364141345\n",
      "epoch:  540, loss: 0.0064547015354037285\n",
      "epoch:  541, loss: 0.006450405344367027\n",
      "epoch:  542, loss: 0.006447579246014357\n",
      "epoch:  543, loss: 0.006444983649998903\n",
      "epoch:  544, loss: 0.006442750804126263\n",
      "epoch:  545, loss: 0.00644053565338254\n",
      "epoch:  546, loss: 0.006438446696847677\n",
      "epoch:  547, loss: 0.006436308845877647\n",
      "epoch:  548, loss: 0.006427401676774025\n",
      "epoch:  549, loss: 0.006419019307941198\n",
      "epoch:  550, loss: 0.006413723807781935\n",
      "epoch:  551, loss: 0.00641025323420763\n",
      "epoch:  552, loss: 0.006407523527741432\n",
      "epoch:  553, loss: 0.00640521664172411\n",
      "epoch:  554, loss: 0.006402963772416115\n",
      "epoch:  555, loss: 0.006400862708687782\n",
      "epoch:  556, loss: 0.0063987113535404205\n",
      "epoch:  557, loss: 0.006388196721673012\n",
      "epoch:  558, loss: 0.006380345672369003\n",
      "epoch:  559, loss: 0.0063758548349142075\n",
      "epoch:  560, loss: 0.006372658535838127\n",
      "epoch:  561, loss: 0.006370021030306816\n",
      "epoch:  562, loss: 0.006367723923176527\n",
      "epoch:  563, loss: 0.006365497596561909\n",
      "epoch:  564, loss: 0.006363369058817625\n",
      "epoch:  565, loss: 0.006361245643347502\n",
      "epoch:  566, loss: 0.006356521975249052\n",
      "epoch:  567, loss: 0.006346079055219889\n",
      "epoch:  568, loss: 0.006339795421808958\n",
      "epoch:  569, loss: 0.006335888523608446\n",
      "epoch:  570, loss: 0.006332931574434042\n",
      "epoch:  571, loss: 0.006330427713692188\n",
      "epoch:  572, loss: 0.0063281129114329815\n",
      "epoch:  573, loss: 0.006325866561383009\n",
      "epoch:  574, loss: 0.0063237291760742664\n",
      "epoch:  575, loss: 0.006321577355265617\n",
      "epoch:  576, loss: 0.00631885277107358\n",
      "epoch:  577, loss: 0.0063066305592656136\n",
      "epoch:  578, loss: 0.006300266832113266\n",
      "epoch:  579, loss: 0.006296130828559399\n",
      "epoch:  580, loss: 0.006293185520917177\n",
      "epoch:  581, loss: 0.006290608551353216\n",
      "epoch:  582, loss: 0.006288306321948767\n",
      "epoch:  583, loss: 0.006286075338721275\n",
      "epoch:  584, loss: 0.006283917929977179\n",
      "epoch:  585, loss: 0.006281780079007149\n",
      "epoch:  586, loss: 0.006279646884649992\n",
      "epoch:  587, loss: 0.006277537439018488\n",
      "epoch:  588, loss: 0.006275407038629055\n",
      "epoch:  589, loss: 0.006273312494158745\n",
      "epoch:  590, loss: 0.006271177437156439\n",
      "epoch:  591, loss: 0.0062673962675035\n",
      "epoch:  592, loss: 0.00625572819262743\n",
      "epoch:  593, loss: 0.006248866207897663\n",
      "epoch:  594, loss: 0.006245064549148083\n",
      "epoch:  595, loss: 0.00624230457469821\n",
      "epoch:  596, loss: 0.006239706184715033\n",
      "epoch:  597, loss: 0.006237451918423176\n",
      "epoch:  598, loss: 0.006235214881598949\n",
      "epoch:  599, loss: 0.006229979917407036\n",
      "epoch:  600, loss: 0.006218543276190758\n",
      "epoch:  601, loss: 0.006212505511939526\n",
      "epoch:  602, loss: 0.006208773702383041\n",
      "epoch:  603, loss: 0.006205656100064516\n",
      "epoch:  604, loss: 0.006203198805451393\n",
      "epoch:  605, loss: 0.006200940813869238\n",
      "epoch:  606, loss: 0.006198662333190441\n",
      "epoch:  607, loss: 0.006196432746946812\n",
      "epoch:  608, loss: 0.006194249261170626\n",
      "epoch:  609, loss: 0.0061888801865279675\n",
      "epoch:  610, loss: 0.006178330164402723\n",
      "epoch:  611, loss: 0.0061721354722976685\n",
      "epoch:  612, loss: 0.006168029736727476\n",
      "epoch:  613, loss: 0.006164972670376301\n",
      "epoch:  614, loss: 0.006162339821457863\n",
      "epoch:  615, loss: 0.006159920245409012\n",
      "epoch:  616, loss: 0.006157686002552509\n",
      "epoch:  617, loss: 0.006146052852272987\n",
      "epoch:  618, loss: 0.006138743832707405\n",
      "epoch:  619, loss: 0.006134639959782362\n",
      "epoch:  620, loss: 0.006130897905677557\n",
      "epoch:  621, loss: 0.00612819055095315\n",
      "epoch:  622, loss: 0.0061257523484528065\n",
      "epoch:  623, loss: 0.006123436614871025\n",
      "epoch:  624, loss: 0.006121150217950344\n",
      "epoch:  625, loss: 0.0061189718544483185\n",
      "epoch:  626, loss: 0.006116775330156088\n",
      "epoch:  627, loss: 0.0061146412044763565\n",
      "epoch:  628, loss: 0.006112493108958006\n",
      "epoch:  629, loss: 0.00611040648072958\n",
      "epoch:  630, loss: 0.006108274683356285\n",
      "epoch:  631, loss: 0.006106168497353792\n",
      "epoch:  632, loss: 0.006104050669819117\n",
      "epoch:  633, loss: 0.006101907696574926\n",
      "epoch:  634, loss: 0.006099737249314785\n",
      "epoch:  635, loss: 0.006099236197769642\n",
      "epoch:  636, loss: 0.006087146233767271\n",
      "epoch:  637, loss: 0.006080391816794872\n",
      "epoch:  638, loss: 0.00607583811506629\n",
      "epoch:  639, loss: 0.006072125397622585\n",
      "epoch:  640, loss: 0.00606911676004529\n",
      "epoch:  641, loss: 0.006066462956368923\n",
      "epoch:  642, loss: 0.006063948851078749\n",
      "epoch:  643, loss: 0.00606168620288372\n",
      "epoch:  644, loss: 0.00605939282104373\n",
      "epoch:  645, loss: 0.006057164166122675\n",
      "epoch:  646, loss: 0.006054991856217384\n",
      "epoch:  647, loss: 0.006053776014596224\n",
      "epoch:  648, loss: 0.006043379195034504\n",
      "epoch:  649, loss: 0.006035495549440384\n",
      "epoch:  650, loss: 0.006031114608049393\n",
      "epoch:  651, loss: 0.006027430295944214\n",
      "epoch:  652, loss: 0.006024484522640705\n",
      "epoch:  653, loss: 0.006021823734045029\n",
      "epoch:  654, loss: 0.006019395776093006\n",
      "epoch:  655, loss: 0.00601710332557559\n",
      "epoch:  656, loss: 0.006014872342348099\n",
      "epoch:  657, loss: 0.006012697704136372\n",
      "epoch:  658, loss: 0.006010531913489103\n",
      "epoch:  659, loss: 0.006008407101035118\n",
      "epoch:  660, loss: 0.006006261333823204\n",
      "epoch:  661, loss: 0.00600415701046586\n",
      "epoch:  662, loss: 0.006004097871482372\n",
      "epoch:  663, loss: 0.005992565304040909\n",
      "epoch:  664, loss: 0.005985634867101908\n",
      "epoch:  665, loss: 0.0059802234172821045\n",
      "epoch:  666, loss: 0.005976542830467224\n",
      "epoch:  667, loss: 0.005973315332084894\n",
      "epoch:  668, loss: 0.005970831960439682\n",
      "epoch:  669, loss: 0.005968336481601\n",
      "epoch:  670, loss: 0.00596616929396987\n",
      "epoch:  671, loss: 0.005963904783129692\n",
      "epoch:  672, loss: 0.005949388723820448\n",
      "epoch:  673, loss: 0.005943766795098782\n",
      "epoch:  674, loss: 0.005940251983702183\n",
      "epoch:  675, loss: 0.005937361158430576\n",
      "epoch:  676, loss: 0.005934732034802437\n",
      "epoch:  677, loss: 0.005932442378252745\n",
      "epoch:  678, loss: 0.0059301583096385\n",
      "epoch:  679, loss: 0.00592800322920084\n",
      "epoch:  680, loss: 0.00592585327103734\n",
      "epoch:  681, loss: 0.005923692137002945\n",
      "epoch:  682, loss: 0.0059215836226940155\n",
      "epoch:  683, loss: 0.005919433198869228\n",
      "epoch:  684, loss: 0.005910441745072603\n",
      "epoch:  685, loss: 0.005904088728129864\n",
      "epoch:  686, loss: 0.0058988649398088455\n",
      "epoch:  687, loss: 0.0058947112411260605\n",
      "epoch:  688, loss: 0.005891520995646715\n",
      "epoch:  689, loss: 0.0058885421603918076\n",
      "epoch:  690, loss: 0.005885967519134283\n",
      "epoch:  691, loss: 0.005883636884391308\n",
      "epoch:  692, loss: 0.005881353747099638\n",
      "epoch:  693, loss: 0.005873269867151976\n",
      "epoch:  694, loss: 0.005866293329745531\n",
      "epoch:  695, loss: 0.005861819256097078\n",
      "epoch:  696, loss: 0.005857011303305626\n",
      "epoch:  697, loss: 0.0058541810140013695\n",
      "epoch:  698, loss: 0.005851200316101313\n",
      "epoch:  699, loss: 0.0058488086797297\n",
      "epoch:  700, loss: 0.005846215877681971\n",
      "epoch:  701, loss: 0.005843792576342821\n",
      "epoch:  702, loss: 0.0058414144441485405\n",
      "epoch:  703, loss: 0.005839160643517971\n",
      "epoch:  704, loss: 0.005836935248225927\n",
      "epoch:  705, loss: 0.005834756884723902\n",
      "epoch:  706, loss: 0.0058325715363025665\n",
      "epoch:  707, loss: 0.005830429960042238\n",
      "epoch:  708, loss: 0.005828285589814186\n",
      "epoch:  709, loss: 0.005826160777360201\n",
      "epoch:  710, loss: 0.005822010803967714\n",
      "epoch:  711, loss: 0.005814210046082735\n",
      "epoch:  712, loss: 0.00580882653594017\n",
      "epoch:  713, loss: 0.005804178770631552\n",
      "epoch:  714, loss: 0.005800474900752306\n",
      "epoch:  715, loss: 0.0057973843067884445\n",
      "epoch:  716, loss: 0.005794514901936054\n",
      "epoch:  717, loss: 0.005791939329355955\n",
      "epoch:  718, loss: 0.005789412185549736\n",
      "epoch:  719, loss: 0.005787088070064783\n",
      "epoch:  720, loss: 0.005784810986369848\n",
      "epoch:  721, loss: 0.00578253623098135\n",
      "epoch:  722, loss: 0.005780327133834362\n",
      "epoch:  723, loss: 0.005778148304671049\n",
      "epoch:  724, loss: 0.005776050500571728\n",
      "epoch:  725, loss: 0.00577388983219862\n",
      "epoch:  726, loss: 0.005771821830421686\n",
      "epoch:  727, loss: 0.005769672803580761\n",
      "epoch:  728, loss: 0.005767631344497204\n",
      "epoch:  729, loss: 0.005765494890511036\n",
      "epoch:  730, loss: 0.005763376131653786\n",
      "epoch:  731, loss: 0.005761275067925453\n",
      "epoch:  732, loss: 0.005759185645729303\n",
      "epoch:  733, loss: 0.005756130907684565\n",
      "epoch:  734, loss: 0.005749962758272886\n",
      "epoch:  735, loss: 0.005744619760662317\n",
      "epoch:  736, loss: 0.00574003579095006\n",
      "epoch:  737, loss: 0.005736189428716898\n",
      "epoch:  738, loss: 0.005732755642384291\n",
      "epoch:  739, loss: 0.005729776807129383\n",
      "epoch:  740, loss: 0.00572701683267951\n",
      "epoch:  741, loss: 0.005724428221583366\n",
      "epoch:  742, loss: 0.005721957888454199\n",
      "epoch:  743, loss: 0.0057195876725018024\n",
      "epoch:  744, loss: 0.005717303603887558\n",
      "epoch:  745, loss: 0.0057150376960635185\n",
      "epoch:  746, loss: 0.00571284769102931\n",
      "epoch:  747, loss: 0.005710659548640251\n",
      "epoch:  748, loss: 0.005708538927137852\n",
      "epoch:  749, loss: 0.00570640666410327\n",
      "epoch:  750, loss: 0.00570414774119854\n",
      "epoch:  751, loss: 0.005701988469809294\n",
      "epoch:  752, loss: 0.005699821747839451\n",
      "epoch:  753, loss: 0.00569771695882082\n",
      "epoch:  754, loss: 0.005695618223398924\n",
      "epoch:  755, loss: 0.005693584214895964\n",
      "epoch:  756, loss: 0.005691505502909422\n",
      "epoch:  757, loss: 0.005689462646842003\n",
      "epoch:  758, loss: 0.005687396042048931\n",
      "epoch:  759, loss: 0.005685345735400915\n",
      "epoch:  760, loss: 0.005683287512511015\n",
      "epoch:  761, loss: 0.005681230220943689\n",
      "epoch:  762, loss: 0.005679172929376364\n",
      "epoch:  763, loss: 0.005677109584212303\n",
      "epoch:  764, loss: 0.005675069056451321\n",
      "epoch:  765, loss: 0.005673001985996962\n",
      "epoch:  766, loss: 0.005670969374477863\n",
      "epoch:  767, loss: 0.0056688906624913216\n",
      "epoch:  768, loss: 0.005666869692504406\n",
      "epoch:  769, loss: 0.005664787255227566\n",
      "epoch:  770, loss: 0.005662713199853897\n",
      "epoch:  771, loss: 0.00566064054146409\n",
      "epoch:  772, loss: 0.005658573471009731\n",
      "epoch:  773, loss: 0.005656754598021507\n",
      "epoch:  774, loss: 0.0056511336006224155\n",
      "epoch:  775, loss: 0.005646880250424147\n",
      "epoch:  776, loss: 0.005642426200211048\n",
      "epoch:  777, loss: 0.005638881120830774\n",
      "epoch:  778, loss: 0.005635198205709457\n",
      "epoch:  779, loss: 0.005632233805954456\n",
      "epoch:  780, loss: 0.005629436578601599\n",
      "epoch:  781, loss: 0.005626732017844915\n",
      "epoch:  782, loss: 0.005624198820441961\n",
      "epoch:  783, loss: 0.005621643271297216\n",
      "epoch:  784, loss: 0.0056191300973296165\n",
      "epoch:  785, loss: 0.0056167179718613625\n",
      "epoch:  786, loss: 0.005614397581666708\n",
      "epoch:  787, loss: 0.005612077657133341\n",
      "epoch:  788, loss: 0.0056098573841154575\n",
      "epoch:  789, loss: 0.005607516970485449\n",
      "epoch:  790, loss: 0.005605134181678295\n",
      "epoch:  791, loss: 0.005602895747870207\n",
      "epoch:  792, loss: 0.005600603297352791\n",
      "epoch:  793, loss: 0.00559842586517334\n",
      "epoch:  794, loss: 0.0055962312035262585\n",
      "epoch:  795, loss: 0.005594097543507814\n",
      "epoch:  796, loss: 0.005591937806457281\n",
      "epoch:  797, loss: 0.005589824169874191\n",
      "epoch:  798, loss: 0.0055877091363072395\n",
      "epoch:  799, loss: 0.005585628096014261\n",
      "epoch:  800, loss: 0.005583546124398708\n",
      "epoch:  801, loss: 0.005581449251621962\n",
      "epoch:  802, loss: 0.005579396616667509\n",
      "epoch:  803, loss: 0.005577294621616602\n",
      "epoch:  804, loss: 0.005575275979936123\n",
      "epoch:  805, loss: 0.0055731856264173985\n",
      "epoch:  806, loss: 0.005571180954575539\n",
      "epoch:  807, loss: 0.005569087341427803\n",
      "epoch:  808, loss: 0.005567124579101801\n",
      "epoch:  809, loss: 0.005565021652728319\n",
      "epoch:  810, loss: 0.0055630868300795555\n",
      "epoch:  811, loss: 0.005560970399528742\n",
      "epoch:  812, loss: 0.005559035111218691\n",
      "epoch:  813, loss: 0.005556911695748568\n",
      "epoch:  814, loss: 0.005555018782615662\n",
      "epoch:  815, loss: 0.005552884191274643\n",
      "epoch:  816, loss: 0.005550804082304239\n",
      "epoch:  817, loss: 0.005548693705350161\n",
      "epoch:  818, loss: 0.005546631757169962\n",
      "epoch:  819, loss: 0.005544543266296387\n",
      "epoch:  820, loss: 0.005542488303035498\n",
      "epoch:  821, loss: 0.005540420301258564\n",
      "epoch:  822, loss: 0.005538382567465305\n",
      "epoch:  823, loss: 0.005536329932510853\n",
      "epoch:  824, loss: 0.005534301977604628\n",
      "epoch:  825, loss: 0.0055322605185210705\n",
      "epoch:  826, loss: 0.005530240014195442\n",
      "epoch:  827, loss: 0.005528207868337631\n",
      "epoch:  828, loss: 0.0055261957459151745\n",
      "epoch:  829, loss: 0.005524171516299248\n",
      "epoch:  830, loss: 0.005522163584828377\n",
      "epoch:  831, loss: 0.005520145408809185\n",
      "epoch:  832, loss: 0.005518140736967325\n",
      "epoch:  833, loss: 0.00551612488925457\n",
      "epoch:  834, loss: 0.005514123011380434\n",
      "epoch:  835, loss: 0.0055121141485869884\n",
      "epoch:  836, loss: 0.005510115064680576\n",
      "epoch:  837, loss: 0.0055081103928387165\n",
      "epoch:  838, loss: 0.005506116896867752\n",
      "epoch:  839, loss: 0.0055041187442839146\n",
      "epoch:  840, loss: 0.00550212524831295\n",
      "epoch:  841, loss: 0.005500129424035549\n",
      "epoch:  842, loss: 0.00549813499674201\n",
      "epoch:  843, loss: 0.00549614243209362\n",
      "epoch:  844, loss: 0.005494152195751667\n",
      "epoch:  845, loss: 0.0054921554401516914\n",
      "epoch:  846, loss: 0.005490161012858152\n",
      "epoch:  847, loss: 0.005488170776516199\n",
      "epoch:  848, loss: 0.005486177280545235\n",
      "epoch:  849, loss: 0.005484192166477442\n",
      "epoch:  850, loss: 0.005482507403939962\n",
      "epoch:  851, loss: 0.00547592481598258\n",
      "epoch:  852, loss: 0.0054730321280658245\n",
      "epoch:  853, loss: 0.00546747213229537\n",
      "epoch:  854, loss: 0.005463812965899706\n",
      "epoch:  855, loss: 0.0054595135152339935\n",
      "epoch:  856, loss: 0.005455687642097473\n",
      "epoch:  857, loss: 0.005452211480587721\n",
      "epoch:  858, loss: 0.005449633114039898\n",
      "epoch:  859, loss: 0.0054467227309942245\n",
      "epoch:  860, loss: 0.005444093141704798\n",
      "epoch:  861, loss: 0.005441686138510704\n",
      "epoch:  862, loss: 0.005438986234366894\n",
      "epoch:  863, loss: 0.0054366327822208405\n",
      "epoch:  864, loss: 0.0054342919029295444\n",
      "epoch:  865, loss: 0.005431962665170431\n",
      "epoch:  866, loss: 0.005429642274975777\n",
      "epoch:  867, loss: 0.005427327938377857\n",
      "epoch:  868, loss: 0.0054250238463282585\n",
      "epoch:  869, loss: 0.005422722082585096\n",
      "epoch:  870, loss: 0.005420421250164509\n",
      "epoch:  871, loss: 0.005418136715888977\n",
      "epoch:  872, loss: 0.005415882915258408\n",
      "epoch:  873, loss: 0.005413708742707968\n",
      "epoch:  874, loss: 0.005411531776189804\n",
      "epoch:  875, loss: 0.005409304052591324\n",
      "epoch:  876, loss: 0.005407084245234728\n",
      "epoch:  877, loss: 0.005404866766184568\n",
      "epoch:  878, loss: 0.005402656737715006\n",
      "epoch:  879, loss: 0.00540044903755188\n",
      "epoch:  880, loss: 0.005398247390985489\n",
      "epoch:  881, loss: 0.00539604714140296\n",
      "epoch:  882, loss: 0.005393847357481718\n",
      "epoch:  883, loss: 0.005391648970544338\n",
      "epoch:  884, loss: 0.005389453377574682\n",
      "epoch:  885, loss: 0.005387261975556612\n",
      "epoch:  886, loss: 0.0053850989788770676\n",
      "epoch:  887, loss: 0.005382993724197149\n",
      "epoch:  888, loss: 0.005380893126130104\n",
      "epoch:  889, loss: 0.005378776229918003\n",
      "epoch:  890, loss: 0.005376726854592562\n",
      "epoch:  891, loss: 0.005374605767428875\n",
      "epoch:  892, loss: 0.005372488405555487\n",
      "epoch:  893, loss: 0.005370378494262695\n",
      "epoch:  894, loss: 0.0053682695142924786\n",
      "epoch:  895, loss: 0.005366172641515732\n",
      "epoch:  896, loss: 0.0053640687838196754\n",
      "epoch:  897, loss: 0.005361978895962238\n",
      "epoch:  898, loss: 0.005359881091862917\n",
      "epoch:  899, loss: 0.005357795394957066\n",
      "epoch:  900, loss: 0.00535570876672864\n",
      "epoch:  901, loss: 0.005353628192096949\n",
      "epoch:  902, loss: 0.005351546220481396\n",
      "epoch:  903, loss: 0.005349470768123865\n",
      "epoch:  904, loss: 0.005347392056137323\n",
      "epoch:  905, loss: 0.005345314275473356\n",
      "epoch:  906, loss: 0.005343246273696423\n",
      "epoch:  907, loss: 0.0053411684930324554\n",
      "epoch:  908, loss: 0.005339094437658787\n",
      "epoch:  909, loss: 0.0053370194509625435\n",
      "epoch:  910, loss: 0.00533495144918561\n",
      "epoch:  911, loss: 0.005332896485924721\n",
      "epoch:  912, loss: 0.005330838728696108\n",
      "epoch:  913, loss: 0.005328782368451357\n",
      "epoch:  914, loss: 0.005326747428625822\n",
      "epoch:  915, loss: 0.005324721336364746\n",
      "epoch:  916, loss: 0.005322709679603577\n",
      "epoch:  917, loss: 0.005320687312632799\n",
      "epoch:  918, loss: 0.005318686831742525\n",
      "epoch:  919, loss: 0.005316663533449173\n",
      "epoch:  920, loss: 0.005314669571816921\n",
      "epoch:  921, loss: 0.005312644876539707\n",
      "epoch:  922, loss: 0.0053106327541172504\n",
      "epoch:  923, loss: 0.005308619700372219\n",
      "epoch:  924, loss: 0.005306604318320751\n",
      "epoch:  925, loss: 0.005304588470607996\n",
      "epoch:  926, loss: 0.005302612204104662\n",
      "epoch:  927, loss: 0.005300834309309721\n",
      "epoch:  928, loss: 0.005298924632370472\n",
      "epoch:  929, loss: 0.005297166295349598\n",
      "epoch:  930, loss: 0.005295339040458202\n",
      "epoch:  931, loss: 0.005293823312968016\n",
      "epoch:  932, loss: 0.005292179994285107\n",
      "epoch:  933, loss: 0.0052907695062458515\n",
      "epoch:  934, loss: 0.0052893017418682575\n",
      "epoch:  935, loss: 0.005287992302328348\n",
      "epoch:  936, loss: 0.005286732688546181\n",
      "epoch:  937, loss: 0.005285580176860094\n",
      "epoch:  938, loss: 0.005284944083541632\n",
      "epoch:  939, loss: 0.005284028593450785\n",
      "epoch:  940, loss: 0.005283725913614035\n",
      "epoch:  941, loss: 0.005283098202198744\n",
      "epoch:  942, loss: 0.0052785081788897514\n",
      "epoch:  943, loss: 0.0052781174890697\n",
      "epoch:  944, loss: 0.00527733750641346\n",
      "epoch:  945, loss: 0.005273307673633099\n",
      "epoch:  946, loss: 0.005272874608635902\n",
      "epoch:  947, loss: 0.00527195492759347\n",
      "epoch:  948, loss: 0.005271931178867817\n",
      "epoch:  949, loss: 0.005271352361887693\n",
      "epoch:  950, loss: 0.005266502965241671\n",
      "epoch:  951, loss: 0.00526638375595212\n",
      "epoch:  952, loss: 0.005265637766569853\n",
      "epoch:  953, loss: 0.005261341109871864\n",
      "epoch:  954, loss: 0.0052611734718084335\n",
      "epoch:  955, loss: 0.005260299891233444\n",
      "epoch:  956, loss: 0.005256418604403734\n",
      "epoch:  957, loss: 0.005256228614598513\n",
      "epoch:  958, loss: 0.005255173891782761\n",
      "epoch:  959, loss: 0.005251641850918531\n",
      "epoch:  960, loss: 0.005250868387520313\n",
      "epoch:  961, loss: 0.00524964788928628\n",
      "epoch:  962, loss: 0.005246670916676521\n",
      "epoch:  963, loss: 0.005245164502412081\n",
      "epoch:  964, loss: 0.005243743769824505\n",
      "epoch:  965, loss: 0.005242733750492334\n",
      "epoch:  966, loss: 0.00524152209982276\n",
      "epoch:  967, loss: 0.0052410936914384365\n",
      "epoch:  968, loss: 0.005240204744040966\n",
      "epoch:  969, loss: 0.00523628294467926\n",
      "epoch:  970, loss: 0.005235376767814159\n",
      "epoch:  971, loss: 0.005234262440353632\n",
      "epoch:  972, loss: 0.005233640782535076\n",
      "epoch:  973, loss: 0.005232838448137045\n",
      "epoch:  974, loss: 0.0052286917343735695\n",
      "epoch:  975, loss: 0.005227862391620874\n",
      "epoch:  976, loss: 0.005226870998740196\n",
      "epoch:  977, loss: 0.005226395558565855\n",
      "epoch:  978, loss: 0.00522575480863452\n",
      "epoch:  979, loss: 0.005225730128586292\n",
      "epoch:  980, loss: 0.0052255745977163315\n",
      "epoch:  981, loss: 0.005219693761318922\n",
      "epoch:  982, loss: 0.005219435319304466\n",
      "epoch:  983, loss: 0.005218973383307457\n",
      "epoch:  984, loss: 0.005214160308241844\n",
      "epoch:  985, loss: 0.005213696975260973\n",
      "epoch:  986, loss: 0.00521305063739419\n",
      "epoch:  987, loss: 0.00521298311650753\n",
      "epoch:  988, loss: 0.005208215210586786\n",
      "epoch:  989, loss: 0.005207370035350323\n",
      "epoch:  990, loss: 0.005207121837884188\n",
      "epoch:  991, loss: 0.005206747446209192\n",
      "epoch:  992, loss: 0.005201998166739941\n",
      "epoch:  993, loss: 0.005201147869229317\n",
      "epoch:  994, loss: 0.005200536921620369\n",
      "epoch:  995, loss: 0.005196585785597563\n",
      "epoch:  996, loss: 0.005195604171603918\n",
      "epoch:  997, loss: 0.005194802302867174\n",
      "epoch:  998, loss: 0.005194621626287699\n",
      "epoch:  999, loss: 0.00519063463434577\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size=X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "# opt = torch_numopt.ConjugateGradient(model.parameters(), lr=1e-4, model=model, c1=1e-4, tau=0.1, line_search_method=\"const\", cg_method=\"PR\")\n",
    "opt = torch_numopt.ConjugateGradient(model.parameters(), lr=10, model=model, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"armijo\", cg_method=\"PR\")\n",
    "# opt = torch_numopt.ConjugateGradient(model.parameters(), lr=10, model=model, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"wolfe\", cg_method=\"PR\")\n",
    "# opt = torch_numopt.ConjugateGradient(model.parameters(), lr=10, model=model, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"strong-wolfe\", cg_method=\"PR\")\n",
    "# opt = torch_numopt.ConjugateGradient(model.parameters(), lr=10, model=model, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"goldstein\", cg_method=\"PR\")\n",
    "\n",
    "\n",
    "all_loss = {}\n",
    "for epoch in range(1000):\n",
    "    print(\"epoch: \", epoch, end=\"\")\n",
    "    all_loss[epoch + 1] = 0\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y, loss_fn)\n",
    "\n",
    "        all_loss[epoch + 1] += loss\n",
    "    all_loss[epoch + 1] /= len(data_loader)\n",
    "    print(\", loss: {}\".format(all_loss[epoch + 1].detach().numpy().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics: R2 = 0.8317877499343602\n",
      "Test metrics:  R2 = 0.8622204538237305\n"
     ]
    }
   ],
   "source": [
    "pred_train = model.forward(X_train).detach()\n",
    "pred_test = model.forward(X_test).detach()\n",
    "print(f\"Train metrics: R2 = {r2_score(pred_train, y_train)}\")\n",
    "print(f\"Test metrics:  R2 = {r2_score(pred_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.3300580680370331\n",
      "epoch:  1, loss: 0.1837853640317917\n",
      "epoch:  2, loss: 0.11212076246738434\n",
      "epoch:  3, loss: 0.07540986686944962\n",
      "epoch:  4, loss: 0.05648688226938248\n",
      "epoch:  5, loss: 0.046762801706790924\n",
      "epoch:  6, loss: 0.041798390448093414\n",
      "epoch:  7, loss: 0.039280347526073456\n",
      "epoch:  8, loss: 0.03800981491804123\n",
      "epoch:  9, loss: 0.03737065941095352\n",
      "epoch:  10, loss: 0.03704943507909775\n",
      "epoch:  11, loss: 0.036887358874082565\n",
      "epoch:  12, loss: 0.03680473193526268\n",
      "epoch:  13, loss: 0.03676164522767067\n",
      "epoch:  14, loss: 0.03673812374472618\n",
      "epoch:  15, loss: 0.0367242731153965\n",
      "epoch:  16, loss: 0.03670588880777359\n",
      "epoch:  17, loss: 0.03668253496289253\n",
      "epoch:  18, loss: 0.03666846826672554\n",
      "epoch:  19, loss: 0.03664163500070572\n",
      "epoch:  20, loss: 0.03661837428808212\n",
      "epoch:  21, loss: 0.03660411387681961\n",
      "epoch:  22, loss: 0.036563947796821594\n",
      "epoch:  23, loss: 0.03654107451438904\n",
      "epoch:  24, loss: 0.03652636706829071\n",
      "epoch:  25, loss: 0.03648769482970238\n",
      "epoch:  26, loss: 0.036465324461460114\n",
      "epoch:  27, loss: 0.0364498607814312\n",
      "epoch:  28, loss: 0.03641270101070404\n",
      "epoch:  29, loss: 0.03639146313071251\n",
      "epoch:  30, loss: 0.03637755662202835\n",
      "epoch:  31, loss: 0.03634151071310043\n",
      "epoch:  32, loss: 0.03632084280252457\n",
      "epoch:  33, loss: 0.036306098103523254\n",
      "epoch:  34, loss: 0.03627151623368263\n",
      "epoch:  35, loss: 0.036251794546842575\n",
      "epoch:  36, loss: 0.03624081611633301\n",
      "epoch:  37, loss: 0.036207374185323715\n",
      "epoch:  38, loss: 0.0361885204911232\n",
      "epoch:  39, loss: 0.03618081286549568\n",
      "epoch:  40, loss: 0.0361480638384819\n",
      "epoch:  41, loss: 0.03612974286079407\n",
      "epoch:  42, loss: 0.03612257540225983\n",
      "epoch:  43, loss: 0.03609035909175873\n",
      "epoch:  44, loss: 0.036072198301553726\n",
      "epoch:  45, loss: 0.03606260567903519\n",
      "epoch:  46, loss: 0.03603044152259827\n",
      "epoch:  47, loss: 0.03601229563355446\n",
      "epoch:  48, loss: 0.036001335829496384\n",
      "epoch:  49, loss: 0.03596922755241394\n",
      "epoch:  50, loss: 0.03595104068517685\n",
      "epoch:  51, loss: 0.035938091576099396\n",
      "epoch:  52, loss: 0.03590559959411621\n",
      "epoch:  53, loss: 0.03588718920946121\n",
      "epoch:  54, loss: 0.03587130457162857\n",
      "epoch:  55, loss: 0.035838570445775986\n",
      "epoch:  56, loss: 0.035819876939058304\n",
      "epoch:  57, loss: 0.03580150753259659\n",
      "epoch:  58, loss: 0.035767920315265656\n",
      "epoch:  59, loss: 0.03574875742197037\n",
      "epoch:  60, loss: 0.035727035254240036\n",
      "epoch:  61, loss: 0.03569291532039642\n",
      "epoch:  62, loss: 0.03567326441407204\n",
      "epoch:  63, loss: 0.03564773499965668\n",
      "epoch:  64, loss: 0.0356125608086586\n",
      "epoch:  65, loss: 0.03559230640530586\n",
      "epoch:  66, loss: 0.03556327894330025\n",
      "epoch:  67, loss: 0.03552698716521263\n",
      "epoch:  68, loss: 0.03550590202212334\n",
      "epoch:  69, loss: 0.03547303378582001\n",
      "epoch:  70, loss: 0.03543509915471077\n",
      "epoch:  71, loss: 0.03541312366724014\n",
      "epoch:  72, loss: 0.03537651151418686\n",
      "epoch:  73, loss: 0.03533710166811943\n",
      "epoch:  74, loss: 0.035314109176397324\n",
      "epoch:  75, loss: 0.03527355566620827\n",
      "epoch:  76, loss: 0.03523169457912445\n",
      "epoch:  77, loss: 0.0352073535323143\n",
      "epoch:  78, loss: 0.03516129404306412\n",
      "epoch:  79, loss: 0.03511665761470795\n",
      "epoch:  80, loss: 0.03509046882390976\n",
      "epoch:  81, loss: 0.03503741696476936\n",
      "epoch:  82, loss: 0.034989386796951294\n",
      "epoch:  83, loss: 0.03496134653687477\n",
      "epoch:  84, loss: 0.0349021852016449\n",
      "epoch:  85, loss: 0.034850820899009705\n",
      "epoch:  86, loss: 0.03482077270746231\n",
      "epoch:  87, loss: 0.034757327288389206\n",
      "epoch:  88, loss: 0.03470130264759064\n",
      "epoch:  89, loss: 0.03466886654496193\n",
      "epoch:  90, loss: 0.03459775075316429\n",
      "epoch:  91, loss: 0.0345371849834919\n",
      "epoch:  92, loss: 0.034501850605010986\n",
      "epoch:  93, loss: 0.034424591809511185\n",
      "epoch:  94, loss: 0.03435690328478813\n",
      "epoch:  95, loss: 0.0343179926276207\n",
      "epoch:  96, loss: 0.03423255309462547\n",
      "epoch:  97, loss: 0.03415805473923683\n",
      "epoch:  98, loss: 0.03411542996764183\n",
      "epoch:  99, loss: 0.03402573987841606\n",
      "epoch:  100, loss: 0.03394182771444321\n",
      "epoch:  101, loss: 0.03389463946223259\n",
      "epoch:  102, loss: 0.03379660099744797\n",
      "epoch:  103, loss: 0.03370235487818718\n",
      "epoch:  104, loss: 0.033649690449237823\n",
      "epoch:  105, loss: 0.03354550898075104\n",
      "epoch:  106, loss: 0.03343835473060608\n",
      "epoch:  107, loss: 0.03337951749563217\n",
      "epoch:  108, loss: 0.033269308507442474\n",
      "epoch:  109, loss: 0.03314772620797157\n",
      "epoch:  110, loss: 0.033081457018852234\n",
      "epoch:  111, loss: 0.03296900913119316\n",
      "epoch:  112, loss: 0.03282659500837326\n",
      "epoch:  113, loss: 0.032751455903053284\n",
      "epoch:  114, loss: 0.03263584151864052\n",
      "epoch:  115, loss: 0.032471492886543274\n",
      "epoch:  116, loss: 0.03238604590296745\n",
      "epoch:  117, loss: 0.03228228539228439\n",
      "epoch:  118, loss: 0.032084494829177856\n",
      "epoch:  119, loss: 0.03198639675974846\n",
      "epoch:  120, loss: 0.031891997903585434\n",
      "epoch:  121, loss: 0.03165833279490471\n",
      "epoch:  122, loss: 0.031544577330350876\n",
      "epoch:  123, loss: 0.03146975859999657\n",
      "epoch:  124, loss: 0.03118443675339222\n",
      "epoch:  125, loss: 0.03105260618031025\n",
      "epoch:  126, loss: 0.03099565953016281\n",
      "epoch:  127, loss: 0.030657343566417694\n",
      "epoch:  128, loss: 0.0305034089833498\n",
      "epoch:  129, loss: 0.030496113002300262\n",
      "epoch:  130, loss: 0.030070381239056587\n",
      "epoch:  131, loss: 0.02988969348371029\n",
      "epoch:  132, loss: 0.029792089015245438\n",
      "epoch:  133, loss: 0.02942432463169098\n",
      "epoch:  134, loss: 0.029211778193712234\n",
      "epoch:  135, loss: 0.0291009359061718\n",
      "epoch:  136, loss: 0.02872142754495144\n",
      "epoch:  137, loss: 0.02846786566078663\n",
      "epoch:  138, loss: 0.02834365889430046\n",
      "epoch:  139, loss: 0.027949834242463112\n",
      "epoch:  140, loss: 0.02765394188463688\n",
      "epoch:  141, loss: 0.027516178786754608\n",
      "epoch:  142, loss: 0.02711641974747181\n",
      "epoch:  143, loss: 0.026765968650579453\n",
      "epoch:  144, loss: 0.026614265516400337\n",
      "epoch:  145, loss: 0.026184575632214546\n",
      "epoch:  146, loss: 0.025799719616770744\n",
      "epoch:  147, loss: 0.025639161467552185\n",
      "epoch:  148, loss: 0.025180160999298096\n",
      "epoch:  149, loss: 0.02475730888545513\n",
      "epoch:  150, loss: 0.024592271074652672\n",
      "epoch:  151, loss: 0.02404595911502838\n",
      "epoch:  152, loss: 0.023632926866412163\n",
      "epoch:  153, loss: 0.02347256988286972\n",
      "epoch:  154, loss: 0.02285139635205269\n",
      "epoch:  155, loss: 0.022448768839240074\n",
      "epoch:  156, loss: 0.022295916453003883\n",
      "epoch:  157, loss: 0.021548056975007057\n",
      "epoch:  158, loss: 0.02121010236442089\n",
      "epoch:  159, loss: 0.021072160452604294\n",
      "epoch:  160, loss: 0.020223967730998993\n",
      "epoch:  161, loss: 0.019957294687628746\n",
      "epoch:  162, loss: 0.019637668505311012\n",
      "epoch:  163, loss: 0.018880570307374\n",
      "epoch:  164, loss: 0.018718333914875984\n",
      "epoch:  165, loss: 0.01795218139886856\n",
      "epoch:  166, loss: 0.017630698159337044\n",
      "epoch:  167, loss: 0.017427299171686172\n",
      "epoch:  168, loss: 0.01660364679992199\n",
      "epoch:  169, loss: 0.016475355252623558\n",
      "epoch:  170, loss: 0.015666747465729713\n",
      "epoch:  171, loss: 0.015505943447351456\n",
      "epoch:  172, loss: 0.014851449988782406\n",
      "epoch:  173, loss: 0.014621678739786148\n",
      "epoch:  174, loss: 0.014075910672545433\n",
      "epoch:  175, loss: 0.013827402144670486\n",
      "epoch:  176, loss: 0.01335968729108572\n",
      "epoch:  177, loss: 0.013129320926964283\n",
      "epoch:  178, loss: 0.012645321898162365\n",
      "epoch:  179, loss: 0.012525610625743866\n",
      "epoch:  180, loss: 0.012068413197994232\n",
      "epoch:  181, loss: 0.012008927762508392\n",
      "epoch:  182, loss: 0.009849981404840946\n",
      "epoch:  183, loss: 0.00975431315600872\n",
      "epoch:  184, loss: 0.009719775058329105\n",
      "epoch:  185, loss: 0.00946963019669056\n",
      "epoch:  186, loss: 0.009440742433071136\n",
      "epoch:  187, loss: 0.009353642351925373\n",
      "epoch:  188, loss: 0.009289640001952648\n",
      "epoch:  189, loss: 0.009278669022023678\n",
      "epoch:  190, loss: 0.009194393642246723\n",
      "epoch:  191, loss: 0.009183394722640514\n",
      "epoch:  192, loss: 0.009122513234615326\n",
      "epoch:  193, loss: 0.009110575541853905\n",
      "epoch:  194, loss: 0.009065763093531132\n",
      "epoch:  195, loss: 0.009050661697983742\n",
      "epoch:  196, loss: 0.009020416997373104\n",
      "epoch:  197, loss: 0.008999574929475784\n",
      "epoch:  198, loss: 0.00897863507270813\n",
      "epoch:  199, loss: 0.008954410441219807\n",
      "epoch:  200, loss: 0.008950266055762768\n",
      "epoch:  201, loss: 0.008789989165961742\n",
      "epoch:  202, loss: 0.008680232800543308\n",
      "epoch:  203, loss: 0.008677205070853233\n",
      "epoch:  204, loss: 0.008666201494634151\n",
      "epoch:  205, loss: 0.00866233091801405\n",
      "epoch:  206, loss: 0.008656918071210384\n",
      "epoch:  207, loss: 0.008649805560708046\n",
      "epoch:  208, loss: 0.008648641407489777\n",
      "epoch:  209, loss: 0.008593796752393246\n",
      "epoch:  210, loss: 0.008569804951548576\n",
      "epoch:  211, loss: 0.008568519726395607\n",
      "epoch:  212, loss: 0.008563762530684471\n",
      "epoch:  213, loss: 0.008561617694795132\n",
      "epoch:  214, loss: 0.008560349233448505\n",
      "epoch:  215, loss: 0.008556040935218334\n",
      "epoch:  216, loss: 0.008555462583899498\n",
      "epoch:  217, loss: 0.008551482111215591\n",
      "epoch:  218, loss: 0.008550482802093029\n",
      "epoch:  219, loss: 0.008547686971724033\n",
      "epoch:  220, loss: 0.008545818738639355\n",
      "epoch:  221, loss: 0.008545382879674435\n",
      "epoch:  222, loss: 0.008524622768163681\n",
      "epoch:  223, loss: 0.008510651998221874\n",
      "epoch:  224, loss: 0.008510046638548374\n",
      "epoch:  225, loss: 0.008508875034749508\n",
      "epoch:  226, loss: 0.008507190272212029\n",
      "epoch:  227, loss: 0.00850690994411707\n",
      "epoch:  228, loss: 0.008490231819450855\n",
      "epoch:  229, loss: 0.008484004996716976\n",
      "epoch:  230, loss: 0.008483617566525936\n",
      "epoch:  231, loss: 0.008481880649924278\n",
      "epoch:  232, loss: 0.008481200784444809\n",
      "epoch:  233, loss: 0.008480636402964592\n",
      "epoch:  234, loss: 0.008478948846459389\n",
      "epoch:  235, loss: 0.008478696458041668\n",
      "epoch:  236, loss: 0.00846782699227333\n",
      "epoch:  237, loss: 0.008458191528916359\n",
      "epoch:  238, loss: 0.00845771748572588\n",
      "epoch:  239, loss: 0.008456231094896793\n",
      "epoch:  240, loss: 0.008455370552837849\n",
      "epoch:  241, loss: 0.00845516100525856\n",
      "epoch:  242, loss: 0.008451745845377445\n",
      "epoch:  243, loss: 0.008438430726528168\n",
      "epoch:  244, loss: 0.008437808603048325\n",
      "epoch:  245, loss: 0.008436784148216248\n",
      "epoch:  246, loss: 0.00843545887619257\n",
      "epoch:  247, loss: 0.008435222320258617\n",
      "epoch:  248, loss: 0.008434396237134933\n",
      "epoch:  249, loss: 0.008420451544225216\n",
      "epoch:  250, loss: 0.008419797755777836\n",
      "epoch:  251, loss: 0.008418373763561249\n",
      "epoch:  252, loss: 0.008417041040956974\n",
      "epoch:  253, loss: 0.008416797034442425\n",
      "epoch:  254, loss: 0.008416006341576576\n",
      "epoch:  255, loss: 0.008403964340686798\n",
      "epoch:  256, loss: 0.008403091691434383\n",
      "epoch:  257, loss: 0.008401695638895035\n",
      "epoch:  258, loss: 0.008399575017392635\n",
      "epoch:  259, loss: 0.00839927140623331\n",
      "epoch:  260, loss: 0.008397403173148632\n",
      "epoch:  261, loss: 0.008396964520215988\n",
      "epoch:  262, loss: 0.008396070450544357\n",
      "epoch:  263, loss: 0.008395032025873661\n",
      "epoch:  264, loss: 0.0083948178216815\n",
      "epoch:  265, loss: 0.008388765156269073\n",
      "epoch:  266, loss: 0.008379066362977028\n",
      "epoch:  267, loss: 0.008378570899367332\n",
      "epoch:  268, loss: 0.008377023041248322\n",
      "epoch:  269, loss: 0.008376121520996094\n",
      "epoch:  270, loss: 0.00837590079754591\n",
      "epoch:  271, loss: 0.008367969654500484\n",
      "epoch:  272, loss: 0.008359638974070549\n",
      "epoch:  273, loss: 0.00835895910859108\n",
      "epoch:  274, loss: 0.008358752354979515\n",
      "epoch:  275, loss: 0.008356078527867794\n",
      "epoch:  276, loss: 0.008355800062417984\n",
      "epoch:  277, loss: 0.008354013785719872\n",
      "epoch:  278, loss: 0.008353553712368011\n",
      "epoch:  279, loss: 0.008353201672434807\n",
      "epoch:  280, loss: 0.008351542986929417\n",
      "epoch:  281, loss: 0.008351271040737629\n",
      "epoch:  282, loss: 0.008349686861038208\n",
      "epoch:  283, loss: 0.008349254727363586\n",
      "epoch:  284, loss: 0.008348813280463219\n",
      "epoch:  285, loss: 0.008347301743924618\n",
      "epoch:  286, loss: 0.008347074501216412\n",
      "epoch:  287, loss: 0.008346112444996834\n",
      "epoch:  288, loss: 0.008345111273229122\n",
      "epoch:  289, loss: 0.008344884030520916\n",
      "epoch:  290, loss: 0.00834324024617672\n",
      "epoch:  291, loss: 0.00834287516772747\n",
      "epoch:  292, loss: 0.008342329412698746\n",
      "epoch:  293, loss: 0.008340892381966114\n",
      "epoch:  294, loss: 0.008340653963387012\n",
      "epoch:  295, loss: 0.008339010179042816\n",
      "epoch:  296, loss: 0.00833863765001297\n",
      "epoch:  297, loss: 0.008338443003594875\n",
      "epoch:  298, loss: 0.008337213657796383\n",
      "epoch:  299, loss: 0.008336436003446579\n",
      "epoch:  300, loss: 0.008336224593222141\n",
      "epoch:  301, loss: 0.008334881626069546\n",
      "epoch:  302, loss: 0.008334193378686905\n",
      "epoch:  303, loss: 0.008333966135978699\n",
      "epoch:  304, loss: 0.008331689052283764\n",
      "epoch:  305, loss: 0.008316338993608952\n",
      "epoch:  306, loss: 0.008315562270581722\n",
      "epoch:  307, loss: 0.008314825594425201\n",
      "epoch:  308, loss: 0.008313460275530815\n",
      "epoch:  309, loss: 0.008313234895467758\n",
      "epoch:  310, loss: 0.008310262113809586\n",
      "epoch:  311, loss: 0.008295662701129913\n",
      "epoch:  312, loss: 0.008294904604554176\n",
      "epoch:  313, loss: 0.008294662460684776\n",
      "epoch:  314, loss: 0.00828858744353056\n",
      "epoch:  315, loss: 0.008277524262666702\n",
      "epoch:  316, loss: 0.008276774547994137\n",
      "epoch:  317, loss: 0.008276707492768764\n",
      "epoch:  318, loss: 0.008274072781205177\n",
      "epoch:  319, loss: 0.00827374029904604\n",
      "epoch:  320, loss: 0.00827205739915371\n",
      "epoch:  321, loss: 0.00827106460928917\n",
      "epoch:  322, loss: 0.00827077403664589\n",
      "epoch:  323, loss: 0.008270008489489555\n",
      "epoch:  324, loss: 0.008253605104982853\n",
      "epoch:  325, loss: 0.008252589963376522\n",
      "epoch:  326, loss: 0.008252236060798168\n",
      "epoch:  327, loss: 0.00824959110468626\n",
      "epoch:  328, loss: 0.008249145932495594\n",
      "epoch:  329, loss: 0.008247449062764645\n",
      "epoch:  330, loss: 0.008246350102126598\n",
      "epoch:  331, loss: 0.008246061392128468\n",
      "epoch:  332, loss: 0.008242535404860973\n",
      "epoch:  333, loss: 0.008224508725106716\n",
      "epoch:  334, loss: 0.008223594166338444\n",
      "epoch:  335, loss: 0.008222854696214199\n",
      "epoch:  336, loss: 0.008220561780035496\n",
      "epoch:  337, loss: 0.008220209740102291\n",
      "epoch:  338, loss: 0.008218123577535152\n",
      "epoch:  339, loss: 0.008217406459152699\n",
      "epoch:  340, loss: 0.00821687187999487\n",
      "epoch:  341, loss: 0.008214708417654037\n",
      "epoch:  342, loss: 0.008214381523430347\n",
      "epoch:  343, loss: 0.008212235756218433\n",
      "epoch:  344, loss: 0.008211600594222546\n",
      "epoch:  345, loss: 0.008211595006287098\n",
      "epoch:  346, loss: 0.008208990097045898\n",
      "epoch:  347, loss: 0.00820862129330635\n",
      "epoch:  348, loss: 0.008206650614738464\n",
      "epoch:  349, loss: 0.008205944672226906\n",
      "epoch:  350, loss: 0.008205658756196499\n",
      "epoch:  351, loss: 0.008203664794564247\n",
      "epoch:  352, loss: 0.008203006349503994\n",
      "epoch:  353, loss: 0.008202729746699333\n",
      "epoch:  354, loss: 0.008188817650079727\n",
      "epoch:  355, loss: 0.008179232478141785\n",
      "epoch:  356, loss: 0.00817857775837183\n",
      "epoch:  357, loss: 0.008178220130503178\n",
      "epoch:  358, loss: 0.008175652474164963\n",
      "epoch:  359, loss: 0.008175297640264034\n",
      "epoch:  360, loss: 0.008173111826181412\n",
      "epoch:  361, loss: 0.0081723527982831\n",
      "epoch:  362, loss: 0.008171889930963516\n",
      "epoch:  363, loss: 0.008169475011527538\n",
      "epoch:  364, loss: 0.008169123902916908\n",
      "epoch:  365, loss: 0.00816686823964119\n",
      "epoch:  366, loss: 0.008166228421032429\n",
      "epoch:  367, loss: 0.00816582515835762\n",
      "epoch:  368, loss: 0.00816342607140541\n",
      "epoch:  369, loss: 0.008163068443536758\n",
      "epoch:  370, loss: 0.008161105215549469\n",
      "epoch:  371, loss: 0.008160226978361607\n",
      "epoch:  372, loss: 0.008159938268363476\n",
      "epoch:  373, loss: 0.008146768435835838\n",
      "epoch:  374, loss: 0.00813403818756342\n",
      "epoch:  375, loss: 0.008133231662213802\n",
      "epoch:  376, loss: 0.008132746443152428\n",
      "epoch:  377, loss: 0.008130055852234364\n",
      "epoch:  378, loss: 0.008129669353365898\n",
      "epoch:  379, loss: 0.008127447217702866\n",
      "epoch:  380, loss: 0.008126568980515003\n",
      "epoch:  381, loss: 0.00812626164406538\n",
      "epoch:  382, loss: 0.00811377726495266\n",
      "epoch:  383, loss: 0.00809802021831274\n",
      "epoch:  384, loss: 0.008096927776932716\n",
      "epoch:  385, loss: 0.008096553385257721\n",
      "epoch:  386, loss: 0.00809438619762659\n",
      "epoch:  387, loss: 0.008093202486634254\n",
      "epoch:  388, loss: 0.008092845790088177\n",
      "epoch:  389, loss: 0.008090060204267502\n",
      "epoch:  390, loss: 0.008089513517916203\n",
      "epoch:  391, loss: 0.008088958449661732\n",
      "epoch:  392, loss: 0.008086246438324451\n",
      "epoch:  393, loss: 0.008085825480520725\n",
      "epoch:  394, loss: 0.008083821274340153\n",
      "epoch:  395, loss: 0.008082594722509384\n",
      "epoch:  396, loss: 0.008082249201834202\n",
      "epoch:  397, loss: 0.008079594001173973\n",
      "epoch:  398, loss: 0.008078992366790771\n",
      "epoch:  399, loss: 0.008077699691057205\n",
      "epoch:  400, loss: 0.008075809106230736\n",
      "epoch:  401, loss: 0.008075413294136524\n",
      "epoch:  402, loss: 0.00807292852550745\n",
      "epoch:  403, loss: 0.008072168566286564\n",
      "epoch:  404, loss: 0.00807183887809515\n",
      "epoch:  405, loss: 0.00806479249149561\n",
      "epoch:  406, loss: 0.008042868226766586\n",
      "epoch:  407, loss: 0.008041606284677982\n",
      "epoch:  408, loss: 0.00804123654961586\n",
      "epoch:  409, loss: 0.008038436062633991\n",
      "epoch:  410, loss: 0.008037630468606949\n",
      "epoch:  411, loss: 0.008036606013774872\n",
      "epoch:  412, loss: 0.008034064434468746\n",
      "epoch:  413, loss: 0.008033650927245617\n",
      "epoch:  414, loss: 0.008030664175748825\n",
      "epoch:  415, loss: 0.008029990829527378\n",
      "epoch:  416, loss: 0.008029534481465816\n",
      "epoch:  417, loss: 0.008026449009776115\n",
      "epoch:  418, loss: 0.00802597589790821\n",
      "epoch:  419, loss: 0.008023448288440704\n",
      "epoch:  420, loss: 0.00802238192409277\n",
      "epoch:  421, loss: 0.008022019639611244\n",
      "epoch:  422, loss: 0.00800073891878128\n",
      "epoch:  423, loss: 0.007989349775016308\n",
      "epoch:  424, loss: 0.007988429628312588\n",
      "epoch:  425, loss: 0.007988138124346733\n",
      "epoch:  426, loss: 0.007984724827110767\n",
      "epoch:  427, loss: 0.007984230294823647\n",
      "epoch:  428, loss: 0.00798171479254961\n",
      "epoch:  429, loss: 0.007980511523783207\n",
      "epoch:  430, loss: 0.00798011478036642\n",
      "epoch:  431, loss: 0.00797840766608715\n",
      "epoch:  432, loss: 0.007945644669234753\n",
      "epoch:  433, loss: 0.007943598553538322\n",
      "epoch:  434, loss: 0.007943111471831799\n",
      "epoch:  435, loss: 0.007939818315207958\n",
      "epoch:  436, loss: 0.007938824594020844\n",
      "epoch:  437, loss: 0.007938764989376068\n",
      "epoch:  438, loss: 0.007934699766337872\n",
      "epoch:  439, loss: 0.007934121415019035\n",
      "epoch:  440, loss: 0.007932404987514019\n",
      "epoch:  441, loss: 0.007929898798465729\n",
      "epoch:  442, loss: 0.007929395884275436\n",
      "epoch:  443, loss: 0.007927128113806248\n",
      "epoch:  444, loss: 0.007925063371658325\n",
      "epoch:  445, loss: 0.007924589328467846\n",
      "epoch:  446, loss: 0.007921451702713966\n",
      "epoch:  447, loss: 0.007920295931398869\n",
      "epoch:  448, loss: 0.0079198544844985\n",
      "epoch:  449, loss: 0.00791963841766119\n",
      "epoch:  450, loss: 0.007882021367549896\n",
      "epoch:  451, loss: 0.007879388518631458\n",
      "epoch:  452, loss: 0.007878744974732399\n",
      "epoch:  453, loss: 0.007874341681599617\n",
      "epoch:  454, loss: 0.007873347029089928\n",
      "epoch:  455, loss: 0.007872101850807667\n",
      "epoch:  456, loss: 0.007868357002735138\n",
      "epoch:  457, loss: 0.007867686450481415\n",
      "epoch:  458, loss: 0.007863443344831467\n",
      "epoch:  459, loss: 0.007862611673772335\n",
      "epoch:  460, loss: 0.007860632613301277\n",
      "epoch:  461, loss: 0.007857421413064003\n",
      "epoch:  462, loss: 0.007856732234358788\n",
      "epoch:  463, loss: 0.007853143848478794\n",
      "epoch:  464, loss: 0.007851348258554935\n",
      "epoch:  465, loss: 0.007850756868720055\n",
      "epoch:  466, loss: 0.007846544496715069\n",
      "epoch:  467, loss: 0.007845291867852211\n",
      "epoch:  468, loss: 0.007844752632081509\n",
      "epoch:  469, loss: 0.00784226693212986\n",
      "epoch:  470, loss: 0.007796412333846092\n",
      "epoch:  471, loss: 0.007792985532432795\n",
      "epoch:  472, loss: 0.007792233023792505\n",
      "epoch:  473, loss: 0.007786871399730444\n",
      "epoch:  474, loss: 0.0077857063151896\n",
      "epoch:  475, loss: 0.007785224821418524\n",
      "epoch:  476, loss: 0.007780179847031832\n",
      "epoch:  477, loss: 0.00777937937527895\n",
      "epoch:  478, loss: 0.007777128368616104\n",
      "epoch:  479, loss: 0.0077737243846058846\n",
      "epoch:  480, loss: 0.0077730100601911545\n",
      "epoch:  481, loss: 0.007769071962684393\n",
      "epoch:  482, loss: 0.007767310831695795\n",
      "epoch:  483, loss: 0.007766666356474161\n",
      "epoch:  484, loss: 0.007763409987092018\n",
      "epoch:  485, loss: 0.007761216722428799\n",
      "epoch:  486, loss: 0.007760569918900728\n",
      "epoch:  487, loss: 0.0077574108727276325\n",
      "epoch:  488, loss: 0.007755115628242493\n",
      "epoch:  489, loss: 0.007754476275295019\n",
      "epoch:  490, loss: 0.007752206176519394\n",
      "epoch:  491, loss: 0.007749021518975496\n",
      "epoch:  492, loss: 0.007748359348624945\n",
      "epoch:  493, loss: 0.007745066657662392\n",
      "epoch:  494, loss: 0.007742669433355331\n",
      "epoch:  495, loss: 0.007741988170892\n",
      "epoch:  496, loss: 0.0077393557876348495\n",
      "epoch:  497, loss: 0.007736028637737036\n",
      "epoch:  498, loss: 0.007735243532806635\n",
      "epoch:  499, loss: 0.00767454132437706\n",
      "epoch:  500, loss: 0.007669075857847929\n",
      "epoch:  501, loss: 0.007668046746402979\n",
      "epoch:  502, loss: 0.007662834133952856\n",
      "epoch:  503, loss: 0.007660198956727982\n",
      "epoch:  504, loss: 0.007659416180104017\n",
      "epoch:  505, loss: 0.007653522305190563\n",
      "epoch:  506, loss: 0.00765208201482892\n",
      "epoch:  507, loss: 0.007651353254914284\n",
      "epoch:  508, loss: 0.007645376957952976\n",
      "epoch:  509, loss: 0.007644240744411945\n",
      "epoch:  510, loss: 0.007643518038094044\n",
      "epoch:  511, loss: 0.007637686561793089\n",
      "epoch:  512, loss: 0.007636652793735266\n",
      "epoch:  513, loss: 0.007634323090314865\n",
      "epoch:  514, loss: 0.007629984058439732\n",
      "epoch:  515, loss: 0.007629146799445152\n",
      "epoch:  516, loss: 0.0076253097504377365\n",
      "epoch:  517, loss: 0.007622365839779377\n",
      "epoch:  518, loss: 0.0076215374283492565\n",
      "epoch:  519, loss: 0.007616806775331497\n",
      "epoch:  520, loss: 0.00761455437168479\n",
      "epoch:  521, loss: 0.007613795343786478\n",
      "epoch:  522, loss: 0.007607904728502035\n",
      "epoch:  523, loss: 0.0076065994799137115\n",
      "epoch:  524, loss: 0.007605834398418665\n",
      "epoch:  525, loss: 0.007598735857754946\n",
      "epoch:  526, loss: 0.007546207867562771\n",
      "epoch:  527, loss: 0.00754209142178297\n",
      "epoch:  528, loss: 0.007540710736066103\n",
      "epoch:  529, loss: 0.007531620096415281\n",
      "epoch:  530, loss: 0.007529356516897678\n",
      "epoch:  531, loss: 0.007528405170887709\n",
      "epoch:  532, loss: 0.007506347261369228\n",
      "epoch:  533, loss: 0.007489397656172514\n",
      "epoch:  534, loss: 0.007485078647732735\n",
      "epoch:  535, loss: 0.007482786662876606\n",
      "epoch:  536, loss: 0.007459855172783136\n",
      "epoch:  537, loss: 0.0074563710950315\n",
      "epoch:  538, loss: 0.007454767823219299\n",
      "epoch:  539, loss: 0.007445407100021839\n",
      "epoch:  540, loss: 0.007441533729434013\n",
      "epoch:  541, loss: 0.007440326735377312\n",
      "epoch:  542, loss: 0.007434474769979715\n",
      "epoch:  543, loss: 0.007430071942508221\n",
      "epoch:  544, loss: 0.007428910117596388\n",
      "epoch:  545, loss: 0.0074223666451871395\n",
      "epoch:  546, loss: 0.0074187982827425\n",
      "epoch:  547, loss: 0.007417691871523857\n",
      "epoch:  548, loss: 0.007414376828819513\n",
      "epoch:  549, loss: 0.007408702280372381\n",
      "epoch:  550, loss: 0.007407463155686855\n",
      "epoch:  551, loss: 0.007404427044093609\n",
      "epoch:  552, loss: 0.007398978341370821\n",
      "epoch:  553, loss: 0.007397745735943317\n",
      "epoch:  554, loss: 0.007396882865577936\n",
      "epoch:  555, loss: 0.00739029236137867\n",
      "epoch:  556, loss: 0.007388063240796328\n",
      "epoch:  557, loss: 0.007387103978544474\n",
      "epoch:  558, loss: 0.007384942378848791\n",
      "epoch:  559, loss: 0.007378329057246447\n",
      "epoch:  560, loss: 0.007376945111900568\n",
      "epoch:  561, loss: 0.007375994697213173\n",
      "epoch:  562, loss: 0.007369293365627527\n",
      "epoch:  563, loss: 0.007366679143160582\n",
      "epoch:  564, loss: 0.007365576457232237\n",
      "epoch:  565, loss: 0.007364119403064251\n",
      "epoch:  566, loss: 0.007356597576290369\n",
      "epoch:  567, loss: 0.007355129811912775\n",
      "epoch:  568, loss: 0.007354161702096462\n",
      "epoch:  569, loss: 0.00734975328668952\n",
      "epoch:  570, loss: 0.007344809826463461\n",
      "epoch:  571, loss: 0.007343521807342768\n",
      "epoch:  572, loss: 0.007342562545090914\n",
      "epoch:  573, loss: 0.007334777154028416\n",
      "epoch:  574, loss: 0.00733286002650857\n",
      "epoch:  575, loss: 0.007331854198127985\n",
      "epoch:  576, loss: 0.0073262364603579044\n",
      "epoch:  577, loss: 0.0073225777596235275\n",
      "epoch:  578, loss: 0.007321345619857311\n",
      "epoch:  579, loss: 0.007319565396755934\n",
      "epoch:  580, loss: 0.007312227506190538\n",
      "epoch:  581, loss: 0.007310665678232908\n",
      "epoch:  582, loss: 0.007309653330594301\n",
      "epoch:  583, loss: 0.0073047783225774765\n",
      "epoch:  584, loss: 0.007300141267478466\n",
      "epoch:  585, loss: 0.0072989435866475105\n",
      "epoch:  586, loss: 0.007297856733202934\n",
      "epoch:  587, loss: 0.007289706263691187\n",
      "epoch:  588, loss: 0.0072880033403635025\n",
      "epoch:  589, loss: 0.00728696770966053\n",
      "epoch:  590, loss: 0.007278225384652615\n",
      "epoch:  591, loss: 0.007276439107954502\n",
      "epoch:  592, loss: 0.007275338284671307\n",
      "epoch:  593, loss: 0.0072676497511565685\n",
      "epoch:  594, loss: 0.007264693267643452\n",
      "epoch:  595, loss: 0.00726352771744132\n",
      "epoch:  596, loss: 0.007257587742060423\n",
      "epoch:  597, loss: 0.007253044750541449\n",
      "epoch:  598, loss: 0.007251662202179432\n",
      "epoch:  599, loss: 0.007248139474540949\n",
      "epoch:  600, loss: 0.007241163402795792\n",
      "epoch:  601, loss: 0.007239694707095623\n",
      "epoch:  602, loss: 0.007238520309329033\n",
      "epoch:  603, loss: 0.007229530718177557\n",
      "epoch:  604, loss: 0.007227652240544558\n",
      "epoch:  605, loss: 0.0072265444323420525\n",
      "epoch:  606, loss: 0.007217742037028074\n",
      "epoch:  607, loss: 0.007215401157736778\n",
      "epoch:  608, loss: 0.007214222103357315\n",
      "epoch:  609, loss: 0.007205532863736153\n",
      "epoch:  610, loss: 0.007202806416898966\n",
      "epoch:  611, loss: 0.007201633416116238\n",
      "epoch:  612, loss: 0.007192745339125395\n",
      "epoch:  613, loss: 0.007189956959336996\n",
      "epoch:  614, loss: 0.007188607472926378\n",
      "epoch:  615, loss: 0.007180585991591215\n",
      "epoch:  616, loss: 0.007176469545811415\n",
      "epoch:  617, loss: 0.007175138685852289\n",
      "epoch:  618, loss: 0.007169910240918398\n",
      "epoch:  619, loss: 0.007163382135331631\n",
      "epoch:  620, loss: 0.007161587011069059\n",
      "epoch:  621, loss: 0.007160040084272623\n",
      "epoch:  622, loss: 0.007149864453822374\n",
      "epoch:  623, loss: 0.00714777410030365\n",
      "epoch:  624, loss: 0.00714648375287652\n",
      "epoch:  625, loss: 0.007137658540159464\n",
      "epoch:  626, loss: 0.007134377956390381\n",
      "epoch:  627, loss: 0.007133015897125006\n",
      "epoch:  628, loss: 0.007127466145902872\n",
      "epoch:  629, loss: 0.007121479604393244\n",
      "epoch:  630, loss: 0.007119821850210428\n",
      "epoch:  631, loss: 0.007118589244782925\n",
      "epoch:  632, loss: 0.007109553087502718\n",
      "epoch:  633, loss: 0.0071068741381168365\n",
      "epoch:  634, loss: 0.007105510216206312\n",
      "epoch:  635, loss: 0.007102445233613253\n",
      "epoch:  636, loss: 0.0070944130420684814\n",
      "epoch:  637, loss: 0.007092372048646212\n",
      "epoch:  638, loss: 0.00709114084020257\n",
      "epoch:  639, loss: 0.0070847743190824986\n",
      "epoch:  640, loss: 0.00707970978692174\n",
      "epoch:  641, loss: 0.007078100461512804\n",
      "epoch:  642, loss: 0.007076917216181755\n",
      "epoch:  643, loss: 0.007068184204399586\n",
      "epoch:  644, loss: 0.0070653255097568035\n",
      "epoch:  645, loss: 0.007063991855829954\n",
      "epoch:  646, loss: 0.007061365991830826\n",
      "epoch:  647, loss: 0.0070528690703213215\n",
      "epoch:  648, loss: 0.007050961256027222\n",
      "epoch:  649, loss: 0.00704977847635746\n",
      "epoch:  650, loss: 0.007041876204311848\n",
      "epoch:  651, loss: 0.0070381974801421165\n",
      "epoch:  652, loss: 0.007036739960312843\n",
      "epoch:  653, loss: 0.007030367385596037\n",
      "epoch:  654, loss: 0.007024886552244425\n",
      "epoch:  655, loss: 0.007023198530077934\n",
      "epoch:  656, loss: 0.0070219398476183414\n",
      "epoch:  657, loss: 0.007012307643890381\n",
      "epoch:  658, loss: 0.0070091914385557175\n",
      "epoch:  659, loss: 0.007007737644016743\n",
      "epoch:  660, loss: 0.007001796271651983\n",
      "epoch:  661, loss: 0.00699553731828928\n",
      "epoch:  662, loss: 0.006993679329752922\n",
      "epoch:  663, loss: 0.006992403883486986\n",
      "epoch:  664, loss: 0.00698322057723999\n",
      "epoch:  665, loss: 0.006979895289987326\n",
      "epoch:  666, loss: 0.006978388410061598\n",
      "epoch:  667, loss: 0.006972706411033869\n",
      "epoch:  668, loss: 0.0069662295281887054\n",
      "epoch:  669, loss: 0.006964394357055426\n",
      "epoch:  670, loss: 0.006963122636079788\n",
      "epoch:  671, loss: 0.006953230127692223\n",
      "epoch:  672, loss: 0.006950377952307463\n",
      "epoch:  673, loss: 0.006948923692107201\n",
      "epoch:  674, loss: 0.006942125968635082\n",
      "epoch:  675, loss: 0.006936569232493639\n",
      "epoch:  676, loss: 0.006934705190360546\n",
      "epoch:  677, loss: 0.006933410651981831\n",
      "epoch:  678, loss: 0.006924539804458618\n",
      "epoch:  679, loss: 0.006920690648257732\n",
      "epoch:  680, loss: 0.006919100880622864\n",
      "epoch:  681, loss: 0.00691741518676281\n",
      "epoch:  682, loss: 0.006906756199896336\n",
      "epoch:  683, loss: 0.006904366426169872\n",
      "epoch:  684, loss: 0.006902928929775953\n",
      "epoch:  685, loss: 0.0068940515629947186\n",
      "epoch:  686, loss: 0.006889404263347387\n",
      "epoch:  687, loss: 0.006887706927955151\n",
      "epoch:  688, loss: 0.00688755651935935\n",
      "epoch:  689, loss: 0.0068754698149859905\n",
      "epoch:  690, loss: 0.0068726921454072\n",
      "epoch:  691, loss: 0.00687120808288455\n",
      "epoch:  692, loss: 0.006863374728709459\n",
      "epoch:  693, loss: 0.006857858970761299\n",
      "epoch:  694, loss: 0.006856022402644157\n",
      "epoch:  695, loss: 0.006854627747088671\n",
      "epoch:  696, loss: 0.006844739429652691\n",
      "epoch:  697, loss: 0.0068411026149988174\n",
      "epoch:  698, loss: 0.0068394215777516365\n",
      "epoch:  699, loss: 0.006838079076260328\n",
      "epoch:  700, loss: 0.006828862242400646\n",
      "epoch:  701, loss: 0.006824502721428871\n",
      "epoch:  702, loss: 0.0068227630108594894\n",
      "epoch:  703, loss: 0.006822684314101934\n",
      "epoch:  704, loss: 0.006810266524553299\n",
      "epoch:  705, loss: 0.006807386875152588\n",
      "epoch:  706, loss: 0.006805796176195145\n",
      "epoch:  707, loss: 0.00680086575448513\n",
      "epoch:  708, loss: 0.006792554631829262\n",
      "epoch:  709, loss: 0.006790118291974068\n",
      "epoch:  710, loss: 0.006788664497435093\n",
      "epoch:  711, loss: 0.006779144983738661\n",
      "epoch:  712, loss: 0.006774525623768568\n",
      "epoch:  713, loss: 0.006772755179554224\n",
      "epoch:  714, loss: 0.0067713288590312\n",
      "epoch:  715, loss: 0.0067601860500872135\n",
      "epoch:  716, loss: 0.006757163908332586\n",
      "epoch:  717, loss: 0.006755561102181673\n",
      "epoch:  718, loss: 0.00674935569986701\n",
      "epoch:  719, loss: 0.0067419749684631824\n",
      "epoch:  720, loss: 0.006739717908203602\n",
      "epoch:  721, loss: 0.006738206837326288\n",
      "epoch:  722, loss: 0.006728691048920155\n",
      "epoch:  723, loss: 0.006724138744175434\n",
      "epoch:  724, loss: 0.006722323130816221\n",
      "epoch:  725, loss: 0.006720858160406351\n",
      "epoch:  726, loss: 0.006710244808346033\n",
      "epoch:  727, loss: 0.006706397980451584\n",
      "epoch:  728, loss: 0.006704594474285841\n",
      "epoch:  729, loss: 0.006703123915940523\n",
      "epoch:  730, loss: 0.006692394148558378\n",
      "epoch:  731, loss: 0.006688520312309265\n",
      "epoch:  732, loss: 0.00668669305741787\n",
      "epoch:  733, loss: 0.006686102598905563\n",
      "epoch:  734, loss: 0.006673025898635387\n",
      "epoch:  735, loss: 0.006669832859188318\n",
      "epoch:  736, loss: 0.006668093614280224\n",
      "epoch:  737, loss: 0.006663248408585787\n",
      "epoch:  738, loss: 0.0066535877995193005\n",
      "epoch:  739, loss: 0.006650735158473253\n",
      "epoch:  740, loss: 0.006649021990597248\n",
      "epoch:  741, loss: 0.006644505076110363\n",
      "epoch:  742, loss: 0.006634883116930723\n",
      "epoch:  743, loss: 0.006632092408835888\n",
      "epoch:  744, loss: 0.006630374118685722\n",
      "epoch:  745, loss: 0.006623886991292238\n",
      "epoch:  746, loss: 0.006615642458200455\n",
      "epoch:  747, loss: 0.006613107863813639\n",
      "epoch:  748, loss: 0.0066114203073084354\n",
      "epoch:  749, loss: 0.006602410227060318\n",
      "epoch:  750, loss: 0.0065963389351964\n",
      "epoch:  751, loss: 0.006594095844775438\n",
      "epoch:  752, loss: 0.006592486519366503\n",
      "epoch:  753, loss: 0.006580716464668512\n",
      "epoch:  754, loss: 0.006576637737452984\n",
      "epoch:  755, loss: 0.006574778351932764\n",
      "epoch:  756, loss: 0.006574102211743593\n",
      "epoch:  757, loss: 0.006560644134879112\n",
      "epoch:  758, loss: 0.006557181943207979\n",
      "epoch:  759, loss: 0.006555374711751938\n",
      "epoch:  760, loss: 0.0065499176271259785\n",
      "epoch:  761, loss: 0.006540351081639528\n",
      "epoch:  762, loss: 0.006537476554512978\n",
      "epoch:  763, loss: 0.006535733118653297\n",
      "epoch:  764, loss: 0.00652742525562644\n",
      "epoch:  765, loss: 0.0065202005207538605\n",
      "epoch:  766, loss: 0.006517606321722269\n",
      "epoch:  767, loss: 0.006515898741781712\n",
      "epoch:  768, loss: 0.006505855359137058\n",
      "epoch:  769, loss: 0.006499934010207653\n",
      "epoch:  770, loss: 0.006497633643448353\n",
      "epoch:  771, loss: 0.006495942361652851\n",
      "epoch:  772, loss: 0.006484979297965765\n",
      "epoch:  773, loss: 0.006479714531451464\n",
      "epoch:  774, loss: 0.006477488204836845\n",
      "epoch:  775, loss: 0.00647579412907362\n",
      "epoch:  776, loss: 0.006464703008532524\n",
      "epoch:  777, loss: 0.006459707394242287\n",
      "epoch:  778, loss: 0.0064575751312077045\n",
      "epoch:  779, loss: 0.006455906201153994\n",
      "epoch:  780, loss: 0.006445745937526226\n",
      "epoch:  781, loss: 0.0064399088732898235\n",
      "epoch:  782, loss: 0.006437609903514385\n",
      "epoch:  783, loss: 0.006435875780880451\n",
      "epoch:  784, loss: 0.006430845707654953\n",
      "epoch:  785, loss: 0.006420356221497059\n",
      "epoch:  786, loss: 0.0064170933328568935\n",
      "epoch:  787, loss: 0.006415119394659996\n",
      "epoch:  788, loss: 0.006413121707737446\n",
      "epoch:  789, loss: 0.006399561185389757\n",
      "epoch:  790, loss: 0.006395840086042881\n",
      "epoch:  791, loss: 0.006393713410943747\n",
      "epoch:  792, loss: 0.006391951348632574\n",
      "epoch:  793, loss: 0.0063792634755373\n",
      "epoch:  794, loss: 0.006374303251504898\n",
      "epoch:  795, loss: 0.006371927447617054\n",
      "epoch:  796, loss: 0.006370093673467636\n",
      "epoch:  797, loss: 0.00635947659611702\n",
      "epoch:  798, loss: 0.006352545227855444\n",
      "epoch:  799, loss: 0.006349918898195028\n",
      "epoch:  800, loss: 0.006347977556288242\n",
      "epoch:  801, loss: 0.006338723469525576\n",
      "epoch:  802, loss: 0.006330609321594238\n",
      "epoch:  803, loss: 0.006327597890049219\n",
      "epoch:  804, loss: 0.0063256085850298405\n",
      "epoch:  805, loss: 0.006317558232694864\n",
      "epoch:  806, loss: 0.006307938136160374\n",
      "epoch:  807, loss: 0.006304637994617224\n",
      "epoch:  808, loss: 0.0063025373965501785\n",
      "epoch:  809, loss: 0.006298199761658907\n",
      "epoch:  810, loss: 0.006285329349339008\n",
      "epoch:  811, loss: 0.006281372159719467\n",
      "epoch:  812, loss: 0.006279137916862965\n",
      "epoch:  813, loss: 0.0062772249802947044\n",
      "epoch:  814, loss: 0.0062630935572087765\n",
      "epoch:  815, loss: 0.0062582469545304775\n",
      "epoch:  816, loss: 0.006255690474063158\n",
      "epoch:  817, loss: 0.006253685802221298\n",
      "epoch:  818, loss: 0.006242432631552219\n",
      "epoch:  819, loss: 0.006234677042812109\n",
      "epoch:  820, loss: 0.006231671664863825\n",
      "epoch:  821, loss: 0.006229502148926258\n",
      "epoch:  822, loss: 0.006222941912710667\n",
      "epoch:  823, loss: 0.006211382802575827\n",
      "epoch:  824, loss: 0.006207423284649849\n",
      "epoch:  825, loss: 0.006205141544342041\n",
      "epoch:  826, loss: 0.006203138269484043\n",
      "epoch:  827, loss: 0.00618975143879652\n",
      "epoch:  828, loss: 0.006183439400047064\n",
      "epoch:  829, loss: 0.006180630996823311\n",
      "epoch:  830, loss: 0.0061784847639501095\n",
      "epoch:  831, loss: 0.006169614382088184\n",
      "epoch:  832, loss: 0.006159250624477863\n",
      "epoch:  833, loss: 0.006155549082905054\n",
      "epoch:  834, loss: 0.006153114605695009\n",
      "epoch:  835, loss: 0.006151041015982628\n",
      "epoch:  836, loss: 0.006138637661933899\n",
      "epoch:  837, loss: 0.006130935158580542\n",
      "epoch:  838, loss: 0.0061275470070540905\n",
      "epoch:  839, loss: 0.0061251819133758545\n",
      "epoch:  840, loss: 0.006117809563875198\n",
      "epoch:  841, loss: 0.006105200853198767\n",
      "epoch:  842, loss: 0.00610090559348464\n",
      "epoch:  843, loss: 0.006098290905356407\n",
      "epoch:  844, loss: 0.006096112076193094\n",
      "epoch:  845, loss: 0.00608332734555006\n",
      "epoch:  846, loss: 0.0060753170400857925\n",
      "epoch:  847, loss: 0.006071997340768576\n",
      "epoch:  848, loss: 0.006069624796509743\n",
      "epoch:  849, loss: 0.006069219671189785\n",
      "epoch:  850, loss: 0.0060515557415783405\n",
      "epoch:  851, loss: 0.006046062335371971\n",
      "epoch:  852, loss: 0.00604309793561697\n",
      "epoch:  853, loss: 0.0060408045537769794\n",
      "epoch:  854, loss: 0.006037076003849506\n",
      "epoch:  855, loss: 0.006021833047270775\n",
      "epoch:  856, loss: 0.006016787141561508\n",
      "epoch:  857, loss: 0.006013935897499323\n",
      "epoch:  858, loss: 0.006011656951159239\n",
      "epoch:  859, loss: 0.006003572139889002\n",
      "epoch:  860, loss: 0.005991709418594837\n",
      "epoch:  861, loss: 0.005987279582768679\n",
      "epoch:  862, loss: 0.0059846388176083565\n",
      "epoch:  863, loss: 0.005982407368719578\n",
      "epoch:  864, loss: 0.005967932287603617\n",
      "epoch:  865, loss: 0.00596047705039382\n",
      "epoch:  866, loss: 0.005957046058028936\n",
      "epoch:  867, loss: 0.005954545922577381\n",
      "epoch:  868, loss: 0.005947577301412821\n",
      "epoch:  869, loss: 0.005933619569987059\n",
      "epoch:  870, loss: 0.005928695667535067\n",
      "epoch:  871, loss: 0.0059257387183606625\n",
      "epoch:  872, loss: 0.005923306569457054\n",
      "epoch:  873, loss: 0.005912987049669027\n",
      "epoch:  874, loss: 0.00590181490406394\n",
      "epoch:  875, loss: 0.00589751498773694\n",
      "epoch:  876, loss: 0.005894756410270929\n",
      "epoch:  877, loss: 0.005892398301512003\n",
      "epoch:  878, loss: 0.0058824168518185616\n",
      "epoch:  879, loss: 0.005871148779988289\n",
      "epoch:  880, loss: 0.005866783671081066\n",
      "epoch:  881, loss: 0.005863990169018507\n",
      "epoch:  882, loss: 0.005861618556082249\n",
      "epoch:  883, loss: 0.005848159082233906\n",
      "epoch:  884, loss: 0.005839433055371046\n",
      "epoch:  885, loss: 0.005835689138621092\n",
      "epoch:  886, loss: 0.005833112634718418\n",
      "epoch:  887, loss: 0.005830691661685705\n",
      "epoch:  888, loss: 0.005813572555780411\n",
      "epoch:  889, loss: 0.005807655397802591\n",
      "epoch:  890, loss: 0.0058044493198394775\n",
      "epoch:  891, loss: 0.005801967345178127\n",
      "epoch:  892, loss: 0.0058005498722195625\n",
      "epoch:  893, loss: 0.0057825553230941296\n",
      "epoch:  894, loss: 0.005776378326117992\n",
      "epoch:  895, loss: 0.0057730828411877155\n",
      "epoch:  896, loss: 0.005770555231720209\n",
      "epoch:  897, loss: 0.005763787776231766\n",
      "epoch:  898, loss: 0.005749599542468786\n",
      "epoch:  899, loss: 0.00574424909427762\n",
      "epoch:  900, loss: 0.005741163622587919\n",
      "epoch:  901, loss: 0.005738660227507353\n",
      "epoch:  902, loss: 0.005732281133532524\n",
      "epoch:  903, loss: 0.005717981606721878\n",
      "epoch:  904, loss: 0.005712474696338177\n",
      "epoch:  905, loss: 0.005709402728825808\n",
      "epoch:  906, loss: 0.005706914234906435\n",
      "epoch:  907, loss: 0.0056970142759382725\n",
      "epoch:  908, loss: 0.005685192532837391\n",
      "epoch:  909, loss: 0.005680421367287636\n",
      "epoch:  910, loss: 0.005677470006048679\n",
      "epoch:  911, loss: 0.005674939136952162\n",
      "epoch:  912, loss: 0.00566879753023386\n",
      "epoch:  913, loss: 0.0056543354876339436\n",
      "epoch:  914, loss: 0.005648787599056959\n",
      "epoch:  915, loss: 0.00564552191644907\n",
      "epoch:  916, loss: 0.005642951000481844\n",
      "epoch:  917, loss: 0.0056416308507323265\n",
      "epoch:  918, loss: 0.005623525939881802\n",
      "epoch:  919, loss: 0.005616877228021622\n",
      "epoch:  920, loss: 0.005613328889012337\n",
      "epoch:  921, loss: 0.0056106410920619965\n",
      "epoch:  922, loss: 0.005607054103165865\n",
      "epoch:  923, loss: 0.005589599721133709\n",
      "epoch:  924, loss: 0.005583364516496658\n",
      "epoch:  925, loss: 0.005579987540841103\n",
      "epoch:  926, loss: 0.005577291827648878\n",
      "epoch:  927, loss: 0.0055717891082167625\n",
      "epoch:  928, loss: 0.005556442309170961\n",
      "epoch:  929, loss: 0.005549957975745201\n",
      "epoch:  930, loss: 0.0055464222095906734\n",
      "epoch:  931, loss: 0.005543598439544439\n",
      "epoch:  932, loss: 0.00554105406627059\n",
      "epoch:  933, loss: 0.005525095853954554\n",
      "epoch:  934, loss: 0.005517097655683756\n",
      "epoch:  935, loss: 0.00551293371245265\n",
      "epoch:  936, loss: 0.00550994835793972\n",
      "epoch:  937, loss: 0.005507302936166525\n",
      "epoch:  938, loss: 0.005506341345608234\n",
      "epoch:  939, loss: 0.005487758666276932\n",
      "epoch:  940, loss: 0.005480533465743065\n",
      "epoch:  941, loss: 0.005476474296301603\n",
      "epoch:  942, loss: 0.005473512224853039\n",
      "epoch:  943, loss: 0.005470938049256802\n",
      "epoch:  944, loss: 0.005464283749461174\n",
      "epoch:  945, loss: 0.005449290852993727\n",
      "epoch:  946, loss: 0.005443156231194735\n",
      "epoch:  947, loss: 0.005439487285912037\n",
      "epoch:  948, loss: 0.005436521023511887\n",
      "epoch:  949, loss: 0.005433853715658188\n",
      "epoch:  950, loss: 0.005420952569693327\n",
      "epoch:  951, loss: 0.005409879144281149\n",
      "epoch:  952, loss: 0.0054047591984272\n",
      "epoch:  953, loss: 0.00540135195478797\n",
      "epoch:  954, loss: 0.005398570094257593\n",
      "epoch:  955, loss: 0.00539453374221921\n",
      "epoch:  956, loss: 0.005377322901040316\n",
      "epoch:  957, loss: 0.005370074417442083\n",
      "epoch:  958, loss: 0.005366018041968346\n",
      "epoch:  959, loss: 0.005362953525036573\n",
      "epoch:  960, loss: 0.0053602163679897785\n",
      "epoch:  961, loss: 0.005353483371436596\n",
      "epoch:  962, loss: 0.005337883252650499\n",
      "epoch:  963, loss: 0.00533116701990366\n",
      "epoch:  964, loss: 0.005327137652784586\n",
      "epoch:  965, loss: 0.005324129015207291\n",
      "epoch:  966, loss: 0.005321354139596224\n",
      "epoch:  967, loss: 0.005310505628585815\n",
      "epoch:  968, loss: 0.005297486204653978\n",
      "epoch:  969, loss: 0.005291425157338381\n",
      "epoch:  970, loss: 0.005287569016218185\n",
      "epoch:  971, loss: 0.005284490529447794\n",
      "epoch:  972, loss: 0.00528161833062768\n",
      "epoch:  973, loss: 0.005268629174679518\n",
      "epoch:  974, loss: 0.005256053060293198\n",
      "epoch:  975, loss: 0.005250217858701944\n",
      "epoch:  976, loss: 0.005246367305517197\n",
      "epoch:  977, loss: 0.005243261344730854\n",
      "epoch:  978, loss: 0.0052404277957975864\n",
      "epoch:  979, loss: 0.0052332282066345215\n",
      "epoch:  980, loss: 0.005217256024479866\n",
      "epoch:  981, loss: 0.005210025701671839\n",
      "epoch:  982, loss: 0.005205739755183458\n",
      "epoch:  983, loss: 0.005202336236834526\n",
      "epoch:  984, loss: 0.005199358332902193\n",
      "epoch:  985, loss: 0.00519646517932415\n",
      "epoch:  986, loss: 0.005180669017136097\n",
      "epoch:  987, loss: 0.005170220509171486\n",
      "epoch:  988, loss: 0.005164823494851589\n",
      "epoch:  989, loss: 0.005161064211279154\n",
      "epoch:  990, loss: 0.005157946143299341\n",
      "epoch:  991, loss: 0.0051550851203501225\n",
      "epoch:  992, loss: 0.005144504830241203\n",
      "epoch:  993, loss: 0.005131071899086237\n",
      "epoch:  994, loss: 0.0051244767382740974\n",
      "epoch:  995, loss: 0.005120232235640287\n",
      "epoch:  996, loss: 0.005116903688758612\n",
      "epoch:  997, loss: 0.00511400680989027\n",
      "epoch:  998, loss: 0.005111176986247301\n",
      "epoch:  999, loss: 0.005094924941658974\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size=X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "# opt = torch_numopt.ConjugateGradient(model.parameters(), lr=1e-4, model=model, c1=1e-4, tau=0.1, line_search_method=\"const\", cg_method=\"FR\")\n",
    "opt = torch_numopt.ConjugateGradient(model.parameters(), lr=10, model=model, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"armijo\", cg_method=\"FR\")\n",
    "# opt = torch_numopt.ConjugateGradient(model.parameters(), lr=10, model=model, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"wolfe\", cg_method=\"FR\")\n",
    "# opt = torch_numopt.ConjugateGradient(model.parameters(), lr=10, model=model, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"strong-wolfe\", cg_method=\"FR\")\n",
    "# opt = torch_numopt.ConjugateGradient(model.parameters(), lr=10, model=model, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"goldstein\", cg_method=\"FR\")\n",
    "\n",
    "\n",
    "all_loss = {}\n",
    "for epoch in range(1000):\n",
    "    print(\"epoch: \", epoch, end=\"\")\n",
    "    all_loss[epoch + 1] = 0\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y, loss_fn)\n",
    "\n",
    "        all_loss[epoch + 1] += loss\n",
    "    all_loss[epoch + 1] /= len(data_loader)\n",
    "    print(\", loss: {}\".format(all_loss[epoch + 1].detach().numpy().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics: R2 = 0.8359038322267865\n",
      "Test metrics:  R2 = 0.8463810076509527\n"
     ]
    }
   ],
   "source": [
    "pred_train = model.forward(X_train).detach()\n",
    "pred_test = model.forward(X_test).detach()\n",
    "print(f\"Train metrics: R2 = {r2_score(pred_train, y_train)}\")\n",
    "print(f\"Test metrics:  R2 = {r2_score(pred_test, y_test)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
