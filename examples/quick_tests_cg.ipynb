{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch_numopt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import *\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.f1 = nn.Linear(input_size, 10, device=device)\n",
    "        self.f2 = nn.Linear(10, 20, device=device)\n",
    "        self.f3 = nn.Linear(20, 20, device=device)\n",
    "        self.f4 = nn.Linear(20, 10, device=device)\n",
    "        self.f5 = nn.Linear(10, 1, device=device)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        # self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.f1(x))\n",
    "        x = self.activation(self.f2(x))\n",
    "        x = self.activation(self.f3(x))\n",
    "        x = self.activation(self.f4(x))\n",
    "        x = self.f5(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([800, 10]) torch.Size([800, 1])\n",
      "torch.Size([200, 10]) torch.Size([200, 1])\n"
     ]
    }
   ],
   "source": [
    "# X, y = load_diabetes(return_X_y=True, scaled=False)\n",
    "# X, y = make_regression(n_samples=1000, n_features=100)\n",
    "X, y = make_friedman1(n_samples=1000, noise=1e-2)\n",
    "\n",
    "X_scaler = MinMaxScaler()\n",
    "X = X_scaler.fit_transform(X)\n",
    "X = torch.Tensor(X).to(device)\n",
    "\n",
    "y_scaler = MinMaxScaler()\n",
    "y = y_scaler.fit_transform(y.reshape((-1, 1)))\n",
    "y = torch.Tensor(y).to(device)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "torch_data = TensorDataset(X_train, y_train)\n",
    "data_loader = DataLoader(torch_data, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.36605796217918396\n",
      "epoch:  1, loss: 0.1960708051919937\n",
      "epoch:  2, loss: 0.10571490973234177\n",
      "epoch:  3, loss: 0.0660625472664833\n",
      "epoch:  4, loss: 0.047632113099098206\n",
      "epoch:  5, loss: 0.03896179795265198\n",
      "epoch:  6, loss: 0.034802209585905075\n",
      "epoch:  7, loss: 0.03277411311864853\n",
      "epoch:  8, loss: 0.03175155818462372\n",
      "epoch:  9, loss: 0.031225211918354034\n",
      "epoch:  10, loss: 0.030953457579016685\n",
      "epoch:  11, loss: 0.030809607356786728\n",
      "epoch:  12, loss: 0.030731569975614548\n",
      "epoch:  13, loss: 0.03068777173757553\n",
      "epoch:  14, loss: 0.03066111169755459\n",
      "epoch:  15, loss: 0.030640363693237305\n",
      "epoch:  16, loss: 0.030640363693237305\n",
      "epoch:  17, loss: 0.03061521053314209\n",
      "epoch:  18, loss: 0.030597424134612083\n",
      "epoch:  19, loss: 0.03058696910738945\n",
      "epoch:  20, loss: 0.030580561608076096\n",
      "epoch:  21, loss: 0.03057183511555195\n",
      "epoch:  22, loss: 0.03057183511555195\n",
      "epoch:  23, loss: 0.030560895800590515\n",
      "epoch:  24, loss: 0.030560895800590515\n",
      "epoch:  25, loss: 0.030542295426130295\n",
      "epoch:  26, loss: 0.030542295426130295\n",
      "epoch:  27, loss: 0.030506812036037445\n",
      "epoch:  28, loss: 0.030506812036037445\n",
      "epoch:  29, loss: 0.030470743775367737\n",
      "epoch:  30, loss: 0.03044605813920498\n",
      "epoch:  31, loss: 0.030432460829615593\n",
      "epoch:  32, loss: 0.0304244477301836\n",
      "epoch:  33, loss: 0.030412957072257996\n",
      "epoch:  34, loss: 0.030412957072257996\n",
      "epoch:  35, loss: 0.030401892960071564\n",
      "epoch:  36, loss: 0.03039398044347763\n",
      "epoch:  37, loss: 0.030389748513698578\n",
      "epoch:  38, loss: 0.030389748513698578\n",
      "epoch:  39, loss: 0.03037770837545395\n",
      "epoch:  40, loss: 0.030369332060217857\n",
      "epoch:  41, loss: 0.030367976054549217\n",
      "epoch:  42, loss: 0.030367976054549217\n",
      "epoch:  43, loss: 0.03035498596727848\n",
      "epoch:  44, loss: 0.03034580685198307\n",
      "epoch:  45, loss: 0.030340222641825676\n",
      "epoch:  46, loss: 0.03033088706433773\n",
      "epoch:  47, loss: 0.03033088706433773\n",
      "epoch:  48, loss: 0.030330028384923935\n",
      "epoch:  49, loss: 0.030330028384923935\n",
      "epoch:  50, loss: 0.030313558876514435\n",
      "epoch:  51, loss: 0.030302144587039948\n",
      "epoch:  52, loss: 0.030295422300696373\n",
      "epoch:  53, loss: 0.030288849025964737\n",
      "epoch:  54, loss: 0.030288847163319588\n",
      "epoch:  55, loss: 0.030278487130999565\n",
      "epoch:  56, loss: 0.030271096155047417\n",
      "epoch:  57, loss: 0.030265245586633682\n",
      "epoch:  58, loss: 0.030265245586633682\n",
      "epoch:  59, loss: 0.03025382198393345\n",
      "epoch:  60, loss: 0.030245637521147728\n",
      "epoch:  61, loss: 0.030241720378398895\n",
      "epoch:  62, loss: 0.030241720378398895\n",
      "epoch:  63, loss: 0.03022870607674122\n",
      "epoch:  64, loss: 0.030219553038477898\n",
      "epoch:  65, loss: 0.030217019841074944\n",
      "epoch:  66, loss: 0.030217019841074944\n",
      "epoch:  67, loss: 0.030202645808458328\n",
      "epoch:  68, loss: 0.030192499980330467\n",
      "epoch:  69, loss: 0.030186276882886887\n",
      "epoch:  70, loss: 0.03017463907599449\n",
      "epoch:  71, loss: 0.03017463907599449\n",
      "epoch:  72, loss: 0.030174314975738525\n",
      "epoch:  73, loss: 0.030174314975738525\n",
      "epoch:  74, loss: 0.030154366046190262\n",
      "epoch:  75, loss: 0.030140578746795654\n",
      "epoch:  76, loss: 0.030132479965686798\n",
      "epoch:  77, loss: 0.03012385591864586\n",
      "epoch:  78, loss: 0.03012385591864586\n",
      "epoch:  79, loss: 0.030111247673630714\n",
      "epoch:  80, loss: 0.03010224550962448\n",
      "epoch:  81, loss: 0.03009486384689808\n",
      "epoch:  82, loss: 0.03009486384689808\n",
      "epoch:  83, loss: 0.03008074127137661\n",
      "epoch:  84, loss: 0.030070647597312927\n",
      "epoch:  85, loss: 0.03006560355424881\n",
      "epoch:  86, loss: 0.03006560355424881\n",
      "epoch:  87, loss: 0.030049145221710205\n",
      "epoch:  88, loss: 0.03003751114010811\n",
      "epoch:  89, loss: 0.03003511391580105\n",
      "epoch:  90, loss: 0.03003511391580105\n",
      "epoch:  91, loss: 0.030016198754310608\n",
      "epoch:  92, loss: 0.030002940446138382\n",
      "epoch:  93, loss: 0.029994888231158257\n",
      "epoch:  94, loss: 0.02997944876551628\n",
      "epoch:  95, loss: 0.02997944876551628\n",
      "epoch:  96, loss: 0.02996711991727352\n",
      "epoch:  97, loss: 0.02995806373655796\n",
      "epoch:  98, loss: 0.02994372323155403\n",
      "epoch:  99, loss: 0.02994372323155403\n",
      "epoch:  100, loss: 0.02992960438132286\n",
      "epoch:  101, loss: 0.02991933375597\n",
      "epoch:  102, loss: 0.029906947165727615\n",
      "epoch:  103, loss: 0.029906947165727615\n",
      "epoch:  104, loss: 0.029890401288866997\n",
      "epoch:  105, loss: 0.029878513887524605\n",
      "epoch:  106, loss: 0.029868677258491516\n",
      "epoch:  107, loss: 0.029868677258491516\n",
      "epoch:  108, loss: 0.029849229380488396\n",
      "epoch:  109, loss: 0.029835456982254982\n",
      "epoch:  110, loss: 0.029829517006874084\n",
      "epoch:  111, loss: 0.029829517006874084\n",
      "epoch:  112, loss: 0.029806416481733322\n",
      "epoch:  113, loss: 0.0297902449965477\n",
      "epoch:  114, loss: 0.029789192602038383\n",
      "epoch:  115, loss: 0.029789192602038383\n",
      "epoch:  116, loss: 0.029761575162410736\n",
      "epoch:  117, loss: 0.029742470011115074\n",
      "epoch:  118, loss: 0.029731053858995438\n",
      "epoch:  119, loss: 0.02971109375357628\n",
      "epoch:  120, loss: 0.02971109375357628\n",
      "epoch:  121, loss: 0.02969294972717762\n",
      "epoch:  122, loss: 0.0296798013150692\n",
      "epoch:  123, loss: 0.029661530628800392\n",
      "epoch:  124, loss: 0.029661530628800392\n",
      "epoch:  125, loss: 0.029640011489391327\n",
      "epoch:  126, loss: 0.029624633491039276\n",
      "epoch:  127, loss: 0.029611291363835335\n",
      "epoch:  128, loss: 0.029611291363835335\n",
      "epoch:  129, loss: 0.029585368931293488\n",
      "epoch:  130, loss: 0.02956710383296013\n",
      "epoch:  131, loss: 0.029558641836047173\n",
      "epoch:  132, loss: 0.029558641836047173\n",
      "epoch:  133, loss: 0.029526911675930023\n",
      "epoch:  134, loss: 0.02950505167245865\n",
      "epoch:  135, loss: 0.029503431171178818\n",
      "epoch:  136, loss: 0.029503431171178818\n",
      "epoch:  137, loss: 0.02946508675813675\n",
      "epoch:  138, loss: 0.029439058154821396\n",
      "epoch:  139, loss: 0.029423745349049568\n",
      "epoch:  140, loss: 0.029393969103693962\n",
      "epoch:  141, loss: 0.029393965378403664\n",
      "epoch:  142, loss: 0.029369473457336426\n",
      "epoch:  143, loss: 0.02935180254280567\n",
      "epoch:  144, loss: 0.029326602816581726\n",
      "epoch:  145, loss: 0.029326602816581726\n",
      "epoch:  146, loss: 0.029296647757291794\n",
      "epoch:  147, loss: 0.029275530949234962\n",
      "epoch:  148, loss: 0.029256870970129967\n",
      "epoch:  149, loss: 0.029256870970129967\n",
      "epoch:  150, loss: 0.029219698160886765\n",
      "epoch:  151, loss: 0.029193883761763573\n",
      "epoch:  152, loss: 0.029185254126787186\n",
      "epoch:  153, loss: 0.029185252264142036\n",
      "epoch:  154, loss: 0.02913801744580269\n",
      "epoch:  155, loss: 0.029106562957167625\n",
      "epoch:  156, loss: 0.02908802032470703\n",
      "epoch:  157, loss: 0.02904619462788105\n",
      "epoch:  158, loss: 0.02904619462788105\n",
      "epoch:  159, loss: 0.029015343636274338\n",
      "epoch:  160, loss: 0.028993092477321625\n",
      "epoch:  161, loss: 0.028958451002836227\n",
      "epoch:  162, loss: 0.028958451002836227\n",
      "epoch:  163, loss: 0.02891855686903\n",
      "epoch:  164, loss: 0.02889108471572399\n",
      "epoch:  165, loss: 0.02886456809937954\n",
      "epoch:  166, loss: 0.02886456809937954\n",
      "epoch:  167, loss: 0.028814248740673065\n",
      "epoch:  168, loss: 0.02877998538315296\n",
      "epoch:  169, loss: 0.028770577162504196\n",
      "epoch:  170, loss: 0.028770577162504196\n",
      "epoch:  171, loss: 0.028704702854156494\n",
      "epoch:  172, loss: 0.028660839423537254\n",
      "epoch:  173, loss: 0.028635647147893906\n",
      "epoch:  174, loss: 0.02857564203441143\n",
      "epoch:  175, loss: 0.02857564203441143\n",
      "epoch:  176, loss: 0.028534244745969772\n",
      "epoch:  177, loss: 0.028504753485322\n",
      "epoch:  178, loss: 0.028455810621380806\n",
      "epoch:  179, loss: 0.028455810621380806\n",
      "epoch:  180, loss: 0.02840154618024826\n",
      "epoch:  181, loss: 0.028364064171910286\n",
      "epoch:  182, loss: 0.028336085379123688\n",
      "epoch:  183, loss: 0.02833608165383339\n",
      "epoch:  184, loss: 0.028262872248888016\n",
      "epoch:  185, loss: 0.028214044868946075\n",
      "epoch:  186, loss: 0.028185224160552025\n",
      "epoch:  187, loss: 0.028106551617383957\n",
      "epoch:  188, loss: 0.028106551617383957\n",
      "epoch:  189, loss: 0.028055446222424507\n",
      "epoch:  190, loss: 0.02801949717104435\n",
      "epoch:  191, loss: 0.02795516327023506\n",
      "epoch:  192, loss: 0.02795516327023506\n",
      "epoch:  193, loss: 0.027886325493454933\n",
      "epoch:  194, loss: 0.027839407324790955\n",
      "epoch:  195, loss: 0.02780148945748806\n",
      "epoch:  196, loss: 0.02780148945748806\n",
      "epoch:  197, loss: 0.027706390246748924\n",
      "epoch:  198, loss: 0.027644837275147438\n",
      "epoch:  199, loss: 0.027608904987573624\n",
      "epoch:  200, loss: 0.027503399178385735\n",
      "epoch:  201, loss: 0.027503399178385735\n",
      "epoch:  202, loss: 0.02743811532855034\n",
      "epoch:  203, loss: 0.027392376214265823\n",
      "epoch:  204, loss: 0.027302443981170654\n",
      "epoch:  205, loss: 0.027302443981170654\n",
      "epoch:  206, loss: 0.02721373364329338\n",
      "epoch:  207, loss: 0.02715456672012806\n",
      "epoch:  208, loss: 0.027103543281555176\n",
      "epoch:  209, loss: 0.027103543281555176\n",
      "epoch:  210, loss: 0.02697652392089367\n",
      "epoch:  211, loss: 0.026895971968770027\n",
      "epoch:  212, loss: 0.02685018628835678\n",
      "epoch:  213, loss: 0.02670213393867016\n",
      "epoch:  214, loss: 0.02670213393867016\n",
      "epoch:  215, loss: 0.026620175689458847\n",
      "epoch:  216, loss: 0.026563823223114014\n",
      "epoch:  217, loss: 0.026447730138897896\n",
      "epoch:  218, loss: 0.026447730138897896\n",
      "epoch:  219, loss: 0.026329979300498962\n",
      "epoch:  220, loss: 0.02625378780066967\n",
      "epoch:  221, loss: 0.026204613968729973\n",
      "epoch:  222, loss: 0.026204613968729973\n",
      "epoch:  223, loss: 0.026024164631962776\n",
      "epoch:  224, loss: 0.025917191058397293\n",
      "epoch:  225, loss: 0.025862345471978188\n",
      "epoch:  226, loss: 0.025670232251286507\n",
      "epoch:  227, loss: 0.025670232251286507\n",
      "epoch:  228, loss: 0.025567026808857918\n",
      "epoch:  229, loss: 0.025498807430267334\n",
      "epoch:  230, loss: 0.02532726339995861\n",
      "epoch:  231, loss: 0.02532726339995861\n",
      "epoch:  232, loss: 0.02519054338335991\n",
      "epoch:  233, loss: 0.02510586753487587\n",
      "epoch:  234, loss: 0.02501077950000763\n",
      "epoch:  235, loss: 0.02501077950000763\n",
      "epoch:  236, loss: 0.02481173910200596\n",
      "epoch:  237, loss: 0.02469513565301895\n",
      "epoch:  238, loss: 0.024632802233099937\n",
      "epoch:  239, loss: 0.024384884163737297\n",
      "epoch:  240, loss: 0.024384884163737297\n",
      "epoch:  241, loss: 0.024261923506855965\n",
      "epoch:  242, loss: 0.024185536429286003\n",
      "epoch:  243, loss: 0.0239805169403553\n",
      "epoch:  244, loss: 0.0239805169403553\n",
      "epoch:  245, loss: 0.02381264977157116\n",
      "epoch:  246, loss: 0.023714395239949226\n",
      "epoch:  247, loss: 0.023575864732265472\n",
      "epoch:  248, loss: 0.023575864732265472\n",
      "epoch:  249, loss: 0.023342693224549294\n",
      "epoch:  250, loss: 0.02321436256170273\n",
      "epoch:  251, loss: 0.02320891059935093\n",
      "epoch:  252, loss: 0.02320891059935093\n",
      "epoch:  253, loss: 0.022851472720503807\n",
      "epoch:  254, loss: 0.02267886884510517\n",
      "epoch:  255, loss: 0.022602194920182228\n",
      "epoch:  256, loss: 0.0222733523696661\n",
      "epoch:  257, loss: 0.0222733523696661\n",
      "epoch:  258, loss: 0.022133802995085716\n",
      "epoch:  259, loss: 0.022047657519578934\n",
      "epoch:  260, loss: 0.021765591576695442\n",
      "epoch:  261, loss: 0.021765591576695442\n",
      "epoch:  262, loss: 0.021570460870862007\n",
      "epoch:  263, loss: 0.021466568112373352\n",
      "epoch:  264, loss: 0.021198637783527374\n",
      "epoch:  265, loss: 0.021198637783527374\n",
      "epoch:  266, loss: 0.020973853766918182\n",
      "epoch:  267, loss: 0.02085830271244049\n",
      "epoch:  268, loss: 0.020656142383813858\n",
      "epoch:  269, loss: 0.020656142383813858\n",
      "epoch:  270, loss: 0.0203598253428936\n",
      "epoch:  271, loss: 0.020226359367370605\n",
      "epoch:  272, loss: 0.02005765214562416\n",
      "epoch:  273, loss: 0.02005765214562416\n",
      "epoch:  274, loss: 0.01971706561744213\n",
      "epoch:  275, loss: 0.019573865458369255\n",
      "epoch:  276, loss: 0.019413677975535393\n",
      "epoch:  277, loss: 0.019413677975535393\n",
      "epoch:  278, loss: 0.01905033178627491\n",
      "epoch:  279, loss: 0.018905140459537506\n",
      "epoch:  280, loss: 0.018726371228694916\n",
      "epoch:  281, loss: 0.018726371228694916\n",
      "epoch:  282, loss: 0.018365252763032913\n",
      "epoch:  283, loss: 0.018228527158498764\n",
      "epoch:  284, loss: 0.017985181882977486\n",
      "epoch:  285, loss: 0.017985181882977486\n",
      "epoch:  286, loss: 0.017672212794423103\n",
      "epoch:  287, loss: 0.01755436137318611\n",
      "epoch:  288, loss: 0.017271917313337326\n",
      "epoch:  289, loss: 0.017271917313337326\n",
      "epoch:  290, loss: 0.017001520842313766\n",
      "epoch:  291, loss: 0.01689288578927517\n",
      "epoch:  292, loss: 0.0165766179561615\n",
      "epoch:  293, loss: 0.016576619818806648\n",
      "epoch:  294, loss: 0.01633531041443348\n",
      "epoch:  295, loss: 0.01624046266078949\n",
      "epoch:  296, loss: 0.015909165143966675\n",
      "epoch:  297, loss: 0.015909165143966675\n",
      "epoch:  298, loss: 0.015712352469563484\n",
      "epoch:  299, loss: 0.01562740094959736\n",
      "epoch:  300, loss: 0.01526019349694252\n",
      "epoch:  301, loss: 0.01526019349694252\n",
      "epoch:  302, loss: 0.015094012022018433\n",
      "epoch:  303, loss: 0.015022928826510906\n",
      "epoch:  304, loss: 0.014848325401544571\n",
      "epoch:  305, loss: 0.014848325401544571\n",
      "epoch:  306, loss: 0.014589359983801842\n",
      "epoch:  307, loss: 0.01450110413134098\n",
      "epoch:  308, loss: 0.014163296669721603\n",
      "epoch:  309, loss: 0.014163296669721603\n",
      "epoch:  310, loss: 0.013997981324791908\n",
      "epoch:  311, loss: 0.013933815993368626\n",
      "epoch:  312, loss: 0.013802146539092064\n",
      "epoch:  313, loss: 0.013802146539092064\n",
      "epoch:  314, loss: 0.013530425727367401\n",
      "epoch:  315, loss: 0.01345355436205864\n",
      "epoch:  316, loss: 0.013153108768165112\n",
      "epoch:  317, loss: 0.013153108768165112\n",
      "epoch:  318, loss: 0.012995157390832901\n",
      "epoch:  319, loss: 0.012937097810208797\n",
      "epoch:  320, loss: 0.012906055897474289\n",
      "epoch:  321, loss: 0.012637903913855553\n",
      "epoch:  322, loss: 0.012637903913855553\n",
      "epoch:  323, loss: 0.01253744401037693\n",
      "epoch:  324, loss: 0.012446146458387375\n",
      "epoch:  325, loss: 0.012446146458387375\n",
      "epoch:  326, loss: 0.012093087658286095\n",
      "epoch:  327, loss: 0.012028295546770096\n",
      "epoch:  328, loss: 0.011966312304139137\n",
      "epoch:  329, loss: 0.011966312304139137\n",
      "epoch:  330, loss: 0.011694258078932762\n",
      "epoch:  331, loss: 0.011638781987130642\n",
      "epoch:  332, loss: 0.011503991670906544\n",
      "epoch:  333, loss: 0.011503991670906544\n",
      "epoch:  334, loss: 0.011319300159811974\n",
      "epoch:  335, loss: 0.011275351978838444\n",
      "epoch:  336, loss: 0.011263344436883926\n",
      "epoch:  337, loss: 0.011263344436883926\n",
      "epoch:  338, loss: 0.011222584173083305\n",
      "epoch:  339, loss: 0.011222584173083305\n",
      "epoch:  340, loss: 0.010915374383330345\n",
      "epoch:  341, loss: 0.010915374383330345\n",
      "epoch:  342, loss: 0.01086786761879921\n",
      "epoch:  343, loss: 0.010860715992748737\n",
      "epoch:  344, loss: 0.010860715992748737\n",
      "epoch:  345, loss: 0.010589352808892727\n",
      "epoch:  346, loss: 0.010546611621975899\n",
      "epoch:  347, loss: 0.010532322339713573\n",
      "epoch:  348, loss: 0.010442649014294147\n",
      "epoch:  349, loss: 0.010442649014294147\n",
      "epoch:  350, loss: 0.010360547341406345\n",
      "epoch:  351, loss: 0.010295833460986614\n",
      "epoch:  352, loss: 0.010295833460986614\n",
      "epoch:  353, loss: 0.010049191303551197\n",
      "epoch:  354, loss: 0.010012106038630009\n",
      "epoch:  355, loss: 0.010012106038630009\n",
      "epoch:  356, loss: 0.0084282997995615\n",
      "epoch:  357, loss: 0.0084282997995615\n",
      "epoch:  358, loss: 0.00823933444917202\n",
      "epoch:  359, loss: 0.0077950311824679375\n",
      "epoch:  360, loss: 0.0077950311824679375\n",
      "epoch:  361, loss: 0.007778121158480644\n",
      "epoch:  362, loss: 0.007702338509261608\n",
      "epoch:  363, loss: 0.007702338509261608\n",
      "epoch:  364, loss: 0.00763457827270031\n",
      "epoch:  365, loss: 0.007631729822605848\n",
      "epoch:  366, loss: 0.007631729822605848\n",
      "epoch:  367, loss: 0.007613556459546089\n",
      "epoch:  368, loss: 0.007609971333295107\n",
      "epoch:  369, loss: 0.007609971333295107\n",
      "epoch:  370, loss: 0.007599454373121262\n",
      "epoch:  371, loss: 0.0075903660617768764\n",
      "epoch:  372, loss: 0.0075903660617768764\n",
      "epoch:  373, loss: 0.007587573025375605\n",
      "epoch:  374, loss: 0.007530878763645887\n",
      "epoch:  375, loss: 0.007530878763645887\n",
      "epoch:  376, loss: 0.0075233192183077335\n",
      "epoch:  377, loss: 0.007516602054238319\n",
      "epoch:  378, loss: 0.007516602054238319\n",
      "epoch:  379, loss: 0.007515202276408672\n",
      "epoch:  380, loss: 0.00748425954952836\n",
      "epoch:  381, loss: 0.00748425954952836\n",
      "epoch:  382, loss: 0.007482819724828005\n",
      "epoch:  383, loss: 0.007482537534087896\n",
      "epoch:  384, loss: 0.007477998733520508\n",
      "epoch:  385, loss: 0.007477998733520508\n",
      "epoch:  386, loss: 0.00747652305290103\n",
      "epoch:  387, loss: 0.0074765123426914215\n",
      "epoch:  388, loss: 0.007471010554581881\n",
      "epoch:  389, loss: 0.007471010554581881\n",
      "epoch:  390, loss: 0.007469755597412586\n",
      "epoch:  391, loss: 0.007467004470527172\n",
      "epoch:  392, loss: 0.007467004470527172\n",
      "epoch:  393, loss: 0.007466410286724567\n",
      "epoch:  394, loss: 0.007466410286724567\n",
      "epoch:  395, loss: 0.007462686859071255\n",
      "epoch:  396, loss: 0.007462686859071255\n",
      "epoch:  397, loss: 0.007462162524461746\n",
      "epoch:  398, loss: 0.007462080102413893\n",
      "epoch:  399, loss: 0.007454096805304289\n",
      "epoch:  400, loss: 0.007453122176229954\n",
      "epoch:  401, loss: 0.007453122176229954\n",
      "epoch:  402, loss: 0.00745046092197299\n",
      "epoch:  403, loss: 0.007450120989233255\n",
      "epoch:  404, loss: 0.00743538374081254\n",
      "epoch:  405, loss: 0.00743538374081254\n",
      "epoch:  406, loss: 0.007425153627991676\n",
      "epoch:  407, loss: 0.007424729876220226\n",
      "epoch:  408, loss: 0.00742350285872817\n",
      "epoch:  409, loss: 0.00742350285872817\n",
      "epoch:  410, loss: 0.00742168165743351\n",
      "epoch:  411, loss: 0.007419594097882509\n",
      "epoch:  412, loss: 0.007419594097882509\n",
      "epoch:  413, loss: 0.007418682798743248\n",
      "epoch:  414, loss: 0.007395835127681494\n",
      "epoch:  415, loss: 0.007395835127681494\n",
      "epoch:  416, loss: 0.007394630927592516\n",
      "epoch:  417, loss: 0.007392319850623608\n",
      "epoch:  418, loss: 0.007392319850623608\n",
      "epoch:  419, loss: 0.007391844876110554\n",
      "epoch:  420, loss: 0.007391795050352812\n",
      "epoch:  421, loss: 0.007391794119030237\n",
      "epoch:  422, loss: 0.007391368038952351\n",
      "epoch:  423, loss: 0.007386190816760063\n",
      "epoch:  424, loss: 0.007386190816760063\n",
      "epoch:  425, loss: 0.007375758141279221\n",
      "epoch:  426, loss: 0.007375427056103945\n",
      "epoch:  427, loss: 0.007375274784862995\n",
      "epoch:  428, loss: 0.0073634968139231205\n",
      "epoch:  429, loss: 0.0073634968139231205\n",
      "epoch:  430, loss: 0.007362968288362026\n",
      "epoch:  431, loss: 0.007362968288362026\n",
      "epoch:  432, loss: 0.007360879797488451\n",
      "epoch:  433, loss: 0.007360879797488451\n",
      "epoch:  434, loss: 0.007360334973782301\n",
      "epoch:  435, loss: 0.007360333111137152\n",
      "epoch:  436, loss: 0.007358929142355919\n",
      "epoch:  437, loss: 0.007358929142355919\n",
      "epoch:  438, loss: 0.007358056493103504\n",
      "epoch:  439, loss: 0.00735683087259531\n",
      "epoch:  440, loss: 0.00735683087259531\n",
      "epoch:  441, loss: 0.007356519810855389\n",
      "epoch:  442, loss: 0.007356519810855389\n",
      "epoch:  443, loss: 0.007354968227446079\n",
      "epoch:  444, loss: 0.007354968227446079\n",
      "epoch:  445, loss: 0.007354480214416981\n",
      "epoch:  446, loss: 0.007353512104600668\n",
      "epoch:  447, loss: 0.007353512104600668\n",
      "epoch:  448, loss: 0.007353158202022314\n",
      "epoch:  449, loss: 0.0073442719876766205\n",
      "epoch:  450, loss: 0.0073442719876766205\n",
      "epoch:  451, loss: 0.007343178614974022\n",
      "epoch:  452, loss: 0.007342183962464333\n",
      "epoch:  453, loss: 0.007342183962464333\n",
      "epoch:  454, loss: 0.007341672666370869\n",
      "epoch:  455, loss: 0.007341133430600166\n",
      "epoch:  456, loss: 0.007341133430600166\n",
      "epoch:  457, loss: 0.007333999965339899\n",
      "epoch:  458, loss: 0.007333710324019194\n",
      "epoch:  459, loss: 0.007331985980272293\n",
      "epoch:  460, loss: 0.007331985980272293\n",
      "epoch:  461, loss: 0.007331719622015953\n",
      "epoch:  462, loss: 0.007330389227718115\n",
      "epoch:  463, loss: 0.007330389227718115\n",
      "epoch:  464, loss: 0.0073224580846726894\n",
      "epoch:  465, loss: 0.007322241552174091\n",
      "epoch:  466, loss: 0.0073210205882787704\n",
      "epoch:  467, loss: 0.0073210205882787704\n",
      "epoch:  468, loss: 0.007320772856473923\n",
      "epoch:  469, loss: 0.007320320699363947\n",
      "epoch:  470, loss: 0.007320320699363947\n",
      "epoch:  471, loss: 0.00731237605214119\n",
      "epoch:  472, loss: 0.00731209060177207\n",
      "epoch:  473, loss: 0.007311243563890457\n",
      "epoch:  474, loss: 0.007311242166906595\n",
      "epoch:  475, loss: 0.00731092132627964\n",
      "epoch:  476, loss: 0.007310319691896439\n",
      "epoch:  477, loss: 0.007310319691896439\n",
      "epoch:  478, loss: 0.007309917360544205\n",
      "epoch:  479, loss: 0.00730924354866147\n",
      "epoch:  480, loss: 0.007309242617338896\n",
      "epoch:  481, loss: 0.007308897562325001\n",
      "epoch:  482, loss: 0.007308362051844597\n",
      "epoch:  483, loss: 0.007308362051844597\n",
      "epoch:  484, loss: 0.007307919207960367\n",
      "epoch:  485, loss: 0.007307329215109348\n",
      "epoch:  486, loss: 0.007307328749448061\n",
      "epoch:  487, loss: 0.0073069254867732525\n",
      "epoch:  488, loss: 0.007306437473744154\n",
      "epoch:  489, loss: 0.007306437473744154\n",
      "epoch:  490, loss: 0.007305967155843973\n",
      "epoch:  491, loss: 0.00730542279779911\n",
      "epoch:  492, loss: 0.00730542279779911\n",
      "epoch:  493, loss: 0.007304985076189041\n",
      "epoch:  494, loss: 0.007304586004465818\n",
      "epoch:  495, loss: 0.007304586004465818\n",
      "epoch:  496, loss: 0.007304023951292038\n",
      "epoch:  497, loss: 0.007303751073777676\n",
      "epoch:  498, loss: 0.007303751073777676\n",
      "epoch:  499, loss: 0.00730303768068552\n",
      "epoch:  500, loss: 0.007302936166524887\n",
      "epoch:  501, loss: 0.007294728886336088\n",
      "epoch:  502, loss: 0.007294728886336088\n",
      "epoch:  503, loss: 0.007290123496204615\n",
      "epoch:  504, loss: 0.007290123496204615\n",
      "epoch:  505, loss: 0.007287989836186171\n",
      "epoch:  506, loss: 0.007287798915058374\n",
      "epoch:  507, loss: 0.007286803796887398\n",
      "epoch:  508, loss: 0.007286803796887398\n",
      "epoch:  509, loss: 0.0072859120555222034\n",
      "epoch:  510, loss: 0.007285775616765022\n",
      "epoch:  511, loss: 0.007285050582140684\n",
      "epoch:  512, loss: 0.007285050582140684\n",
      "epoch:  513, loss: 0.007284277584403753\n",
      "epoch:  514, loss: 0.0072841523215174675\n",
      "epoch:  515, loss: 0.007283327169716358\n",
      "epoch:  516, loss: 0.007283326238393784\n",
      "epoch:  517, loss: 0.007282906211912632\n",
      "epoch:  518, loss: 0.007282655220478773\n",
      "epoch:  519, loss: 0.007282655220478773\n",
      "epoch:  520, loss: 0.0072820172645151615\n",
      "epoch:  521, loss: 0.007281981874257326\n",
      "epoch:  522, loss: 0.007281981874257326\n",
      "epoch:  523, loss: 0.007281141355633736\n",
      "epoch:  524, loss: 0.007281047757714987\n",
      "epoch:  525, loss: 0.007277183700352907\n",
      "epoch:  526, loss: 0.007277183700352907\n",
      "epoch:  527, loss: 0.007273002993315458\n",
      "epoch:  528, loss: 0.007272809743881226\n",
      "epoch:  529, loss: 0.00727192172780633\n",
      "epoch:  530, loss: 0.00727192172780633\n",
      "epoch:  531, loss: 0.007271723821759224\n",
      "epoch:  532, loss: 0.007270975038409233\n",
      "epoch:  533, loss: 0.007270975038409233\n",
      "epoch:  534, loss: 0.007270802743732929\n",
      "epoch:  535, loss: 0.007270044181495905\n",
      "epoch:  536, loss: 0.007270044181495905\n",
      "epoch:  537, loss: 0.0072698951698839664\n",
      "epoch:  538, loss: 0.007269199937582016\n",
      "epoch:  539, loss: 0.007269199937582016\n",
      "epoch:  540, loss: 0.0072690509259700775\n",
      "epoch:  541, loss: 0.007267720531672239\n",
      "epoch:  542, loss: 0.007267720531672239\n",
      "epoch:  543, loss: 0.007261584512889385\n",
      "epoch:  544, loss: 0.007261251099407673\n",
      "epoch:  545, loss: 0.007261150516569614\n",
      "epoch:  546, loss: 0.007256629411131144\n",
      "epoch:  547, loss: 0.007256629411131144\n",
      "epoch:  548, loss: 0.007253216113895178\n",
      "epoch:  549, loss: 0.007253042887896299\n",
      "epoch:  550, loss: 0.007251264993101358\n",
      "epoch:  551, loss: 0.007251264993101358\n",
      "epoch:  552, loss: 0.007245490327477455\n",
      "epoch:  553, loss: 0.007245218846946955\n",
      "epoch:  554, loss: 0.007244115695357323\n",
      "epoch:  555, loss: 0.007244115695357323\n",
      "epoch:  556, loss: 0.0072439382784068584\n",
      "epoch:  557, loss: 0.007236899808049202\n",
      "epoch:  558, loss: 0.007236899808049202\n",
      "epoch:  559, loss: 0.007236271630972624\n",
      "epoch:  560, loss: 0.007235604338347912\n",
      "epoch:  561, loss: 0.007235604338347912\n",
      "epoch:  562, loss: 0.007234978023916483\n",
      "epoch:  563, loss: 0.007234860211610794\n",
      "epoch:  564, loss: 0.007234860211610794\n",
      "epoch:  565, loss: 0.0072328452952206135\n",
      "epoch:  566, loss: 0.0072328452952206135\n",
      "epoch:  567, loss: 0.007227787747979164\n",
      "epoch:  568, loss: 0.007227486930787563\n",
      "epoch:  569, loss: 0.007220663595944643\n",
      "epoch:  570, loss: 0.007220663595944643\n",
      "epoch:  571, loss: 0.007219721097499132\n",
      "epoch:  572, loss: 0.007218942046165466\n",
      "epoch:  573, loss: 0.007218941580504179\n",
      "epoch:  574, loss: 0.007217168342322111\n",
      "epoch:  575, loss: 0.007216932717710733\n",
      "epoch:  576, loss: 0.007215161342173815\n",
      "epoch:  577, loss: 0.007215161342173815\n",
      "epoch:  578, loss: 0.007214718963950872\n",
      "epoch:  579, loss: 0.007214261684566736\n",
      "epoch:  580, loss: 0.007214261684566736\n",
      "epoch:  581, loss: 0.007213287521153688\n",
      "epoch:  582, loss: 0.007213127799332142\n",
      "epoch:  583, loss: 0.007211757358163595\n",
      "epoch:  584, loss: 0.007211756892502308\n",
      "epoch:  585, loss: 0.0072115333750844\n",
      "epoch:  586, loss: 0.007210666313767433\n",
      "epoch:  587, loss: 0.007210666313767433\n",
      "epoch:  588, loss: 0.007204133085906506\n",
      "epoch:  589, loss: 0.007203578948974609\n",
      "epoch:  590, loss: 0.007203066721558571\n",
      "epoch:  591, loss: 0.007203066721558571\n",
      "epoch:  592, loss: 0.007201041094958782\n",
      "epoch:  593, loss: 0.007200790569186211\n",
      "epoch:  594, loss: 0.007199965883046389\n",
      "epoch:  595, loss: 0.007199965883046389\n",
      "epoch:  596, loss: 0.00719895726069808\n",
      "epoch:  597, loss: 0.007198774255812168\n",
      "epoch:  598, loss: 0.007197473663836718\n",
      "epoch:  599, loss: 0.007197473663836718\n",
      "epoch:  600, loss: 0.00719720683991909\n",
      "epoch:  601, loss: 0.007196118123829365\n",
      "epoch:  602, loss: 0.007196118123829365\n",
      "epoch:  603, loss: 0.0071958559565246105\n",
      "epoch:  604, loss: 0.007195443380624056\n",
      "epoch:  605, loss: 0.007195443380624056\n",
      "epoch:  606, loss: 0.007194771897047758\n",
      "epoch:  607, loss: 0.007194624748080969\n",
      "epoch:  608, loss: 0.007194484584033489\n",
      "epoch:  609, loss: 0.007193507626652718\n",
      "epoch:  610, loss: 0.00719350716099143\n",
      "epoch:  611, loss: 0.00719328923150897\n",
      "epoch:  612, loss: 0.007192926947027445\n",
      "epoch:  613, loss: 0.007192926947027445\n",
      "epoch:  614, loss: 0.007192355580627918\n",
      "epoch:  615, loss: 0.007192249409854412\n",
      "epoch:  616, loss: 0.007191197481006384\n",
      "epoch:  617, loss: 0.007191197481006384\n",
      "epoch:  618, loss: 0.007190933916717768\n",
      "epoch:  619, loss: 0.00719081424176693\n",
      "epoch:  620, loss: 0.00719081424176693\n",
      "epoch:  621, loss: 0.007190066389739513\n",
      "epoch:  622, loss: 0.00718995975330472\n",
      "epoch:  623, loss: 0.007180937100201845\n",
      "epoch:  624, loss: 0.007180937100201845\n",
      "epoch:  625, loss: 0.007178196217864752\n",
      "epoch:  626, loss: 0.007177977357059717\n",
      "epoch:  627, loss: 0.007176821120083332\n",
      "epoch:  628, loss: 0.007176821120083332\n",
      "epoch:  629, loss: 0.0071764602325856686\n",
      "epoch:  630, loss: 0.0071763647720217705\n",
      "epoch:  631, loss: 0.0071754478849470615\n",
      "epoch:  632, loss: 0.0071754478849470615\n",
      "epoch:  633, loss: 0.007174966391175985\n",
      "epoch:  634, loss: 0.007174860220402479\n",
      "epoch:  635, loss: 0.007169724907726049\n",
      "epoch:  636, loss: 0.007169724907726049\n",
      "epoch:  637, loss: 0.007163200993090868\n",
      "epoch:  638, loss: 0.007162732537835836\n",
      "epoch:  639, loss: 0.007162540685385466\n",
      "epoch:  640, loss: 0.007162539754062891\n",
      "epoch:  641, loss: 0.007161316927522421\n",
      "epoch:  642, loss: 0.007161316927522421\n",
      "epoch:  643, loss: 0.007160916458815336\n",
      "epoch:  644, loss: 0.007160575594753027\n",
      "epoch:  645, loss: 0.007160575594753027\n",
      "epoch:  646, loss: 0.007159670814871788\n",
      "epoch:  647, loss: 0.007159513421356678\n",
      "epoch:  648, loss: 0.00715866032987833\n",
      "epoch:  649, loss: 0.00715866032987833\n",
      "epoch:  650, loss: 0.007158074527978897\n",
      "epoch:  651, loss: 0.0071578919887542725\n",
      "epoch:  652, loss: 0.0071578919887542725\n",
      "epoch:  653, loss: 0.007156867999583483\n",
      "epoch:  654, loss: 0.007156738080084324\n",
      "epoch:  655, loss: 0.007149357348680496\n",
      "epoch:  656, loss: 0.007149357348680496\n",
      "epoch:  657, loss: 0.007146370597183704\n",
      "epoch:  658, loss: 0.0071461689658463\n",
      "epoch:  659, loss: 0.0071371132507920265\n",
      "epoch:  660, loss: 0.0071371132507920265\n",
      "epoch:  661, loss: 0.007134447805583477\n",
      "epoch:  662, loss: 0.007134208455681801\n",
      "epoch:  663, loss: 0.007134208455681801\n",
      "epoch:  664, loss: 0.007133075967431068\n",
      "epoch:  665, loss: 0.007133075967431068\n",
      "epoch:  666, loss: 0.007131992373615503\n",
      "epoch:  667, loss: 0.007131855469197035\n",
      "epoch:  668, loss: 0.007131855469197035\n",
      "epoch:  669, loss: 0.007130289450287819\n",
      "epoch:  670, loss: 0.007130289450287819\n",
      "epoch:  671, loss: 0.007130023092031479\n",
      "epoch:  672, loss: 0.007129482459276915\n",
      "epoch:  673, loss: 0.007129482459276915\n",
      "epoch:  674, loss: 0.00712905777618289\n",
      "epoch:  675, loss: 0.007128455210477114\n",
      "epoch:  676, loss: 0.007128455210477114\n",
      "epoch:  677, loss: 0.007127915509045124\n",
      "epoch:  678, loss: 0.007127800490707159\n",
      "epoch:  679, loss: 0.007126812823116779\n",
      "epoch:  680, loss: 0.007126812823116779\n",
      "epoch:  681, loss: 0.007126140408217907\n",
      "epoch:  682, loss: 0.007126022595912218\n",
      "epoch:  683, loss: 0.0071249669417738914\n",
      "epoch:  684, loss: 0.0071249669417738914\n",
      "epoch:  685, loss: 0.007124385330826044\n",
      "epoch:  686, loss: 0.007124294061213732\n",
      "epoch:  687, loss: 0.007113214582204819\n",
      "epoch:  688, loss: 0.007113214582204819\n",
      "epoch:  689, loss: 0.007111449725925922\n",
      "epoch:  690, loss: 0.00711122015491128\n",
      "epoch:  691, loss: 0.007109654136002064\n",
      "epoch:  692, loss: 0.007109654136002064\n",
      "epoch:  693, loss: 0.007109167519956827\n",
      "epoch:  694, loss: 0.007109136786311865\n",
      "epoch:  695, loss: 0.007109136786311865\n",
      "epoch:  696, loss: 0.007107927929610014\n",
      "epoch:  697, loss: 0.007107728160917759\n",
      "epoch:  698, loss: 0.007107196841388941\n",
      "epoch:  699, loss: 0.007107196841388941\n",
      "epoch:  700, loss: 0.007106097415089607\n",
      "epoch:  701, loss: 0.007105889730155468\n",
      "epoch:  702, loss: 0.007105715107172728\n",
      "epoch:  703, loss: 0.007105233147740364\n",
      "epoch:  704, loss: 0.007105233147740364\n",
      "epoch:  705, loss: 0.0070923008024692535\n",
      "epoch:  706, loss: 0.007091609761118889\n",
      "epoch:  707, loss: 0.007087198086082935\n",
      "epoch:  708, loss: 0.007087198086082935\n",
      "epoch:  709, loss: 0.007074673194438219\n",
      "epoch:  710, loss: 0.0070740836672484875\n",
      "epoch:  711, loss: 0.007061376702040434\n",
      "epoch:  712, loss: 0.007061376702040434\n",
      "epoch:  713, loss: 0.007056772243231535\n",
      "epoch:  714, loss: 0.00705627491697669\n",
      "epoch:  715, loss: 0.007052871398627758\n",
      "epoch:  716, loss: 0.007052871398627758\n",
      "epoch:  717, loss: 0.007052072789520025\n",
      "epoch:  718, loss: 0.007051818538457155\n",
      "epoch:  719, loss: 0.007037578150629997\n",
      "epoch:  720, loss: 0.007037578150629997\n",
      "epoch:  721, loss: 0.007033800706267357\n",
      "epoch:  722, loss: 0.007033163215965033\n",
      "epoch:  723, loss: 0.007030976936221123\n",
      "epoch:  724, loss: 0.007030976936221123\n",
      "epoch:  725, loss: 0.007028200663626194\n",
      "epoch:  726, loss: 0.007027819287031889\n",
      "epoch:  727, loss: 0.0070250798016786575\n",
      "epoch:  728, loss: 0.0070250798016786575\n",
      "epoch:  729, loss: 0.007024313323199749\n",
      "epoch:  730, loss: 0.007022776175290346\n",
      "epoch:  731, loss: 0.007022776175290346\n",
      "epoch:  732, loss: 0.007021461613476276\n",
      "epoch:  733, loss: 0.007021184545010328\n",
      "epoch:  734, loss: 0.0070158494636416435\n",
      "epoch:  735, loss: 0.0070158494636416435\n",
      "epoch:  736, loss: 0.007000222336500883\n",
      "epoch:  737, loss: 0.0069990456104278564\n",
      "epoch:  738, loss: 0.0069965338334441185\n",
      "epoch:  739, loss: 0.0069965338334441185\n",
      "epoch:  740, loss: 0.006994183175265789\n",
      "epoch:  741, loss: 0.006993851624429226\n",
      "epoch:  742, loss: 0.006971891038119793\n",
      "epoch:  743, loss: 0.006971889641135931\n",
      "epoch:  744, loss: 0.006968108005821705\n",
      "epoch:  745, loss: 0.006968108005821705\n",
      "epoch:  746, loss: 0.006967433262616396\n",
      "epoch:  747, loss: 0.006965260487049818\n",
      "epoch:  748, loss: 0.006965260021388531\n",
      "epoch:  749, loss: 0.00696423789486289\n",
      "epoch:  750, loss: 0.00696375360712409\n",
      "epoch:  751, loss: 0.00696375360712409\n",
      "epoch:  752, loss: 0.00696155708283186\n",
      "epoch:  753, loss: 0.006961212959140539\n",
      "epoch:  754, loss: 0.00695912167429924\n",
      "epoch:  755, loss: 0.00695912167429924\n",
      "epoch:  756, loss: 0.006958304438740015\n",
      "epoch:  757, loss: 0.006958044599741697\n",
      "epoch:  758, loss: 0.006957262754440308\n",
      "epoch:  759, loss: 0.006957262754440308\n",
      "epoch:  760, loss: 0.006955298595130444\n",
      "epoch:  761, loss: 0.006955001503229141\n",
      "epoch:  762, loss: 0.0069531025364995\n",
      "epoch:  763, loss: 0.0069531025364995\n",
      "epoch:  764, loss: 0.00695235887542367\n",
      "epoch:  765, loss: 0.00695195933803916\n",
      "epoch:  766, loss: 0.00695195933803916\n",
      "epoch:  767, loss: 0.0069501823745667934\n",
      "epoch:  768, loss: 0.006949895527213812\n",
      "epoch:  769, loss: 0.00694799842312932\n",
      "epoch:  770, loss: 0.00694799842312932\n",
      "epoch:  771, loss: 0.006947195157408714\n",
      "epoch:  772, loss: 0.0069469851441681385\n",
      "epoch:  773, loss: 0.0069448924623429775\n",
      "epoch:  774, loss: 0.0069448924623429775\n",
      "epoch:  775, loss: 0.006944336928427219\n",
      "epoch:  776, loss: 0.006943760439753532\n",
      "epoch:  777, loss: 0.006943760439753532\n",
      "epoch:  778, loss: 0.006942402105778456\n",
      "epoch:  779, loss: 0.006942151580005884\n",
      "epoch:  780, loss: 0.0069400048814713955\n",
      "epoch:  781, loss: 0.0069400048814713955\n",
      "epoch:  782, loss: 0.006939517799764872\n",
      "epoch:  783, loss: 0.0069387066178023815\n",
      "epoch:  784, loss: 0.0069387066178023815\n",
      "epoch:  785, loss: 0.00693780230358243\n",
      "epoch:  786, loss: 0.006937586702406406\n",
      "epoch:  787, loss: 0.006935585755854845\n",
      "epoch:  788, loss: 0.006935584358870983\n",
      "epoch:  789, loss: 0.006935077253729105\n",
      "epoch:  790, loss: 0.0069344984367489815\n",
      "epoch:  791, loss: 0.0069344984367489815\n",
      "epoch:  792, loss: 0.006933319382369518\n",
      "epoch:  793, loss: 0.0069330609403550625\n",
      "epoch:  794, loss: 0.006931643467396498\n",
      "epoch:  795, loss: 0.006931643467396498\n",
      "epoch:  796, loss: 0.006930472794920206\n",
      "epoch:  797, loss: 0.006930214818567038\n",
      "epoch:  798, loss: 0.006929789669811726\n",
      "epoch:  799, loss: 0.006929789669811726\n",
      "epoch:  800, loss: 0.00692770117893815\n",
      "epoch:  801, loss: 0.006927349139004946\n",
      "epoch:  802, loss: 0.006926966831088066\n",
      "epoch:  803, loss: 0.006926966831088066\n",
      "epoch:  804, loss: 0.006924849934875965\n",
      "epoch:  805, loss: 0.0069244927726686\n",
      "epoch:  806, loss: 0.006923777982592583\n",
      "epoch:  807, loss: 0.006923777982592583\n",
      "epoch:  808, loss: 0.0069220601581037045\n",
      "epoch:  809, loss: 0.0069217258132994175\n",
      "epoch:  810, loss: 0.006921480875462294\n",
      "epoch:  811, loss: 0.006920473650097847\n",
      "epoch:  812, loss: 0.006920473650097847\n",
      "epoch:  813, loss: 0.006907546892762184\n",
      "epoch:  814, loss: 0.006906664930284023\n",
      "epoch:  815, loss: 0.006905689835548401\n",
      "epoch:  816, loss: 0.006905689835548401\n",
      "epoch:  817, loss: 0.006902499124407768\n",
      "epoch:  818, loss: 0.006902148015797138\n",
      "epoch:  819, loss: 0.006900510750710964\n",
      "epoch:  820, loss: 0.006900510750710964\n",
      "epoch:  821, loss: 0.0068987347185611725\n",
      "epoch:  822, loss: 0.0068984562531113625\n",
      "epoch:  823, loss: 0.006897228304296732\n",
      "epoch:  824, loss: 0.006897228304296732\n",
      "epoch:  825, loss: 0.006895189173519611\n",
      "epoch:  826, loss: 0.006894898600876331\n",
      "epoch:  827, loss: 0.006892621982842684\n",
      "epoch:  828, loss: 0.006892621982842684\n",
      "epoch:  829, loss: 0.006891751196235418\n",
      "epoch:  830, loss: 0.0068915290758013725\n",
      "epoch:  831, loss: 0.006889683194458485\n",
      "epoch:  832, loss: 0.006889683194458485\n",
      "epoch:  833, loss: 0.0068884785287082195\n",
      "epoch:  834, loss: 0.006888240110129118\n",
      "epoch:  835, loss: 0.0068860845640301704\n",
      "epoch:  836, loss: 0.0068860845640301704\n",
      "epoch:  837, loss: 0.006885235197842121\n",
      "epoch:  838, loss: 0.006885018199682236\n",
      "epoch:  839, loss: 0.00688296789303422\n",
      "epoch:  840, loss: 0.0068829674273729324\n",
      "epoch:  841, loss: 0.006882122252136469\n",
      "epoch:  842, loss: 0.006881906185299158\n",
      "epoch:  843, loss: 0.006880114786326885\n",
      "epoch:  844, loss: 0.006880113855004311\n",
      "epoch:  845, loss: 0.006879027001559734\n",
      "epoch:  846, loss: 0.006878789514303207\n",
      "epoch:  847, loss: 0.006877545733004808\n",
      "epoch:  848, loss: 0.006877545733004808\n",
      "epoch:  849, loss: 0.006875971332192421\n",
      "epoch:  850, loss: 0.006875695660710335\n",
      "epoch:  851, loss: 0.006874874234199524\n",
      "epoch:  852, loss: 0.006874874234199524\n",
      "epoch:  853, loss: 0.006872962694615126\n",
      "epoch:  854, loss: 0.006872637663036585\n",
      "epoch:  855, loss: 0.006872363854199648\n",
      "epoch:  856, loss: 0.006872357334941626\n",
      "epoch:  857, loss: 0.006872095633298159\n",
      "epoch:  858, loss: 0.006855608429759741\n",
      "epoch:  859, loss: 0.006855608429759741\n",
      "epoch:  860, loss: 0.00685085728764534\n",
      "epoch:  861, loss: 0.006850310135632753\n",
      "epoch:  862, loss: 0.0068491725251078606\n",
      "epoch:  863, loss: 0.0068491725251078606\n",
      "epoch:  864, loss: 0.006846923381090164\n",
      "epoch:  865, loss: 0.006846598349511623\n",
      "epoch:  866, loss: 0.006844368763267994\n",
      "epoch:  867, loss: 0.006844368763267994\n",
      "epoch:  868, loss: 0.006843514274805784\n",
      "epoch:  869, loss: 0.006843309383839369\n",
      "epoch:  870, loss: 0.0068412660621106625\n",
      "epoch:  871, loss: 0.0068412660621106625\n",
      "epoch:  872, loss: 0.006840384099632502\n",
      "epoch:  873, loss: 0.006840154994279146\n",
      "epoch:  874, loss: 0.006839856039732695\n",
      "epoch:  875, loss: 0.006836718879640102\n",
      "epoch:  876, loss: 0.006836718879640102\n",
      "epoch:  877, loss: 0.006818229332566261\n",
      "epoch:  878, loss: 0.006816276349127293\n",
      "epoch:  879, loss: 0.006815803702920675\n",
      "epoch:  880, loss: 0.006815803702920675\n",
      "epoch:  881, loss: 0.0068154530599713326\n",
      "epoch:  882, loss: 0.006801475305110216\n",
      "epoch:  883, loss: 0.006801475305110216\n",
      "epoch:  884, loss: 0.006792296655476093\n",
      "epoch:  885, loss: 0.006791360210627317\n",
      "epoch:  886, loss: 0.006790842395275831\n",
      "epoch:  887, loss: 0.006768071558326483\n",
      "epoch:  888, loss: 0.006768071558326483\n",
      "epoch:  889, loss: 0.006762199569493532\n",
      "epoch:  890, loss: 0.006762199569493532\n",
      "epoch:  891, loss: 0.006760866846889257\n",
      "epoch:  892, loss: 0.006759854033589363\n",
      "epoch:  893, loss: 0.006759854033589363\n",
      "epoch:  894, loss: 0.006757567636668682\n",
      "epoch:  895, loss: 0.0067571853287518024\n",
      "epoch:  896, loss: 0.006756748538464308\n",
      "epoch:  897, loss: 0.006756746210157871\n",
      "epoch:  898, loss: 0.006756178103387356\n",
      "epoch:  899, loss: 0.006741674151271582\n",
      "epoch:  900, loss: 0.006741674151271582\n",
      "epoch:  901, loss: 0.006734577007591724\n",
      "epoch:  902, loss: 0.006734038703143597\n",
      "epoch:  903, loss: 0.006732919719070196\n",
      "epoch:  904, loss: 0.006729504093527794\n",
      "epoch:  905, loss: 0.006729504093527794\n",
      "epoch:  906, loss: 0.006712968461215496\n",
      "epoch:  907, loss: 0.00671098567545414\n",
      "epoch:  908, loss: 0.006709134206175804\n",
      "epoch:  909, loss: 0.006705956533551216\n",
      "epoch:  910, loss: 0.006705812644213438\n",
      "epoch:  911, loss: 0.006705812644213438\n",
      "epoch:  912, loss: 0.006704296451061964\n",
      "epoch:  913, loss: 0.006701878271996975\n",
      "epoch:  914, loss: 0.006701878271996975\n",
      "epoch:  915, loss: 0.006691154092550278\n",
      "epoch:  916, loss: 0.006690133363008499\n",
      "epoch:  917, loss: 0.0066888825967907906\n",
      "epoch:  918, loss: 0.006681709084659815\n",
      "epoch:  919, loss: 0.0066801561042666435\n",
      "epoch:  920, loss: 0.0066801561042666435\n",
      "epoch:  921, loss: 0.006678172852844\n",
      "epoch:  922, loss: 0.00667727692052722\n",
      "epoch:  923, loss: 0.0066772764548659325\n",
      "epoch:  924, loss: 0.006674055941402912\n",
      "epoch:  925, loss: 0.006673514377325773\n",
      "epoch:  926, loss: 0.006673514377325773\n",
      "epoch:  927, loss: 0.006669163703918457\n",
      "epoch:  928, loss: 0.006669163703918457\n",
      "epoch:  929, loss: 0.006667435169219971\n",
      "epoch:  930, loss: 0.006666939239948988\n",
      "epoch:  931, loss: 0.006666939239948988\n",
      "epoch:  932, loss: 0.006663610227406025\n",
      "epoch:  933, loss: 0.006663047708570957\n",
      "epoch:  934, loss: 0.006662554573267698\n",
      "epoch:  935, loss: 0.006662533152848482\n",
      "epoch:  936, loss: 0.006662533152848482\n",
      "epoch:  937, loss: 0.006661851424723864\n",
      "epoch:  938, loss: 0.006638970226049423\n",
      "epoch:  939, loss: 0.006638970226049423\n",
      "epoch:  940, loss: 0.006631535477936268\n",
      "epoch:  941, loss: 0.006630735471844673\n",
      "epoch:  942, loss: 0.006629962474107742\n",
      "epoch:  943, loss: 0.006629962474107742\n",
      "epoch:  944, loss: 0.006625493988394737\n",
      "epoch:  945, loss: 0.006625493988394737\n",
      "epoch:  946, loss: 0.006623465567827225\n",
      "epoch:  947, loss: 0.006622493267059326\n",
      "epoch:  948, loss: 0.006622493267059326\n",
      "epoch:  949, loss: 0.006619178224354982\n",
      "epoch:  950, loss: 0.006618565414100885\n",
      "epoch:  951, loss: 0.006618006154894829\n",
      "epoch:  952, loss: 0.006614625453948975\n",
      "epoch:  953, loss: 0.006614625453948975\n",
      "epoch:  954, loss: 0.006613448727875948\n",
      "epoch:  955, loss: 0.00661314744502306\n",
      "epoch:  956, loss: 0.0066093020141124725\n",
      "epoch:  957, loss: 0.0066093020141124725\n",
      "epoch:  958, loss: 0.006607691757380962\n",
      "epoch:  959, loss: 0.006607370916754007\n",
      "epoch:  960, loss: 0.006604733411222696\n",
      "epoch:  961, loss: 0.006604733411222696\n",
      "epoch:  962, loss: 0.006601942703127861\n",
      "epoch:  963, loss: 0.006601480767130852\n",
      "epoch:  964, loss: 0.006599424872547388\n",
      "epoch:  965, loss: 0.006599424872547388\n",
      "epoch:  966, loss: 0.006596163380891085\n",
      "epoch:  967, loss: 0.0065956260077655315\n",
      "epoch:  968, loss: 0.006595088168978691\n",
      "epoch:  969, loss: 0.006595067214220762\n",
      "epoch:  970, loss: 0.006595066748559475\n",
      "epoch:  971, loss: 0.006594356149435043\n",
      "epoch:  972, loss: 0.006592379882931709\n",
      "epoch:  973, loss: 0.006592379882931709\n",
      "epoch:  974, loss: 0.0065908730030059814\n",
      "epoch:  975, loss: 0.006590513978153467\n",
      "epoch:  976, loss: 0.006587371230125427\n",
      "epoch:  977, loss: 0.006587371230125427\n",
      "epoch:  978, loss: 0.006585369817912579\n",
      "epoch:  979, loss: 0.006584937684237957\n",
      "epoch:  980, loss: 0.006583871319890022\n",
      "epoch:  981, loss: 0.006583871319890022\n",
      "epoch:  982, loss: 0.00658002682030201\n",
      "epoch:  983, loss: 0.0065793246030807495\n",
      "epoch:  984, loss: 0.006578763946890831\n",
      "epoch:  985, loss: 0.006577393505722284\n",
      "epoch:  986, loss: 0.006577393505722284\n",
      "epoch:  987, loss: 0.006575120147317648\n",
      "epoch:  988, loss: 0.00657467357814312\n",
      "epoch:  989, loss: 0.006574138533324003\n",
      "epoch:  990, loss: 0.006573894061148167\n",
      "epoch:  991, loss: 0.006573894061148167\n",
      "epoch:  992, loss: 0.006573325954377651\n",
      "epoch:  993, loss: 0.0065704709850251675\n",
      "epoch:  994, loss: 0.00657047051936388\n",
      "epoch:  995, loss: 0.006569432560354471\n",
      "epoch:  996, loss: 0.006569121032953262\n",
      "epoch:  997, loss: 0.006566151976585388\n",
      "epoch:  998, loss: 0.006566151976585388\n",
      "epoch:  999, loss: 0.006563968025147915\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size=X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# opt = torch_numopt.ConjugateGradient(model=model, lr=1e-4, line_search_method=\"const\", cg_method=\"PR\")\n",
    "opt = torch_numopt.ConjugateGradient(model=model, lr=10, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"armijo\", cg_method=\"PR\")\n",
    "# opt = torch_numopt.ConjugateGradient(model=model, lr=10, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"wolfe\", cg_method=\"PR\")\n",
    "# opt = torch_numopt.ConjugateGradient(model=model, lr=10, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"strong-wolfe\", cg_method=\"PR\")\n",
    "# opt = torch_numopt.ConjugateGradient(model=model, lr=10, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"goldstein\", cg_method=\"PR\")\n",
    "\n",
    "\n",
    "all_loss = {}\n",
    "for epoch in range(1000):\n",
    "    print(\"epoch: \", epoch, end=\"\")\n",
    "    all_loss[epoch + 1] = 0\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y, loss_fn)\n",
    "\n",
    "        all_loss[epoch + 1] += loss\n",
    "    all_loss[epoch + 1] /= len(data_loader)\n",
    "    print(\", loss: {}\".format(all_loss[epoch + 1].detach().numpy().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics: R2 = 0.7252108677464713\n",
      "Test metrics:  R2 = 0.6898091145881076\n"
     ]
    }
   ],
   "source": [
    "pred_train = model.forward(X_train).detach()\n",
    "pred_test = model.forward(X_test).detach()\n",
    "print(f\"Train metrics: R2 = {r2_score(pred_train, y_train)}\")\n",
    "print(f\"Test metrics:  R2 = {r2_score(pred_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.8062394857406616\n",
      "epoch:  1, loss: 0.4199889302253723\n",
      "epoch:  2, loss: 0.4199889302253723\n",
      "epoch:  3, loss: 0.2094123512506485\n",
      "epoch:  4, loss: 0.2094123512506485\n",
      "epoch:  5, loss: 0.06591888517141342\n",
      "epoch:  6, loss: 0.028708448633551598\n",
      "epoch:  7, loss: 0.027985699474811554\n",
      "epoch:  8, loss: 0.027625033631920815\n",
      "epoch:  9, loss: 0.027625033631920815\n",
      "epoch:  10, loss: 0.02762504294514656\n",
      "epoch:  11, loss: 0.02762504294514656\n",
      "epoch:  12, loss: 0.02762628346681595\n",
      "epoch:  13, loss: 0.02745325118303299\n",
      "epoch:  14, loss: 0.02673564851284027\n",
      "epoch:  15, loss: 0.02653760462999344\n",
      "epoch:  16, loss: 0.02653760462999344\n",
      "epoch:  17, loss: 0.026541614904999733\n",
      "epoch:  18, loss: 0.02642006427049637\n",
      "epoch:  19, loss: 0.02642006427049637\n",
      "epoch:  20, loss: 0.02642008475959301\n",
      "epoch:  21, loss: 0.026124833151698112\n",
      "epoch:  22, loss: 0.026124833151698112\n",
      "epoch:  23, loss: 0.026125632226467133\n",
      "epoch:  24, loss: 0.026125632226467133\n",
      "epoch:  25, loss: 0.02153407409787178\n",
      "epoch:  26, loss: 0.01514775026589632\n",
      "epoch:  27, loss: 0.014070914126932621\n",
      "epoch:  28, loss: 0.014070914126932621\n",
      "epoch:  29, loss: 0.01355926413089037\n",
      "epoch:  30, loss: 0.01355926413089037\n",
      "epoch:  31, loss: 0.013563912361860275\n",
      "epoch:  32, loss: 0.012896602973341942\n",
      "epoch:  33, loss: 0.012824063189327717\n",
      "epoch:  34, loss: 0.012821928597986698\n",
      "epoch:  35, loss: 0.012821928597986698\n",
      "epoch:  36, loss: 0.012821928597986698\n",
      "epoch:  37, loss: 0.012821928597986698\n",
      "epoch:  38, loss: 0.012821928597986698\n",
      "epoch:  39, loss: 0.012821928597986698\n",
      "epoch:  40, loss: 0.012821928597986698\n",
      "epoch:  41, loss: 0.012821926735341549\n",
      "epoch:  42, loss: 0.012821926735341549\n",
      "epoch:  43, loss: 0.01276563759893179\n",
      "epoch:  44, loss: 0.01276563759893179\n",
      "epoch:  45, loss: 0.012699753977358341\n",
      "epoch:  46, loss: 0.01262942235916853\n",
      "epoch:  47, loss: 0.01262942235916853\n",
      "epoch:  48, loss: 0.012629534117877483\n",
      "epoch:  49, loss: 0.012561611831188202\n",
      "epoch:  50, loss: 0.012561611831188202\n",
      "epoch:  51, loss: 0.012556534260511398\n",
      "epoch:  52, loss: 0.012488359585404396\n",
      "epoch:  53, loss: 0.012488359585404396\n",
      "epoch:  54, loss: 0.012488389387726784\n",
      "epoch:  55, loss: 0.012452976778149605\n",
      "epoch:  56, loss: 0.012452976778149605\n",
      "epoch:  57, loss: 0.012444959953427315\n",
      "epoch:  58, loss: 0.012176275253295898\n",
      "epoch:  59, loss: 0.012176274321973324\n",
      "epoch:  60, loss: 0.011820617131888866\n",
      "epoch:  61, loss: 0.011785642243921757\n",
      "epoch:  62, loss: 0.01173961441963911\n",
      "epoch:  63, loss: 0.010656017810106277\n",
      "epoch:  64, loss: 0.010656017810106277\n",
      "epoch:  65, loss: 0.010014403611421585\n",
      "epoch:  66, loss: 0.010014403611421585\n",
      "epoch:  67, loss: 0.009629106149077415\n",
      "epoch:  68, loss: 0.009629106149077415\n",
      "epoch:  69, loss: 0.8836723566055298\n",
      "epoch:  70, loss: 0.8836723566055298\n",
      "epoch:  71, loss: 0.9332926869392395\n",
      "epoch:  72, loss: 0.03961962088942528\n",
      "epoch:  73, loss: 0.03437686339020729\n",
      "epoch:  74, loss: 0.03437686339020729\n",
      "epoch:  75, loss: 0.032172009348869324\n",
      "epoch:  76, loss: 0.0297074131667614\n",
      "epoch:  77, loss: 0.027859441936016083\n",
      "epoch:  78, loss: 0.02783496305346489\n",
      "epoch:  79, loss: 0.02783496305346489\n",
      "epoch:  80, loss: 0.027834966778755188\n",
      "epoch:  81, loss: 0.027834966778755188\n",
      "epoch:  82, loss: 0.027835005894303322\n",
      "epoch:  83, loss: 0.027775846421718597\n",
      "epoch:  84, loss: 0.027743665501475334\n",
      "epoch:  85, loss: 0.027641015127301216\n",
      "epoch:  86, loss: 0.02755274996161461\n",
      "epoch:  87, loss: 0.02755233459174633\n",
      "epoch:  88, loss: 0.025824155658483505\n",
      "epoch:  89, loss: 0.025824155658483505\n",
      "epoch:  90, loss: 0.019256407395005226\n",
      "epoch:  91, loss: 0.01785464584827423\n",
      "epoch:  92, loss: 0.015629885718226433\n",
      "epoch:  93, loss: 0.015439879149198532\n",
      "epoch:  94, loss: 0.01520348060876131\n",
      "epoch:  95, loss: 0.01520348060876131\n",
      "epoch:  96, loss: 0.01510712131857872\n",
      "epoch:  97, loss: 0.014802814461290836\n",
      "epoch:  98, loss: 0.014802814461290836\n",
      "epoch:  99, loss: 0.014802820980548859\n",
      "epoch:  100, loss: 0.01407044380903244\n",
      "epoch:  101, loss: 0.01407044380903244\n",
      "epoch:  102, loss: 0.013595769181847572\n",
      "epoch:  103, loss: 0.013045437633991241\n",
      "epoch:  104, loss: 0.013045437633991241\n",
      "epoch:  105, loss: 0.012816649861633778\n",
      "epoch:  106, loss: 0.011507618241012096\n",
      "epoch:  107, loss: 0.01138980034738779\n",
      "epoch:  108, loss: 0.01086112018674612\n",
      "epoch:  109, loss: 0.010574131272733212\n",
      "epoch:  110, loss: 0.010177841410040855\n",
      "epoch:  111, loss: 0.009935633279383183\n",
      "epoch:  112, loss: 0.009935633279383183\n",
      "epoch:  113, loss: 0.009824897162616253\n",
      "epoch:  114, loss: 0.009824897162616253\n",
      "epoch:  115, loss: 0.009737015701830387\n",
      "epoch:  116, loss: 0.009664981625974178\n",
      "epoch:  117, loss: 0.009662187658250332\n",
      "epoch:  118, loss: 0.009662186726927757\n",
      "epoch:  119, loss: 0.009638688527047634\n",
      "epoch:  120, loss: 0.009552991017699242\n",
      "epoch:  121, loss: 0.009535444900393486\n",
      "epoch:  122, loss: 0.009480836801230907\n",
      "epoch:  123, loss: 0.009410018101334572\n",
      "epoch:  124, loss: 0.00931642297655344\n",
      "epoch:  125, loss: 0.009264071471989155\n",
      "epoch:  126, loss: 0.009264071471989155\n",
      "epoch:  127, loss: 0.009214596822857857\n",
      "epoch:  128, loss: 0.008891521953046322\n",
      "epoch:  129, loss: 0.00880347192287445\n",
      "epoch:  130, loss: 0.008591369725763798\n",
      "epoch:  131, loss: 0.008591369725763798\n",
      "epoch:  132, loss: 0.00858363602310419\n",
      "epoch:  133, loss: 0.008510991930961609\n",
      "epoch:  134, loss: 0.008510991930961609\n",
      "epoch:  135, loss: 0.008511008694767952\n",
      "epoch:  136, loss: 0.008287264965474606\n",
      "epoch:  137, loss: 0.008192788809537888\n",
      "epoch:  138, loss: 0.008192733861505985\n",
      "epoch:  139, loss: 0.008192733861505985\n",
      "epoch:  140, loss: 0.008192733861505985\n",
      "epoch:  141, loss: 0.00819274690002203\n",
      "epoch:  142, loss: 0.008181337267160416\n",
      "epoch:  143, loss: 0.008172904141247272\n",
      "epoch:  144, loss: 0.008169562555849552\n",
      "epoch:  145, loss: 0.008169562555849552\n",
      "epoch:  146, loss: 0.008162636309862137\n",
      "epoch:  147, loss: 0.008152531459927559\n",
      "epoch:  148, loss: 0.007999890483915806\n",
      "epoch:  149, loss: 0.007911484688520432\n",
      "epoch:  150, loss: 0.007894176989793777\n",
      "epoch:  151, loss: 0.007867586798965931\n",
      "epoch:  152, loss: 0.007861245423555374\n",
      "epoch:  153, loss: 0.00785344559699297\n",
      "epoch:  154, loss: 0.007843001745641232\n",
      "epoch:  155, loss: 0.007839019410312176\n",
      "epoch:  156, loss: 0.007830053567886353\n",
      "epoch:  157, loss: 0.007829553447663784\n",
      "epoch:  158, loss: 0.007829553447663784\n",
      "epoch:  159, loss: 0.007829566486179829\n",
      "epoch:  160, loss: 0.007826931774616241\n",
      "epoch:  161, loss: 0.007824317552149296\n",
      "epoch:  162, loss: 0.007824317552149296\n",
      "epoch:  163, loss: 0.007805428467690945\n",
      "epoch:  164, loss: 0.007794011849910021\n",
      "epoch:  165, loss: 0.00779237225651741\n",
      "epoch:  166, loss: 0.00779237225651741\n",
      "epoch:  167, loss: 0.007761103566735983\n",
      "epoch:  168, loss: 0.007759121246635914\n",
      "epoch:  169, loss: 0.007756891194730997\n",
      "epoch:  170, loss: 0.007756891194730997\n",
      "epoch:  171, loss: 0.007756891194730997\n",
      "epoch:  172, loss: 0.007756891194730997\n",
      "epoch:  173, loss: 0.00775047717615962\n",
      "epoch:  174, loss: 0.007750402670353651\n",
      "epoch:  175, loss: 0.007741807494312525\n",
      "epoch:  176, loss: 0.007735431659966707\n",
      "epoch:  177, loss: 0.007733533158898354\n",
      "epoch:  178, loss: 0.0077332849614322186\n",
      "epoch:  179, loss: 0.007733284030109644\n",
      "epoch:  180, loss: 0.007733284030109644\n",
      "epoch:  181, loss: 0.007730259094387293\n",
      "epoch:  182, loss: 0.007730009034276009\n",
      "epoch:  183, loss: 0.007729657460004091\n",
      "epoch:  184, loss: 0.007729656528681517\n",
      "epoch:  185, loss: 0.007725032977759838\n",
      "epoch:  186, loss: 0.007723556831479073\n",
      "epoch:  187, loss: 0.007722772192209959\n",
      "epoch:  188, loss: 0.007722771726548672\n",
      "epoch:  189, loss: 0.007718608248978853\n",
      "epoch:  190, loss: 0.007715265266597271\n",
      "epoch:  191, loss: 0.00771297886967659\n",
      "epoch:  192, loss: 0.007712977938354015\n",
      "epoch:  193, loss: 0.007710959762334824\n",
      "epoch:  194, loss: 0.0077097462490201\n",
      "epoch:  195, loss: 0.007709500379860401\n",
      "epoch:  196, loss: 0.007709500379860401\n",
      "epoch:  197, loss: 0.00769064761698246\n",
      "epoch:  198, loss: 0.007690039929002523\n",
      "epoch:  199, loss: 0.0076887477189302444\n",
      "epoch:  200, loss: 0.0076887477189302444\n",
      "epoch:  201, loss: 0.007688747253268957\n",
      "epoch:  202, loss: 0.007688747253268957\n",
      "epoch:  203, loss: 0.007688659243285656\n",
      "epoch:  204, loss: 0.007688208017498255\n",
      "epoch:  205, loss: 0.007688133046030998\n",
      "epoch:  206, loss: 0.007688133046030998\n",
      "epoch:  207, loss: 0.007688133046030998\n",
      "epoch:  208, loss: 0.007688133046030998\n",
      "epoch:  209, loss: 0.007688133046030998\n",
      "epoch:  210, loss: 0.007688133046030998\n",
      "epoch:  211, loss: 0.0076881349086761475\n",
      "epoch:  212, loss: 0.007688126992434263\n",
      "epoch:  213, loss: 0.007688126992434263\n",
      "epoch:  214, loss: 0.007688127923756838\n",
      "epoch:  215, loss: 0.007685060612857342\n",
      "epoch:  216, loss: 0.007684728596359491\n",
      "epoch:  217, loss: 0.007684674113988876\n",
      "epoch:  218, loss: 0.00768457492813468\n",
      "epoch:  219, loss: 0.007678255904465914\n",
      "epoch:  220, loss: 0.007670387160032988\n",
      "epoch:  221, loss: 0.007668893784284592\n",
      "epoch:  222, loss: 0.007666404824703932\n",
      "epoch:  223, loss: 0.0076654148288071156\n",
      "epoch:  224, loss: 0.0076654148288071156\n",
      "epoch:  225, loss: 0.007665425539016724\n",
      "epoch:  226, loss: 0.007661791052669287\n",
      "epoch:  227, loss: 0.007661591749638319\n",
      "epoch:  228, loss: 0.007661591283977032\n",
      "epoch:  229, loss: 0.007661591283977032\n",
      "epoch:  230, loss: 0.007661591283977032\n",
      "epoch:  231, loss: 0.007661591283977032\n",
      "epoch:  232, loss: 0.00766098964959383\n",
      "epoch:  233, loss: 0.007660161703824997\n",
      "epoch:  234, loss: 0.007660161703824997\n",
      "epoch:  235, loss: 0.0076601626351475716\n",
      "epoch:  236, loss: 0.0076600308530032635\n",
      "epoch:  237, loss: 0.0076600308530032635\n",
      "epoch:  238, loss: 0.007660031318664551\n",
      "epoch:  239, loss: 0.0076590776443481445\n",
      "epoch:  240, loss: 0.0076590776443481445\n",
      "epoch:  241, loss: 0.0076590776443481445\n",
      "epoch:  242, loss: 0.0076590776443481445\n",
      "epoch:  243, loss: 0.0076590790413320065\n",
      "epoch:  244, loss: 0.00765490485355258\n",
      "epoch:  245, loss: 0.007654903456568718\n",
      "epoch:  246, loss: 0.007654903456568718\n",
      "epoch:  247, loss: 0.007654903922230005\n",
      "epoch:  248, loss: 0.007654483430087566\n",
      "epoch:  249, loss: 0.007654483430087566\n",
      "epoch:  250, loss: 0.007653229869902134\n",
      "epoch:  251, loss: 0.0076516661792993546\n",
      "epoch:  252, loss: 0.007646384183317423\n",
      "epoch:  253, loss: 0.00764604052528739\n",
      "epoch:  254, loss: 0.00764604052528739\n",
      "epoch:  255, loss: 0.00764604052528739\n",
      "epoch:  256, loss: 0.00764604052528739\n",
      "epoch:  257, loss: 0.007636330556124449\n",
      "epoch:  258, loss: 0.007635395508259535\n",
      "epoch:  259, loss: 0.0076351952739059925\n",
      "epoch:  260, loss: 0.0076351952739059925\n",
      "epoch:  261, loss: 0.0076351952739059925\n",
      "epoch:  262, loss: 0.007585030980408192\n",
      "epoch:  263, loss: 0.007574863266199827\n",
      "epoch:  264, loss: 0.007556781638413668\n",
      "epoch:  265, loss: 0.007554496638476849\n",
      "epoch:  266, loss: 0.0075495122000575066\n",
      "epoch:  267, loss: 0.007549211848527193\n",
      "epoch:  268, loss: 0.0075483741238713264\n",
      "epoch:  269, loss: 0.007547981571406126\n",
      "epoch:  270, loss: 0.007547981571406126\n",
      "epoch:  271, loss: 0.007547981571406126\n",
      "epoch:  272, loss: 0.007547585293650627\n",
      "epoch:  273, loss: 0.007546402979642153\n",
      "epoch:  274, loss: 0.007546402979642153\n",
      "epoch:  275, loss: 0.007545988075435162\n",
      "epoch:  276, loss: 0.007545988075435162\n",
      "epoch:  277, loss: 0.007545988075435162\n",
      "epoch:  278, loss: 0.00754397502169013\n",
      "epoch:  279, loss: 0.0075418371707201\n",
      "epoch:  280, loss: 0.007541012018918991\n",
      "epoch:  281, loss: 0.0075409384444355965\n",
      "epoch:  282, loss: 0.0075409384444355965\n",
      "epoch:  283, loss: 0.0075409384444355965\n",
      "epoch:  284, loss: 0.007540745660662651\n",
      "epoch:  285, loss: 0.0075339083559811115\n",
      "epoch:  286, loss: 0.0075339083559811115\n",
      "epoch:  287, loss: 0.007534588687121868\n",
      "epoch:  288, loss: 0.007516621146351099\n",
      "epoch:  289, loss: 0.007515296805649996\n",
      "epoch:  290, loss: 0.007515052333474159\n",
      "epoch:  291, loss: 0.007513408549129963\n",
      "epoch:  292, loss: 0.007513408083468676\n",
      "epoch:  293, loss: 0.007513408083468676\n",
      "epoch:  294, loss: 0.007513408083468676\n",
      "epoch:  295, loss: 0.007509549148380756\n",
      "epoch:  296, loss: 0.007508948445320129\n",
      "epoch:  297, loss: 0.007508948445320129\n",
      "epoch:  298, loss: 0.007508948445320129\n",
      "epoch:  299, loss: 0.007508944720029831\n",
      "epoch:  300, loss: 0.007508944720029831\n",
      "epoch:  301, loss: 0.007508944720029831\n",
      "epoch:  302, loss: 0.007508944720029831\n",
      "epoch:  303, loss: 0.007508944254368544\n",
      "epoch:  304, loss: 0.007508944254368544\n",
      "epoch:  305, loss: 0.007508944254368544\n",
      "epoch:  306, loss: 0.007508944254368544\n",
      "epoch:  307, loss: 0.007508944254368544\n",
      "epoch:  308, loss: 0.007508809678256512\n",
      "epoch:  309, loss: 0.00750725157558918\n",
      "epoch:  310, loss: 0.00750725157558918\n",
      "epoch:  311, loss: 0.00750725157558918\n",
      "epoch:  312, loss: 0.00750725157558918\n",
      "epoch:  313, loss: 0.007507252506911755\n",
      "epoch:  314, loss: 0.007507252506911755\n",
      "epoch:  315, loss: 0.007507058791816235\n",
      "epoch:  316, loss: 0.007506894879043102\n",
      "epoch:  317, loss: 0.007506666239351034\n",
      "epoch:  318, loss: 0.007506666239351034\n",
      "epoch:  319, loss: 0.007499038707464933\n",
      "epoch:  320, loss: 0.007497076876461506\n",
      "epoch:  321, loss: 0.007496678736060858\n",
      "epoch:  322, loss: 0.007495722267776728\n",
      "epoch:  323, loss: 0.007495722267776728\n",
      "epoch:  324, loss: 0.007495722733438015\n",
      "epoch:  325, loss: 0.007495427038520575\n",
      "epoch:  326, loss: 0.007495032157748938\n",
      "epoch:  327, loss: 0.007494934368878603\n",
      "epoch:  328, loss: 0.007494909688830376\n",
      "epoch:  329, loss: 0.007494909688830376\n",
      "epoch:  330, loss: 0.007494887802749872\n",
      "epoch:  331, loss: 0.007494000252336264\n",
      "epoch:  332, loss: 0.007494000252336264\n",
      "epoch:  333, loss: 0.007493988145142794\n",
      "epoch:  334, loss: 0.007493774872273207\n",
      "epoch:  335, loss: 0.007493712939321995\n",
      "epoch:  336, loss: 0.007493712939321995\n",
      "epoch:  337, loss: 0.007493695244193077\n",
      "epoch:  338, loss: 0.007493053562939167\n",
      "epoch:  339, loss: 0.00749305309727788\n",
      "epoch:  340, loss: 0.007493052631616592\n",
      "epoch:  341, loss: 0.007493052631616592\n",
      "epoch:  342, loss: 0.00749305309727788\n",
      "epoch:  343, loss: 0.007492816541343927\n",
      "epoch:  344, loss: 0.007492723874747753\n",
      "epoch:  345, loss: 0.007492723874747753\n",
      "epoch:  346, loss: 0.007492723874747753\n",
      "epoch:  347, loss: 0.007491040043532848\n",
      "epoch:  348, loss: 0.007490905467420816\n",
      "epoch:  349, loss: 0.007487807888537645\n",
      "epoch:  350, loss: 0.007487807888537645\n",
      "epoch:  351, loss: 0.0074878353625535965\n",
      "epoch:  352, loss: 0.007480910513550043\n",
      "epoch:  353, loss: 0.007480754051357508\n",
      "epoch:  354, loss: 0.007479980122298002\n",
      "epoch:  355, loss: 0.00747962249442935\n",
      "epoch:  356, loss: 0.007479621563106775\n",
      "epoch:  357, loss: 0.007479621563106775\n",
      "epoch:  358, loss: 0.0074789500795304775\n",
      "epoch:  359, loss: 0.007478773593902588\n",
      "epoch:  360, loss: 0.0074732000939548016\n",
      "epoch:  361, loss: 0.0074732000939548016\n",
      "epoch:  362, loss: 0.00747261568903923\n",
      "epoch:  363, loss: 0.00747261568903923\n",
      "epoch:  364, loss: 0.007472523953765631\n",
      "epoch:  365, loss: 0.007468423806130886\n",
      "epoch:  366, loss: 0.007467735558748245\n",
      "epoch:  367, loss: 0.007467694114893675\n",
      "epoch:  368, loss: 0.00746746314689517\n",
      "epoch:  369, loss: 0.00746746314689517\n",
      "epoch:  370, loss: 0.007467247080057859\n",
      "epoch:  371, loss: 0.007466946728527546\n",
      "epoch:  372, loss: 0.0074662030674517155\n",
      "epoch:  373, loss: 0.0074662030674517155\n",
      "epoch:  374, loss: 0.0074662030674517155\n",
      "epoch:  375, loss: 0.0074662030674517155\n",
      "epoch:  376, loss: 0.0074662030674517155\n",
      "epoch:  377, loss: 0.0074662030674517155\n",
      "epoch:  378, loss: 0.007466203533113003\n",
      "epoch:  379, loss: 0.007466098293662071\n",
      "epoch:  380, loss: 0.007465357892215252\n",
      "epoch:  381, loss: 0.00746535649523139\n",
      "epoch:  382, loss: 0.00746535649523139\n",
      "epoch:  383, loss: 0.0074646854773163795\n",
      "epoch:  384, loss: 0.0074639953672885895\n",
      "epoch:  385, loss: 0.007463925052434206\n",
      "epoch:  386, loss: 0.007463925052434206\n",
      "epoch:  387, loss: 0.007463925052434206\n",
      "epoch:  388, loss: 0.007463925052434206\n",
      "epoch:  389, loss: 0.007463808171451092\n",
      "epoch:  390, loss: 0.007463451009243727\n",
      "epoch:  391, loss: 0.007463450543582439\n",
      "epoch:  392, loss: 0.007463450543582439\n",
      "epoch:  393, loss: 0.007463450543582439\n",
      "epoch:  394, loss: 0.007463450543582439\n",
      "epoch:  395, loss: 0.007463450543582439\n",
      "epoch:  396, loss: 0.007463042624294758\n",
      "epoch:  397, loss: 0.007463042624294758\n",
      "epoch:  398, loss: 0.007462833542376757\n",
      "epoch:  399, loss: 0.007462684065103531\n",
      "epoch:  400, loss: 0.007462680339813232\n",
      "epoch:  401, loss: 0.007462573703378439\n",
      "epoch:  402, loss: 0.007461742963641882\n",
      "epoch:  403, loss: 0.007461742963641882\n",
      "epoch:  404, loss: 0.007461669389158487\n",
      "epoch:  405, loss: 0.007461669389158487\n",
      "epoch:  406, loss: 0.0074615878984332085\n",
      "epoch:  407, loss: 0.0074607557617127895\n",
      "epoch:  408, loss: 0.007460517808794975\n",
      "epoch:  409, loss: 0.0074603138491511345\n",
      "epoch:  410, loss: 0.0074603138491511345\n",
      "epoch:  411, loss: 0.0074603138491511345\n",
      "epoch:  412, loss: 0.007453722879290581\n",
      "epoch:  413, loss: 0.007452844176441431\n",
      "epoch:  414, loss: 0.007452532649040222\n",
      "epoch:  415, loss: 0.007452532649040222\n",
      "epoch:  416, loss: 0.007452548015862703\n",
      "epoch:  417, loss: 0.007452548015862703\n",
      "epoch:  418, loss: 0.007452548015862703\n",
      "epoch:  419, loss: 0.007452273275703192\n",
      "epoch:  420, loss: 0.007451779209077358\n",
      "epoch:  421, loss: 0.007451779209077358\n",
      "epoch:  422, loss: 0.00745127908885479\n",
      "epoch:  423, loss: 0.007449719123542309\n",
      "epoch:  424, loss: 0.007449039723724127\n",
      "epoch:  425, loss: 0.007448758464306593\n",
      "epoch:  426, loss: 0.007448678836226463\n",
      "epoch:  427, loss: 0.007448678836226463\n",
      "epoch:  428, loss: 0.007448677904903889\n",
      "epoch:  429, loss: 0.0074474262073636055\n",
      "epoch:  430, loss: 0.0074474262073636055\n",
      "epoch:  431, loss: 0.007446978706866503\n",
      "epoch:  432, loss: 0.007446052972227335\n",
      "epoch:  433, loss: 0.007445401046425104\n",
      "epoch:  434, loss: 0.007445401046425104\n",
      "epoch:  435, loss: 0.0098897535353899\n",
      "epoch:  436, loss: 0.008247136138379574\n",
      "epoch:  437, loss: 0.007530295290052891\n",
      "epoch:  438, loss: 0.0075253383256495\n",
      "epoch:  439, loss: 0.007520264480262995\n",
      "epoch:  440, loss: 0.007518315222114325\n",
      "epoch:  441, loss: 0.0075129796750843525\n",
      "epoch:  442, loss: 0.0075129796750843525\n",
      "epoch:  443, loss: 0.007481136824935675\n",
      "epoch:  444, loss: 0.007479532156139612\n",
      "epoch:  445, loss: 0.007479532156139612\n",
      "epoch:  446, loss: 0.0074790590442717075\n",
      "epoch:  447, loss: 0.007472740951925516\n",
      "epoch:  448, loss: 0.00746585987508297\n",
      "epoch:  449, loss: 0.007465073373168707\n",
      "epoch:  450, loss: 0.007465073373168707\n",
      "epoch:  451, loss: 0.007465073373168707\n",
      "epoch:  452, loss: 0.007465073373168707\n",
      "epoch:  453, loss: 0.007465073373168707\n",
      "epoch:  454, loss: 0.007465073373168707\n",
      "epoch:  455, loss: 0.007465073373168707\n",
      "epoch:  456, loss: 0.007465073373168707\n",
      "epoch:  457, loss: 0.007465073373168707\n",
      "epoch:  458, loss: 0.007465073373168707\n",
      "epoch:  459, loss: 0.007465073373168707\n",
      "epoch:  460, loss: 0.007465073373168707\n",
      "epoch:  461, loss: 0.007465073373168707\n",
      "epoch:  462, loss: 0.007461996749043465\n",
      "epoch:  463, loss: 0.007461996749043465\n",
      "epoch:  464, loss: 0.007461996749043465\n",
      "epoch:  465, loss: 0.007461996749043465\n",
      "epoch:  466, loss: 0.007461996749043465\n",
      "epoch:  467, loss: 0.007461996749043465\n",
      "epoch:  468, loss: 0.007461996749043465\n",
      "epoch:  469, loss: 0.007461997680366039\n",
      "epoch:  470, loss: 0.007459534332156181\n",
      "epoch:  471, loss: 0.007456738967448473\n",
      "epoch:  472, loss: 0.007456734776496887\n",
      "epoch:  473, loss: 0.007456734776496887\n",
      "epoch:  474, loss: 0.007456651888787746\n",
      "epoch:  475, loss: 0.007456297054886818\n",
      "epoch:  476, loss: 0.007456296589225531\n",
      "epoch:  477, loss: 0.007456296589225531\n",
      "epoch:  478, loss: 0.007456296589225531\n",
      "epoch:  479, loss: 0.007454051170498133\n",
      "epoch:  480, loss: 0.007453663740307093\n",
      "epoch:  481, loss: 0.007450273260474205\n",
      "epoch:  482, loss: 0.007450273260474205\n",
      "epoch:  483, loss: 0.0074487547390162945\n",
      "epoch:  484, loss: 0.007447880692780018\n",
      "epoch:  485, loss: 0.007447587326169014\n",
      "epoch:  486, loss: 0.00744262570515275\n",
      "epoch:  487, loss: 0.007442625239491463\n",
      "epoch:  488, loss: 0.007442625239491463\n",
      "epoch:  489, loss: 0.007442624773830175\n",
      "epoch:  490, loss: 0.007442624773830175\n",
      "epoch:  491, loss: 0.007442197296768427\n",
      "epoch:  492, loss: 0.007439870852977037\n",
      "epoch:  493, loss: 0.007439121138304472\n",
      "epoch:  494, loss: 0.007438414264470339\n",
      "epoch:  495, loss: 0.007438413333147764\n",
      "epoch:  496, loss: 0.007438413333147764\n",
      "epoch:  497, loss: 0.007438413333147764\n",
      "epoch:  498, loss: 0.007435467094182968\n",
      "epoch:  499, loss: 0.007434855680912733\n",
      "epoch:  500, loss: 0.007434727158397436\n",
      "epoch:  501, loss: 0.00743437185883522\n",
      "epoch:  502, loss: 0.00743437185883522\n",
      "epoch:  503, loss: 0.007433778140693903\n",
      "epoch:  504, loss: 0.007433571852743626\n",
      "epoch:  505, loss: 0.007433571852743626\n",
      "epoch:  506, loss: 0.007433571852743626\n",
      "epoch:  507, loss: 0.007432416547089815\n",
      "epoch:  508, loss: 0.007432108744978905\n",
      "epoch:  509, loss: 0.007430408149957657\n",
      "epoch:  510, loss: 0.007430406287312508\n",
      "epoch:  511, loss: 0.007430406287312508\n",
      "epoch:  512, loss: 0.007430406287312508\n",
      "epoch:  513, loss: 0.00743032107129693\n",
      "epoch:  514, loss: 0.0074301850982010365\n",
      "epoch:  515, loss: 0.007430184632539749\n",
      "epoch:  516, loss: 0.007430184632539749\n",
      "epoch:  517, loss: 0.007428532466292381\n",
      "epoch:  518, loss: 0.0074278926476836205\n",
      "epoch:  519, loss: 0.007427569013088942\n",
      "epoch:  520, loss: 0.007427568547427654\n",
      "epoch:  521, loss: 0.007427197881042957\n",
      "epoch:  522, loss: 0.007426813244819641\n",
      "epoch:  523, loss: 0.007426812779158354\n",
      "epoch:  524, loss: 0.007426812779158354\n",
      "epoch:  525, loss: 0.007426148280501366\n",
      "epoch:  526, loss: 0.007425057236105204\n",
      "epoch:  527, loss: 0.00742482440546155\n",
      "epoch:  528, loss: 0.007424470037221909\n",
      "epoch:  529, loss: 0.007424470037221909\n",
      "epoch:  530, loss: 0.007492695469409227\n",
      "epoch:  531, loss: 0.007432149723172188\n",
      "epoch:  532, loss: 0.007416156120598316\n",
      "epoch:  533, loss: 0.00741519033908844\n",
      "epoch:  534, loss: 0.007414999417960644\n",
      "epoch:  535, loss: 0.007414999417960644\n",
      "epoch:  536, loss: 0.007414997089654207\n",
      "epoch:  537, loss: 0.007412576116621494\n",
      "epoch:  538, loss: 0.00741247134283185\n",
      "epoch:  539, loss: 0.007411932572722435\n",
      "epoch:  540, loss: 0.007411932572722435\n",
      "epoch:  541, loss: 0.007411719765514135\n",
      "epoch:  542, loss: 0.007411339320242405\n",
      "epoch:  543, loss: 0.007410889957100153\n",
      "epoch:  544, loss: 0.007410647813230753\n",
      "epoch:  545, loss: 0.007410647813230753\n",
      "epoch:  546, loss: 0.00741064827889204\n",
      "epoch:  547, loss: 0.007405034266412258\n",
      "epoch:  548, loss: 0.007405034266412258\n",
      "epoch:  549, loss: 0.007405034266412258\n",
      "epoch:  550, loss: 0.007405034266412258\n",
      "epoch:  551, loss: 0.007405034266412258\n",
      "epoch:  552, loss: 0.007405034266412258\n",
      "epoch:  553, loss: 0.007405034266412258\n",
      "epoch:  554, loss: 0.007405034266412258\n",
      "epoch:  555, loss: 0.007405034266412258\n",
      "epoch:  556, loss: 0.007403652183711529\n",
      "epoch:  557, loss: 0.007402552291750908\n",
      "epoch:  558, loss: 0.007402175106108189\n",
      "epoch:  559, loss: 0.007402175106108189\n",
      "epoch:  560, loss: 0.007400041446089745\n",
      "epoch:  561, loss: 0.007400029804557562\n",
      "epoch:  562, loss: 0.007400029804557562\n",
      "epoch:  563, loss: 0.007400029804557562\n",
      "epoch:  564, loss: 0.0073992968536913395\n",
      "epoch:  565, loss: 0.0073992968536913395\n",
      "epoch:  566, loss: 0.0073992968536913395\n",
      "epoch:  567, loss: 0.007399005349725485\n",
      "epoch:  568, loss: 0.00739899929612875\n",
      "epoch:  569, loss: 0.007398969493806362\n",
      "epoch:  570, loss: 0.0073988973163068295\n",
      "epoch:  571, loss: 0.007398772984743118\n",
      "epoch:  572, loss: 0.007398566231131554\n",
      "epoch:  573, loss: 0.007398497313261032\n",
      "epoch:  574, loss: 0.007398140616714954\n",
      "epoch:  575, loss: 0.007397850975394249\n",
      "epoch:  576, loss: 0.007397850975394249\n",
      "epoch:  577, loss: 0.007397703360766172\n",
      "epoch:  578, loss: 0.007396882865577936\n",
      "epoch:  579, loss: 0.007396882865577936\n",
      "epoch:  580, loss: 0.007396882865577936\n",
      "epoch:  581, loss: 0.007396882399916649\n",
      "epoch:  582, loss: 0.007396882399916649\n",
      "epoch:  583, loss: 0.007396882399916649\n",
      "epoch:  584, loss: 0.0073968106880784035\n",
      "epoch:  585, loss: 0.00739599484950304\n",
      "epoch:  586, loss: 0.00739577179774642\n",
      "epoch:  587, loss: 0.007387001533061266\n",
      "epoch:  588, loss: 0.007387001533061266\n",
      "epoch:  589, loss: 0.007387001533061266\n",
      "epoch:  590, loss: 0.007385889068245888\n",
      "epoch:  591, loss: 0.007385070901364088\n",
      "epoch:  592, loss: 0.0073848203755915165\n",
      "epoch:  593, loss: 0.007384819909930229\n",
      "epoch:  594, loss: 0.0073846373707056046\n",
      "epoch:  595, loss: 0.00738446693867445\n",
      "epoch:  596, loss: 0.00738446693867445\n",
      "epoch:  597, loss: 0.00738446693867445\n",
      "epoch:  598, loss: 0.007384244352579117\n",
      "epoch:  599, loss: 0.007384243421256542\n",
      "epoch:  600, loss: 0.0073838867247104645\n",
      "epoch:  601, loss: 0.007383527234196663\n",
      "epoch:  602, loss: 0.007383527234196663\n",
      "epoch:  603, loss: 0.007383368443697691\n",
      "epoch:  604, loss: 0.007382595911622047\n",
      "epoch:  605, loss: 0.007382595911622047\n",
      "epoch:  606, loss: 0.007382243871688843\n",
      "epoch:  607, loss: 0.007381998468190432\n",
      "epoch:  608, loss: 0.007376841735094786\n",
      "epoch:  609, loss: 0.007376198656857014\n",
      "epoch:  610, loss: 0.0073754978366196156\n",
      "epoch:  611, loss: 0.0073621273040771484\n",
      "epoch:  612, loss: 0.0073621273040771484\n",
      "epoch:  613, loss: 0.007362127769738436\n",
      "epoch:  614, loss: 0.007346593774855137\n",
      "epoch:  615, loss: 0.007246264722198248\n",
      "epoch:  616, loss: 0.007246264256536961\n",
      "epoch:  617, loss: 0.007246265187859535\n",
      "epoch:  618, loss: 0.007244560867547989\n",
      "epoch:  619, loss: 0.007244560867547989\n",
      "epoch:  620, loss: 0.007228335831314325\n",
      "epoch:  621, loss: 0.007219348102807999\n",
      "epoch:  622, loss: 0.007217948324978352\n",
      "epoch:  623, loss: 0.007208111695945263\n",
      "epoch:  624, loss: 0.007208111695945263\n",
      "epoch:  625, loss: 0.007180480752140284\n",
      "epoch:  626, loss: 0.007174984086304903\n",
      "epoch:  627, loss: 0.007173244841396809\n",
      "epoch:  628, loss: 0.007173243910074234\n",
      "epoch:  629, loss: 0.007173244841396809\n",
      "epoch:  630, loss: 0.007163123693317175\n",
      "epoch:  631, loss: 0.007162543945014477\n",
      "epoch:  632, loss: 0.007161370478570461\n",
      "epoch:  633, loss: 0.007161370478570461\n",
      "epoch:  634, loss: 0.007159029133617878\n",
      "epoch:  635, loss: 0.007152368314564228\n",
      "epoch:  636, loss: 0.0071523129008710384\n",
      "epoch:  637, loss: 0.0071523129008710384\n",
      "epoch:  638, loss: 0.007149022072553635\n",
      "epoch:  639, loss: 0.00714796781539917\n",
      "epoch:  640, loss: 0.00714796781539917\n",
      "epoch:  641, loss: 0.00714796781539917\n",
      "epoch:  642, loss: 0.007147397380322218\n",
      "epoch:  643, loss: 0.007146960590034723\n",
      "epoch:  644, loss: 0.007146960590034723\n",
      "epoch:  645, loss: 0.0071469563990831375\n",
      "epoch:  646, loss: 0.00714506022632122\n",
      "epoch:  647, loss: 0.007144436240196228\n",
      "epoch:  648, loss: 0.007144436240196228\n",
      "epoch:  649, loss: 0.007144436705857515\n",
      "epoch:  650, loss: 0.007137986831367016\n",
      "epoch:  651, loss: 0.007137986831367016\n",
      "epoch:  652, loss: 0.007128276862204075\n",
      "epoch:  653, loss: 0.007127490825951099\n",
      "epoch:  654, loss: 0.007127490825951099\n",
      "epoch:  655, loss: 0.007116946391761303\n",
      "epoch:  656, loss: 0.007114007603377104\n",
      "epoch:  657, loss: 0.007112996652722359\n",
      "epoch:  658, loss: 0.007112996652722359\n",
      "epoch:  659, loss: 0.007112592458724976\n",
      "epoch:  660, loss: 0.007111480925232172\n",
      "epoch:  661, loss: 0.007111480925232172\n",
      "epoch:  662, loss: 0.007111480925232172\n",
      "epoch:  663, loss: 0.0071067423559725285\n",
      "epoch:  664, loss: 0.0071067423559725285\n",
      "epoch:  665, loss: 0.013395794667303562\n",
      "epoch:  666, loss: 0.009357945993542671\n",
      "epoch:  667, loss: 0.008053325116634369\n",
      "epoch:  668, loss: 0.00805332325398922\n",
      "epoch:  669, loss: 0.007844790816307068\n",
      "epoch:  670, loss: 0.007669046986848116\n",
      "epoch:  671, loss: 0.007663541007786989\n",
      "epoch:  672, loss: 0.00766353914514184\n",
      "epoch:  673, loss: 0.007661806885153055\n",
      "epoch:  674, loss: 0.007652729284018278\n",
      "epoch:  675, loss: 0.007644543424248695\n",
      "epoch:  676, loss: 0.00763654476031661\n",
      "epoch:  677, loss: 0.007635618560016155\n",
      "epoch:  678, loss: 0.007635618560016155\n",
      "epoch:  679, loss: 0.007635641843080521\n",
      "epoch:  680, loss: 0.007602286525070667\n",
      "epoch:  681, loss: 0.0075840731151402\n",
      "epoch:  682, loss: 0.007556506432592869\n",
      "epoch:  683, loss: 0.00754251005128026\n",
      "epoch:  684, loss: 0.007511101197451353\n",
      "epoch:  685, loss: 0.007503019645810127\n",
      "epoch:  686, loss: 0.007501835934817791\n",
      "epoch:  687, loss: 0.007501835934817791\n",
      "epoch:  688, loss: 0.007501835934817791\n",
      "epoch:  689, loss: 0.007501835934817791\n",
      "epoch:  690, loss: 0.007501836866140366\n",
      "epoch:  691, loss: 0.00748932920396328\n",
      "epoch:  692, loss: 0.0074841333553195\n",
      "epoch:  693, loss: 0.007478073239326477\n",
      "epoch:  694, loss: 0.00747667858377099\n",
      "epoch:  695, loss: 0.00747667858377099\n",
      "epoch:  696, loss: 0.00747667858377099\n",
      "epoch:  697, loss: 0.00747667858377099\n",
      "epoch:  698, loss: 0.007476679515093565\n",
      "epoch:  699, loss: 0.007470916491001844\n",
      "epoch:  700, loss: 0.007466452196240425\n",
      "epoch:  701, loss: 0.007464858703315258\n",
      "epoch:  702, loss: 0.007464563939720392\n",
      "epoch:  703, loss: 0.007464563939720392\n",
      "epoch:  704, loss: 0.007464529946446419\n",
      "epoch:  705, loss: 0.0074455346912145615\n",
      "epoch:  706, loss: 0.0074455346912145615\n",
      "epoch:  707, loss: 0.007394534535706043\n",
      "epoch:  708, loss: 0.007393212523311377\n",
      "epoch:  709, loss: 0.007393212523311377\n",
      "epoch:  710, loss: 0.007392257917672396\n",
      "epoch:  711, loss: 0.007386355195194483\n",
      "epoch:  712, loss: 0.0073847644962370396\n",
      "epoch:  713, loss: 0.007382606156170368\n",
      "epoch:  714, loss: 0.007381673902273178\n",
      "epoch:  715, loss: 0.007381673902273178\n",
      "epoch:  716, loss: 0.007381674833595753\n",
      "epoch:  717, loss: 0.007377034053206444\n",
      "epoch:  718, loss: 0.007377034053206444\n",
      "epoch:  719, loss: 0.007374831475317478\n",
      "epoch:  720, loss: 0.007371746469289064\n",
      "epoch:  721, loss: 0.0073709446005523205\n",
      "epoch:  722, loss: 0.007366667501628399\n",
      "epoch:  723, loss: 0.0073638916946947575\n",
      "epoch:  724, loss: 0.007352426182478666\n",
      "epoch:  725, loss: 0.007320216856896877\n",
      "epoch:  726, loss: 0.0073196180164813995\n",
      "epoch:  727, loss: 0.0073196180164813995\n",
      "epoch:  728, loss: 0.0072959004901349545\n",
      "epoch:  729, loss: 0.007295133080333471\n",
      "epoch:  730, loss: 0.007295133080333471\n",
      "epoch:  731, loss: 0.007295133545994759\n",
      "epoch:  732, loss: 0.007291063200682402\n",
      "epoch:  733, loss: 0.007290358655154705\n",
      "epoch:  734, loss: 0.007287262473255396\n",
      "epoch:  735, loss: 0.007287261541932821\n",
      "epoch:  736, loss: 0.007287261541932821\n",
      "epoch:  737, loss: 0.007287261541932821\n",
      "epoch:  738, loss: 0.007287261541932821\n",
      "epoch:  739, loss: 0.007287261541932821\n",
      "epoch:  740, loss: 0.007287261541932821\n",
      "epoch:  741, loss: 0.007287261541932821\n",
      "epoch:  742, loss: 0.007287261541932821\n",
      "epoch:  743, loss: 0.007287261541932821\n",
      "epoch:  744, loss: 0.007286335341632366\n",
      "epoch:  745, loss: 0.007278109900653362\n",
      "epoch:  746, loss: 0.007278109900653362\n",
      "epoch:  747, loss: 0.007278109900653362\n",
      "epoch:  748, loss: 0.007278109900653362\n",
      "epoch:  749, loss: 0.007278109900653362\n",
      "epoch:  750, loss: 0.007278109900653362\n",
      "epoch:  751, loss: 0.007274197414517403\n",
      "epoch:  752, loss: 0.007271025329828262\n",
      "epoch:  753, loss: 0.007268048357218504\n",
      "epoch:  754, loss: 0.00726688839495182\n",
      "epoch:  755, loss: 0.007266887463629246\n",
      "epoch:  756, loss: 0.007266887463629246\n",
      "epoch:  757, loss: 0.007266887463629246\n",
      "epoch:  758, loss: 0.007264907471835613\n",
      "epoch:  759, loss: 0.007264486514031887\n",
      "epoch:  760, loss: 0.0072644841857254505\n",
      "epoch:  761, loss: 0.0072644841857254505\n",
      "epoch:  762, loss: 0.007264483720064163\n",
      "epoch:  763, loss: 0.007262285333126783\n",
      "epoch:  764, loss: 0.007262285333126783\n",
      "epoch:  765, loss: 0.007260242477059364\n",
      "epoch:  766, loss: 0.007259774953126907\n",
      "epoch:  767, loss: 0.007259774953126907\n",
      "epoch:  768, loss: 0.007243633735924959\n",
      "epoch:  769, loss: 0.0072374893352389336\n",
      "epoch:  770, loss: 0.007230320945382118\n",
      "epoch:  771, loss: 0.007230320945382118\n",
      "epoch:  772, loss: 0.007230320945382118\n",
      "epoch:  773, loss: 0.0072276052087545395\n",
      "epoch:  774, loss: 0.007227182388305664\n",
      "epoch:  775, loss: 0.007217085920274258\n",
      "epoch:  776, loss: 0.007217085920274258\n",
      "epoch:  777, loss: 0.007222292013466358\n",
      "epoch:  778, loss: 0.007219092920422554\n",
      "epoch:  779, loss: 0.007219092920422554\n",
      "epoch:  780, loss: 0.007218264974653721\n",
      "epoch:  781, loss: 0.0072156633250415325\n",
      "epoch:  782, loss: 0.007211996242403984\n",
      "epoch:  783, loss: 0.007211996242403984\n",
      "epoch:  784, loss: 0.007211499381810427\n",
      "epoch:  785, loss: 0.007211499381810427\n",
      "epoch:  786, loss: 0.007211499381810427\n",
      "epoch:  787, loss: 0.007199427112936974\n",
      "epoch:  788, loss: 0.007199426181614399\n",
      "epoch:  789, loss: 0.007199426181614399\n",
      "epoch:  790, loss: 0.0071987854316830635\n",
      "epoch:  791, loss: 0.0071987854316830635\n",
      "epoch:  792, loss: 0.007191400043666363\n",
      "epoch:  793, loss: 0.007187085226178169\n",
      "epoch:  794, loss: 0.007182499393820763\n",
      "epoch:  795, loss: 0.007182121276855469\n",
      "epoch:  796, loss: 0.007182037457823753\n",
      "epoch:  797, loss: 0.007182037457823753\n",
      "epoch:  798, loss: 0.007181819528341293\n",
      "epoch:  799, loss: 0.007181208115071058\n",
      "epoch:  800, loss: 0.007181208115071058\n",
      "epoch:  801, loss: 0.007181208115071058\n",
      "epoch:  802, loss: 0.007181208115071058\n",
      "epoch:  803, loss: 0.007181208580732346\n",
      "epoch:  804, loss: 0.007163167465478182\n",
      "epoch:  805, loss: 0.007163167465478182\n",
      "epoch:  806, loss: 0.007159789092838764\n",
      "epoch:  807, loss: 0.007157418876886368\n",
      "epoch:  808, loss: 0.007153655868023634\n",
      "epoch:  809, loss: 0.007142777554690838\n",
      "epoch:  810, loss: 0.007142777554690838\n",
      "epoch:  811, loss: 0.007141066249459982\n",
      "epoch:  812, loss: 0.007136933039873838\n",
      "epoch:  813, loss: 0.007136933039873838\n",
      "epoch:  814, loss: 0.007136933039873838\n",
      "epoch:  815, loss: 0.007136079482734203\n",
      "epoch:  816, loss: 0.007136079482734203\n",
      "epoch:  817, loss: 0.007134087849408388\n",
      "epoch:  818, loss: 0.007133535575121641\n",
      "epoch:  819, loss: 0.007133535575121641\n",
      "epoch:  820, loss: 0.00713353930041194\n",
      "epoch:  821, loss: 0.00713353930041194\n",
      "epoch:  822, loss: 0.007132786326110363\n",
      "epoch:  823, loss: 0.007125428412109613\n",
      "epoch:  824, loss: 0.007125428412109613\n",
      "epoch:  825, loss: 0.00712491013109684\n",
      "epoch:  826, loss: 0.007124295923858881\n",
      "epoch:  827, loss: 0.007124295458197594\n",
      "epoch:  828, loss: 0.007124295458197594\n",
      "epoch:  829, loss: 0.007124295458197594\n",
      "epoch:  830, loss: 0.007124295923858881\n",
      "epoch:  831, loss: 0.007120027672499418\n",
      "epoch:  832, loss: 0.007120027672499418\n",
      "epoch:  833, loss: 0.007119069807231426\n",
      "epoch:  834, loss: 0.007117467001080513\n",
      "epoch:  835, loss: 0.007115792017430067\n",
      "epoch:  836, loss: 0.007114003412425518\n",
      "epoch:  837, loss: 0.007113414816558361\n",
      "epoch:  838, loss: 0.007113414816558361\n",
      "epoch:  839, loss: 0.007113436236977577\n",
      "epoch:  840, loss: 0.007112856954336166\n",
      "epoch:  841, loss: 0.007112856954336166\n",
      "epoch:  842, loss: 0.007112856954336166\n",
      "epoch:  843, loss: 0.007107205223292112\n",
      "epoch:  844, loss: 0.007107205223292112\n",
      "epoch:  845, loss: 0.00710724713280797\n",
      "epoch:  846, loss: 0.007105569820851088\n",
      "epoch:  847, loss: 0.007105010561645031\n",
      "epoch:  848, loss: 0.007105010561645031\n",
      "epoch:  849, loss: 0.007105010561645031\n",
      "epoch:  850, loss: 0.007105010561645031\n",
      "epoch:  851, loss: 0.0071032666601240635\n",
      "epoch:  852, loss: 0.0071032666601240635\n",
      "epoch:  853, loss: 0.007102642673999071\n",
      "epoch:  854, loss: 0.007100626826286316\n",
      "epoch:  855, loss: 0.007100626826286316\n",
      "epoch:  856, loss: 0.007100626826286316\n",
      "epoch:  857, loss: 0.007100626826286316\n",
      "epoch:  858, loss: 0.007099528331309557\n",
      "epoch:  859, loss: 0.0070983185432851315\n",
      "epoch:  860, loss: 0.007098257541656494\n",
      "epoch:  861, loss: 0.007098257541656494\n",
      "epoch:  862, loss: 0.007098258938640356\n",
      "epoch:  863, loss: 0.007097573485225439\n",
      "epoch:  864, loss: 0.0070975725539028645\n",
      "epoch:  865, loss: 0.0070975725539028645\n",
      "epoch:  866, loss: 0.0070975725539028645\n",
      "epoch:  867, loss: 0.007096622604876757\n",
      "epoch:  868, loss: 0.007096088491380215\n",
      "epoch:  869, loss: 0.007096088491380215\n",
      "epoch:  870, loss: 0.007096088491380215\n",
      "epoch:  871, loss: 0.00708753289654851\n",
      "epoch:  872, loss: 0.007061246782541275\n",
      "epoch:  873, loss: 0.007057053968310356\n",
      "epoch:  874, loss: 0.007052934262901545\n",
      "epoch:  875, loss: 0.007052934262901545\n",
      "epoch:  876, loss: 0.007052934728562832\n",
      "epoch:  877, loss: 0.007050491403788328\n",
      "epoch:  878, loss: 0.007050491403788328\n",
      "epoch:  879, loss: 0.007048162166029215\n",
      "epoch:  880, loss: 0.007044090423732996\n",
      "epoch:  881, loss: 0.00703717116266489\n",
      "epoch:  882, loss: 0.00703717116266489\n",
      "epoch:  883, loss: 0.007036380935460329\n",
      "epoch:  884, loss: 0.007036380935460329\n",
      "epoch:  885, loss: 0.007036364637315273\n",
      "epoch:  886, loss: 0.007033172063529491\n",
      "epoch:  887, loss: 0.007006730884313583\n",
      "epoch:  888, loss: 0.007006730884313583\n",
      "epoch:  889, loss: 0.007006730884313583\n",
      "epoch:  890, loss: 0.007006508763879538\n",
      "epoch:  891, loss: 0.007006508763879538\n",
      "epoch:  892, loss: 0.006997009739279747\n",
      "epoch:  893, loss: 0.00699259340763092\n",
      "epoch:  894, loss: 0.006986111868172884\n",
      "epoch:  895, loss: 0.006986111868172884\n",
      "epoch:  896, loss: 0.006981300190091133\n",
      "epoch:  897, loss: 0.006978186313062906\n",
      "epoch:  898, loss: 0.006977328564971685\n",
      "epoch:  899, loss: 0.006977328564971685\n",
      "epoch:  900, loss: 0.006976403761655092\n",
      "epoch:  901, loss: 0.006975764408707619\n",
      "epoch:  902, loss: 0.006975764408707619\n",
      "epoch:  903, loss: 0.006975764408707619\n",
      "epoch:  904, loss: 0.006975764408707619\n",
      "epoch:  905, loss: 0.006975764408707619\n",
      "epoch:  906, loss: 0.006974213756620884\n",
      "epoch:  907, loss: 0.006972346920520067\n",
      "epoch:  908, loss: 0.006969531998038292\n",
      "epoch:  909, loss: 0.006967885885387659\n",
      "epoch:  910, loss: 0.006967885885387659\n",
      "epoch:  911, loss: 0.006963619031012058\n",
      "epoch:  912, loss: 0.006961128208786249\n",
      "epoch:  913, loss: 0.006960746832191944\n",
      "epoch:  914, loss: 0.006960746832191944\n",
      "epoch:  915, loss: 0.006958878133445978\n",
      "epoch:  916, loss: 0.006955931894481182\n",
      "epoch:  917, loss: 0.006920715793967247\n",
      "epoch:  918, loss: 0.006920399609953165\n",
      "epoch:  919, loss: 0.006919168867170811\n",
      "epoch:  920, loss: 0.006919168401509523\n",
      "epoch:  921, loss: 0.006889923941344023\n",
      "epoch:  922, loss: 0.006887318100780249\n",
      "epoch:  923, loss: 0.006887318100780249\n",
      "epoch:  924, loss: 0.006887318566441536\n",
      "epoch:  925, loss: 0.00687467260286212\n",
      "epoch:  926, loss: 0.00687467260286212\n",
      "epoch:  927, loss: 0.006874674465507269\n",
      "epoch:  928, loss: 0.006874674465507269\n",
      "epoch:  929, loss: 0.00685462960973382\n",
      "epoch:  930, loss: 0.006834342610090971\n",
      "epoch:  931, loss: 0.006808349397033453\n",
      "epoch:  932, loss: 0.006808348931372166\n",
      "epoch:  933, loss: 0.006808348931372166\n",
      "epoch:  934, loss: 0.006808348931372166\n",
      "epoch:  935, loss: 0.006805971264839172\n",
      "epoch:  936, loss: 0.006797332316637039\n",
      "epoch:  937, loss: 0.006797332316637039\n",
      "epoch:  938, loss: 0.006796436384320259\n",
      "epoch:  939, loss: 0.006796435918658972\n",
      "epoch:  940, loss: 0.006793832406401634\n",
      "epoch:  941, loss: 0.006792728789150715\n",
      "epoch:  942, loss: 0.006792728789150715\n",
      "epoch:  943, loss: 0.006792728789150715\n",
      "epoch:  944, loss: 0.006792217493057251\n",
      "epoch:  945, loss: 0.006792217493057251\n",
      "epoch:  946, loss: 0.006791448686271906\n",
      "epoch:  947, loss: 0.006790666375309229\n",
      "epoch:  948, loss: 0.006790666375309229\n",
      "epoch:  949, loss: 0.006739403586834669\n",
      "epoch:  950, loss: 0.006739403586834669\n",
      "epoch:  951, loss: 0.006737419869750738\n",
      "epoch:  952, loss: 0.006734596565365791\n",
      "epoch:  953, loss: 0.006734596099704504\n",
      "epoch:  954, loss: 0.006734596099704504\n",
      "epoch:  955, loss: 0.006734689697623253\n",
      "epoch:  956, loss: 0.006734689697623253\n",
      "epoch:  957, loss: 0.006725700106471777\n",
      "epoch:  958, loss: 0.006709059700369835\n",
      "epoch:  959, loss: 0.0067037558183074\n",
      "epoch:  960, loss: 0.006703009828925133\n",
      "epoch:  961, loss: 0.006702176760882139\n",
      "epoch:  962, loss: 0.006702176760882139\n",
      "epoch:  963, loss: 0.006702177692204714\n",
      "epoch:  964, loss: 0.006702176760882139\n",
      "epoch:  965, loss: 0.006696464028209448\n",
      "epoch:  966, loss: 0.006693446543067694\n",
      "epoch:  967, loss: 0.006693446077406406\n",
      "epoch:  968, loss: 0.006693446543067694\n",
      "epoch:  969, loss: 0.006692834664136171\n",
      "epoch:  970, loss: 0.006678966339677572\n",
      "epoch:  971, loss: 0.006678966339677572\n",
      "epoch:  972, loss: 0.006675228476524353\n",
      "epoch:  973, loss: 0.006673908792436123\n",
      "epoch:  974, loss: 0.006673908792436123\n",
      "epoch:  975, loss: 0.006672090385109186\n",
      "epoch:  976, loss: 0.006671490613371134\n",
      "epoch:  977, loss: 0.006671490613371134\n",
      "epoch:  978, loss: 0.006671492010354996\n",
      "epoch:  979, loss: 0.006669765338301659\n",
      "epoch:  980, loss: 0.006669765338301659\n",
      "epoch:  981, loss: 0.006669050548225641\n",
      "epoch:  982, loss: 0.0066188182681798935\n",
      "epoch:  983, loss: 0.0066188182681798935\n",
      "epoch:  984, loss: 0.006618818733841181\n",
      "epoch:  985, loss: 0.006616904865950346\n",
      "epoch:  986, loss: 0.006615316495299339\n",
      "epoch:  987, loss: 0.006608086172491312\n",
      "epoch:  988, loss: 0.006606084294617176\n",
      "epoch:  989, loss: 0.006606084294617176\n",
      "epoch:  990, loss: 0.006604071706533432\n",
      "epoch:  991, loss: 0.0066009103320539\n",
      "epoch:  992, loss: 0.0066009103320539\n",
      "epoch:  993, loss: 0.00659304391592741\n",
      "epoch:  994, loss: 0.0065922862850129604\n",
      "epoch:  995, loss: 0.0065922862850129604\n",
      "epoch:  996, loss: 0.006592001765966415\n",
      "epoch:  997, loss: 0.00658416049554944\n",
      "epoch:  998, loss: 0.00658416049554944\n",
      "epoch:  999, loss: 0.006582443602383137\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size=X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# opt = torch_numopt.ConjugateGradient(model=model, lr=1e-4, line_search_method=\"const\", cg_method=\"FR\")\n",
    "opt = torch_numopt.ConjugateGradient(model=model, lr=10, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"armijo\", cg_method=\"DY\")\n",
    "# opt = torch_numopt.ConjugateGradient(model=model, lr=10, c1=0.1, c2=0.9, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"wolfe\", cg_method=\"FR\")\n",
    "# opt = torch_numopt.ConjugateGradient(model=model, lr=10, c1=0.1, c2=0.9, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"strong-wolfe\", cg_method=\"FR\")\n",
    "# opt = torch_numopt.ConjugateGradient(model=model, lr=10, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"goldstein\", cg_method=\"FR\")\n",
    "\n",
    "all_loss = {}\n",
    "for epoch in range(1000):\n",
    "    print(\"epoch: \", epoch, end=\"\")\n",
    "    all_loss[epoch + 1] = 0\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y, loss_fn)\n",
    "\n",
    "        all_loss[epoch + 1] += loss\n",
    "    all_loss[epoch + 1] /= len(data_loader)\n",
    "    print(\", loss: {}\".format(all_loss[epoch + 1].detach().numpy().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics: R2 = 0.7222837551830337\n",
      "Test metrics:  R2 = 0.7124056969067776\n"
     ]
    }
   ],
   "source": [
    "pred_train = model.forward(X_train).detach()\n",
    "pred_test = model.forward(X_test).detach()\n",
    "print(f\"Train metrics: R2 = {r2_score(pred_train, y_train)}\")\n",
    "print(f\"Test metrics:  R2 = {r2_score(pred_test, y_test)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_numopt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
