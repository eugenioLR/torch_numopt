{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch_numopt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import *\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.f1 = nn.Linear(input_size, 10, device=device)\n",
    "        self.f2 = nn.Linear(10, 20, device=device)\n",
    "        self.f3 = nn.Linear(20, 20, device=device)\n",
    "        self.f4 = nn.Linear(20, 10, device=device)\n",
    "        self.f5 = nn.Linear(10, 1, device=device)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        # self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.f1(x))\n",
    "        x = self.activation(self.f2(x))\n",
    "        x = self.activation(self.f3(x))\n",
    "        x = self.activation(self.f4(x))\n",
    "        x = self.f5(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([800, 10]) torch.Size([800, 1])\n",
      "torch.Size([200, 10]) torch.Size([200, 1])\n"
     ]
    }
   ],
   "source": [
    "# X, y = load_diabetes(return_X_y=True, scaled=False)\n",
    "# X, y = make_regression(n_samples=1000, n_features=100)\n",
    "X, y = make_friedman1(n_samples=1000, noise=1e-2)\n",
    "\n",
    "X_scaler = MinMaxScaler()\n",
    "X = X_scaler.fit_transform(X)\n",
    "X = torch.Tensor(X).to(device)\n",
    "\n",
    "y_scaler = MinMaxScaler()\n",
    "y = y_scaler.fit_transform(y.reshape((-1, 1)))\n",
    "y = torch.Tensor(y).to(device)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "torch_data = TensorDataset(X_train, y_train)\n",
    "data_loader = DataLoader(torch_data, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.5963084697723389\n",
      "epoch:  1, loss: 0.27325356006622314\n",
      "epoch:  2, loss: 0.14448611438274384\n",
      "epoch:  3, loss: 0.08463037014007568\n",
      "epoch:  4, loss: 0.0563373826444149\n",
      "epoch:  5, loss: 0.043102383613586426\n",
      "epoch:  6, loss: 0.03699345514178276\n",
      "epoch:  7, loss: 0.034211233258247375\n",
      "epoch:  8, loss: 0.0329512394964695\n",
      "epoch:  9, loss: 0.03237833082675934\n",
      "epoch:  10, loss: 0.03211156278848648\n",
      "epoch:  11, loss: 0.031985148787498474\n",
      "epoch:  12, loss: 0.031922705471515656\n",
      "epoch:  13, loss: 0.031887587159872055\n",
      "epoch:  14, loss: 0.03183499723672867\n",
      "epoch:  15, loss: 0.03176741302013397\n",
      "epoch:  16, loss: 0.03172875940799713\n",
      "epoch:  17, loss: 0.031658802181482315\n",
      "epoch:  18, loss: 0.031584758311510086\n",
      "epoch:  19, loss: 0.031568530946969986\n",
      "epoch:  20, loss: 0.031408678740262985\n",
      "epoch:  21, loss: 0.031320881098508835\n",
      "epoch:  22, loss: 0.031252212822437286\n",
      "epoch:  23, loss: 0.03106769174337387\n",
      "epoch:  24, loss: 0.03096788562834263\n",
      "epoch:  25, loss: 0.03088846057653427\n",
      "epoch:  26, loss: 0.030690303072333336\n",
      "epoch:  27, loss: 0.030589375644922256\n",
      "epoch:  28, loss: 0.03053075633943081\n",
      "epoch:  29, loss: 0.030423207208514214\n",
      "epoch:  30, loss: 0.030306758359074593\n",
      "epoch:  31, loss: 0.03024599887430668\n",
      "epoch:  32, loss: 0.030180063098669052\n",
      "epoch:  33, loss: 0.030047612264752388\n",
      "epoch:  34, loss: 0.029980439692735672\n",
      "epoch:  35, loss: 0.02993268333375454\n",
      "epoch:  36, loss: 0.029777316376566887\n",
      "epoch:  37, loss: 0.029700640588998795\n",
      "epoch:  38, loss: 0.02966747246682644\n",
      "epoch:  39, loss: 0.029486095532774925\n",
      "epoch:  40, loss: 0.02939899079501629\n",
      "epoch:  41, loss: 0.029397092759609222\n",
      "epoch:  42, loss: 0.02918311022222042\n",
      "epoch:  43, loss: 0.02908380888402462\n",
      "epoch:  44, loss: 0.029028480872511864\n",
      "epoch:  45, loss: 0.028865402564406395\n",
      "epoch:  46, loss: 0.028746670112013817\n",
      "epoch:  47, loss: 0.02868388593196869\n",
      "epoch:  48, loss: 0.028515877202153206\n",
      "epoch:  49, loss: 0.02838124707341194\n",
      "epoch:  50, loss: 0.028311312198638916\n",
      "epoch:  51, loss: 0.028147989884018898\n",
      "epoch:  52, loss: 0.027984121814370155\n",
      "epoch:  53, loss: 0.027904771268367767\n",
      "epoch:  54, loss: 0.02774318866431713\n",
      "epoch:  55, loss: 0.027553042396903038\n",
      "epoch:  56, loss: 0.027463683858513832\n",
      "epoch:  57, loss: 0.02731727622449398\n",
      "epoch:  58, loss: 0.027088485658168793\n",
      "epoch:  59, loss: 0.026988008990883827\n",
      "epoch:  60, loss: 0.026853477582335472\n",
      "epoch:  61, loss: 0.026590049266815186\n",
      "epoch:  62, loss: 0.026475491002202034\n",
      "epoch:  63, loss: 0.026369847357273102\n",
      "epoch:  64, loss: 0.026046261191368103\n",
      "epoch:  65, loss: 0.025918403640389442\n",
      "epoch:  66, loss: 0.025810644030570984\n",
      "epoch:  67, loss: 0.025456538423895836\n",
      "epoch:  68, loss: 0.025318380445241928\n",
      "epoch:  69, loss: 0.02523263543844223\n",
      "epoch:  70, loss: 0.024815363809466362\n",
      "epoch:  71, loss: 0.024667009711265564\n",
      "epoch:  72, loss: 0.024568093940615654\n",
      "epoch:  73, loss: 0.02411593869328499\n",
      "epoch:  74, loss: 0.023957587778568268\n",
      "epoch:  75, loss: 0.023846441879868507\n",
      "epoch:  76, loss: 0.02334611862897873\n",
      "epoch:  77, loss: 0.023188544437289238\n",
      "epoch:  78, loss: 0.022980961948633194\n",
      "epoch:  79, loss: 0.02251981385052204\n",
      "epoch:  80, loss: 0.02237110771238804\n",
      "epoch:  81, loss: 0.02207299694418907\n",
      "epoch:  82, loss: 0.02163654938340187\n",
      "epoch:  83, loss: 0.02150416560471058\n",
      "epoch:  84, loss: 0.021047033369541168\n",
      "epoch:  85, loss: 0.020716747269034386\n",
      "epoch:  86, loss: 0.02060101181268692\n",
      "epoch:  87, loss: 0.02003326453268528\n",
      "epoch:  88, loss: 0.019772885367274284\n",
      "epoch:  89, loss: 0.019753962755203247\n",
      "epoch:  90, loss: 0.01898234151303768\n",
      "epoch:  91, loss: 0.01882445253431797\n",
      "epoch:  92, loss: 0.01837371103465557\n",
      "epoch:  93, loss: 0.017983470112085342\n",
      "epoch:  94, loss: 0.017879566177725792\n",
      "epoch:  95, loss: 0.01717947982251644\n",
      "epoch:  96, loss: 0.017028747126460075\n",
      "epoch:  97, loss: 0.016506683081388474\n",
      "epoch:  98, loss: 0.01620495319366455\n",
      "epoch:  99, loss: 0.01604895107448101\n",
      "epoch:  100, loss: 0.015422450378537178\n",
      "epoch:  101, loss: 0.01533244363963604\n",
      "epoch:  102, loss: 0.01468604989349842\n",
      "epoch:  103, loss: 0.014591227285563946\n",
      "epoch:  104, loss: 0.014017028734087944\n",
      "epoch:  105, loss: 0.013906776905059814\n",
      "epoch:  106, loss: 0.013388033024966717\n",
      "epoch:  107, loss: 0.013284147717058659\n",
      "epoch:  108, loss: 0.012388343922793865\n",
      "epoch:  109, loss: 0.01009430643171072\n",
      "epoch:  110, loss: 0.00995267927646637\n",
      "epoch:  111, loss: 0.009916165843605995\n",
      "epoch:  112, loss: 0.009671458974480629\n",
      "epoch:  113, loss: 0.009623004123568535\n",
      "epoch:  114, loss: 0.009536441415548325\n",
      "epoch:  115, loss: 0.009432239457964897\n",
      "epoch:  116, loss: 0.009416447021067142\n",
      "epoch:  117, loss: 0.00910219643265009\n",
      "epoch:  118, loss: 0.008690780960023403\n",
      "epoch:  119, loss: 0.008536504581570625\n",
      "epoch:  120, loss: 0.008487901650369167\n",
      "epoch:  121, loss: 0.008475310169160366\n",
      "epoch:  122, loss: 0.008430327288806438\n",
      "epoch:  123, loss: 0.00842608418315649\n",
      "epoch:  124, loss: 0.008396999910473824\n",
      "epoch:  125, loss: 0.008392184972763062\n",
      "epoch:  126, loss: 0.008378371596336365\n",
      "epoch:  127, loss: 0.008365740068256855\n",
      "epoch:  128, loss: 0.008363306522369385\n",
      "epoch:  129, loss: 0.008322504349052906\n",
      "epoch:  130, loss: 0.008211100473999977\n",
      "epoch:  131, loss: 0.00820627436041832\n",
      "epoch:  132, loss: 0.008204076439142227\n",
      "epoch:  133, loss: 0.008196097798645496\n",
      "epoch:  134, loss: 0.008195034228265285\n",
      "epoch:  135, loss: 0.00818820670247078\n",
      "epoch:  136, loss: 0.00818675383925438\n",
      "epoch:  137, loss: 0.00818442739546299\n",
      "epoch:  138, loss: 0.008179458789527416\n",
      "epoch:  139, loss: 0.0081786485388875\n",
      "epoch:  140, loss: 0.008173393085598946\n",
      "epoch:  141, loss: 0.008171805180609226\n",
      "epoch:  142, loss: 0.008171161636710167\n",
      "epoch:  143, loss: 0.008147538639605045\n",
      "epoch:  144, loss: 0.008121735416352749\n",
      "epoch:  145, loss: 0.008120527490973473\n",
      "epoch:  146, loss: 0.008118386380374432\n",
      "epoch:  147, loss: 0.00811589602380991\n",
      "epoch:  148, loss: 0.008115431293845177\n",
      "epoch:  149, loss: 0.008111811242997646\n",
      "epoch:  150, loss: 0.008111193776130676\n",
      "epoch:  151, loss: 0.00810975395143032\n",
      "epoch:  152, loss: 0.00810744147747755\n",
      "epoch:  153, loss: 0.00810699537396431\n",
      "epoch:  154, loss: 0.008104479871690273\n",
      "epoch:  155, loss: 0.008103316649794579\n",
      "epoch:  156, loss: 0.008102943189442158\n",
      "epoch:  157, loss: 0.008099893108010292\n",
      "epoch:  158, loss: 0.008099295198917389\n",
      "epoch:  159, loss: 0.008097285404801369\n",
      "epoch:  160, loss: 0.008095712400972843\n",
      "epoch:  161, loss: 0.00809534639120102\n",
      "epoch:  162, loss: 0.008090530522167683\n",
      "epoch:  163, loss: 0.008065717294812202\n",
      "epoch:  164, loss: 0.008064365945756435\n",
      "epoch:  165, loss: 0.008063975721597672\n",
      "epoch:  166, loss: 0.008061283268034458\n",
      "epoch:  167, loss: 0.008060473017394543\n",
      "epoch:  168, loss: 0.008060119114816189\n",
      "epoch:  169, loss: 0.008057382889091969\n",
      "epoch:  170, loss: 0.008056937716901302\n",
      "epoch:  171, loss: 0.008055916987359524\n",
      "epoch:  172, loss: 0.008053808473050594\n",
      "epoch:  173, loss: 0.008053434081375599\n",
      "epoch:  174, loss: 0.008051886223256588\n",
      "epoch:  175, loss: 0.0080503448843956\n",
      "epoch:  176, loss: 0.008049999363720417\n",
      "epoch:  177, loss: 0.008047578856348991\n",
      "epoch:  178, loss: 0.008046877570450306\n",
      "epoch:  179, loss: 0.00804661214351654\n",
      "epoch:  180, loss: 0.008044022135436535\n",
      "epoch:  181, loss: 0.008043547160923481\n",
      "epoch:  182, loss: 0.008041122928261757\n",
      "epoch:  183, loss: 0.008040334098041058\n",
      "epoch:  184, loss: 0.008039957843720913\n",
      "epoch:  185, loss: 0.008037159219384193\n",
      "epoch:  186, loss: 0.008036608807742596\n",
      "epoch:  187, loss: 0.008036269806325436\n",
      "epoch:  188, loss: 0.008033966645598412\n",
      "epoch:  189, loss: 0.008033232763409615\n",
      "epoch:  190, loss: 0.008032933808863163\n",
      "epoch:  191, loss: 0.008027665317058563\n",
      "epoch:  192, loss: 0.008008706383407116\n",
      "epoch:  193, loss: 0.008007443509995937\n",
      "epoch:  194, loss: 0.00800695363432169\n",
      "epoch:  195, loss: 0.008003140799701214\n",
      "epoch:  196, loss: 0.008002645336091518\n",
      "epoch:  197, loss: 0.008000986650586128\n",
      "epoch:  198, loss: 0.007999742403626442\n",
      "epoch:  199, loss: 0.007999434135854244\n",
      "epoch:  200, loss: 0.007996084168553352\n",
      "epoch:  201, loss: 0.007976310327649117\n",
      "epoch:  202, loss: 0.007975262589752674\n",
      "epoch:  203, loss: 0.007972565479576588\n",
      "epoch:  204, loss: 0.007970605976879597\n",
      "epoch:  205, loss: 0.007970177568495274\n",
      "epoch:  206, loss: 0.00796728115528822\n",
      "epoch:  207, loss: 0.00796682108193636\n",
      "epoch:  208, loss: 0.007965479046106339\n",
      "epoch:  209, loss: 0.007963967509567738\n",
      "epoch:  210, loss: 0.007963581942021847\n",
      "epoch:  211, loss: 0.00796158891171217\n",
      "epoch:  212, loss: 0.00796075351536274\n",
      "epoch:  213, loss: 0.007960471324622631\n",
      "epoch:  214, loss: 0.007958371192216873\n",
      "epoch:  215, loss: 0.007957694120705128\n",
      "epoch:  216, loss: 0.007956801913678646\n",
      "epoch:  217, loss: 0.007955026812851429\n",
      "epoch:  218, loss: 0.007954631932079792\n",
      "epoch:  219, loss: 0.007953853346407413\n",
      "epoch:  220, loss: 0.007951891049742699\n",
      "epoch:  221, loss: 0.007951540872454643\n",
      "epoch:  222, loss: 0.007949846796691418\n",
      "epoch:  223, loss: 0.007948803715407848\n",
      "epoch:  224, loss: 0.007948508486151695\n",
      "epoch:  225, loss: 0.007946238853037357\n",
      "epoch:  226, loss: 0.007945772260427475\n",
      "epoch:  227, loss: 0.00794429425150156\n",
      "epoch:  228, loss: 0.007943089120090008\n",
      "epoch:  229, loss: 0.007942728698253632\n",
      "epoch:  230, loss: 0.007940411567687988\n",
      "epoch:  231, loss: 0.007939807139337063\n",
      "epoch:  232, loss: 0.00793914869427681\n",
      "epoch:  233, loss: 0.007937014102935791\n",
      "epoch:  234, loss: 0.007936622947454453\n",
      "epoch:  235, loss: 0.007935190573334694\n",
      "epoch:  236, loss: 0.007933735847473145\n",
      "epoch:  237, loss: 0.007933387532830238\n",
      "epoch:  238, loss: 0.0079313013702631\n",
      "epoch:  239, loss: 0.007930466905236244\n",
      "epoch:  240, loss: 0.007930136285722256\n",
      "epoch:  241, loss: 0.007928217761218548\n",
      "epoch:  242, loss: 0.007927167229354382\n",
      "epoch:  243, loss: 0.00792684406042099\n",
      "epoch:  244, loss: 0.007924502715468407\n",
      "epoch:  245, loss: 0.007923832163214684\n",
      "epoch:  246, loss: 0.007923520170152187\n",
      "epoch:  247, loss: 0.007913876324892044\n",
      "epoch:  248, loss: 0.007897102274000645\n",
      "epoch:  249, loss: 0.007895901799201965\n",
      "epoch:  250, loss: 0.00789551343768835\n",
      "epoch:  251, loss: 0.00789270456880331\n",
      "epoch:  252, loss: 0.00789210107177496\n",
      "epoch:  253, loss: 0.007890187203884125\n",
      "epoch:  254, loss: 0.007888923399150372\n",
      "epoch:  255, loss: 0.00788856204599142\n",
      "epoch:  256, loss: 0.007886945270001888\n",
      "epoch:  257, loss: 0.00788557156920433\n",
      "epoch:  258, loss: 0.007885209284722805\n",
      "epoch:  259, loss: 0.00788329727947712\n",
      "epoch:  260, loss: 0.00788216944783926\n",
      "epoch:  261, loss: 0.007881845347583294\n",
      "epoch:  262, loss: 0.007877347059547901\n",
      "epoch:  263, loss: 0.007856251671910286\n",
      "epoch:  264, loss: 0.007855170406401157\n",
      "epoch:  265, loss: 0.007854195311665535\n",
      "epoch:  266, loss: 0.007851249538362026\n",
      "epoch:  267, loss: 0.007850821129977703\n",
      "epoch:  268, loss: 0.007848812267184258\n",
      "epoch:  269, loss: 0.007847658358514309\n",
      "epoch:  270, loss: 0.007847335189580917\n",
      "epoch:  271, loss: 0.00784105435013771\n",
      "epoch:  272, loss: 0.007820596918463707\n",
      "epoch:  273, loss: 0.007819452323019505\n",
      "epoch:  274, loss: 0.007819241844117641\n",
      "epoch:  275, loss: 0.007815837860107422\n",
      "epoch:  276, loss: 0.007815323770046234\n",
      "epoch:  277, loss: 0.007812606170773506\n",
      "epoch:  278, loss: 0.007811988238245249\n",
      "epoch:  279, loss: 0.007811666466295719\n",
      "epoch:  280, loss: 0.007810567505657673\n",
      "epoch:  281, loss: 0.007808550260961056\n",
      "epoch:  282, loss: 0.007808180991560221\n",
      "epoch:  283, loss: 0.007806309498846531\n",
      "epoch:  284, loss: 0.0078049772419035435\n",
      "epoch:  285, loss: 0.00780462147668004\n",
      "epoch:  286, loss: 0.007802464999258518\n",
      "epoch:  287, loss: 0.007801424711942673\n",
      "epoch:  288, loss: 0.00780109129846096\n",
      "epoch:  289, loss: 0.007799387909471989\n",
      "epoch:  290, loss: 0.007797994650900364\n",
      "epoch:  291, loss: 0.007797572296112776\n",
      "epoch:  292, loss: 0.0077969771809875965\n",
      "epoch:  293, loss: 0.0077944002114236355\n",
      "epoch:  294, loss: 0.007793950382620096\n",
      "epoch:  295, loss: 0.007792547810822725\n",
      "epoch:  296, loss: 0.007790775038301945\n",
      "epoch:  297, loss: 0.007790339644998312\n",
      "epoch:  298, loss: 0.007789180148392916\n",
      "epoch:  299, loss: 0.0077871656976640224\n",
      "epoch:  300, loss: 0.007786786183714867\n",
      "epoch:  301, loss: 0.0077854301780462265\n",
      "epoch:  302, loss: 0.007783715613186359\n",
      "epoch:  303, loss: 0.007783286273479462\n",
      "epoch:  304, loss: 0.0077818152494728565\n",
      "epoch:  305, loss: 0.007780148647725582\n",
      "epoch:  306, loss: 0.007779772859066725\n",
      "epoch:  307, loss: 0.007777994032949209\n",
      "epoch:  308, loss: 0.007776609621942043\n",
      "epoch:  309, loss: 0.007776238024234772\n",
      "epoch:  310, loss: 0.007775165140628815\n",
      "epoch:  311, loss: 0.007773126009851694\n",
      "epoch:  312, loss: 0.007772716227918863\n",
      "epoch:  313, loss: 0.007771281525492668\n",
      "epoch:  314, loss: 0.007769527845084667\n",
      "epoch:  315, loss: 0.007769123185425997\n",
      "epoch:  316, loss: 0.007768169976770878\n",
      "epoch:  317, loss: 0.007765987887978554\n",
      "epoch:  318, loss: 0.0077655259519815445\n",
      "epoch:  319, loss: 0.007765209302306175\n",
      "epoch:  320, loss: 0.007762328255921602\n",
      "epoch:  321, loss: 0.007761836983263493\n",
      "epoch:  322, loss: 0.007761671207845211\n",
      "epoch:  323, loss: 0.007758560124784708\n",
      "epoch:  324, loss: 0.007758031133562326\n",
      "epoch:  325, loss: 0.007756085135042667\n",
      "epoch:  326, loss: 0.007754686288535595\n",
      "epoch:  327, loss: 0.007754249963909388\n",
      "epoch:  328, loss: 0.0077539365738630295\n",
      "epoch:  329, loss: 0.007752431556582451\n",
      "epoch:  330, loss: 0.007750614080578089\n",
      "epoch:  331, loss: 0.007750188931822777\n",
      "epoch:  332, loss: 0.007748735602945089\n",
      "epoch:  333, loss: 0.007746943272650242\n",
      "epoch:  334, loss: 0.007746471092104912\n",
      "epoch:  335, loss: 0.007745000068098307\n",
      "epoch:  336, loss: 0.007743305992335081\n",
      "epoch:  337, loss: 0.007742857560515404\n",
      "epoch:  338, loss: 0.007741713896393776\n",
      "epoch:  339, loss: 0.0077397702261805534\n",
      "epoch:  340, loss: 0.007739278953522444\n",
      "epoch:  341, loss: 0.007737257517874241\n",
      "epoch:  342, loss: 0.007735937833786011\n",
      "epoch:  343, loss: 0.007735562976449728\n",
      "epoch:  344, loss: 0.007735152263194323\n",
      "epoch:  345, loss: 0.00773239228874445\n",
      "epoch:  346, loss: 0.007731882855296135\n",
      "epoch:  347, loss: 0.0077315340749919415\n",
      "epoch:  348, loss: 0.007728960830718279\n",
      "epoch:  349, loss: 0.007728172466158867\n",
      "epoch:  350, loss: 0.0077278027310967445\n",
      "epoch:  351, loss: 0.007724975235760212\n",
      "epoch:  352, loss: 0.007724417373538017\n",
      "epoch:  353, loss: 0.007724056486040354\n",
      "epoch:  354, loss: 0.007722204551100731\n",
      "epoch:  355, loss: 0.007720681838691235\n",
      "epoch:  356, loss: 0.0077202413231134415\n",
      "epoch:  357, loss: 0.0077197859063744545\n",
      "epoch:  358, loss: 0.007716867607086897\n",
      "epoch:  359, loss: 0.007716372609138489\n",
      "epoch:  360, loss: 0.00771408760920167\n",
      "epoch:  361, loss: 0.007712876424193382\n",
      "epoch:  362, loss: 0.0077124303206801414\n",
      "epoch:  363, loss: 0.007710406556725502\n",
      "epoch:  364, loss: 0.007708788849413395\n",
      "epoch:  365, loss: 0.007708356250077486\n",
      "epoch:  366, loss: 0.007708223070949316\n",
      "epoch:  367, loss: 0.007704875431954861\n",
      "epoch:  368, loss: 0.00770433759316802\n",
      "epoch:  369, loss: 0.007703961338847876\n",
      "epoch:  370, loss: 0.007689621299505234\n",
      "epoch:  371, loss: 0.007673710584640503\n",
      "epoch:  372, loss: 0.007672260981053114\n",
      "epoch:  373, loss: 0.007671588100492954\n",
      "epoch:  374, loss: 0.007666764315217733\n",
      "epoch:  375, loss: 0.007666115649044514\n",
      "epoch:  376, loss: 0.007665425539016724\n",
      "epoch:  377, loss: 0.007662308868020773\n",
      "epoch:  378, loss: 0.007661808747798204\n",
      "epoch:  379, loss: 0.0076590655371546745\n",
      "epoch:  380, loss: 0.0076579987071454525\n",
      "epoch:  381, loss: 0.007657600566744804\n",
      "epoch:  382, loss: 0.0076545351184904575\n",
      "epoch:  383, loss: 0.00762482825666666\n",
      "epoch:  384, loss: 0.007622658275067806\n",
      "epoch:  385, loss: 0.007622136734426022\n",
      "epoch:  386, loss: 0.00761897349730134\n",
      "epoch:  387, loss: 0.0076180086471140385\n",
      "epoch:  388, loss: 0.007617587689310312\n",
      "epoch:  389, loss: 0.0076156482100486755\n",
      "epoch:  390, loss: 0.007613891735672951\n",
      "epoch:  391, loss: 0.0076134526170790195\n",
      "epoch:  392, loss: 0.007610882166773081\n",
      "epoch:  393, loss: 0.007609702181071043\n",
      "epoch:  394, loss: 0.007609280291944742\n",
      "epoch:  395, loss: 0.0076063345186412334\n",
      "epoch:  396, loss: 0.007605425547808409\n",
      "epoch:  397, loss: 0.0076050362549722195\n",
      "epoch:  398, loss: 0.007594413589686155\n",
      "epoch:  399, loss: 0.007569754030555487\n",
      "epoch:  400, loss: 0.007568023167550564\n",
      "epoch:  401, loss: 0.007567479275166988\n",
      "epoch:  402, loss: 0.007564118131995201\n",
      "epoch:  403, loss: 0.007562869694083929\n",
      "epoch:  404, loss: 0.007562398910522461\n",
      "epoch:  405, loss: 0.007558689918369055\n",
      "epoch:  406, loss: 0.007557865232229233\n",
      "epoch:  407, loss: 0.007557816803455353\n",
      "epoch:  408, loss: 0.007553575094789267\n",
      "epoch:  409, loss: 0.00755294319242239\n",
      "epoch:  410, loss: 0.007551357615739107\n",
      "epoch:  411, loss: 0.0075485240668058395\n",
      "epoch:  412, loss: 0.0075479233637452126\n",
      "epoch:  413, loss: 0.007545159664005041\n",
      "epoch:  414, loss: 0.007543401326984167\n",
      "epoch:  415, loss: 0.00754288537427783\n",
      "epoch:  416, loss: 0.007539735641330481\n",
      "epoch:  417, loss: 0.007538357749581337\n",
      "epoch:  418, loss: 0.007537846453487873\n",
      "epoch:  419, loss: 0.007534804753959179\n",
      "epoch:  420, loss: 0.00753324618563056\n",
      "epoch:  421, loss: 0.007532732095569372\n",
      "epoch:  422, loss: 0.007530503906309605\n",
      "epoch:  423, loss: 0.0075281402096152306\n",
      "epoch:  424, loss: 0.007527564652264118\n",
      "epoch:  425, loss: 0.007526783738285303\n",
      "epoch:  426, loss: 0.007523054722696543\n",
      "epoch:  427, loss: 0.007522430270910263\n",
      "epoch:  428, loss: 0.00752062676474452\n",
      "epoch:  429, loss: 0.007517778314650059\n",
      "epoch:  430, loss: 0.007517179474234581\n",
      "epoch:  431, loss: 0.007514612283557653\n",
      "epoch:  432, loss: 0.007512567099183798\n",
      "epoch:  433, loss: 0.007512054871767759\n",
      "epoch:  434, loss: 0.007511137519031763\n",
      "epoch:  435, loss: 0.00750769255682826\n",
      "epoch:  436, loss: 0.007507025264203548\n",
      "epoch:  437, loss: 0.0075063141994178295\n",
      "epoch:  438, loss: 0.00750265596434474\n",
      "epoch:  439, loss: 0.007502003572881222\n",
      "epoch:  440, loss: 0.007500145118683577\n",
      "epoch:  441, loss: 0.007497522514313459\n",
      "epoch:  442, loss: 0.007496930193156004\n",
      "epoch:  443, loss: 0.007494242396205664\n",
      "epoch:  444, loss: 0.00749234901741147\n",
      "epoch:  445, loss: 0.007491824682801962\n",
      "epoch:  446, loss: 0.007489391602575779\n",
      "epoch:  447, loss: 0.007487158756703138\n",
      "epoch:  448, loss: 0.007486539892852306\n",
      "epoch:  449, loss: 0.007483241613954306\n",
      "epoch:  450, loss: 0.007481729611754417\n",
      "epoch:  451, loss: 0.007481174077838659\n",
      "epoch:  452, loss: 0.007479177322238684\n",
      "epoch:  453, loss: 0.007476428523659706\n",
      "epoch:  454, loss: 0.007475794292986393\n",
      "epoch:  455, loss: 0.007474883459508419\n",
      "epoch:  456, loss: 0.007470902521163225\n",
      "epoch:  457, loss: 0.0074702659621834755\n",
      "epoch:  458, loss: 0.007467242423444986\n",
      "epoch:  459, loss: 0.007465153932571411\n",
      "epoch:  460, loss: 0.007464547175914049\n",
      "epoch:  461, loss: 0.007460583932697773\n",
      "epoch:  462, loss: 0.007459387648850679\n",
      "epoch:  463, loss: 0.007458444684743881\n",
      "epoch:  464, loss: 0.007454452570527792\n",
      "epoch:  465, loss: 0.00745374895632267\n",
      "epoch:  466, loss: 0.0074495188891887665\n",
      "epoch:  467, loss: 0.0074484203942120075\n",
      "epoch:  468, loss: 0.007447888609021902\n",
      "epoch:  469, loss: 0.007438013330101967\n",
      "epoch:  470, loss: 0.007402054499834776\n",
      "epoch:  471, loss: 0.007399551570415497\n",
      "epoch:  472, loss: 0.007398856803774834\n",
      "epoch:  473, loss: 0.00739558320492506\n",
      "epoch:  474, loss: 0.007393578067421913\n",
      "epoch:  475, loss: 0.007392995059490204\n",
      "epoch:  476, loss: 0.007388886064291\n",
      "epoch:  477, loss: 0.007387693505734205\n",
      "epoch:  478, loss: 0.007387168239802122\n",
      "epoch:  479, loss: 0.0073774890042841434\n",
      "epoch:  480, loss: 0.007342005148530006\n",
      "epoch:  481, loss: 0.007339743431657553\n",
      "epoch:  482, loss: 0.007338875904679298\n",
      "epoch:  483, loss: 0.007333992514759302\n",
      "epoch:  484, loss: 0.007332640700042248\n",
      "epoch:  485, loss: 0.007329899352043867\n",
      "epoch:  486, loss: 0.007327052764594555\n",
      "epoch:  487, loss: 0.007326296530663967\n",
      "epoch:  488, loss: 0.007323061116039753\n",
      "epoch:  489, loss: 0.0073210750706493855\n",
      "epoch:  490, loss: 0.007320376578718424\n",
      "epoch:  491, loss: 0.007316694129258394\n",
      "epoch:  492, loss: 0.0073148999363183975\n",
      "epoch:  493, loss: 0.0073142885230481625\n",
      "epoch:  494, loss: 0.007312105502933264\n",
      "epoch:  495, loss: 0.007308965548872948\n",
      "epoch:  496, loss: 0.007308166939765215\n",
      "epoch:  497, loss: 0.007307359483093023\n",
      "epoch:  498, loss: 0.007302660960704088\n",
      "epoch:  499, loss: 0.0073019214905798435\n",
      "epoch:  500, loss: 0.00729773985221982\n",
      "epoch:  501, loss: 0.0072962818667292595\n",
      "epoch:  502, loss: 0.007295575458556414\n",
      "epoch:  503, loss: 0.007292905822396278\n",
      "epoch:  504, loss: 0.007289764937013388\n",
      "epoch:  505, loss: 0.007289128843694925\n",
      "epoch:  506, loss: 0.0072884452529251575\n",
      "epoch:  507, loss: 0.007283554878085852\n",
      "epoch:  508, loss: 0.007282608654350042\n",
      "epoch:  509, loss: 0.0072808777913451195\n",
      "epoch:  510, loss: 0.007276497781276703\n",
      "epoch:  511, loss: 0.0072756740264594555\n",
      "epoch:  512, loss: 0.00727454898878932\n",
      "epoch:  513, loss: 0.007269624155014753\n",
      "epoch:  514, loss: 0.007268640212714672\n",
      "epoch:  515, loss: 0.007266812026500702\n",
      "epoch:  516, loss: 0.007262521423399448\n",
      "epoch:  517, loss: 0.007261668331921101\n",
      "epoch:  518, loss: 0.007258933037519455\n",
      "epoch:  519, loss: 0.007255333475768566\n",
      "epoch:  520, loss: 0.007254479918628931\n",
      "epoch:  521, loss: 0.007250016555190086\n",
      "epoch:  522, loss: 0.0072478121146559715\n",
      "epoch:  523, loss: 0.0072470782324671745\n",
      "epoch:  524, loss: 0.007242841646075249\n",
      "epoch:  525, loss: 0.00724048400297761\n",
      "epoch:  526, loss: 0.007239708676934242\n",
      "epoch:  527, loss: 0.007234584074467421\n",
      "epoch:  528, loss: 0.007232643663883209\n",
      "epoch:  529, loss: 0.00723190838471055\n",
      "epoch:  530, loss: 0.007226518355309963\n",
      "epoch:  531, loss: 0.007224988657981157\n",
      "epoch:  532, loss: 0.007224265951663256\n",
      "epoch:  533, loss: 0.007219495717436075\n",
      "epoch:  534, loss: 0.007217245642095804\n",
      "epoch:  535, loss: 0.007216448429971933\n",
      "epoch:  536, loss: 0.007211190182715654\n",
      "epoch:  537, loss: 0.007209299132227898\n",
      "epoch:  538, loss: 0.007208536844700575\n",
      "epoch:  539, loss: 0.007203663233667612\n",
      "epoch:  540, loss: 0.007201466243714094\n",
      "epoch:  541, loss: 0.0072006783448159695\n",
      "epoch:  542, loss: 0.007195528596639633\n",
      "epoch:  543, loss: 0.007193525787442923\n",
      "epoch:  544, loss: 0.00719281192868948\n",
      "epoch:  545, loss: 0.007187682669609785\n",
      "epoch:  546, loss: 0.007186157628893852\n",
      "epoch:  547, loss: 0.007185457739979029\n",
      "epoch:  548, loss: 0.007180039770901203\n",
      "epoch:  549, loss: 0.007178828585892916\n",
      "epoch:  550, loss: 0.007178134750574827\n",
      "epoch:  551, loss: 0.007172629237174988\n",
      "epoch:  552, loss: 0.007171228993684053\n",
      "epoch:  553, loss: 0.007170533761382103\n",
      "epoch:  554, loss: 0.007165988441556692\n",
      "epoch:  555, loss: 0.007163563743233681\n",
      "epoch:  556, loss: 0.007162143010646105\n",
      "epoch:  557, loss: 0.007156164385378361\n",
      "epoch:  558, loss: 0.007154957856982946\n",
      "epoch:  559, loss: 0.007154184393584728\n",
      "epoch:  560, loss: 0.007148647680878639\n",
      "epoch:  561, loss: 0.007146646734327078\n",
      "epoch:  562, loss: 0.007145894691348076\n",
      "epoch:  563, loss: 0.007140886504203081\n",
      "epoch:  564, loss: 0.0071390843950212\n",
      "epoch:  565, loss: 0.007138350512832403\n",
      "epoch:  566, loss: 0.007134776096791029\n",
      "epoch:  567, loss: 0.007131985388696194\n",
      "epoch:  568, loss: 0.007131205406039953\n",
      "epoch:  569, loss: 0.0071305702440440655\n",
      "epoch:  570, loss: 0.007126555312424898\n",
      "epoch:  571, loss: 0.007124302443116903\n",
      "epoch:  572, loss: 0.0071235341019928455\n",
      "epoch:  573, loss: 0.007120418827980757\n",
      "epoch:  574, loss: 0.00711727375164628\n",
      "epoch:  575, loss: 0.0071164583787322044\n",
      "epoch:  576, loss: 0.007115823682397604\n",
      "epoch:  577, loss: 0.0071104178205132484\n",
      "epoch:  578, loss: 0.007109445054084063\n",
      "epoch:  579, loss: 0.007108769845217466\n",
      "epoch:  580, loss: 0.007103741634637117\n",
      "epoch:  581, loss: 0.0071024298667907715\n",
      "epoch:  582, loss: 0.007101715076714754\n",
      "epoch:  583, loss: 0.007098337169736624\n",
      "epoch:  584, loss: 0.00709551339969039\n",
      "epoch:  585, loss: 0.007094646338373423\n",
      "epoch:  586, loss: 0.0070940000005066395\n",
      "epoch:  587, loss: 0.00709132757037878\n",
      "epoch:  588, loss: 0.007087758742272854\n",
      "epoch:  589, loss: 0.007086902856826782\n",
      "epoch:  590, loss: 0.0070850239135324955\n",
      "epoch:  591, loss: 0.007080794312059879\n",
      "epoch:  592, loss: 0.007079842034727335\n",
      "epoch:  593, loss: 0.0070791649632155895\n",
      "epoch:  594, loss: 0.007074141874909401\n",
      "epoch:  595, loss: 0.007072826381772757\n",
      "epoch:  596, loss: 0.00707210274413228\n",
      "epoch:  597, loss: 0.007068765815347433\n",
      "epoch:  598, loss: 0.007065849844366312\n",
      "epoch:  599, loss: 0.007064979989081621\n",
      "epoch:  600, loss: 0.007062780205160379\n",
      "epoch:  601, loss: 0.0070588295347988605\n",
      "epoch:  602, loss: 0.007057803217321634\n",
      "epoch:  603, loss: 0.007057139649987221\n",
      "epoch:  604, loss: 0.007054421119391918\n",
      "epoch:  605, loss: 0.007050902117043734\n",
      "epoch:  606, loss: 0.007050001993775368\n",
      "epoch:  607, loss: 0.007049356587231159\n",
      "epoch:  608, loss: 0.00704388739541173\n",
      "epoch:  609, loss: 0.007042760029435158\n",
      "epoch:  610, loss: 0.007042123004794121\n",
      "epoch:  611, loss: 0.007040377706289291\n",
      "epoch:  612, loss: 0.007035862188786268\n",
      "epoch:  613, loss: 0.007034804672002792\n",
      "epoch:  614, loss: 0.007034142967313528\n",
      "epoch:  615, loss: 0.007031835149973631\n",
      "epoch:  616, loss: 0.007027673535048962\n",
      "epoch:  617, loss: 0.0070267328992486\n",
      "epoch:  618, loss: 0.007024480029940605\n",
      "epoch:  619, loss: 0.007019958924502134\n",
      "epoch:  620, loss: 0.007019059732556343\n",
      "epoch:  621, loss: 0.007018327713012695\n",
      "epoch:  622, loss: 0.007012535352259874\n",
      "epoch:  623, loss: 0.007011427078396082\n",
      "epoch:  624, loss: 0.0070107076317071915\n",
      "epoch:  625, loss: 0.0070077585987746716\n",
      "epoch:  626, loss: 0.007004048675298691\n",
      "epoch:  627, loss: 0.007003075908869505\n",
      "epoch:  628, loss: 0.007002206519246101\n",
      "epoch:  629, loss: 0.006996593903750181\n",
      "epoch:  630, loss: 0.006995444186031818\n",
      "epoch:  631, loss: 0.006994735449552536\n",
      "epoch:  632, loss: 0.006989108398556709\n",
      "epoch:  633, loss: 0.006987819913774729\n",
      "epoch:  634, loss: 0.006987061817198992\n",
      "epoch:  635, loss: 0.006982118356972933\n",
      "epoch:  636, loss: 0.006980275735259056\n",
      "epoch:  637, loss: 0.0069794305600225925\n",
      "epoch:  638, loss: 0.00697667570784688\n",
      "epoch:  639, loss: 0.006972758565098047\n",
      "epoch:  640, loss: 0.0069717830047011375\n",
      "epoch:  641, loss: 0.00697105610743165\n",
      "epoch:  642, loss: 0.006965744774788618\n",
      "epoch:  643, loss: 0.0069640884175896645\n",
      "epoch:  644, loss: 0.006963321007788181\n",
      "epoch:  645, loss: 0.006958378478884697\n",
      "epoch:  646, loss: 0.006956506986171007\n",
      "epoch:  647, loss: 0.006955609191209078\n",
      "epoch:  648, loss: 0.006953028496354818\n",
      "epoch:  649, loss: 0.006948948372155428\n",
      "epoch:  650, loss: 0.006947956047952175\n",
      "epoch:  651, loss: 0.006947241257876158\n",
      "epoch:  652, loss: 0.006942682899534702\n",
      "epoch:  653, loss: 0.006940433755517006\n",
      "epoch:  654, loss: 0.006939629092812538\n",
      "epoch:  655, loss: 0.006938926875591278\n",
      "epoch:  656, loss: 0.0069337282329797745\n",
      "epoch:  657, loss: 0.0069320788607001305\n",
      "epoch:  658, loss: 0.006931303534656763\n",
      "epoch:  659, loss: 0.0069299242459237576\n",
      "epoch:  660, loss: 0.006924986839294434\n",
      "epoch:  661, loss: 0.006923687178641558\n",
      "epoch:  662, loss: 0.006922963075339794\n",
      "epoch:  663, loss: 0.006918198429048061\n",
      "epoch:  664, loss: 0.006916106678545475\n",
      "epoch:  665, loss: 0.006915276870131493\n",
      "epoch:  666, loss: 0.006912306882441044\n",
      "epoch:  667, loss: 0.00690854899585247\n",
      "epoch:  668, loss: 0.006907558999955654\n",
      "epoch:  669, loss: 0.006906872149556875\n",
      "epoch:  670, loss: 0.006901666987687349\n",
      "epoch:  671, loss: 0.006899864412844181\n",
      "epoch:  672, loss: 0.006899095606058836\n",
      "epoch:  673, loss: 0.006895329337567091\n",
      "epoch:  674, loss: 0.006892287638038397\n",
      "epoch:  675, loss: 0.006891262251883745\n",
      "epoch:  676, loss: 0.0068906741216778755\n",
      "epoch:  677, loss: 0.006884581875056028\n",
      "epoch:  678, loss: 0.006883374415338039\n",
      "epoch:  679, loss: 0.006882643327116966\n",
      "epoch:  680, loss: 0.006877133622765541\n",
      "epoch:  681, loss: 0.0068754767999053\n",
      "epoch:  682, loss: 0.006874685641378164\n",
      "epoch:  683, loss: 0.006870790384709835\n",
      "epoch:  684, loss: 0.006867699325084686\n",
      "epoch:  685, loss: 0.006866755895316601\n",
      "epoch:  686, loss: 0.006866034120321274\n",
      "epoch:  687, loss: 0.006861078552901745\n",
      "epoch:  688, loss: 0.006858982145786285\n",
      "epoch:  689, loss: 0.006858113221824169\n",
      "epoch:  690, loss: 0.006855115760117769\n",
      "epoch:  691, loss: 0.006851195357739925\n",
      "epoch:  692, loss: 0.0068501499481499195\n",
      "epoch:  693, loss: 0.006849424913525581\n",
      "epoch:  694, loss: 0.006844974122941494\n",
      "epoch:  695, loss: 0.006842419970780611\n",
      "epoch:  696, loss: 0.006841497495770454\n",
      "epoch:  697, loss: 0.006840785499662161\n",
      "epoch:  698, loss: 0.006835775449872017\n",
      "epoch:  699, loss: 0.006833616178482771\n",
      "epoch:  700, loss: 0.006832784041762352\n",
      "epoch:  701, loss: 0.006832065526396036\n",
      "epoch:  702, loss: 0.006826726254075766\n",
      "epoch:  703, loss: 0.0068248575553298\n",
      "epoch:  704, loss: 0.0068240296095609665\n",
      "epoch:  705, loss: 0.006823316216468811\n",
      "epoch:  706, loss: 0.006817277520895004\n",
      "epoch:  707, loss: 0.006816031876951456\n",
      "epoch:  708, loss: 0.006815264001488686\n",
      "epoch:  709, loss: 0.006810461636632681\n",
      "epoch:  710, loss: 0.006808060687035322\n",
      "epoch:  711, loss: 0.0068071880377829075\n",
      "epoch:  712, loss: 0.0068060122430324554\n",
      "epoch:  713, loss: 0.006800558418035507\n",
      "epoch:  714, loss: 0.006799164693802595\n",
      "epoch:  715, loss: 0.006798416376113892\n",
      "epoch:  716, loss: 0.006795077584683895\n",
      "epoch:  717, loss: 0.006791502702981234\n",
      "epoch:  718, loss: 0.00679033761844039\n",
      "epoch:  719, loss: 0.006789603270590305\n",
      "epoch:  720, loss: 0.006784119643270969\n",
      "epoch:  721, loss: 0.0067823478020727634\n",
      "epoch:  722, loss: 0.006781501695513725\n",
      "epoch:  723, loss: 0.006778084672987461\n",
      "epoch:  724, loss: 0.0067744324915111065\n",
      "epoch:  725, loss: 0.0067734126932919025\n",
      "epoch:  726, loss: 0.006772693712264299\n",
      "epoch:  727, loss: 0.006768978666514158\n",
      "epoch:  728, loss: 0.006765586789697409\n",
      "epoch:  729, loss: 0.006764580961316824\n",
      "epoch:  730, loss: 0.006763783749192953\n",
      "epoch:  731, loss: 0.006757925730198622\n",
      "epoch:  732, loss: 0.006756529677659273\n",
      "epoch:  733, loss: 0.006755731999874115\n",
      "epoch:  734, loss: 0.00675298273563385\n",
      "epoch:  735, loss: 0.006748723331838846\n",
      "epoch:  736, loss: 0.006747531704604626\n",
      "epoch:  737, loss: 0.006746775470674038\n",
      "epoch:  738, loss: 0.0067427679896354675\n",
      "epoch:  739, loss: 0.0067397127859294415\n",
      "epoch:  740, loss: 0.006738650146871805\n",
      "epoch:  741, loss: 0.00673788832500577\n",
      "epoch:  742, loss: 0.006732487119734287\n",
      "epoch:  743, loss: 0.006730574183166027\n",
      "epoch:  744, loss: 0.006729620508849621\n",
      "epoch:  745, loss: 0.006727845408022404\n",
      "epoch:  746, loss: 0.006722609978169203\n",
      "epoch:  747, loss: 0.006721421144902706\n",
      "epoch:  748, loss: 0.006720608100295067\n",
      "epoch:  749, loss: 0.006717017851769924\n",
      "epoch:  750, loss: 0.006713369861245155\n",
      "epoch:  751, loss: 0.0067123412154614925\n",
      "epoch:  752, loss: 0.006711562164127827\n",
      "epoch:  753, loss: 0.006706753745675087\n",
      "epoch:  754, loss: 0.006704158615320921\n",
      "epoch:  755, loss: 0.006703241728246212\n",
      "epoch:  756, loss: 0.006702486891299486\n",
      "epoch:  757, loss: 0.0066965119913220406\n",
      "epoch:  758, loss: 0.006694978103041649\n",
      "epoch:  759, loss: 0.006694102194160223\n",
      "epoch:  760, loss: 0.006693348754197359\n",
      "epoch:  761, loss: 0.006690741516649723\n",
      "epoch:  762, loss: 0.006686185486614704\n",
      "epoch:  763, loss: 0.006684994325041771\n",
      "epoch:  764, loss: 0.006684195250272751\n",
      "epoch:  765, loss: 0.0066803633235394955\n",
      "epoch:  766, loss: 0.006676902994513512\n",
      "epoch:  767, loss: 0.006675844546407461\n",
      "epoch:  768, loss: 0.0066750263795256615\n",
      "epoch:  769, loss: 0.00667348550632596\n",
      "epoch:  770, loss: 0.006668120622634888\n",
      "epoch:  771, loss: 0.0066667161881923676\n",
      "epoch:  772, loss: 0.006665878929197788\n",
      "epoch:  773, loss: 0.0066637396812438965\n",
      "epoch:  774, loss: 0.006658843718469143\n",
      "epoch:  775, loss: 0.006657503545284271\n",
      "epoch:  776, loss: 0.006656650919467211\n",
      "epoch:  777, loss: 0.006655113771557808\n",
      "epoch:  778, loss: 0.006649631541222334\n",
      "epoch:  779, loss: 0.0066482070833444595\n",
      "epoch:  780, loss: 0.006647392176091671\n",
      "epoch:  781, loss: 0.0066458252258598804\n",
      "epoch:  782, loss: 0.006640565115958452\n",
      "epoch:  783, loss: 0.00663895346224308\n",
      "epoch:  784, loss: 0.00663813715800643\n",
      "epoch:  785, loss: 0.006635564379394054\n",
      "epoch:  786, loss: 0.006631126161664724\n",
      "epoch:  787, loss: 0.006629684008657932\n",
      "epoch:  788, loss: 0.0066288430243730545\n",
      "epoch:  789, loss: 0.006626443937420845\n",
      "epoch:  790, loss: 0.006621872540563345\n",
      "epoch:  791, loss: 0.006620394065976143\n",
      "epoch:  792, loss: 0.006619547493755817\n",
      "epoch:  793, loss: 0.006619440857321024\n",
      "epoch:  794, loss: 0.0066129351034760475\n",
      "epoch:  795, loss: 0.006611260585486889\n",
      "epoch:  796, loss: 0.00661034369841218\n",
      "epoch:  797, loss: 0.006607339717447758\n",
      "epoch:  798, loss: 0.006603182293474674\n",
      "epoch:  799, loss: 0.006601912900805473\n",
      "epoch:  800, loss: 0.00660110916942358\n",
      "epoch:  801, loss: 0.006597419735044241\n",
      "epoch:  802, loss: 0.006593854166567326\n",
      "epoch:  803, loss: 0.006592649035155773\n",
      "epoch:  804, loss: 0.0065918052569031715\n",
      "epoch:  805, loss: 0.0065872943960130215\n",
      "epoch:  806, loss: 0.006584289949387312\n",
      "epoch:  807, loss: 0.006583203095942736\n",
      "epoch:  808, loss: 0.0065824114717543125\n",
      "epoch:  809, loss: 0.006577492691576481\n",
      "epoch:  810, loss: 0.0065748062916100025\n",
      "epoch:  811, loss: 0.006573820021003485\n",
      "epoch:  812, loss: 0.0065730176866054535\n",
      "epoch:  813, loss: 0.006567702163010836\n",
      "epoch:  814, loss: 0.00656534917652607\n",
      "epoch:  815, loss: 0.006564359646290541\n",
      "epoch:  816, loss: 0.006563528440892696\n",
      "epoch:  817, loss: 0.006558682769536972\n",
      "epoch:  818, loss: 0.0065557644702494144\n",
      "epoch:  819, loss: 0.006554718594998121\n",
      "epoch:  820, loss: 0.0065538701601326466\n",
      "epoch:  821, loss: 0.00654992088675499\n",
      "epoch:  822, loss: 0.0065461122430861\n",
      "epoch:  823, loss: 0.006544855888932943\n",
      "epoch:  824, loss: 0.006544013507664204\n",
      "epoch:  825, loss: 0.006540194153785706\n",
      "epoch:  826, loss: 0.006536124739795923\n",
      "epoch:  827, loss: 0.006534872576594353\n",
      "epoch:  828, loss: 0.006533997133374214\n",
      "epoch:  829, loss: 0.006528615020215511\n",
      "epoch:  830, loss: 0.006525948643684387\n",
      "epoch:  831, loss: 0.0065248869359493256\n",
      "epoch:  832, loss: 0.006524013355374336\n",
      "epoch:  833, loss: 0.0065208254382014275\n",
      "epoch:  834, loss: 0.006516315042972565\n",
      "epoch:  835, loss: 0.006515026092529297\n",
      "epoch:  836, loss: 0.006514105014503002\n",
      "epoch:  837, loss: 0.006510511506348848\n",
      "epoch:  838, loss: 0.006506452802568674\n",
      "epoch:  839, loss: 0.006505119614303112\n",
      "epoch:  840, loss: 0.006504187360405922\n",
      "epoch:  841, loss: 0.0065033831633627415\n",
      "epoch:  842, loss: 0.006499422714114189\n",
      "epoch:  843, loss: 0.00649568997323513\n",
      "epoch:  844, loss: 0.0064943525940179825\n",
      "epoch:  845, loss: 0.006493504624813795\n",
      "epoch:  846, loss: 0.0064926897175610065\n",
      "epoch:  847, loss: 0.006488163489848375\n",
      "epoch:  848, loss: 0.006484950426965952\n",
      "epoch:  849, loss: 0.006483763922005892\n",
      "epoch:  850, loss: 0.006482853554189205\n",
      "epoch:  851, loss: 0.006481918040663004\n",
      "epoch:  852, loss: 0.006475778762251139\n",
      "epoch:  853, loss: 0.006474093534052372\n",
      "epoch:  854, loss: 0.006473076064139605\n",
      "epoch:  855, loss: 0.00647230027243495\n",
      "epoch:  856, loss: 0.006469764746725559\n",
      "epoch:  857, loss: 0.006465021055191755\n",
      "epoch:  858, loss: 0.006463541649281979\n",
      "epoch:  859, loss: 0.006462612189352512\n",
      "epoch:  860, loss: 0.006461809854954481\n",
      "epoch:  861, loss: 0.00645813811570406\n",
      "epoch:  862, loss: 0.006454274524003267\n",
      "epoch:  863, loss: 0.00645305123180151\n",
      "epoch:  864, loss: 0.00645220372825861\n",
      "epoch:  865, loss: 0.006450672633945942\n",
      "epoch:  866, loss: 0.006445213221013546\n",
      "epoch:  867, loss: 0.006443491205573082\n",
      "epoch:  868, loss: 0.006442574318498373\n",
      "epoch:  869, loss: 0.006441779434680939\n",
      "epoch:  870, loss: 0.006437957286834717\n",
      "epoch:  871, loss: 0.006434417795389891\n",
      "epoch:  872, loss: 0.006433127913624048\n",
      "epoch:  873, loss: 0.006432235240936279\n",
      "epoch:  874, loss: 0.0064314911141991615\n",
      "epoch:  875, loss: 0.006430720444768667\n",
      "epoch:  876, loss: 0.006428879220038652\n",
      "epoch:  877, loss: 0.006423745770007372\n",
      "epoch:  878, loss: 0.0064222016371786594\n",
      "epoch:  879, loss: 0.006421243771910667\n",
      "epoch:  880, loss: 0.006420455873012543\n",
      "epoch:  881, loss: 0.0064170584082603455\n",
      "epoch:  882, loss: 0.006413154304027557\n",
      "epoch:  883, loss: 0.0064119803719222546\n",
      "epoch:  884, loss: 0.006411099806427956\n",
      "epoch:  885, loss: 0.0064094820991158485\n",
      "epoch:  886, loss: 0.006404236890375614\n",
      "epoch:  887, loss: 0.006402584724128246\n",
      "epoch:  888, loss: 0.006401677615940571\n",
      "epoch:  889, loss: 0.00639960216358304\n",
      "epoch:  890, loss: 0.006394640542566776\n",
      "epoch:  891, loss: 0.00639317324385047\n",
      "epoch:  892, loss: 0.006392234470695257\n",
      "epoch:  893, loss: 0.006391444243490696\n",
      "epoch:  894, loss: 0.006388664711266756\n",
      "epoch:  895, loss: 0.006384215783327818\n",
      "epoch:  896, loss: 0.006382803898304701\n",
      "epoch:  897, loss: 0.006381888408213854\n",
      "epoch:  898, loss: 0.006381101906299591\n",
      "epoch:  899, loss: 0.006376628763973713\n",
      "epoch:  900, loss: 0.006373622454702854\n",
      "epoch:  901, loss: 0.006372495554387569\n",
      "epoch:  902, loss: 0.006371614057570696\n",
      "epoch:  903, loss: 0.006370848044753075\n",
      "epoch:  904, loss: 0.006366740446537733\n",
      "epoch:  905, loss: 0.00636347895488143\n",
      "epoch:  906, loss: 0.006362289655953646\n",
      "epoch:  907, loss: 0.006361441686749458\n",
      "epoch:  908, loss: 0.0063576847314834595\n",
      "epoch:  909, loss: 0.006354328244924545\n",
      "epoch:  910, loss: 0.0063529531471431255\n",
      "epoch:  911, loss: 0.006352075841277838\n",
      "epoch:  912, loss: 0.006351311691105366\n",
      "epoch:  913, loss: 0.006347603630274534\n",
      "epoch:  914, loss: 0.006344251334667206\n",
      "epoch:  915, loss: 0.006342814303934574\n",
      "epoch:  916, loss: 0.00634196400642395\n",
      "epoch:  917, loss: 0.006339828949421644\n",
      "epoch:  918, loss: 0.006335071288049221\n",
      "epoch:  919, loss: 0.006333660800009966\n",
      "epoch:  920, loss: 0.006332678254693747\n",
      "epoch:  921, loss: 0.0063318777829408646\n",
      "epoch:  922, loss: 0.0063270265236496925\n",
      "epoch:  923, loss: 0.0063243890181183815\n",
      "epoch:  924, loss: 0.0063233948312699795\n",
      "epoch:  925, loss: 0.00632249703630805\n",
      "epoch:  926, loss: 0.006321749649941921\n",
      "epoch:  927, loss: 0.006318740546703339\n",
      "epoch:  928, loss: 0.006314774043858051\n",
      "epoch:  929, loss: 0.006313409190624952\n",
      "epoch:  930, loss: 0.006312386132776737\n",
      "epoch:  931, loss: 0.006311595439910889\n",
      "epoch:  932, loss: 0.006309386342763901\n",
      "epoch:  933, loss: 0.006304607726633549\n",
      "epoch:  934, loss: 0.00630310270935297\n",
      "epoch:  935, loss: 0.006302178837358952\n",
      "epoch:  936, loss: 0.00630010524764657\n",
      "epoch:  937, loss: 0.006295276805758476\n",
      "epoch:  938, loss: 0.006293566897511482\n",
      "epoch:  939, loss: 0.006292637437582016\n",
      "epoch:  940, loss: 0.0062917862087488174\n",
      "epoch:  941, loss: 0.0062909964472055435\n",
      "epoch:  942, loss: 0.0062857745215296745\n",
      "epoch:  943, loss: 0.006283306982368231\n",
      "epoch:  944, loss: 0.006282297894358635\n",
      "epoch:  945, loss: 0.006281424779444933\n",
      "epoch:  946, loss: 0.006278106942772865\n",
      "epoch:  947, loss: 0.006274436600506306\n",
      "epoch:  948, loss: 0.006272941827774048\n",
      "epoch:  949, loss: 0.006271961610764265\n",
      "epoch:  950, loss: 0.006271172780543566\n",
      "epoch:  951, loss: 0.0062693022191524506\n",
      "epoch:  952, loss: 0.006264419294893742\n",
      "epoch:  953, loss: 0.0062626805156469345\n",
      "epoch:  954, loss: 0.006261688191443682\n",
      "epoch:  955, loss: 0.006260899361222982\n",
      "epoch:  956, loss: 0.0062601338140666485\n",
      "epoch:  957, loss: 0.006259397137910128\n",
      "epoch:  958, loss: 0.006259368732571602\n",
      "epoch:  959, loss: 0.0062529556453228\n",
      "epoch:  960, loss: 0.0062511092983186245\n",
      "epoch:  961, loss: 0.006250008009374142\n",
      "epoch:  962, loss: 0.006249194033443928\n",
      "epoch:  963, loss: 0.00624843779951334\n",
      "epoch:  964, loss: 0.006244427990168333\n",
      "epoch:  965, loss: 0.006241215392947197\n",
      "epoch:  966, loss: 0.006239930167794228\n",
      "epoch:  967, loss: 0.006238993257284164\n",
      "epoch:  968, loss: 0.006238211411982775\n",
      "epoch:  969, loss: 0.006236712913960218\n",
      "epoch:  970, loss: 0.006231419276446104\n",
      "epoch:  971, loss: 0.006229690741747618\n",
      "epoch:  972, loss: 0.006228666752576828\n",
      "epoch:  973, loss: 0.006227834615856409\n",
      "epoch:  974, loss: 0.006227759178727865\n",
      "epoch:  975, loss: 0.006221259478479624\n",
      "epoch:  976, loss: 0.006219259928911924\n",
      "epoch:  977, loss: 0.006218208000063896\n",
      "epoch:  978, loss: 0.0062173474580049515\n",
      "epoch:  979, loss: 0.006214925087988377\n",
      "epoch:  980, loss: 0.006210173945873976\n",
      "epoch:  981, loss: 0.006208532955497503\n",
      "epoch:  982, loss: 0.006207537371665239\n",
      "epoch:  983, loss: 0.00620673643425107\n",
      "epoch:  984, loss: 0.006205921061336994\n",
      "epoch:  985, loss: 0.006202505901455879\n",
      "epoch:  986, loss: 0.006198594346642494\n",
      "epoch:  987, loss: 0.006197154521942139\n",
      "epoch:  988, loss: 0.006196150090545416\n",
      "epoch:  989, loss: 0.006195324473083019\n",
      "epoch:  990, loss: 0.006194473244249821\n",
      "epoch:  991, loss: 0.0061885397881269455\n",
      "epoch:  992, loss: 0.006186557933688164\n",
      "epoch:  993, loss: 0.006185557693243027\n",
      "epoch:  994, loss: 0.006184710189700127\n",
      "epoch:  995, loss: 0.006183427292853594\n",
      "epoch:  996, loss: 0.006177710369229317\n",
      "epoch:  997, loss: 0.006175959948450327\n",
      "epoch:  998, loss: 0.006174931302666664\n",
      "epoch:  999, loss: 0.006174141075462103\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size=X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# opt = torch_numopt.ConjugateGradient(model=model, lr=1e-4, line_search_method=\"const\", cg_method=\"PR\")\n",
    "opt = torch_numopt.ConjugateGradient(model=model, lr=10, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"armijo\", cg_method=\"PR\")\n",
    "# opt = torch_numopt.ConjugateGradient(model=model, lr=10, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"wolfe\", cg_method=\"PR\")\n",
    "# opt = torch_numopt.ConjugateGradient(model=model, lr=10, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"strong-wolfe\", cg_method=\"PR\")\n",
    "# opt = torch_numopt.ConjugateGradient(model=model, lr=10, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"goldstein\", cg_method=\"PR\")\n",
    "\n",
    "\n",
    "all_loss = {}\n",
    "for epoch in range(1000):\n",
    "    print(\"epoch: \", epoch, end=\"\")\n",
    "    all_loss[epoch + 1] = 0\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y, loss_fn)\n",
    "\n",
    "        all_loss[epoch + 1] += loss\n",
    "    all_loss[epoch + 1] /= len(data_loader)\n",
    "    print(\", loss: {}\".format(all_loss[epoch + 1].cpu().detach().numpy().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics: R2 = 0.7631731348323153\n",
      "Test metrics:  R2 = 0.8241349821170552\n"
     ]
    }
   ],
   "source": [
    "pred_train = model.forward(X_train).cpu().detach()\n",
    "pred_test = model.forward(X_test).cpu().detach()\n",
    "print(f\"Train metrics: R2 = {r2_score(pred_train, y_train.cpu())}\")\n",
    "print(f\"Test metrics:  R2 = {r2_score(pred_test, y_test.cpu())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.1518639326095581\n",
      "epoch:  1, loss: 0.09925659000873566\n",
      "epoch:  2, loss: 0.06943614035844803\n",
      "epoch:  3, loss: 0.05270099639892578\n",
      "epoch:  4, loss: 0.043459419161081314\n",
      "epoch:  5, loss: 0.03843265771865845\n",
      "epoch:  6, loss: 0.035732947289943695\n",
      "epoch:  7, loss: 0.034297212958335876\n",
      "epoch:  8, loss: 0.03353939577937126\n",
      "epoch:  9, loss: 0.03314173221588135\n",
      "epoch:  10, loss: 0.03293376788496971\n",
      "epoch:  11, loss: 0.032825078815221786\n",
      "epoch:  12, loss: 0.03276817873120308\n",
      "epoch:  13, loss: 0.03273823484778404\n",
      "epoch:  14, loss: 0.03272224962711334\n",
      "epoch:  15, loss: 0.032713521271944046\n",
      "epoch:  16, loss: 0.032708510756492615\n",
      "epoch:  17, loss: 0.03270817920565605\n",
      "epoch:  18, loss: 0.032700322568416595\n",
      "epoch:  19, loss: 0.03269578143954277\n",
      "epoch:  20, loss: 0.0326940156519413\n",
      "epoch:  21, loss: 0.032687004655599594\n",
      "epoch:  22, loss: 0.0326828770339489\n",
      "epoch:  23, loss: 0.03267928957939148\n",
      "epoch:  24, loss: 0.03267284482717514\n",
      "epoch:  25, loss: 0.032668955624103546\n",
      "epoch:  26, loss: 0.03266401216387749\n",
      "epoch:  27, loss: 0.03265833482146263\n",
      "epoch:  28, loss: 0.03265487775206566\n",
      "epoch:  29, loss: 0.03265013545751572\n",
      "epoch:  30, loss: 0.032645128667354584\n",
      "epoch:  31, loss: 0.0326450914144516\n",
      "epoch:  32, loss: 0.032637134194374084\n",
      "epoch:  33, loss: 0.0326324924826622\n",
      "epoch:  34, loss: 0.03263021260499954\n",
      "epoch:  35, loss: 0.03262315317988396\n",
      "epoch:  36, loss: 0.032619014382362366\n",
      "epoch:  37, loss: 0.032616425305604935\n",
      "epoch:  38, loss: 0.03261010721325874\n",
      "epoch:  39, loss: 0.032606326043605804\n",
      "epoch:  40, loss: 0.03260264918208122\n",
      "epoch:  41, loss: 0.03259698674082756\n",
      "epoch:  42, loss: 0.032593581825494766\n",
      "epoch:  43, loss: 0.032589469105005264\n",
      "epoch:  44, loss: 0.03258441761136055\n",
      "epoch:  45, loss: 0.03258439525961876\n",
      "epoch:  46, loss: 0.03257637470960617\n",
      "epoch:  47, loss: 0.0325716994702816\n",
      "epoch:  48, loss: 0.03256946802139282\n",
      "epoch:  49, loss: 0.03256232291460037\n",
      "epoch:  50, loss: 0.03255812078714371\n",
      "epoch:  51, loss: 0.03255487605929375\n",
      "epoch:  52, loss: 0.032548364251852036\n",
      "epoch:  53, loss: 0.03254442289471626\n",
      "epoch:  54, loss: 0.03253958001732826\n",
      "epoch:  55, loss: 0.032533787190914154\n",
      "epoch:  56, loss: 0.03253024443984032\n",
      "epoch:  57, loss: 0.032524943351745605\n",
      "epoch:  58, loss: 0.032519686967134476\n",
      "epoch:  59, loss: 0.032518208026885986\n",
      "epoch:  60, loss: 0.032510098069906235\n",
      "epoch:  61, loss: 0.03250535577535629\n",
      "epoch:  62, loss: 0.03250281512737274\n",
      "epoch:  63, loss: 0.03249554708600044\n",
      "epoch:  64, loss: 0.03249125927686691\n",
      "epoch:  65, loss: 0.032487835735082626\n",
      "epoch:  66, loss: 0.03248133882880211\n",
      "epoch:  67, loss: 0.032477445900440216\n",
      "epoch:  68, loss: 0.032473038882017136\n",
      "epoch:  69, loss: 0.03246709331870079\n",
      "epoch:  70, loss: 0.03246672451496124\n",
      "epoch:  71, loss: 0.032457418739795685\n",
      "epoch:  72, loss: 0.03245203569531441\n",
      "epoch:  73, loss: 0.032450247555971146\n",
      "epoch:  74, loss: 0.03244182839989662\n",
      "epoch:  75, loss: 0.03243687003850937\n",
      "epoch:  76, loss: 0.032433416694402695\n",
      "epoch:  77, loss: 0.03242585062980652\n",
      "epoch:  78, loss: 0.03242134675383568\n",
      "epoch:  79, loss: 0.03241678699851036\n",
      "epoch:  80, loss: 0.032409943640232086\n",
      "epoch:  81, loss: 0.0324058011174202\n",
      "epoch:  82, loss: 0.03240026533603668\n",
      "epoch:  83, loss: 0.03239407017827034\n",
      "epoch:  84, loss: 0.032393623143434525\n",
      "epoch:  85, loss: 0.03238382562994957\n",
      "epoch:  86, loss: 0.03237816318869591\n",
      "epoch:  87, loss: 0.032375793904066086\n",
      "epoch:  88, loss: 0.032366927713155746\n",
      "epoch:  89, loss: 0.03236168995499611\n",
      "epoch:  90, loss: 0.032357435673475266\n",
      "epoch:  91, loss: 0.0323493629693985\n",
      "epoch:  92, loss: 0.03234453126788139\n",
      "epoch:  93, loss: 0.032338857650756836\n",
      "epoch:  94, loss: 0.03233150765299797\n",
      "epoch:  95, loss: 0.03233104944229126\n",
      "epoch:  96, loss: 0.03231919929385185\n",
      "epoch:  97, loss: 0.032312311232089996\n",
      "epoch:  98, loss: 0.032309241592884064\n",
      "epoch:  99, loss: 0.03229851275682449\n",
      "epoch:  100, loss: 0.032292187213897705\n",
      "epoch:  101, loss: 0.03228721395134926\n",
      "epoch:  102, loss: 0.03227751702070236\n",
      "epoch:  103, loss: 0.03227177634835243\n",
      "epoch:  104, loss: 0.03226584196090698\n",
      "epoch:  105, loss: 0.03225698322057724\n",
      "epoch:  106, loss: 0.03225161135196686\n",
      "epoch:  107, loss: 0.03224337846040726\n",
      "epoch:  108, loss: 0.03223514184355736\n",
      "epoch:  109, loss: 0.03223241865634918\n",
      "epoch:  110, loss: 0.03221924602985382\n",
      "epoch:  111, loss: 0.0322115458548069\n",
      "epoch:  112, loss: 0.03220599517226219\n",
      "epoch:  113, loss: 0.03219367936253548\n",
      "epoch:  114, loss: 0.03218632563948631\n",
      "epoch:  115, loss: 0.032177310436964035\n",
      "epoch:  116, loss: 0.032165925949811935\n",
      "epoch:  117, loss: 0.0321589931845665\n",
      "epoch:  118, loss: 0.03214816004037857\n",
      "epoch:  119, loss: 0.03213769197463989\n",
      "epoch:  120, loss: 0.03213512897491455\n",
      "epoch:  121, loss: 0.03211821988224983\n",
      "epoch:  122, loss: 0.03210844472050667\n",
      "epoch:  123, loss: 0.0321040116250515\n",
      "epoch:  124, loss: 0.03208858519792557\n",
      "epoch:  125, loss: 0.032079584896564484\n",
      "epoch:  126, loss: 0.0320744514465332\n",
      "epoch:  127, loss: 0.03206012025475502\n",
      "epoch:  128, loss: 0.03205173835158348\n",
      "epoch:  129, loss: 0.03204524517059326\n",
      "epoch:  130, loss: 0.0320318341255188\n",
      "epoch:  131, loss: 0.032023921608924866\n",
      "epoch:  132, loss: 0.0320151261985302\n",
      "epoch:  133, loss: 0.032002583146095276\n",
      "epoch:  134, loss: 0.031995031982660294\n",
      "epoch:  135, loss: 0.03198414295911789\n",
      "epoch:  136, loss: 0.0319722518324852\n",
      "epoch:  137, loss: 0.031971536576747894\n",
      "epoch:  138, loss: 0.03195187821984291\n",
      "epoch:  139, loss: 0.031940631568431854\n",
      "epoch:  140, loss: 0.031936608254909515\n",
      "epoch:  141, loss: 0.03191797062754631\n",
      "epoch:  142, loss: 0.03190720081329346\n",
      "epoch:  143, loss: 0.03190016373991966\n",
      "epoch:  144, loss: 0.03188261017203331\n",
      "epoch:  145, loss: 0.03187231719493866\n",
      "epoch:  146, loss: 0.03186223283410072\n",
      "epoch:  147, loss: 0.03184548392891884\n",
      "epoch:  148, loss: 0.03183555603027344\n",
      "epoch:  149, loss: 0.03182262182235718\n",
      "epoch:  150, loss: 0.03180673345923424\n",
      "epoch:  151, loss: 0.031797144562006\n",
      "epoch:  152, loss: 0.03178141638636589\n",
      "epoch:  153, loss: 0.03176610916852951\n",
      "epoch:  154, loss: 0.031764205545186996\n",
      "epoch:  155, loss: 0.03173826262354851\n",
      "epoch:  156, loss: 0.03172346204519272\n",
      "epoch:  157, loss: 0.03171787038445473\n",
      "epoch:  158, loss: 0.03169265389442444\n",
      "epoch:  159, loss: 0.03167825937271118\n",
      "epoch:  160, loss: 0.03166915103793144\n",
      "epoch:  161, loss: 0.031645048409700394\n",
      "epoch:  162, loss: 0.03163102641701698\n",
      "epoch:  163, loss: 0.03161817416548729\n",
      "epoch:  164, loss: 0.031594738364219666\n",
      "epoch:  165, loss: 0.03158101439476013\n",
      "epoch:  166, loss: 0.03156457468867302\n",
      "epoch:  167, loss: 0.03154193237423897\n",
      "epoch:  168, loss: 0.031528446823358536\n",
      "epoch:  169, loss: 0.031508609652519226\n",
      "epoch:  170, loss: 0.03148633986711502\n",
      "epoch:  171, loss: 0.03147298842668533\n",
      "epoch:  172, loss: 0.03144947811961174\n",
      "epoch:  173, loss: 0.03142768144607544\n",
      "epoch:  174, loss: 0.03142537549138069\n",
      "epoch:  175, loss: 0.03138689324259758\n",
      "epoch:  176, loss: 0.031365346163511276\n",
      "epoch:  177, loss: 0.031358618289232254\n",
      "epoch:  178, loss: 0.03132086619734764\n",
      "epoch:  179, loss: 0.031299423426389694\n",
      "epoch:  180, loss: 0.0312882661819458\n",
      "epoch:  181, loss: 0.03125055879354477\n",
      "epoch:  182, loss: 0.031229224056005478\n",
      "epoch:  183, loss: 0.03121369704604149\n",
      "epoch:  184, loss: 0.03117651864886284\n",
      "epoch:  185, loss: 0.03115518018603325\n",
      "epoch:  186, loss: 0.03113485313951969\n",
      "epoch:  187, loss: 0.031097497791051865\n",
      "epoch:  188, loss: 0.03107599727809429\n",
      "epoch:  189, loss: 0.0310518778860569\n",
      "epoch:  190, loss: 0.03101443313062191\n",
      "epoch:  191, loss: 0.030992640182375908\n",
      "epoch:  192, loss: 0.03096422553062439\n",
      "epoch:  193, loss: 0.030926134437322617\n",
      "epoch:  194, loss: 0.030903931707143784\n",
      "epoch:  195, loss: 0.0308708343654871\n",
      "epoch:  196, loss: 0.03083212859928608\n",
      "epoch:  197, loss: 0.030809344723820686\n",
      "epoch:  198, loss: 0.030771564692258835\n",
      "epoch:  199, loss: 0.030731620267033577\n",
      "epoch:  200, loss: 0.03070812113583088\n",
      "epoch:  201, loss: 0.030666345730423927\n",
      "epoch:  202, loss: 0.030625490471720695\n",
      "epoch:  203, loss: 0.030601248145103455\n",
      "epoch:  204, loss: 0.030555108562111855\n",
      "epoch:  205, loss: 0.030512355268001556\n",
      "epoch:  206, loss: 0.030487069860100746\n",
      "epoch:  207, loss: 0.030436983332037926\n",
      "epoch:  208, loss: 0.030392533168196678\n",
      "epoch:  209, loss: 0.030366076156497\n",
      "epoch:  210, loss: 0.030311712995171547\n",
      "epoch:  211, loss: 0.03026447631418705\n",
      "epoch:  212, loss: 0.0302366241812706\n",
      "epoch:  213, loss: 0.0301775261759758\n",
      "epoch:  214, loss: 0.0301281176507473\n",
      "epoch:  215, loss: 0.030098838731646538\n",
      "epoch:  216, loss: 0.030035508796572685\n",
      "epoch:  217, loss: 0.029982533305883408\n",
      "epoch:  218, loss: 0.0299514289945364\n",
      "epoch:  219, loss: 0.029883040115237236\n",
      "epoch:  220, loss: 0.02982676587998867\n",
      "epoch:  221, loss: 0.02979356050491333\n",
      "epoch:  222, loss: 0.02972051501274109\n",
      "epoch:  223, loss: 0.029659228399395943\n",
      "epoch:  224, loss: 0.02962358482182026\n",
      "epoch:  225, loss: 0.02954617515206337\n",
      "epoch:  226, loss: 0.02947983704507351\n",
      "epoch:  227, loss: 0.029441343620419502\n",
      "epoch:  228, loss: 0.02935945615172386\n",
      "epoch:  229, loss: 0.02928616851568222\n",
      "epoch:  230, loss: 0.02924427017569542\n",
      "epoch:  231, loss: 0.02915806509554386\n",
      "epoch:  232, loss: 0.02907750755548477\n",
      "epoch:  233, loss: 0.029031744226813316\n",
      "epoch:  234, loss: 0.028943832963705063\n",
      "epoch:  235, loss: 0.028853939846158028\n",
      "epoch:  236, loss: 0.02880416065454483\n",
      "epoch:  237, loss: 0.028717735782265663\n",
      "epoch:  238, loss: 0.028617773205041885\n",
      "epoch:  239, loss: 0.02856297418475151\n",
      "epoch:  240, loss: 0.0284795630723238\n",
      "epoch:  241, loss: 0.02836501970887184\n",
      "epoch:  242, loss: 0.02830418013036251\n",
      "epoch:  243, loss: 0.028223585337400436\n",
      "epoch:  244, loss: 0.028095562011003494\n",
      "epoch:  245, loss: 0.02802802063524723\n",
      "epoch:  246, loss: 0.027957148849964142\n",
      "epoch:  247, loss: 0.027807051315903664\n",
      "epoch:  248, loss: 0.027730945497751236\n",
      "epoch:  249, loss: 0.0276719331741333\n",
      "epoch:  250, loss: 0.02749955654144287\n",
      "epoch:  251, loss: 0.02741362527012825\n",
      "epoch:  252, loss: 0.027378184720873833\n",
      "epoch:  253, loss: 0.027170846238732338\n",
      "epoch:  254, loss: 0.02707284316420555\n",
      "epoch:  255, loss: 0.027061406522989273\n",
      "epoch:  256, loss: 0.026822561398148537\n",
      "epoch:  257, loss: 0.02671058662235737\n",
      "epoch:  258, loss: 0.026647567749023438\n",
      "epoch:  259, loss: 0.026453327387571335\n",
      "epoch:  260, loss: 0.02632005698978901\n",
      "epoch:  261, loss: 0.026248715817928314\n",
      "epoch:  262, loss: 0.02605356089770794\n",
      "epoch:  263, loss: 0.025899337604641914\n",
      "epoch:  264, loss: 0.025819286704063416\n",
      "epoch:  265, loss: 0.025636345148086548\n",
      "epoch:  266, loss: 0.025450430810451508\n",
      "epoch:  267, loss: 0.02535994164645672\n",
      "epoch:  268, loss: 0.025183962658047676\n",
      "epoch:  269, loss: 0.02497192844748497\n",
      "epoch:  270, loss: 0.024871379137039185\n",
      "epoch:  271, loss: 0.024724403396248817\n",
      "epoch:  272, loss: 0.024465905502438545\n",
      "epoch:  273, loss: 0.02435227669775486\n",
      "epoch:  274, loss: 0.024230925366282463\n",
      "epoch:  275, loss: 0.023928232491016388\n",
      "epoch:  276, loss: 0.02380022034049034\n",
      "epoch:  277, loss: 0.023729566484689713\n",
      "epoch:  278, loss: 0.023362061008810997\n",
      "epoch:  279, loss: 0.023218465968966484\n",
      "epoch:  280, loss: 0.023176951333880424\n",
      "epoch:  281, loss: 0.022760214284062386\n",
      "epoch:  282, loss: 0.02260187640786171\n",
      "epoch:  283, loss: 0.022519150748848915\n",
      "epoch:  284, loss: 0.022135738283395767\n",
      "epoch:  285, loss: 0.02195766009390354\n",
      "epoch:  286, loss: 0.021870693191885948\n",
      "epoch:  287, loss: 0.021470125764608383\n",
      "epoch:  288, loss: 0.021286487579345703\n",
      "epoch:  289, loss: 0.021196942776441574\n",
      "epoch:  290, loss: 0.02079835906624794\n",
      "epoch:  291, loss: 0.020595639944076538\n",
      "epoch:  292, loss: 0.020504282787442207\n",
      "epoch:  293, loss: 0.02009100466966629\n",
      "epoch:  294, loss: 0.019887330010533333\n",
      "epoch:  295, loss: 0.019796118140220642\n",
      "epoch:  296, loss: 0.019360337406396866\n",
      "epoch:  297, loss: 0.019163496792316437\n",
      "epoch:  298, loss: 0.01907600834965706\n",
      "epoch:  299, loss: 0.018607206642627716\n",
      "epoch:  300, loss: 0.018433481454849243\n",
      "epoch:  301, loss: 0.018415583297610283\n",
      "epoch:  302, loss: 0.0178502406924963\n",
      "epoch:  303, loss: 0.017710978165268898\n",
      "epoch:  304, loss: 0.017484353855252266\n",
      "epoch:  305, loss: 0.017100106924772263\n",
      "epoch:  306, loss: 0.01699107326567173\n",
      "epoch:  307, loss: 0.01663631945848465\n",
      "epoch:  308, loss: 0.016363156959414482\n",
      "epoch:  309, loss: 0.016277171671390533\n",
      "epoch:  310, loss: 0.01582062430679798\n",
      "epoch:  311, loss: 0.01565343327820301\n",
      "epoch:  312, loss: 0.0155585166066885\n",
      "epoch:  313, loss: 0.015081271529197693\n",
      "epoch:  314, loss: 0.014986040070652962\n",
      "epoch:  315, loss: 0.014631305821239948\n",
      "epoch:  316, loss: 0.01442044135183096\n",
      "epoch:  317, loss: 0.014354660175740719\n",
      "epoch:  318, loss: 0.01391006913036108\n",
      "epoch:  319, loss: 0.013811526820063591\n",
      "epoch:  320, loss: 0.013512663543224335\n",
      "epoch:  321, loss: 0.01330871507525444\n",
      "epoch:  322, loss: 0.013254012912511826\n",
      "epoch:  323, loss: 0.012856344692409039\n",
      "epoch:  324, loss: 0.012783934362232685\n",
      "epoch:  325, loss: 0.012477433308959007\n",
      "epoch:  326, loss: 0.01235171314328909\n",
      "epoch:  327, loss: 0.012177945114672184\n",
      "epoch:  328, loss: 0.011954228393733501\n",
      "epoch:  329, loss: 0.011911338195204735\n",
      "epoch:  330, loss: 0.011599994264543056\n",
      "epoch:  331, loss: 0.011546937748789787\n",
      "epoch:  332, loss: 0.011294517666101456\n",
      "epoch:  333, loss: 0.011218974366784096\n",
      "epoch:  334, loss: 0.01101228129118681\n",
      "epoch:  335, loss: 0.010917534120380878\n",
      "epoch:  336, loss: 0.010752007365226746\n",
      "epoch:  337, loss: 0.010639581829309464\n",
      "epoch:  338, loss: 0.010510889813303947\n",
      "epoch:  339, loss: 0.010386372916400433\n",
      "epoch:  340, loss: 0.010224518366158009\n",
      "epoch:  341, loss: 0.010158101096749306\n",
      "epoch:  342, loss: 0.010001959279179573\n",
      "epoch:  343, loss: 0.009952313266694546\n",
      "epoch:  344, loss: 0.009795311838388443\n",
      "epoch:  345, loss: 0.009768201038241386\n",
      "epoch:  346, loss: 0.00923954974859953\n",
      "epoch:  347, loss: 0.008617298677563667\n",
      "epoch:  348, loss: 0.0086089251562953\n",
      "epoch:  349, loss: 0.008340679109096527\n",
      "epoch:  350, loss: 0.00815820787101984\n",
      "epoch:  351, loss: 0.008150516077876091\n",
      "epoch:  352, loss: 0.008101061917841434\n",
      "epoch:  353, loss: 0.008096425794064999\n",
      "epoch:  354, loss: 0.00805666297674179\n",
      "epoch:  355, loss: 0.00790317915380001\n",
      "epoch:  356, loss: 0.007888122461736202\n",
      "epoch:  357, loss: 0.007848485372960567\n",
      "epoch:  358, loss: 0.007810579147189856\n",
      "epoch:  359, loss: 0.007805360481142998\n",
      "epoch:  360, loss: 0.007777536753565073\n",
      "epoch:  361, loss: 0.00777022959664464\n",
      "epoch:  362, loss: 0.007767782546579838\n",
      "epoch:  363, loss: 0.007752651814371347\n",
      "epoch:  364, loss: 0.007639483083039522\n",
      "epoch:  365, loss: 0.007629667408764362\n",
      "epoch:  366, loss: 0.00761024234816432\n",
      "epoch:  367, loss: 0.00758412154391408\n",
      "epoch:  368, loss: 0.0075808074325323105\n",
      "epoch:  369, loss: 0.007573890965431929\n",
      "epoch:  370, loss: 0.007562275975942612\n",
      "epoch:  371, loss: 0.0075602359138429165\n",
      "epoch:  372, loss: 0.007549841422587633\n",
      "epoch:  373, loss: 0.0075445217080414295\n",
      "epoch:  374, loss: 0.007542924024164677\n",
      "epoch:  375, loss: 0.007531156297773123\n",
      "epoch:  376, loss: 0.007528180256485939\n",
      "epoch:  377, loss: 0.007526755332946777\n",
      "epoch:  378, loss: 0.007509585469961166\n",
      "epoch:  379, loss: 0.0074113779701292515\n",
      "epoch:  380, loss: 0.007402477320283651\n",
      "epoch:  381, loss: 0.007400555536150932\n",
      "epoch:  382, loss: 0.007397336419671774\n",
      "epoch:  383, loss: 0.007388164754956961\n",
      "epoch:  384, loss: 0.007386387325823307\n",
      "epoch:  385, loss: 0.007382702082395554\n",
      "epoch:  386, loss: 0.00737479655072093\n",
      "epoch:  387, loss: 0.00737308245152235\n",
      "epoch:  388, loss: 0.0073691545985639095\n",
      "epoch:  389, loss: 0.00736210448667407\n",
      "epoch:  390, loss: 0.0073606460355222225\n",
      "epoch:  391, loss: 0.0073549142107367516\n",
      "epoch:  392, loss: 0.00734966155141592\n",
      "epoch:  393, loss: 0.007348269689828157\n",
      "epoch:  394, loss: 0.007345159072428942\n",
      "epoch:  395, loss: 0.0073377094231545925\n",
      "epoch:  396, loss: 0.007335974834859371\n",
      "epoch:  397, loss: 0.007334847468882799\n",
      "epoch:  398, loss: 0.00732576847076416\n",
      "epoch:  399, loss: 0.007323681376874447\n",
      "epoch:  400, loss: 0.007322571240365505\n",
      "epoch:  401, loss: 0.00731407618150115\n",
      "epoch:  402, loss: 0.007311752997338772\n",
      "epoch:  403, loss: 0.007310635410249233\n",
      "epoch:  404, loss: 0.007303055375814438\n",
      "epoch:  405, loss: 0.007300379686057568\n",
      "epoch:  406, loss: 0.007299275603145361\n",
      "epoch:  407, loss: 0.007292599882930517\n",
      "epoch:  408, loss: 0.007289764937013388\n",
      "epoch:  409, loss: 0.0072886827401816845\n",
      "epoch:  410, loss: 0.007283232174813747\n",
      "epoch:  411, loss: 0.00727950781583786\n",
      "epoch:  412, loss: 0.007278306409716606\n",
      "epoch:  413, loss: 0.00727414945140481\n",
      "epoch:  414, loss: 0.007269163616001606\n",
      "epoch:  415, loss: 0.0072678085416555405\n",
      "epoch:  416, loss: 0.007267428562045097\n",
      "epoch:  417, loss: 0.007259168662130833\n",
      "epoch:  418, loss: 0.007257494609802961\n",
      "epoch:  419, loss: 0.007256553042680025\n",
      "epoch:  420, loss: 0.007249651942402124\n",
      "epoch:  421, loss: 0.007247185334563255\n",
      "epoch:  422, loss: 0.007246079389005899\n",
      "epoch:  423, loss: 0.0072407969273626804\n",
      "epoch:  424, loss: 0.007236312609165907\n",
      "epoch:  425, loss: 0.00723500968888402\n",
      "epoch:  426, loss: 0.007231945637613535\n",
      "epoch:  427, loss: 0.007225364446640015\n",
      "epoch:  428, loss: 0.007223815657198429\n",
      "epoch:  429, loss: 0.007223410531878471\n",
      "epoch:  430, loss: 0.007214365992695093\n",
      "epoch:  431, loss: 0.007212430238723755\n",
      "epoch:  432, loss: 0.007211392745375633\n",
      "epoch:  433, loss: 0.007203458342701197\n",
      "epoch:  434, loss: 0.007201170548796654\n",
      "epoch:  435, loss: 0.007200074847787619\n",
      "epoch:  436, loss: 0.0071954187005758286\n",
      "epoch:  437, loss: 0.007190489210188389\n",
      "epoch:  438, loss: 0.007189122494310141\n",
      "epoch:  439, loss: 0.007188163697719574\n",
      "epoch:  440, loss: 0.007180437911301851\n",
      "epoch:  441, loss: 0.007178537081927061\n",
      "epoch:  442, loss: 0.007177557796239853\n",
      "epoch:  443, loss: 0.007171247620135546\n",
      "epoch:  444, loss: 0.007168480195105076\n",
      "epoch:  445, loss: 0.007167361211031675\n",
      "epoch:  446, loss: 0.007163587491959333\n",
      "epoch:  447, loss: 0.007158645428717136\n",
      "epoch:  448, loss: 0.007157330401241779\n",
      "epoch:  449, loss: 0.007156406994909048\n",
      "epoch:  450, loss: 0.0071488916873931885\n",
      "epoch:  451, loss: 0.007147260941565037\n",
      "epoch:  452, loss: 0.007146305404603481\n",
      "epoch:  453, loss: 0.0071389805525541306\n",
      "epoch:  454, loss: 0.007137012202292681\n",
      "epoch:  455, loss: 0.007135994266718626\n",
      "epoch:  456, loss: 0.007130063138902187\n",
      "epoch:  457, loss: 0.0071266754530370235\n",
      "epoch:  458, loss: 0.007125494536012411\n",
      "epoch:  459, loss: 0.007125352043658495\n",
      "epoch:  460, loss: 0.0071169123984873295\n",
      "epoch:  461, loss: 0.007115022279322147\n",
      "epoch:  462, loss: 0.007114026229828596\n",
      "epoch:  463, loss: 0.007109206635504961\n",
      "epoch:  464, loss: 0.007104901596903801\n",
      "epoch:  465, loss: 0.007103521842509508\n",
      "epoch:  466, loss: 0.007102572824805975\n",
      "epoch:  467, loss: 0.00709626916795969\n",
      "epoch:  468, loss: 0.007093233987689018\n",
      "epoch:  469, loss: 0.007091946434229612\n",
      "epoch:  470, loss: 0.007091007195413113\n",
      "epoch:  471, loss: 0.007084256038069725\n",
      "epoch:  472, loss: 0.0070815179497003555\n",
      "epoch:  473, loss: 0.0070803724229335785\n",
      "epoch:  474, loss: 0.0070787821896374226\n",
      "epoch:  475, loss: 0.0070717548951506615\n",
      "epoch:  476, loss: 0.0070699844509363174\n",
      "epoch:  477, loss: 0.007069004699587822\n",
      "epoch:  478, loss: 0.0070647201500833035\n",
      "epoch:  479, loss: 0.007060184143483639\n",
      "epoch:  480, loss: 0.00705881929025054\n",
      "epoch:  481, loss: 0.0070578837767243385\n",
      "epoch:  482, loss: 0.007051205262541771\n",
      "epoch:  483, loss: 0.007048760540783405\n",
      "epoch:  484, loss: 0.007047684863209724\n",
      "epoch:  485, loss: 0.007045126985758543\n",
      "epoch:  486, loss: 0.007038978394120932\n",
      "epoch:  487, loss: 0.007037282921373844\n",
      "epoch:  488, loss: 0.007036290597170591\n",
      "epoch:  489, loss: 0.007030811160802841\n",
      "epoch:  490, loss: 0.0070271738804876804\n",
      "epoch:  491, loss: 0.007025862578302622\n",
      "epoch:  492, loss: 0.0070249103009700775\n",
      "epoch:  493, loss: 0.007018469274044037\n",
      "epoch:  494, loss: 0.007015577517449856\n",
      "epoch:  495, loss: 0.00701431417837739\n",
      "epoch:  496, loss: 0.007013347465544939\n",
      "epoch:  497, loss: 0.007006255444139242\n",
      "epoch:  498, loss: 0.007003832142800093\n",
      "epoch:  499, loss: 0.00700266333296895\n",
      "epoch:  500, loss: 0.007001251447945833\n",
      "epoch:  501, loss: 0.006993994116783142\n",
      "epoch:  502, loss: 0.006992152892053127\n",
      "epoch:  503, loss: 0.0069911000318825245\n",
      "epoch:  504, loss: 0.006987427826970816\n",
      "epoch:  505, loss: 0.006981738843023777\n",
      "epoch:  506, loss: 0.0069800978526473045\n",
      "epoch:  507, loss: 0.006978990975767374\n",
      "epoch:  508, loss: 0.006974143907427788\n",
      "epoch:  509, loss: 0.006969134788960218\n",
      "epoch:  510, loss: 0.006967546883970499\n",
      "epoch:  511, loss: 0.0069664595648646355\n",
      "epoch:  512, loss: 0.0069619715213775635\n",
      "epoch:  513, loss: 0.006956927478313446\n",
      "epoch:  514, loss: 0.006955280900001526\n",
      "epoch:  515, loss: 0.006954213138669729\n",
      "epoch:  516, loss: 0.006953306496143341\n",
      "epoch:  517, loss: 0.006945557426661253\n",
      "epoch:  518, loss: 0.006943489890545607\n",
      "epoch:  519, loss: 0.006942341104149818\n",
      "epoch:  520, loss: 0.006942306645214558\n",
      "epoch:  521, loss: 0.0069338660687208176\n",
      "epoch:  522, loss: 0.006931595504283905\n",
      "epoch:  523, loss: 0.006930421572178602\n",
      "epoch:  524, loss: 0.006929444149136543\n",
      "epoch:  525, loss: 0.006924778688699007\n",
      "epoch:  526, loss: 0.0069201430305838585\n",
      "epoch:  527, loss: 0.006918524391949177\n",
      "epoch:  528, loss: 0.006917445454746485\n",
      "epoch:  529, loss: 0.006916674319654703\n",
      "epoch:  530, loss: 0.0069088623858988285\n",
      "epoch:  531, loss: 0.006906670052558184\n",
      "epoch:  532, loss: 0.006905484478920698\n",
      "epoch:  533, loss: 0.006904517766088247\n",
      "epoch:  534, loss: 0.006898627616465092\n",
      "epoch:  535, loss: 0.006895148660987616\n",
      "epoch:  536, loss: 0.006893741432577372\n",
      "epoch:  537, loss: 0.006892709527164698\n",
      "epoch:  538, loss: 0.006889707874506712\n",
      "epoch:  539, loss: 0.006884007249027491\n",
      "epoch:  540, loss: 0.00688202166929841\n",
      "epoch:  541, loss: 0.0068808505311608315\n",
      "epoch:  542, loss: 0.006879877764731646\n",
      "epoch:  543, loss: 0.006874292157590389\n",
      "epoch:  544, loss: 0.006870472338050604\n",
      "epoch:  545, loss: 0.0068689510226249695\n",
      "epoch:  546, loss: 0.006867856718599796\n",
      "epoch:  547, loss: 0.00686688395217061\n",
      "epoch:  548, loss: 0.006861367262899876\n",
      "epoch:  549, loss: 0.006857770960777998\n",
      "epoch:  550, loss: 0.006856310646981001\n",
      "epoch:  551, loss: 0.006855237763375044\n",
      "epoch:  552, loss: 0.0068543110974133015\n",
      "epoch:  553, loss: 0.006850865203887224\n",
      "epoch:  554, loss: 0.006845798809081316\n",
      "epoch:  555, loss: 0.00684411870315671\n",
      "epoch:  556, loss: 0.0068429987877607346\n",
      "epoch:  557, loss: 0.006842009723186493\n",
      "epoch:  558, loss: 0.006836657412350178\n",
      "epoch:  559, loss: 0.0068327318876981735\n",
      "epoch:  560, loss: 0.00683109275996685\n",
      "epoch:  561, loss: 0.00682997889816761\n",
      "epoch:  562, loss: 0.0068294815719127655\n",
      "epoch:  563, loss: 0.006821601185947657\n",
      "epoch:  564, loss: 0.006819149479269981\n",
      "epoch:  565, loss: 0.00681786984205246\n",
      "epoch:  566, loss: 0.006816858891397715\n",
      "epoch:  567, loss: 0.006811685860157013\n",
      "epoch:  568, loss: 0.00680755078792572\n",
      "epoch:  569, loss: 0.006805812008678913\n",
      "epoch:  570, loss: 0.006804709322750568\n",
      "epoch:  571, loss: 0.006803726311773062\n",
      "epoch:  572, loss: 0.006796876434236765\n",
      "epoch:  573, loss: 0.0067942189052701\n",
      "epoch:  574, loss: 0.006792865693569183\n",
      "epoch:  575, loss: 0.006791844964027405\n",
      "epoch:  576, loss: 0.006790915969759226\n",
      "epoch:  577, loss: 0.006784597411751747\n",
      "epoch:  578, loss: 0.0067817592062056065\n",
      "epoch:  579, loss: 0.006780391559004784\n",
      "epoch:  580, loss: 0.0067793820053339005\n",
      "epoch:  581, loss: 0.006778451148420572\n",
      "epoch:  582, loss: 0.006772489286959171\n",
      "epoch:  583, loss: 0.006769334431737661\n",
      "epoch:  584, loss: 0.006767901126295328\n",
      "epoch:  585, loss: 0.006766799371689558\n",
      "epoch:  586, loss: 0.006765821948647499\n",
      "epoch:  587, loss: 0.00676128501072526\n",
      "epoch:  588, loss: 0.006756724789738655\n",
      "epoch:  589, loss: 0.0067550442181527615\n",
      "epoch:  590, loss: 0.0067539154551923275\n",
      "epoch:  591, loss: 0.006752938963472843\n",
      "epoch:  592, loss: 0.00674774032086134\n",
      "epoch:  593, loss: 0.006743859965354204\n",
      "epoch:  594, loss: 0.0067422054708004\n",
      "epoch:  595, loss: 0.006741143297404051\n",
      "epoch:  596, loss: 0.00674014538526535\n",
      "epoch:  597, loss: 0.006737850606441498\n",
      "epoch:  598, loss: 0.006731867790222168\n",
      "epoch:  599, loss: 0.006729771848767996\n",
      "epoch:  600, loss: 0.006728517357259989\n",
      "epoch:  601, loss: 0.006727522239089012\n",
      "epoch:  602, loss: 0.006726572290062904\n",
      "epoch:  603, loss: 0.006721296347677708\n",
      "epoch:  604, loss: 0.00671769306063652\n",
      "epoch:  605, loss: 0.006716128904372454\n",
      "epoch:  606, loss: 0.006715002935379744\n",
      "epoch:  607, loss: 0.00671402458101511\n",
      "epoch:  608, loss: 0.006708530709147453\n",
      "epoch:  609, loss: 0.006705001927912235\n",
      "epoch:  610, loss: 0.006703270133584738\n",
      "epoch:  611, loss: 0.006702157203108072\n",
      "epoch:  612, loss: 0.006701138336211443\n",
      "epoch:  613, loss: 0.006699366495013237\n",
      "epoch:  614, loss: 0.006692575290799141\n",
      "epoch:  615, loss: 0.006690340582281351\n",
      "epoch:  616, loss: 0.006689072120934725\n",
      "epoch:  617, loss: 0.0066880714148283005\n",
      "epoch:  618, loss: 0.006686612032353878\n",
      "epoch:  619, loss: 0.006679924204945564\n",
      "epoch:  620, loss: 0.006677411496639252\n",
      "epoch:  621, loss: 0.006676084827631712\n",
      "epoch:  622, loss: 0.006675037555396557\n",
      "epoch:  623, loss: 0.006674068979918957\n",
      "epoch:  624, loss: 0.0066686091013252735\n",
      "epoch:  625, loss: 0.006664904300123453\n",
      "epoch:  626, loss: 0.006663216277956963\n",
      "epoch:  627, loss: 0.006662057712674141\n",
      "epoch:  628, loss: 0.006661066319793463\n",
      "epoch:  629, loss: 0.006660266313701868\n",
      "epoch:  630, loss: 0.006652868818491697\n",
      "epoch:  631, loss: 0.006650395225733519\n",
      "epoch:  632, loss: 0.00664899917319417\n",
      "epoch:  633, loss: 0.006647932343184948\n",
      "epoch:  634, loss: 0.00664691673591733\n",
      "epoch:  635, loss: 0.006641990505158901\n",
      "epoch:  636, loss: 0.006637364625930786\n",
      "epoch:  637, loss: 0.006635487545281649\n",
      "epoch:  638, loss: 0.006634219083935022\n",
      "epoch:  639, loss: 0.006633150391280651\n",
      "epoch:  640, loss: 0.006631091702729464\n",
      "epoch:  641, loss: 0.006624124012887478\n",
      "epoch:  642, loss: 0.0066214376129209995\n",
      "epoch:  643, loss: 0.006619906052947044\n",
      "epoch:  644, loss: 0.00661873584613204\n",
      "epoch:  645, loss: 0.006618727929890156\n",
      "epoch:  646, loss: 0.0066095981746912\n",
      "epoch:  647, loss: 0.0066063725389540195\n",
      "epoch:  648, loss: 0.006604598835110664\n",
      "epoch:  649, loss: 0.00660324189811945\n",
      "epoch:  650, loss: 0.006602021399885416\n",
      "epoch:  651, loss: 0.006594156846404076\n",
      "epoch:  652, loss: 0.006589570082724094\n",
      "epoch:  653, loss: 0.0065874275751411915\n",
      "epoch:  654, loss: 0.0065859160386025906\n",
      "epoch:  655, loss: 0.0065845618955791\n",
      "epoch:  656, loss: 0.00658003194257617\n",
      "epoch:  657, loss: 0.0065733278170228004\n",
      "epoch:  658, loss: 0.006570677738636732\n",
      "epoch:  659, loss: 0.006569000892341137\n",
      "epoch:  660, loss: 0.006567717529833317\n",
      "epoch:  661, loss: 0.006566493306308985\n",
      "epoch:  662, loss: 0.006560320500284433\n",
      "epoch:  663, loss: 0.006555406842380762\n",
      "epoch:  664, loss: 0.006553218234330416\n",
      "epoch:  665, loss: 0.006551702506840229\n",
      "epoch:  666, loss: 0.006550470367074013\n",
      "epoch:  667, loss: 0.0065492913126945496\n",
      "epoch:  668, loss: 0.006543298251926899\n",
      "epoch:  669, loss: 0.006538332439959049\n",
      "epoch:  670, loss: 0.006536128465086222\n",
      "epoch:  671, loss: 0.006534604821354151\n",
      "epoch:  672, loss: 0.006533371284604073\n",
      "epoch:  673, loss: 0.006532175466418266\n",
      "epoch:  674, loss: 0.006523667369037867\n",
      "epoch:  675, loss: 0.006520469207316637\n",
      "epoch:  676, loss: 0.006518704816699028\n",
      "epoch:  677, loss: 0.006517364643514156\n",
      "epoch:  678, loss: 0.006516169290989637\n",
      "epoch:  679, loss: 0.0065101319923996925\n",
      "epoch:  680, loss: 0.006505340803414583\n",
      "epoch:  681, loss: 0.006503163371235132\n",
      "epoch:  682, loss: 0.006501710042357445\n",
      "epoch:  683, loss: 0.006500501651316881\n",
      "epoch:  684, loss: 0.006499363109469414\n",
      "epoch:  685, loss: 0.006492959335446358\n",
      "epoch:  686, loss: 0.0064887371845543385\n",
      "epoch:  687, loss: 0.006486704107373953\n",
      "epoch:  688, loss: 0.006485297344624996\n",
      "epoch:  689, loss: 0.006484098732471466\n",
      "epoch:  690, loss: 0.006482941564172506\n",
      "epoch:  691, loss: 0.006479757837951183\n",
      "epoch:  692, loss: 0.006473385728895664\n",
      "epoch:  693, loss: 0.006470630411058664\n",
      "epoch:  694, loss: 0.006469004787504673\n",
      "epoch:  695, loss: 0.006467761471867561\n",
      "epoch:  696, loss: 0.0064666080288589\n",
      "epoch:  697, loss: 0.006461635697633028\n",
      "epoch:  698, loss: 0.006456482689827681\n",
      "epoch:  699, loss: 0.006454170681536198\n",
      "epoch:  700, loss: 0.006452734116464853\n",
      "epoch:  701, loss: 0.00645151361823082\n",
      "epoch:  702, loss: 0.006450389511883259\n",
      "epoch:  703, loss: 0.006445420440286398\n",
      "epoch:  704, loss: 0.0064402613788843155\n",
      "epoch:  705, loss: 0.006437948904931545\n",
      "epoch:  706, loss: 0.006436456460505724\n",
      "epoch:  707, loss: 0.006435228046029806\n",
      "epoch:  708, loss: 0.00643409788608551\n",
      "epoch:  709, loss: 0.006428554654121399\n",
      "epoch:  710, loss: 0.006423874758183956\n",
      "epoch:  711, loss: 0.006421653088182211\n",
      "epoch:  712, loss: 0.006420218851417303\n",
      "epoch:  713, loss: 0.00641899649053812\n",
      "epoch:  714, loss: 0.006417883560061455\n",
      "epoch:  715, loss: 0.006417183671146631\n",
      "epoch:  716, loss: 0.006409325636923313\n",
      "epoch:  717, loss: 0.006406073458492756\n",
      "epoch:  718, loss: 0.006404310930520296\n",
      "epoch:  719, loss: 0.006403001025319099\n",
      "epoch:  720, loss: 0.006401860620826483\n",
      "epoch:  721, loss: 0.006400764919817448\n",
      "epoch:  722, loss: 0.0063997600227594376\n",
      "epoch:  723, loss: 0.0063921138644218445\n",
      "epoch:  724, loss: 0.00638905493542552\n",
      "epoch:  725, loss: 0.006387266796082258\n",
      "epoch:  726, loss: 0.006385991349816322\n",
      "epoch:  727, loss: 0.0063848006539046764\n",
      "epoch:  728, loss: 0.006383704021573067\n",
      "epoch:  729, loss: 0.006382590159773827\n",
      "epoch:  730, loss: 0.006377297453582287\n",
      "epoch:  731, loss: 0.006372662261128426\n",
      "epoch:  732, loss: 0.00637035584077239\n",
      "epoch:  733, loss: 0.006368877831846476\n",
      "epoch:  734, loss: 0.00636763870716095\n",
      "epoch:  735, loss: 0.006366496905684471\n",
      "epoch:  736, loss: 0.006365388631820679\n",
      "epoch:  737, loss: 0.006364309694617987\n",
      "epoch:  738, loss: 0.006363216321915388\n",
      "epoch:  739, loss: 0.00636214530095458\n",
      "epoch:  740, loss: 0.006361635867506266\n",
      "epoch:  741, loss: 0.006353725679218769\n",
      "epoch:  742, loss: 0.0063505712896585464\n",
      "epoch:  743, loss: 0.00634864903986454\n",
      "epoch:  744, loss: 0.006347288843244314\n",
      "epoch:  745, loss: 0.006346079986542463\n",
      "epoch:  746, loss: 0.006344946566969156\n",
      "epoch:  747, loss: 0.006339740008115768\n",
      "epoch:  748, loss: 0.006334967445582151\n",
      "epoch:  749, loss: 0.006332708522677422\n",
      "epoch:  750, loss: 0.006331207696348429\n",
      "epoch:  751, loss: 0.006329964846372604\n",
      "epoch:  752, loss: 0.006328837014734745\n",
      "epoch:  753, loss: 0.006327714771032333\n",
      "epoch:  754, loss: 0.006323300302028656\n",
      "epoch:  755, loss: 0.006318110041320324\n",
      "epoch:  756, loss: 0.00631556985899806\n",
      "epoch:  757, loss: 0.006313959136605263\n",
      "epoch:  758, loss: 0.006312673445791006\n",
      "epoch:  759, loss: 0.006311503704637289\n",
      "epoch:  760, loss: 0.006310465279966593\n",
      "epoch:  761, loss: 0.006302960216999054\n",
      "epoch:  762, loss: 0.006299545522779226\n",
      "epoch:  763, loss: 0.006297642365098\n",
      "epoch:  764, loss: 0.006296250503510237\n",
      "epoch:  765, loss: 0.006295042112469673\n",
      "epoch:  766, loss: 0.006293890532106161\n",
      "epoch:  767, loss: 0.0062882183119654655\n",
      "epoch:  768, loss: 0.006283740978688002\n",
      "epoch:  769, loss: 0.006281455513089895\n",
      "epoch:  770, loss: 0.006279869005084038\n",
      "epoch:  771, loss: 0.006278607528656721\n",
      "epoch:  772, loss: 0.006277414504438639\n",
      "epoch:  773, loss: 0.006275908090174198\n",
      "epoch:  774, loss: 0.00626875227317214\n",
      "epoch:  775, loss: 0.006265438161790371\n",
      "epoch:  776, loss: 0.006263564806431532\n",
      "epoch:  777, loss: 0.006262172944843769\n",
      "epoch:  778, loss: 0.006260932423174381\n",
      "epoch:  779, loss: 0.006259760819375515\n",
      "epoch:  780, loss: 0.006258618086576462\n",
      "epoch:  781, loss: 0.006254489067941904\n",
      "epoch:  782, loss: 0.006248928140848875\n",
      "epoch:  783, loss: 0.006246230565011501\n",
      "epoch:  784, loss: 0.006244490388780832\n",
      "epoch:  785, loss: 0.0062431273981928825\n",
      "epoch:  786, loss: 0.006241858936846256\n",
      "epoch:  787, loss: 0.006240680813789368\n",
      "epoch:  788, loss: 0.006238629575818777\n",
      "epoch:  789, loss: 0.006231638137251139\n",
      "epoch:  790, loss: 0.006228418089449406\n",
      "epoch:  791, loss: 0.006226456258445978\n",
      "epoch:  792, loss: 0.006224947981536388\n",
      "epoch:  793, loss: 0.006223692558705807\n",
      "epoch:  794, loss: 0.0062224650755524635\n",
      "epoch:  795, loss: 0.006221300456672907\n",
      "epoch:  796, loss: 0.006221282295882702\n",
      "epoch:  797, loss: 0.0062129804864525795\n",
      "epoch:  798, loss: 0.006209418643265963\n",
      "epoch:  799, loss: 0.00620729336515069\n",
      "epoch:  800, loss: 0.006205783225595951\n",
      "epoch:  801, loss: 0.006204464938491583\n",
      "epoch:  802, loss: 0.00620327377691865\n",
      "epoch:  803, loss: 0.006202072836458683\n",
      "epoch:  804, loss: 0.006201018113642931\n",
      "epoch:  805, loss: 0.006193339824676514\n",
      "epoch:  806, loss: 0.006190017331391573\n",
      "epoch:  807, loss: 0.0061879525892436504\n",
      "epoch:  808, loss: 0.00618643406778574\n",
      "epoch:  809, loss: 0.006185127422213554\n",
      "epoch:  810, loss: 0.006183909252285957\n",
      "epoch:  811, loss: 0.006182707380503416\n",
      "epoch:  812, loss: 0.006181538105010986\n",
      "epoch:  813, loss: 0.006176056805998087\n",
      "epoch:  814, loss: 0.00617147795855999\n",
      "epoch:  815, loss: 0.006168830674141645\n",
      "epoch:  816, loss: 0.006167122162878513\n",
      "epoch:  817, loss: 0.006165690254420042\n",
      "epoch:  818, loss: 0.0061644297093153\n",
      "epoch:  819, loss: 0.006163222249597311\n",
      "epoch:  820, loss: 0.0061631272546947\n",
      "epoch:  821, loss: 0.006155245006084442\n",
      "epoch:  822, loss: 0.006151529494673014\n",
      "epoch:  823, loss: 0.0061492943204939365\n",
      "epoch:  824, loss: 0.0061476812697947025\n",
      "epoch:  825, loss: 0.006146344356238842\n",
      "epoch:  826, loss: 0.006145083345472813\n",
      "epoch:  827, loss: 0.006143920123577118\n",
      "epoch:  828, loss: 0.0061427452601492405\n",
      "epoch:  829, loss: 0.006140880286693573\n",
      "epoch:  830, loss: 0.00613456079736352\n",
      "epoch:  831, loss: 0.006131092086434364\n",
      "epoch:  832, loss: 0.006128931883722544\n",
      "epoch:  833, loss: 0.006127253640443087\n",
      "epoch:  834, loss: 0.006125870626419783\n",
      "epoch:  835, loss: 0.00612462917342782\n",
      "epoch:  836, loss: 0.006123415194451809\n",
      "epoch:  837, loss: 0.006122271064668894\n",
      "epoch:  838, loss: 0.006121103186160326\n",
      "epoch:  839, loss: 0.006115117110311985\n",
      "epoch:  840, loss: 0.00611115200445056\n",
      "epoch:  841, loss: 0.006108701229095459\n",
      "epoch:  842, loss: 0.006107031367719173\n",
      "epoch:  843, loss: 0.0061055817641317844\n",
      "epoch:  844, loss: 0.006104291416704655\n",
      "epoch:  845, loss: 0.006103073246777058\n",
      "epoch:  846, loss: 0.006101859733462334\n",
      "epoch:  847, loss: 0.00610069977119565\n",
      "epoch:  848, loss: 0.006099532824009657\n",
      "epoch:  849, loss: 0.006096552591770887\n",
      "epoch:  850, loss: 0.006090865004807711\n",
      "epoch:  851, loss: 0.0060877189971506596\n",
      "epoch:  852, loss: 0.0060855732299387455\n",
      "epoch:  853, loss: 0.0060838935896754265\n",
      "epoch:  854, loss: 0.006082402542233467\n",
      "epoch:  855, loss: 0.006081119645386934\n",
      "epoch:  856, loss: 0.006079882849007845\n",
      "epoch:  857, loss: 0.006078697741031647\n",
      "epoch:  858, loss: 0.006077126134186983\n",
      "epoch:  859, loss: 0.006070389412343502\n",
      "epoch:  860, loss: 0.006067127920687199\n",
      "epoch:  861, loss: 0.0060647581703960896\n",
      "epoch:  862, loss: 0.006063101813197136\n",
      "epoch:  863, loss: 0.006061657331883907\n",
      "epoch:  864, loss: 0.00606036651879549\n",
      "epoch:  865, loss: 0.006059137638658285\n",
      "epoch:  866, loss: 0.006057932041585445\n",
      "epoch:  867, loss: 0.006056746002286673\n",
      "epoch:  868, loss: 0.0060563297010958195\n",
      "epoch:  869, loss: 0.00604909285902977\n",
      "epoch:  870, loss: 0.006045537069439888\n",
      "epoch:  871, loss: 0.006042910739779472\n",
      "epoch:  872, loss: 0.006041265558451414\n",
      "epoch:  873, loss: 0.006039765197783709\n",
      "epoch:  874, loss: 0.006038466468453407\n",
      "epoch:  875, loss: 0.006037230137735605\n",
      "epoch:  876, loss: 0.006036025006324053\n",
      "epoch:  877, loss: 0.0060348473489284515\n",
      "epoch:  878, loss: 0.006029261276125908\n",
      "epoch:  879, loss: 0.006024934351444244\n",
      "epoch:  880, loss: 0.006022509187459946\n",
      "epoch:  881, loss: 0.006020511034876108\n",
      "epoch:  882, loss: 0.006019077263772488\n",
      "epoch:  883, loss: 0.006017717067152262\n",
      "epoch:  884, loss: 0.0060164532624185085\n",
      "epoch:  885, loss: 0.006015218328684568\n",
      "epoch:  886, loss: 0.006014018319547176\n",
      "epoch:  887, loss: 0.006012805737555027\n",
      "epoch:  888, loss: 0.006011622492223978\n",
      "epoch:  889, loss: 0.006010431330651045\n",
      "epoch:  890, loss: 0.0060066781006753445\n",
      "epoch:  891, loss: 0.00600135512650013\n",
      "epoch:  892, loss: 0.005998529028147459\n",
      "epoch:  893, loss: 0.005996264051645994\n",
      "epoch:  894, loss: 0.00599473062902689\n",
      "epoch:  895, loss: 0.005993226543068886\n",
      "epoch:  896, loss: 0.0059919655323028564\n",
      "epoch:  897, loss: 0.005990718957036734\n",
      "epoch:  898, loss: 0.005989512894302607\n",
      "epoch:  899, loss: 0.0059883007779717445\n",
      "epoch:  900, loss: 0.005987139884382486\n",
      "epoch:  901, loss: 0.0059859356842935085\n",
      "epoch:  902, loss: 0.005980811081826687\n",
      "epoch:  903, loss: 0.00597630487754941\n",
      "epoch:  904, loss: 0.005973624065518379\n",
      "epoch:  905, loss: 0.005971582606434822\n",
      "epoch:  906, loss: 0.005970109719783068\n",
      "epoch:  907, loss: 0.005968648474663496\n",
      "epoch:  908, loss: 0.005967389792203903\n",
      "epoch:  909, loss: 0.005966150667518377\n",
      "epoch:  910, loss: 0.00596496881917119\n",
      "epoch:  911, loss: 0.005963743664324284\n",
      "epoch:  912, loss: 0.0059625511057674885\n",
      "epoch:  913, loss: 0.005961364600807428\n",
      "epoch:  914, loss: 0.005959089379757643\n",
      "epoch:  915, loss: 0.005953424610197544\n",
      "epoch:  916, loss: 0.005949963815510273\n",
      "epoch:  917, loss: 0.005947502329945564\n",
      "epoch:  918, loss: 0.005945788696408272\n",
      "epoch:  919, loss: 0.005944186821579933\n",
      "epoch:  920, loss: 0.00594283314421773\n",
      "epoch:  921, loss: 0.005941550713032484\n",
      "epoch:  922, loss: 0.005940309725701809\n",
      "epoch:  923, loss: 0.0059329611249268055\n",
      "epoch:  924, loss: 0.005929599981755018\n",
      "epoch:  925, loss: 0.00592738576233387\n",
      "epoch:  926, loss: 0.005925531033426523\n",
      "epoch:  927, loss: 0.00592406140640378\n",
      "epoch:  928, loss: 0.005922720301896334\n",
      "epoch:  929, loss: 0.005921451840549707\n",
      "epoch:  930, loss: 0.005920206196606159\n",
      "epoch:  931, loss: 0.005918922368437052\n",
      "epoch:  932, loss: 0.0059126135893166065\n",
      "epoch:  933, loss: 0.005909034051001072\n",
      "epoch:  934, loss: 0.005906365811824799\n",
      "epoch:  935, loss: 0.005904434714466333\n",
      "epoch:  936, loss: 0.005902769509702921\n",
      "epoch:  937, loss: 0.00590131152421236\n",
      "epoch:  938, loss: 0.0058999680913984776\n",
      "epoch:  939, loss: 0.005898666102439165\n",
      "epoch:  940, loss: 0.005897396244108677\n",
      "epoch:  941, loss: 0.0058961473405361176\n",
      "epoch:  942, loss: 0.0058923158794641495\n",
      "epoch:  943, loss: 0.005887467879801989\n",
      "epoch:  944, loss: 0.005884137004613876\n",
      "epoch:  945, loss: 0.005881798919290304\n",
      "epoch:  946, loss: 0.005879868287593126\n",
      "epoch:  947, loss: 0.00587827805429697\n",
      "epoch:  948, loss: 0.005876816343516111\n",
      "epoch:  949, loss: 0.005875446833670139\n",
      "epoch:  950, loss: 0.0058741383254528046\n",
      "epoch:  951, loss: 0.005872827488929033\n",
      "epoch:  952, loss: 0.005871547851711512\n",
      "epoch:  953, loss: 0.005865947343409061\n",
      "epoch:  954, loss: 0.005861642304807901\n",
      "epoch:  955, loss: 0.0058587659150362015\n",
      "epoch:  956, loss: 0.005856367293745279\n",
      "epoch:  957, loss: 0.005854482296854258\n",
      "epoch:  958, loss: 0.005852820817381144\n",
      "epoch:  959, loss: 0.005851312540471554\n",
      "epoch:  960, loss: 0.005849883425980806\n",
      "epoch:  961, loss: 0.005848487373441458\n",
      "epoch:  962, loss: 0.005847147200256586\n",
      "epoch:  963, loss: 0.00584579911082983\n",
      "epoch:  964, loss: 0.005842150188982487\n",
      "epoch:  965, loss: 0.005837311502546072\n",
      "epoch:  966, loss: 0.0058335838839411736\n",
      "epoch:  967, loss: 0.005830821115523577\n",
      "epoch:  968, loss: 0.0058286297135055065\n",
      "epoch:  969, loss: 0.005826829932630062\n",
      "epoch:  970, loss: 0.005825205706059933\n",
      "epoch:  971, loss: 0.0058237300254404545\n",
      "epoch:  972, loss: 0.005822286941111088\n",
      "epoch:  973, loss: 0.0058208960108459\n",
      "epoch:  974, loss: 0.0058195400051772594\n",
      "epoch:  975, loss: 0.005818196572363377\n",
      "epoch:  976, loss: 0.005816866178065538\n",
      "epoch:  977, loss: 0.005815545096993446\n",
      "epoch:  978, loss: 0.005814197473227978\n",
      "epoch:  979, loss: 0.005812890827655792\n",
      "epoch:  980, loss: 0.005811699666082859\n",
      "epoch:  981, loss: 0.005805484019219875\n",
      "epoch:  982, loss: 0.005801472347229719\n",
      "epoch:  983, loss: 0.005798504687845707\n",
      "epoch:  984, loss: 0.005796091631054878\n",
      "epoch:  985, loss: 0.005794125143438578\n",
      "epoch:  986, loss: 0.005792438052594662\n",
      "epoch:  987, loss: 0.005790880881249905\n",
      "epoch:  988, loss: 0.00578945130109787\n",
      "epoch:  989, loss: 0.005788028240203857\n",
      "epoch:  990, loss: 0.00578666664659977\n",
      "epoch:  991, loss: 0.0057852971367537975\n",
      "epoch:  992, loss: 0.00578398909419775\n",
      "epoch:  993, loss: 0.005782628897577524\n",
      "epoch:  994, loss: 0.005781295243650675\n",
      "epoch:  995, loss: 0.005779518745839596\n",
      "epoch:  996, loss: 0.005773765500634909\n",
      "epoch:  997, loss: 0.00576991681009531\n",
      "epoch:  998, loss: 0.005766949616372585\n",
      "epoch:  999, loss: 0.005764778237789869\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size=X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# opt = torch_numopt.ConjugateGradient(model=model, lr=1e-4, line_search_method=\"const\", cg_method=\"FR\")\n",
    "opt = torch_numopt.ConjugateGradient(model=model, lr=10, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"armijo\", cg_method=\"FR\")\n",
    "# opt = torch_numopt.ConjugateGradient(model=model, lr=10, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"wolfe\", cg_method=\"FR\")\n",
    "# opt = torch_numopt.ConjugateGradient(model=model, lr=10, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"strong-wolfe\", cg_method=\"FR\")\n",
    "# opt = torch_numopt.ConjugateGradient(model=model, lr=10, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"goldstein\", cg_method=\"FR\")\n",
    "\n",
    "all_loss = {}\n",
    "for epoch in range(1000):\n",
    "    print(\"epoch: \", epoch, end=\"\")\n",
    "    all_loss[epoch + 1] = 0\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y, loss_fn)\n",
    "\n",
    "        all_loss[epoch + 1] += loss\n",
    "    all_loss[epoch + 1] /= len(data_loader)\n",
    "    print(\", loss: {}\".format(all_loss[epoch + 1].cpu().detach().numpy().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics: R2 = 0.7791152601841428\n",
      "Test metrics:  R2 = 0.8343205966773746\n"
     ]
    }
   ],
   "source": [
    "pred_train = model.forward(X_train).cpu().detach()\n",
    "pred_test = model.forward(X_test).cpu().detach()\n",
    "print(f\"Train metrics: R2 = {r2_score(pred_train, y_train.cpu())}\")\n",
    "print(f\"Test metrics:  R2 = {r2_score(pred_test, y_test.cpu())}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_numopt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
