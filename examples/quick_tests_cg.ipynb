{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch_numopt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import *\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.f1 = nn.Linear(input_size, 10, device=device)\n",
    "        self.f2 = nn.Linear(10, 20, device=device)\n",
    "        self.f3 = nn.Linear(20, 20, device=device)\n",
    "        self.f4 = nn.Linear(20, 10, device=device)\n",
    "        self.f5 = nn.Linear(10, 1, device=device)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        # self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.f1(x))\n",
    "        x = self.activation(self.f2(x))\n",
    "        x = self.activation(self.f3(x))\n",
    "        x = self.activation(self.f4(x))\n",
    "        x = self.f5(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([800, 10]) torch.Size([800, 1])\n",
      "torch.Size([200, 10]) torch.Size([200, 1])\n"
     ]
    }
   ],
   "source": [
    "# X, y = load_diabetes(return_X_y=True, scaled=False)\n",
    "# X, y = make_regression(n_samples=1000, n_features=100)\n",
    "X, y = make_friedman1(n_samples=1000, noise=1e-2)\n",
    "\n",
    "X_scaler = MinMaxScaler()\n",
    "X = X_scaler.fit_transform(X)\n",
    "X = torch.Tensor(X).to(device)\n",
    "\n",
    "y_scaler = MinMaxScaler()\n",
    "y = y_scaler.fit_transform(y.reshape((-1, 1)))\n",
    "y = torch.Tensor(y).to(device)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "torch_data = TensorDataset(X_train, y_train)\n",
    "data_loader = DataLoader(torch_data, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.3369813859462738\n",
      "epoch:  1, loss: 0.18830543756484985\n",
      "epoch:  2, loss: 0.11335527151823044\n",
      "epoch:  3, loss: 0.075849249958992\n",
      "epoch:  4, loss: 0.05692458525300026\n",
      "epoch:  5, loss: 0.04745115339756012\n",
      "epoch:  6, loss: 0.042780984193086624\n",
      "epoch:  7, loss: 0.040504369884729385\n",
      "epoch:  8, loss: 0.03939918056130409\n",
      "epoch:  9, loss: 0.0388604961335659\n",
      "epoch:  10, loss: 0.038592249155044556\n",
      "epoch:  11, loss: 0.03845079988241196\n",
      "epoch:  12, loss: 0.038372207432985306\n",
      "epoch:  13, loss: 0.03835640102624893\n",
      "epoch:  14, loss: 0.03823895752429962\n",
      "epoch:  15, loss: 0.03817911446094513\n",
      "epoch:  16, loss: 0.03814564272761345\n",
      "epoch:  17, loss: 0.038098569959402084\n",
      "epoch:  18, loss: 0.03803489729762077\n",
      "epoch:  19, loss: 0.0379982553422451\n",
      "epoch:  20, loss: 0.03794249892234802\n",
      "epoch:  21, loss: 0.037877846509218216\n",
      "epoch:  22, loss: 0.03784297779202461\n",
      "epoch:  23, loss: 0.037826891988515854\n",
      "epoch:  24, loss: 0.03776106983423233\n",
      "epoch:  25, loss: 0.03772670775651932\n",
      "epoch:  26, loss: 0.03770652785897255\n",
      "epoch:  27, loss: 0.037661291658878326\n",
      "epoch:  28, loss: 0.03762492537498474\n",
      "epoch:  29, loss: 0.03760397434234619\n",
      "epoch:  30, loss: 0.03756105527281761\n",
      "epoch:  31, loss: 0.03752287104725838\n",
      "epoch:  32, loss: 0.03750096261501312\n",
      "epoch:  33, loss: 0.03745974600315094\n",
      "epoch:  34, loss: 0.03741832077503204\n",
      "epoch:  35, loss: 0.037395063787698746\n",
      "epoch:  36, loss: 0.03735371679067612\n",
      "epoch:  37, loss: 0.037309616804122925\n",
      "epoch:  38, loss: 0.037284668534994125\n",
      "epoch:  39, loss: 0.03724339231848717\n",
      "epoch:  40, loss: 0.037194304168224335\n",
      "epoch:  41, loss: 0.03716697171330452\n",
      "epoch:  42, loss: 0.037123359739780426\n",
      "epoch:  43, loss: 0.03706999868154526\n",
      "epoch:  44, loss: 0.03704030439257622\n",
      "epoch:  45, loss: 0.03699615225195885\n",
      "epoch:  46, loss: 0.036936528980731964\n",
      "epoch:  47, loss: 0.03690401464700699\n",
      "epoch:  48, loss: 0.03686041757464409\n",
      "epoch:  49, loss: 0.03679415583610535\n",
      "epoch:  50, loss: 0.03675827383995056\n",
      "epoch:  51, loss: 0.036714818328619\n",
      "epoch:  52, loss: 0.03664030134677887\n",
      "epoch:  53, loss: 0.036600518971681595\n",
      "epoch:  54, loss: 0.036555659025907516\n",
      "epoch:  55, loss: 0.03647362068295479\n",
      "epoch:  56, loss: 0.03642980009317398\n",
      "epoch:  57, loss: 0.03638538718223572\n",
      "epoch:  58, loss: 0.0362919345498085\n",
      "epoch:  59, loss: 0.036243222653865814\n",
      "epoch:  60, loss: 0.036200620234012604\n",
      "epoch:  61, loss: 0.036096323281526566\n",
      "epoch:  62, loss: 0.036042071878910065\n",
      "epoch:  63, loss: 0.036004964262247086\n",
      "epoch:  64, loss: 0.03588356822729111\n",
      "epoch:  65, loss: 0.03582232818007469\n",
      "epoch:  66, loss: 0.03579014167189598\n",
      "epoch:  67, loss: 0.03565230965614319\n",
      "epoch:  68, loss: 0.03558291494846344\n",
      "epoch:  69, loss: 0.03556269779801369\n",
      "epoch:  70, loss: 0.03539933264255524\n",
      "epoch:  71, loss: 0.03532014042139053\n",
      "epoch:  72, loss: 0.03530837222933769\n",
      "epoch:  73, loss: 0.035121820867061615\n",
      "epoch:  74, loss: 0.035031404346227646\n",
      "epoch:  75, loss: 0.034979481250047684\n",
      "epoch:  76, loss: 0.03482053428888321\n",
      "epoch:  77, loss: 0.034713055938482285\n",
      "epoch:  78, loss: 0.03465375304222107\n",
      "epoch:  79, loss: 0.03448515385389328\n",
      "epoch:  80, loss: 0.03436115384101868\n",
      "epoch:  81, loss: 0.03429359570145607\n",
      "epoch:  82, loss: 0.03412134572863579\n",
      "epoch:  83, loss: 0.03397321328520775\n",
      "epoch:  84, loss: 0.03389601781964302\n",
      "epoch:  85, loss: 0.033718276768922806\n",
      "epoch:  86, loss: 0.03354434296488762\n",
      "epoch:  87, loss: 0.03345610946416855\n",
      "epoch:  88, loss: 0.033280812203884125\n",
      "epoch:  89, loss: 0.03306960687041283\n",
      "epoch:  90, loss: 0.032967451959848404\n",
      "epoch:  91, loss: 0.03279215469956398\n",
      "epoch:  92, loss: 0.032542988657951355\n",
      "epoch:  93, loss: 0.03242509439587593\n",
      "epoch:  94, loss: 0.032261189073324203\n",
      "epoch:  95, loss: 0.03195787966251373\n",
      "epoch:  96, loss: 0.03182373568415642\n",
      "epoch:  97, loss: 0.031667642295360565\n",
      "epoch:  98, loss: 0.031311601400375366\n",
      "epoch:  99, loss: 0.03115643560886383\n",
      "epoch:  100, loss: 0.031034041196107864\n",
      "epoch:  101, loss: 0.03059186041355133\n",
      "epoch:  102, loss: 0.030414022505283356\n",
      "epoch:  103, loss: 0.030290458351373672\n",
      "epoch:  104, loss: 0.029786985367536545\n",
      "epoch:  105, loss: 0.02958822064101696\n",
      "epoch:  106, loss: 0.0294917281717062\n",
      "epoch:  107, loss: 0.028888236731290817\n",
      "epoch:  108, loss: 0.0286706630140543\n",
      "epoch:  109, loss: 0.02853870391845703\n",
      "epoch:  110, loss: 0.027889113873243332\n",
      "epoch:  111, loss: 0.027657993137836456\n",
      "epoch:  112, loss: 0.027494439855217934\n",
      "epoch:  113, loss: 0.026775451377034187\n",
      "epoch:  114, loss: 0.026543542742729187\n",
      "epoch:  115, loss: 0.026224171742796898\n",
      "epoch:  116, loss: 0.025554094463586807\n",
      "epoch:  117, loss: 0.02533341944217682\n",
      "epoch:  118, loss: 0.024879775941371918\n",
      "epoch:  119, loss: 0.02422912046313286\n",
      "epoch:  120, loss: 0.02402735874056816\n",
      "epoch:  121, loss: 0.023314815014600754\n",
      "epoch:  122, loss: 0.02281876653432846\n",
      "epoch:  123, loss: 0.022642817348241806\n",
      "epoch:  124, loss: 0.0217287614941597\n",
      "epoch:  125, loss: 0.02134879305958748\n",
      "epoch:  126, loss: 0.021182889118790627\n",
      "epoch:  127, loss: 0.020084239542484283\n",
      "epoch:  128, loss: 0.01986219733953476\n",
      "epoch:  129, loss: 0.01900523714721203\n",
      "epoch:  130, loss: 0.01853058859705925\n",
      "epoch:  131, loss: 0.01832912117242813\n",
      "epoch:  132, loss: 0.017256610095500946\n",
      "epoch:  133, loss: 0.017094144597649574\n",
      "epoch:  134, loss: 0.0160855520516634\n",
      "epoch:  135, loss: 0.015868939459323883\n",
      "epoch:  136, loss: 0.015006372705101967\n",
      "epoch:  137, loss: 0.014747539535164833\n",
      "epoch:  138, loss: 0.013993706554174423\n",
      "epoch:  139, loss: 0.013750379905104637\n",
      "epoch:  140, loss: 0.013034660369157791\n",
      "epoch:  141, loss: 0.012893136590719223\n",
      "epoch:  142, loss: 0.011243468150496483\n",
      "epoch:  143, loss: 0.010674812830984592\n",
      "epoch:  144, loss: 0.010506968945264816\n",
      "epoch:  145, loss: 0.010407375171780586\n",
      "epoch:  146, loss: 0.009898318909108639\n",
      "epoch:  147, loss: 0.009684559889137745\n",
      "epoch:  148, loss: 0.009635349735617638\n",
      "epoch:  149, loss: 0.009546206332743168\n",
      "epoch:  150, loss: 0.009350713342428207\n",
      "epoch:  151, loss: 0.009322665631771088\n",
      "epoch:  152, loss: 0.009288527071475983\n",
      "epoch:  153, loss: 0.009181629866361618\n",
      "epoch:  154, loss: 0.009166857227683067\n",
      "epoch:  155, loss: 0.009122240357100964\n",
      "epoch:  156, loss: 0.009081999771296978\n",
      "epoch:  157, loss: 0.009074371308088303\n",
      "epoch:  158, loss: 0.00903457123786211\n",
      "epoch:  159, loss: 0.009015810675919056\n",
      "epoch:  160, loss: 0.009010628797113895\n",
      "epoch:  161, loss: 0.008975464850664139\n",
      "epoch:  162, loss: 0.008965418674051762\n",
      "epoch:  163, loss: 0.0089615723118186\n",
      "epoch:  164, loss: 0.008931235410273075\n",
      "epoch:  165, loss: 0.008925368078052998\n",
      "epoch:  166, loss: 0.00891253910958767\n",
      "epoch:  167, loss: 0.00889299064874649\n",
      "epoch:  168, loss: 0.008889488875865936\n",
      "epoch:  169, loss: 0.008868468925356865\n",
      "epoch:  170, loss: 0.008861127309501171\n",
      "epoch:  171, loss: 0.008858434855937958\n",
      "epoch:  172, loss: 0.00884257536381483\n",
      "epoch:  173, loss: 0.008686728775501251\n",
      "epoch:  174, loss: 0.008677360601723194\n",
      "epoch:  175, loss: 0.00867606233805418\n",
      "epoch:  176, loss: 0.008670646697282791\n",
      "epoch:  177, loss: 0.008666815236210823\n",
      "epoch:  178, loss: 0.008665839210152626\n",
      "epoch:  179, loss: 0.008659406565129757\n",
      "epoch:  180, loss: 0.00865744985640049\n",
      "epoch:  181, loss: 0.008656630292534828\n",
      "epoch:  182, loss: 0.008642864413559437\n",
      "epoch:  183, loss: 0.008591187186539173\n",
      "epoch:  184, loss: 0.008587268181145191\n",
      "epoch:  185, loss: 0.008586354553699493\n",
      "epoch:  186, loss: 0.00858220737427473\n",
      "epoch:  187, loss: 0.008579418063163757\n",
      "epoch:  188, loss: 0.008578620851039886\n",
      "epoch:  189, loss: 0.008573927916586399\n",
      "epoch:  190, loss: 0.008571974001824856\n",
      "epoch:  191, loss: 0.00857122428715229\n",
      "epoch:  192, loss: 0.008567309007048607\n",
      "epoch:  193, loss: 0.008565098978579044\n",
      "epoch:  194, loss: 0.008564446121454239\n",
      "epoch:  195, loss: 0.00856320746243\n",
      "epoch:  196, loss: 0.008558907546103\n",
      "epoch:  197, loss: 0.008558124303817749\n",
      "epoch:  198, loss: 0.008557564578950405\n",
      "epoch:  199, loss: 0.008553840219974518\n",
      "epoch:  200, loss: 0.008552219718694687\n",
      "epoch:  201, loss: 0.008551581762731075\n",
      "epoch:  202, loss: 0.008547982200980186\n",
      "epoch:  203, loss: 0.008546143770217896\n",
      "epoch:  204, loss: 0.008545548655092716\n",
      "epoch:  205, loss: 0.00854199007153511\n",
      "epoch:  206, loss: 0.008540288545191288\n",
      "epoch:  207, loss: 0.008539718575775623\n",
      "epoch:  208, loss: 0.008538400754332542\n",
      "epoch:  209, loss: 0.008535314351320267\n",
      "epoch:  210, loss: 0.0085347481071949\n",
      "epoch:  211, loss: 0.008533640764653683\n",
      "epoch:  212, loss: 0.00853061955422163\n",
      "epoch:  213, loss: 0.008530008606612682\n",
      "epoch:  214, loss: 0.008529230952262878\n",
      "epoch:  215, loss: 0.008526072837412357\n",
      "epoch:  216, loss: 0.008525566197931767\n",
      "epoch:  217, loss: 0.008523799479007721\n",
      "epoch:  218, loss: 0.008521655574440956\n",
      "epoch:  219, loss: 0.00852111540734768\n",
      "epoch:  220, loss: 0.00851902924478054\n",
      "epoch:  221, loss: 0.008517264388501644\n",
      "epoch:  222, loss: 0.008516835048794746\n",
      "epoch:  223, loss: 0.008514437824487686\n",
      "epoch:  224, loss: 0.008512958884239197\n",
      "epoch:  225, loss: 0.008512499742209911\n",
      "epoch:  226, loss: 0.008510040119290352\n",
      "epoch:  227, loss: 0.008508672006428242\n",
      "epoch:  228, loss: 0.008508242666721344\n",
      "epoch:  229, loss: 0.008506657555699348\n",
      "epoch:  230, loss: 0.008504552766680717\n",
      "epoch:  231, loss: 0.008504059165716171\n",
      "epoch:  232, loss: 0.008502784185111523\n",
      "epoch:  233, loss: 0.008500398136675358\n",
      "epoch:  234, loss: 0.008499883115291595\n",
      "epoch:  235, loss: 0.008499207906425\n",
      "epoch:  236, loss: 0.0084963608533144\n",
      "epoch:  237, loss: 0.008495798334479332\n",
      "epoch:  238, loss: 0.00849542673677206\n",
      "epoch:  239, loss: 0.008492586202919483\n",
      "epoch:  240, loss: 0.008491776883602142\n",
      "epoch:  241, loss: 0.00849137082695961\n",
      "epoch:  242, loss: 0.008490216918289661\n",
      "epoch:  243, loss: 0.008487951941788197\n",
      "epoch:  244, loss: 0.008487466722726822\n",
      "epoch:  245, loss: 0.008487091399729252\n",
      "epoch:  246, loss: 0.008484970778226852\n",
      "epoch:  247, loss: 0.00848363246768713\n",
      "epoch:  248, loss: 0.008483183570206165\n",
      "epoch:  249, loss: 0.008482827804982662\n",
      "epoch:  250, loss: 0.008480008691549301\n",
      "epoch:  251, loss: 0.008479374460875988\n",
      "epoch:  252, loss: 0.008478990755975246\n",
      "epoch:  253, loss: 0.008478772826492786\n",
      "epoch:  254, loss: 0.008475806564092636\n",
      "epoch:  255, loss: 0.00847532507032156\n",
      "epoch:  256, loss: 0.008474943228065968\n",
      "epoch:  257, loss: 0.008472690358757973\n",
      "epoch:  258, loss: 0.008471706882119179\n",
      "epoch:  259, loss: 0.008471328765153885\n",
      "epoch:  260, loss: 0.008469412103295326\n",
      "epoch:  261, loss: 0.008468146435916424\n",
      "epoch:  262, loss: 0.008467753417789936\n",
      "epoch:  263, loss: 0.008467424660921097\n",
      "epoch:  264, loss: 0.008467174135148525\n",
      "epoch:  265, loss: 0.00846441276371479\n",
      "epoch:  266, loss: 0.008463919162750244\n",
      "epoch:  267, loss: 0.008463584817945957\n",
      "epoch:  268, loss: 0.008461407385766506\n",
      "epoch:  269, loss: 0.008460414595901966\n",
      "epoch:  270, loss: 0.00846002995967865\n",
      "epoch:  271, loss: 0.008459716103971004\n",
      "epoch:  272, loss: 0.008459413424134254\n",
      "epoch:  273, loss: 0.008458847180008888\n",
      "epoch:  274, loss: 0.008456411771476269\n",
      "epoch:  275, loss: 0.008455943316221237\n",
      "epoch:  276, loss: 0.008455611765384674\n",
      "epoch:  277, loss: 0.008454340510070324\n",
      "epoch:  278, loss: 0.008452682755887508\n",
      "epoch:  279, loss: 0.00845219288021326\n",
      "epoch:  280, loss: 0.008451856672763824\n",
      "epoch:  281, loss: 0.008450360968708992\n",
      "epoch:  282, loss: 0.008448762819170952\n",
      "epoch:  283, loss: 0.008448366075754166\n",
      "epoch:  284, loss: 0.008448035456240177\n",
      "epoch:  285, loss: 0.00844539888203144\n",
      "epoch:  286, loss: 0.00844485592097044\n",
      "epoch:  287, loss: 0.008444475010037422\n",
      "epoch:  288, loss: 0.00844417791813612\n",
      "epoch:  289, loss: 0.008443133905529976\n",
      "epoch:  290, loss: 0.008441196754574776\n",
      "epoch:  291, loss: 0.008440781384706497\n",
      "epoch:  292, loss: 0.008440463803708553\n",
      "epoch:  293, loss: 0.008437957614660263\n",
      "epoch:  294, loss: 0.008437412790954113\n",
      "epoch:  295, loss: 0.008437049575150013\n",
      "epoch:  296, loss: 0.008436654694378376\n",
      "epoch:  297, loss: 0.008434312418103218\n",
      "epoch:  298, loss: 0.008433766663074493\n",
      "epoch:  299, loss: 0.008433442562818527\n",
      "epoch:  300, loss: 0.00843135081231594\n",
      "epoch:  301, loss: 0.008430473506450653\n",
      "epoch:  302, loss: 0.008430096320807934\n",
      "epoch:  303, loss: 0.008429789915680885\n",
      "epoch:  304, loss: 0.0084284832701087\n",
      "epoch:  305, loss: 0.008426942862570286\n",
      "epoch:  306, loss: 0.008426498621702194\n",
      "epoch:  307, loss: 0.008426202461123466\n",
      "epoch:  308, loss: 0.008425910025835037\n",
      "epoch:  309, loss: 0.008423948660492897\n",
      "epoch:  310, loss: 0.008423015475273132\n",
      "epoch:  311, loss: 0.008422693237662315\n",
      "epoch:  312, loss: 0.00842237751930952\n",
      "epoch:  313, loss: 0.008421746082603931\n",
      "epoch:  314, loss: 0.00841965340077877\n",
      "epoch:  315, loss: 0.008419200778007507\n",
      "epoch:  316, loss: 0.008418888784945011\n",
      "epoch:  317, loss: 0.008416936732828617\n",
      "epoch:  318, loss: 0.008416002616286278\n",
      "epoch:  319, loss: 0.00841567013412714\n",
      "epoch:  320, loss: 0.008414728567004204\n",
      "epoch:  321, loss: 0.008412881754338741\n",
      "epoch:  322, loss: 0.008412390016019344\n",
      "epoch:  323, loss: 0.008412092924118042\n",
      "epoch:  324, loss: 0.00840964075177908\n",
      "epoch:  325, loss: 0.008409096859395504\n",
      "epoch:  326, loss: 0.008408769965171814\n",
      "epoch:  327, loss: 0.008407142013311386\n",
      "epoch:  328, loss: 0.00840589590370655\n",
      "epoch:  329, loss: 0.00840551033616066\n",
      "epoch:  330, loss: 0.008405220694839954\n",
      "epoch:  331, loss: 0.008403101935982704\n",
      "epoch:  332, loss: 0.00840233825147152\n",
      "epoch:  333, loss: 0.008401992730796337\n",
      "epoch:  334, loss: 0.008400417864322662\n",
      "epoch:  335, loss: 0.008399137295782566\n",
      "epoch:  336, loss: 0.008398717269301414\n",
      "epoch:  337, loss: 0.008398416452109814\n",
      "epoch:  338, loss: 0.008396649733185768\n",
      "epoch:  339, loss: 0.008395624347031116\n",
      "epoch:  340, loss: 0.00839521549642086\n",
      "epoch:  341, loss: 0.00839491281658411\n",
      "epoch:  342, loss: 0.008393940515816212\n",
      "epoch:  343, loss: 0.008392177522182465\n",
      "epoch:  344, loss: 0.008391700685024261\n",
      "epoch:  345, loss: 0.008391400799155235\n",
      "epoch:  346, loss: 0.008389681577682495\n",
      "epoch:  347, loss: 0.008388501591980457\n",
      "epoch:  348, loss: 0.008388139307498932\n",
      "epoch:  349, loss: 0.008387843146920204\n",
      "epoch:  350, loss: 0.008386577479541302\n",
      "epoch:  351, loss: 0.008385082706809044\n",
      "epoch:  352, loss: 0.008384705521166325\n",
      "epoch:  353, loss: 0.008384422399103642\n",
      "epoch:  354, loss: 0.00838245078921318\n",
      "epoch:  355, loss: 0.008381742052733898\n",
      "epoch:  356, loss: 0.00838140957057476\n",
      "epoch:  357, loss: 0.00838134903460741\n",
      "epoch:  358, loss: 0.008378843776881695\n",
      "epoch:  359, loss: 0.008378393948078156\n",
      "epoch:  360, loss: 0.0083781061694026\n",
      "epoch:  361, loss: 0.00837633479386568\n",
      "epoch:  362, loss: 0.008375367149710655\n",
      "epoch:  363, loss: 0.008374997414648533\n",
      "epoch:  364, loss: 0.00837408471852541\n",
      "epoch:  365, loss: 0.008372332900762558\n",
      "epoch:  366, loss: 0.008371914736926556\n",
      "epoch:  367, loss: 0.00837162509560585\n",
      "epoch:  368, loss: 0.008369246497750282\n",
      "epoch:  369, loss: 0.008368796668946743\n",
      "epoch:  370, loss: 0.00836849119514227\n",
      "epoch:  371, loss: 0.008366385474801064\n",
      "epoch:  372, loss: 0.008365696296095848\n",
      "epoch:  373, loss: 0.008365380577743053\n",
      "epoch:  374, loss: 0.00836509931832552\n",
      "epoch:  375, loss: 0.008358700200915337\n",
      "epoch:  376, loss: 0.0083421990275383\n",
      "epoch:  377, loss: 0.008340638130903244\n",
      "epoch:  378, loss: 0.00834027212113142\n",
      "epoch:  379, loss: 0.008338919840753078\n",
      "epoch:  380, loss: 0.008337438106536865\n",
      "epoch:  381, loss: 0.008337072096765041\n",
      "epoch:  382, loss: 0.008335691876709461\n",
      "epoch:  383, loss: 0.008334329351782799\n",
      "epoch:  384, loss: 0.008333978243172169\n",
      "epoch:  385, loss: 0.008333705365657806\n",
      "epoch:  386, loss: 0.008332053199410439\n",
      "epoch:  387, loss: 0.008331168442964554\n",
      "epoch:  388, loss: 0.008330877870321274\n",
      "epoch:  389, loss: 0.008330631069839\n",
      "epoch:  390, loss: 0.008328870870172977\n",
      "epoch:  391, loss: 0.00832818727940321\n",
      "epoch:  392, loss: 0.008327887393534184\n",
      "epoch:  393, loss: 0.008327633142471313\n",
      "epoch:  394, loss: 0.00832581240683794\n",
      "epoch:  395, loss: 0.00832518469542265\n",
      "epoch:  396, loss: 0.008324913680553436\n",
      "epoch:  397, loss: 0.008323818445205688\n",
      "epoch:  398, loss: 0.008322570472955704\n",
      "epoch:  399, loss: 0.008322244510054588\n",
      "epoch:  400, loss: 0.008322000503540039\n",
      "epoch:  401, loss: 0.008321345783770084\n",
      "epoch:  402, loss: 0.008319747634232044\n",
      "epoch:  403, loss: 0.008319420740008354\n",
      "epoch:  404, loss: 0.008319173008203506\n",
      "epoch:  405, loss: 0.0083179185166955\n",
      "epoch:  406, loss: 0.008316941559314728\n",
      "epoch:  407, loss: 0.00831662304699421\n",
      "epoch:  408, loss: 0.008316392078995705\n",
      "epoch:  409, loss: 0.008314522914588451\n",
      "epoch:  410, loss: 0.008314118720591068\n",
      "epoch:  411, loss: 0.008313847705721855\n",
      "epoch:  412, loss: 0.008312411606311798\n",
      "epoch:  413, loss: 0.008311636745929718\n",
      "epoch:  414, loss: 0.008311334997415543\n",
      "epoch:  415, loss: 0.008311104960739613\n",
      "epoch:  416, loss: 0.008309287019073963\n",
      "epoch:  417, loss: 0.008292704820632935\n",
      "epoch:  418, loss: 0.008291169069707394\n",
      "epoch:  419, loss: 0.008290749043226242\n",
      "epoch:  420, loss: 0.008288421668112278\n",
      "epoch:  421, loss: 0.008287495002150536\n",
      "epoch:  422, loss: 0.008287202566862106\n",
      "epoch:  423, loss: 0.008286000229418278\n",
      "epoch:  424, loss: 0.008284716866910458\n",
      "epoch:  425, loss: 0.008284387178719044\n",
      "epoch:  426, loss: 0.008283511735498905\n",
      "epoch:  427, loss: 0.008282048627734184\n",
      "epoch:  428, loss: 0.00828173290938139\n",
      "epoch:  429, loss: 0.008281489834189415\n",
      "epoch:  430, loss: 0.00827967282384634\n",
      "epoch:  431, loss: 0.00827917829155922\n",
      "epoch:  432, loss: 0.008278920315206051\n",
      "epoch:  433, loss: 0.008277432061731815\n",
      "epoch:  434, loss: 0.008276689797639847\n",
      "epoch:  435, loss: 0.008276375941932201\n",
      "epoch:  436, loss: 0.008276134729385376\n",
      "epoch:  437, loss: 0.008274247869849205\n",
      "epoch:  438, loss: 0.00827386137098074\n",
      "epoch:  439, loss: 0.008273602463304996\n",
      "epoch:  440, loss: 0.008271978236734867\n",
      "epoch:  441, loss: 0.00827137753367424\n",
      "epoch:  442, loss: 0.008271094411611557\n",
      "epoch:  443, loss: 0.008270869962871075\n",
      "epoch:  444, loss: 0.00826972909271717\n",
      "epoch:  445, loss: 0.008268645033240318\n",
      "epoch:  446, loss: 0.00826833862811327\n",
      "epoch:  447, loss: 0.008268093690276146\n",
      "epoch:  448, loss: 0.008266500197350979\n",
      "epoch:  449, loss: 0.008265692740678787\n",
      "epoch:  450, loss: 0.00826540682464838\n",
      "epoch:  451, loss: 0.008265379816293716\n",
      "epoch:  452, loss: 0.008263183757662773\n",
      "epoch:  453, loss: 0.008262722752988338\n",
      "epoch:  454, loss: 0.008262465707957745\n",
      "epoch:  455, loss: 0.008261527866125107\n",
      "epoch:  456, loss: 0.008260242640972137\n",
      "epoch:  457, loss: 0.008259870111942291\n",
      "epoch:  458, loss: 0.008259616792201996\n",
      "epoch:  459, loss: 0.0082577308639884\n",
      "epoch:  460, loss: 0.008257204666733742\n",
      "epoch:  461, loss: 0.008256928995251656\n",
      "epoch:  462, loss: 0.008255776017904282\n",
      "epoch:  463, loss: 0.008254563435912132\n",
      "epoch:  464, loss: 0.008254197426140308\n",
      "epoch:  465, loss: 0.008253936655819416\n",
      "epoch:  466, loss: 0.008252760395407677\n",
      "epoch:  467, loss: 0.008251563645899296\n",
      "epoch:  468, loss: 0.008251224644482136\n",
      "epoch:  469, loss: 0.008250975981354713\n",
      "epoch:  470, loss: 0.008249556645751\n",
      "epoch:  471, loss: 0.008248616009950638\n",
      "epoch:  472, loss: 0.008248315192759037\n",
      "epoch:  473, loss: 0.008248070254921913\n",
      "epoch:  474, loss: 0.008246646262705326\n",
      "epoch:  475, loss: 0.008245770819485188\n",
      "epoch:  476, loss: 0.00824545044451952\n",
      "epoch:  477, loss: 0.008245202712714672\n",
      "epoch:  478, loss: 0.008244983851909637\n",
      "epoch:  479, loss: 0.008244755677878857\n",
      "epoch:  480, loss: 0.008243460208177567\n",
      "epoch:  481, loss: 0.008242488838732243\n",
      "epoch:  482, loss: 0.008242156356573105\n",
      "epoch:  483, loss: 0.008241929113864899\n",
      "epoch:  484, loss: 0.008241694420576096\n",
      "epoch:  485, loss: 0.008241424337029457\n",
      "epoch:  486, loss: 0.008239531889557838\n",
      "epoch:  487, loss: 0.008239165879786015\n",
      "epoch:  488, loss: 0.008238893933594227\n",
      "epoch:  489, loss: 0.008237686939537525\n",
      "epoch:  490, loss: 0.008236641064286232\n",
      "epoch:  491, loss: 0.008236296474933624\n",
      "epoch:  492, loss: 0.008236056193709373\n",
      "epoch:  493, loss: 0.008234518580138683\n",
      "epoch:  494, loss: 0.008233812637627125\n",
      "epoch:  495, loss: 0.008233518339693546\n",
      "epoch:  496, loss: 0.008233285509049892\n",
      "epoch:  497, loss: 0.008232186548411846\n",
      "epoch:  498, loss: 0.008231185376644135\n",
      "epoch:  499, loss: 0.00823086779564619\n",
      "epoch:  500, loss: 0.008230622857809067\n",
      "epoch:  501, loss: 0.008229583501815796\n",
      "epoch:  502, loss: 0.008228504098951817\n",
      "epoch:  503, loss: 0.00822817999869585\n",
      "epoch:  504, loss: 0.008227947168052197\n",
      "epoch:  505, loss: 0.008227168582379818\n",
      "epoch:  506, loss: 0.008225835859775543\n",
      "epoch:  507, loss: 0.008225502446293831\n",
      "epoch:  508, loss: 0.008225275203585625\n",
      "epoch:  509, loss: 0.008224019780755043\n",
      "epoch:  510, loss: 0.008223084732890129\n",
      "epoch:  511, loss: 0.008222784847021103\n",
      "epoch:  512, loss: 0.008222544565796852\n",
      "epoch:  513, loss: 0.008222315460443497\n",
      "epoch:  514, loss: 0.008220418356359005\n",
      "epoch:  515, loss: 0.008220006711781025\n",
      "epoch:  516, loss: 0.00821975152939558\n",
      "epoch:  517, loss: 0.008218602277338505\n",
      "epoch:  518, loss: 0.008217524737119675\n",
      "epoch:  519, loss: 0.00821719877421856\n",
      "epoch:  520, loss: 0.008216958492994308\n",
      "epoch:  521, loss: 0.008215237408876419\n",
      "epoch:  522, loss: 0.008214721456170082\n",
      "epoch:  523, loss: 0.008214447647333145\n",
      "epoch:  524, loss: 0.008214227855205536\n",
      "epoch:  525, loss: 0.008213144727051258\n",
      "epoch:  526, loss: 0.00821200292557478\n",
      "epoch:  527, loss: 0.008211709558963776\n",
      "epoch:  528, loss: 0.008211469277739525\n",
      "epoch:  529, loss: 0.008211104199290276\n",
      "epoch:  530, loss: 0.008209383115172386\n",
      "epoch:  531, loss: 0.008209005929529667\n",
      "epoch:  532, loss: 0.008208769373595715\n",
      "epoch:  533, loss: 0.008207813836634159\n",
      "epoch:  534, loss: 0.008206598460674286\n",
      "epoch:  535, loss: 0.00820628646761179\n",
      "epoch:  536, loss: 0.008206053636968136\n",
      "epoch:  537, loss: 0.008204341866075993\n",
      "epoch:  538, loss: 0.008203837089240551\n",
      "epoch:  539, loss: 0.008203571662306786\n",
      "epoch:  540, loss: 0.0082033546641469\n",
      "epoch:  541, loss: 0.008202376775443554\n",
      "epoch:  542, loss: 0.008201244287192822\n",
      "epoch:  543, loss: 0.008200937882065773\n",
      "epoch:  544, loss: 0.00820077583193779\n",
      "epoch:  545, loss: 0.008198801428079605\n",
      "epoch:  546, loss: 0.008198443800210953\n",
      "epoch:  547, loss: 0.008198206312954426\n",
      "epoch:  548, loss: 0.008197319693863392\n",
      "epoch:  549, loss: 0.008195985108613968\n",
      "epoch:  550, loss: 0.008195631206035614\n",
      "epoch:  551, loss: 0.008195365779101849\n",
      "epoch:  552, loss: 0.00819396786391735\n",
      "epoch:  553, loss: 0.00819286610931158\n",
      "epoch:  554, loss: 0.008192529901862144\n",
      "epoch:  555, loss: 0.008192270062863827\n",
      "epoch:  556, loss: 0.008190275169909\n",
      "epoch:  557, loss: 0.008189858868718147\n",
      "epoch:  558, loss: 0.008189583197236061\n",
      "epoch:  559, loss: 0.008188391104340553\n",
      "epoch:  560, loss: 0.008187190629541874\n",
      "epoch:  561, loss: 0.008186867460608482\n",
      "epoch:  562, loss: 0.008186605758965015\n",
      "epoch:  563, loss: 0.008185145445168018\n",
      "epoch:  564, loss: 0.008184206672012806\n",
      "epoch:  565, loss: 0.008183866739273071\n",
      "epoch:  566, loss: 0.00818343460559845\n",
      "epoch:  567, loss: 0.008181605488061905\n",
      "epoch:  568, loss: 0.008181178942322731\n",
      "epoch:  569, loss: 0.00818093866109848\n",
      "epoch:  570, loss: 0.008180102333426476\n",
      "epoch:  571, loss: 0.008178722113370895\n",
      "epoch:  572, loss: 0.008178411982953548\n",
      "epoch:  573, loss: 0.008178170770406723\n",
      "epoch:  574, loss: 0.00817679613828659\n",
      "epoch:  575, loss: 0.008176055736839771\n",
      "epoch:  576, loss: 0.008175748400390148\n",
      "epoch:  577, loss: 0.008175525814294815\n",
      "epoch:  578, loss: 0.008173712529242039\n",
      "epoch:  579, loss: 0.00817335769534111\n",
      "epoch:  580, loss: 0.008173123002052307\n",
      "epoch:  581, loss: 0.008171522989869118\n",
      "epoch:  582, loss: 0.008171016350388527\n",
      "epoch:  583, loss: 0.008170735090970993\n",
      "epoch:  584, loss: 0.008169817738234997\n",
      "epoch:  585, loss: 0.00816867221146822\n",
      "epoch:  586, loss: 0.008168351836502552\n",
      "epoch:  587, loss: 0.008168138563632965\n",
      "epoch:  588, loss: 0.00816791970282793\n",
      "epoch:  589, loss: 0.008166426792740822\n",
      "epoch:  590, loss: 0.008165806531906128\n",
      "epoch:  591, loss: 0.008165545761585236\n",
      "epoch:  592, loss: 0.00816531665623188\n",
      "epoch:  593, loss: 0.008163616061210632\n",
      "epoch:  594, loss: 0.008163184858858585\n",
      "epoch:  595, loss: 0.00816294364631176\n",
      "epoch:  596, loss: 0.008161939680576324\n",
      "epoch:  597, loss: 0.00816090777516365\n",
      "epoch:  598, loss: 0.008160579949617386\n",
      "epoch:  599, loss: 0.008160374127328396\n",
      "epoch:  600, loss: 0.008160234428942204\n",
      "epoch:  601, loss: 0.008158405311405659\n",
      "epoch:  602, loss: 0.008158069103956223\n",
      "epoch:  603, loss: 0.008157838135957718\n",
      "epoch:  604, loss: 0.008157235570251942\n",
      "epoch:  605, loss: 0.008155868388712406\n",
      "epoch:  606, loss: 0.008155551739037037\n",
      "epoch:  607, loss: 0.008155335672199726\n",
      "epoch:  608, loss: 0.008154074661433697\n",
      "epoch:  609, loss: 0.00815331656485796\n",
      "epoch:  610, loss: 0.008153054863214493\n",
      "epoch:  611, loss: 0.008152845315635204\n",
      "epoch:  612, loss: 0.00815125647932291\n",
      "epoch:  613, loss: 0.008150809444487095\n",
      "epoch:  614, loss: 0.008150583133101463\n",
      "epoch:  615, loss: 0.008150233887135983\n",
      "epoch:  616, loss: 0.008148666471242905\n",
      "epoch:  617, loss: 0.00814831629395485\n",
      "epoch:  618, loss: 0.00814812071621418\n",
      "epoch:  619, loss: 0.008147912099957466\n",
      "epoch:  620, loss: 0.008146172389388084\n",
      "epoch:  621, loss: 0.008145874366164207\n",
      "epoch:  622, loss: 0.00814564898610115\n",
      "epoch:  623, loss: 0.008144750259816647\n",
      "epoch:  624, loss: 0.008143722079694271\n",
      "epoch:  625, loss: 0.008143401704728603\n",
      "epoch:  626, loss: 0.008143195882439613\n",
      "epoch:  627, loss: 0.008142110891640186\n",
      "epoch:  628, loss: 0.008141200989484787\n",
      "epoch:  629, loss: 0.008140918798744678\n",
      "epoch:  630, loss: 0.008140706457197666\n",
      "epoch:  631, loss: 0.008139023557305336\n",
      "epoch:  632, loss: 0.008138695731759071\n",
      "epoch:  633, loss: 0.00813846755772829\n",
      "epoch:  634, loss: 0.008137008175253868\n",
      "epoch:  635, loss: 0.008136477321386337\n",
      "epoch:  636, loss: 0.008136222139000893\n",
      "epoch:  637, loss: 0.008135772310197353\n",
      "epoch:  638, loss: 0.008134391158819199\n",
      "epoch:  639, loss: 0.008134032599627972\n",
      "epoch:  640, loss: 0.008133824914693832\n",
      "epoch:  641, loss: 0.008128665387630463\n",
      "epoch:  642, loss: 0.008117485791444778\n",
      "epoch:  643, loss: 0.008116374723613262\n",
      "epoch:  644, loss: 0.00811605341732502\n",
      "epoch:  645, loss: 0.008116002194583416\n",
      "epoch:  646, loss: 0.008113951422274113\n",
      "epoch:  647, loss: 0.008113580755889416\n",
      "epoch:  648, loss: 0.008113384246826172\n",
      "epoch:  649, loss: 0.008112860843539238\n",
      "epoch:  650, loss: 0.008111538365483284\n",
      "epoch:  651, loss: 0.00811123475432396\n",
      "epoch:  652, loss: 0.008111034519970417\n",
      "epoch:  653, loss: 0.008109436370432377\n",
      "epoch:  654, loss: 0.008109044283628464\n",
      "epoch:  655, loss: 0.008108837530016899\n",
      "epoch:  656, loss: 0.008107680827379227\n",
      "epoch:  657, loss: 0.008106881752610207\n",
      "epoch:  658, loss: 0.008106619119644165\n",
      "epoch:  659, loss: 0.008105992339551449\n",
      "epoch:  660, loss: 0.008104684762656689\n",
      "epoch:  661, loss: 0.0081044165417552\n",
      "epoch:  662, loss: 0.008104149252176285\n",
      "epoch:  663, loss: 0.008102620020508766\n",
      "epoch:  664, loss: 0.008102268911898136\n",
      "epoch:  665, loss: 0.008102062158286572\n",
      "epoch:  666, loss: 0.008100911043584347\n",
      "epoch:  667, loss: 0.008100204169750214\n",
      "epoch:  668, loss: 0.008099932223558426\n",
      "epoch:  669, loss: 0.008099745959043503\n",
      "epoch:  670, loss: 0.008099045604467392\n",
      "epoch:  671, loss: 0.00809798389673233\n",
      "epoch:  672, loss: 0.008097711019217968\n",
      "epoch:  673, loss: 0.008097514510154724\n",
      "epoch:  674, loss: 0.00809614546597004\n",
      "epoch:  675, loss: 0.00809566117823124\n",
      "epoch:  676, loss: 0.008095459081232548\n",
      "epoch:  677, loss: 0.00809487421065569\n",
      "epoch:  678, loss: 0.008093730546534061\n",
      "epoch:  679, loss: 0.008093442767858505\n",
      "epoch:  680, loss: 0.008093245327472687\n",
      "epoch:  681, loss: 0.008092197589576244\n",
      "epoch:  682, loss: 0.00809147022664547\n",
      "epoch:  683, loss: 0.00809122808277607\n",
      "epoch:  684, loss: 0.00809103436768055\n",
      "epoch:  685, loss: 0.008089659735560417\n",
      "epoch:  686, loss: 0.00808925461024046\n",
      "epoch:  687, loss: 0.00808902457356453\n",
      "epoch:  688, loss: 0.00808884110301733\n",
      "epoch:  689, loss: 0.008087573572993279\n",
      "epoch:  690, loss: 0.008074425160884857\n",
      "epoch:  691, loss: 0.008072905242443085\n",
      "epoch:  692, loss: 0.008072496391832829\n",
      "epoch:  693, loss: 0.00807226449251175\n",
      "epoch:  694, loss: 0.008071043528616428\n",
      "epoch:  695, loss: 0.008070291951298714\n",
      "epoch:  696, loss: 0.00806998647749424\n",
      "epoch:  697, loss: 0.008069721050560474\n",
      "epoch:  698, loss: 0.008068173192441463\n",
      "epoch:  699, loss: 0.00806780718266964\n",
      "epoch:  700, loss: 0.008067595772445202\n",
      "epoch:  701, loss: 0.008066471666097641\n",
      "epoch:  702, loss: 0.008065659552812576\n",
      "epoch:  703, loss: 0.008065380156040192\n",
      "epoch:  704, loss: 0.008065185509622097\n",
      "epoch:  705, loss: 0.008064058609306812\n",
      "epoch:  706, loss: 0.008063270710408688\n",
      "epoch:  707, loss: 0.008062995038926601\n",
      "epoch:  708, loss: 0.008062787353992462\n",
      "epoch:  709, loss: 0.008061735890805721\n",
      "epoch:  710, loss: 0.008060922846198082\n",
      "epoch:  711, loss: 0.008060642518103123\n",
      "epoch:  712, loss: 0.008060425519943237\n",
      "epoch:  713, loss: 0.008059295825660229\n",
      "epoch:  714, loss: 0.008058547973632812\n",
      "epoch:  715, loss: 0.008058269508183002\n",
      "epoch:  716, loss: 0.00805805716663599\n",
      "epoch:  717, loss: 0.008056706748902798\n",
      "epoch:  718, loss: 0.008056167513132095\n",
      "epoch:  719, loss: 0.00805593654513359\n",
      "epoch:  720, loss: 0.008055102080106735\n",
      "epoch:  721, loss: 0.008053992874920368\n",
      "epoch:  722, loss: 0.008053679019212723\n",
      "epoch:  723, loss: 0.008053470402956009\n",
      "epoch:  724, loss: 0.008052337914705276\n",
      "epoch:  725, loss: 0.008051532320678234\n",
      "epoch:  726, loss: 0.008051247335970402\n",
      "epoch:  727, loss: 0.008050631731748581\n",
      "epoch:  728, loss: 0.008049344643950462\n",
      "epoch:  729, loss: 0.008049027062952518\n",
      "epoch:  730, loss: 0.008048798888921738\n",
      "epoch:  731, loss: 0.008047576062381268\n",
      "epoch:  732, loss: 0.008046853356063366\n",
      "epoch:  733, loss: 0.008046602830290794\n",
      "epoch:  734, loss: 0.008046405389904976\n",
      "epoch:  735, loss: 0.008045203983783722\n",
      "epoch:  736, loss: 0.008044498041272163\n",
      "epoch:  737, loss: 0.008044256828725338\n",
      "epoch:  738, loss: 0.008044058457016945\n",
      "epoch:  739, loss: 0.008043056353926659\n",
      "epoch:  740, loss: 0.008042208850383759\n",
      "epoch:  741, loss: 0.008041929453611374\n",
      "epoch:  742, loss: 0.008041730150580406\n",
      "epoch:  743, loss: 0.008040990680456161\n",
      "epoch:  744, loss: 0.008039940148591995\n",
      "epoch:  745, loss: 0.008039664477109909\n",
      "epoch:  746, loss: 0.008039462380111217\n",
      "epoch:  747, loss: 0.008039277978241444\n",
      "epoch:  748, loss: 0.00803794339299202\n",
      "epoch:  749, loss: 0.00803745724260807\n",
      "epoch:  750, loss: 0.008037234656512737\n",
      "epoch:  751, loss: 0.008037053048610687\n",
      "epoch:  752, loss: 0.008035860024392605\n",
      "epoch:  753, loss: 0.008035223931074142\n",
      "epoch:  754, loss: 0.008034984581172466\n",
      "epoch:  755, loss: 0.008034791797399521\n",
      "epoch:  756, loss: 0.008033983409404755\n",
      "epoch:  757, loss: 0.008032993413507938\n",
      "epoch:  758, loss: 0.008032691664993763\n",
      "epoch:  759, loss: 0.008032489567995071\n",
      "epoch:  760, loss: 0.008032002486288548\n",
      "epoch:  761, loss: 0.008030716329813004\n",
      "epoch:  762, loss: 0.008030402474105358\n",
      "epoch:  763, loss: 0.00803020317107439\n",
      "epoch:  764, loss: 0.00802901666611433\n",
      "epoch:  765, loss: 0.008028347045183182\n",
      "epoch:  766, loss: 0.008028107695281506\n",
      "epoch:  767, loss: 0.008027919568121433\n",
      "epoch:  768, loss: 0.00802643783390522\n",
      "epoch:  769, loss: 0.008026007562875748\n",
      "epoch:  770, loss: 0.008025789633393288\n",
      "epoch:  771, loss: 0.0080253966152668\n",
      "epoch:  772, loss: 0.008024034090340137\n",
      "epoch:  773, loss: 0.00802367553114891\n",
      "epoch:  774, loss: 0.00802344549447298\n",
      "epoch:  775, loss: 0.008022436872124672\n",
      "epoch:  776, loss: 0.008021433837711811\n",
      "epoch:  777, loss: 0.008021079935133457\n",
      "epoch:  778, loss: 0.008020851761102676\n",
      "epoch:  779, loss: 0.008020368404686451\n",
      "epoch:  780, loss: 0.008018813095986843\n",
      "epoch:  781, loss: 0.008018407970666885\n",
      "epoch:  782, loss: 0.008018177933990955\n",
      "epoch:  783, loss: 0.008016985841095448\n",
      "epoch:  784, loss: 0.008016020059585571\n",
      "epoch:  785, loss: 0.008015708066523075\n",
      "epoch:  786, loss: 0.008015482686460018\n",
      "epoch:  787, loss: 0.008014755323529243\n",
      "epoch:  788, loss: 0.008013395592570305\n",
      "epoch:  789, loss: 0.008013042621314526\n",
      "epoch:  790, loss: 0.008012802340090275\n",
      "epoch:  791, loss: 0.008011717349290848\n",
      "epoch:  792, loss: 0.008010629564523697\n",
      "epoch:  793, loss: 0.008010311052203178\n",
      "epoch:  794, loss: 0.008010084740817547\n",
      "epoch:  795, loss: 0.008008704520761967\n",
      "epoch:  796, loss: 0.008007884956896305\n",
      "epoch:  797, loss: 0.008007570169866085\n",
      "epoch:  798, loss: 0.008007335476577282\n",
      "epoch:  799, loss: 0.008005732670426369\n",
      "epoch:  800, loss: 0.008004970848560333\n",
      "epoch:  801, loss: 0.008004668168723583\n",
      "epoch:  802, loss: 0.00800349097698927\n",
      "epoch:  803, loss: 0.008002378977835178\n",
      "epoch:  804, loss: 0.008002019487321377\n",
      "epoch:  805, loss: 0.008001774549484253\n",
      "epoch:  806, loss: 0.00800102949142456\n",
      "epoch:  807, loss: 0.007999682798981667\n",
      "epoch:  808, loss: 0.007999272085726261\n",
      "epoch:  809, loss: 0.00799901969730854\n",
      "epoch:  810, loss: 0.007998805493116379\n",
      "epoch:  811, loss: 0.00799817405641079\n",
      "epoch:  812, loss: 0.007996717467904091\n",
      "epoch:  813, loss: 0.007996381260454655\n",
      "epoch:  814, loss: 0.007996142841875553\n",
      "epoch:  815, loss: 0.007994566112756729\n",
      "epoch:  816, loss: 0.007993937470018864\n",
      "epoch:  817, loss: 0.007993626408278942\n",
      "epoch:  818, loss: 0.00799337774515152\n",
      "epoch:  819, loss: 0.007991474121809006\n",
      "epoch:  820, loss: 0.007991018705070019\n",
      "epoch:  821, loss: 0.007990765385329723\n",
      "epoch:  822, loss: 0.007990091107785702\n",
      "epoch:  823, loss: 0.007988597266376019\n",
      "epoch:  824, loss: 0.007988187484443188\n",
      "epoch:  825, loss: 0.007987919263541698\n",
      "epoch:  826, loss: 0.007987718097865582\n",
      "epoch:  827, loss: 0.00798584520816803\n",
      "epoch:  828, loss: 0.007985382340848446\n",
      "epoch:  829, loss: 0.007985103875398636\n",
      "epoch:  830, loss: 0.007984868250787258\n",
      "epoch:  831, loss: 0.00798308290541172\n",
      "epoch:  832, loss: 0.007982623763382435\n",
      "epoch:  833, loss: 0.007982327602803707\n",
      "epoch:  834, loss: 0.007982087321579456\n",
      "epoch:  835, loss: 0.007981033064424992\n",
      "epoch:  836, loss: 0.007979966700077057\n",
      "epoch:  837, loss: 0.007979662157595158\n",
      "epoch:  838, loss: 0.007979430258274078\n",
      "epoch:  839, loss: 0.00797179900109768\n",
      "epoch:  840, loss: 0.007962444797158241\n",
      "epoch:  841, loss: 0.007961217314004898\n",
      "epoch:  842, loss: 0.00796075165271759\n",
      "epoch:  843, loss: 0.007958102971315384\n",
      "epoch:  844, loss: 0.007957112975418568\n",
      "epoch:  845, loss: 0.007956767454743385\n",
      "epoch:  846, loss: 0.007955992594361305\n",
      "epoch:  847, loss: 0.00795433484017849\n",
      "epoch:  848, loss: 0.007953914813697338\n",
      "epoch:  849, loss: 0.007953666150569916\n",
      "epoch:  850, loss: 0.007952103391289711\n",
      "epoch:  851, loss: 0.007951430045068264\n",
      "epoch:  852, loss: 0.007951121777296066\n",
      "epoch:  853, loss: 0.00795037392526865\n",
      "epoch:  854, loss: 0.007949072867631912\n",
      "epoch:  855, loss: 0.007948712445795536\n",
      "epoch:  856, loss: 0.007948515005409718\n",
      "epoch:  857, loss: 0.007946634665131569\n",
      "epoch:  858, loss: 0.007946230471134186\n",
      "epoch:  859, loss: 0.007945996709167957\n",
      "epoch:  860, loss: 0.007944634184241295\n",
      "epoch:  861, loss: 0.007943796925246716\n",
      "epoch:  862, loss: 0.007943498902022839\n",
      "epoch:  863, loss: 0.007942713797092438\n",
      "epoch:  864, loss: 0.00794146303087473\n",
      "epoch:  865, loss: 0.007941046729683876\n",
      "epoch:  866, loss: 0.007940809242427349\n",
      "epoch:  867, loss: 0.007939567789435387\n",
      "epoch:  868, loss: 0.007938617840409279\n",
      "epoch:  869, loss: 0.007938316091895103\n",
      "epoch:  870, loss: 0.00793733261525631\n",
      "epoch:  871, loss: 0.007936005480587482\n",
      "epoch:  872, loss: 0.007935632951557636\n",
      "epoch:  873, loss: 0.00793540570884943\n",
      "epoch:  874, loss: 0.007934223860502243\n",
      "epoch:  875, loss: 0.007933266460895538\n",
      "epoch:  876, loss: 0.007932936772704124\n",
      "epoch:  877, loss: 0.007932713255286217\n",
      "epoch:  878, loss: 0.007930840365588665\n",
      "epoch:  879, loss: 0.007930479012429714\n",
      "epoch:  880, loss: 0.007930249907076359\n",
      "epoch:  881, loss: 0.00792846642434597\n",
      "epoch:  882, loss: 0.007928065955638885\n",
      "epoch:  883, loss: 0.00792782288044691\n",
      "epoch:  884, loss: 0.007926554419100285\n",
      "epoch:  885, loss: 0.00792568176984787\n",
      "epoch:  886, loss: 0.007925399579107761\n",
      "epoch:  887, loss: 0.007925178855657578\n",
      "epoch:  888, loss: 0.007923782803118229\n",
      "epoch:  889, loss: 0.00792301632463932\n",
      "epoch:  890, loss: 0.00792274996638298\n",
      "epoch:  891, loss: 0.007922531105577946\n",
      "epoch:  892, loss: 0.007920973002910614\n",
      "epoch:  893, loss: 0.007920327596366405\n",
      "epoch:  894, loss: 0.00792006403207779\n",
      "epoch:  895, loss: 0.00791951734572649\n",
      "epoch:  896, loss: 0.007917980663478374\n",
      "epoch:  897, loss: 0.007917607203125954\n",
      "epoch:  898, loss: 0.007917376235127449\n",
      "epoch:  899, loss: 0.007915583439171314\n",
      "epoch:  900, loss: 0.007915121503174305\n",
      "epoch:  901, loss: 0.007914865389466286\n",
      "epoch:  902, loss: 0.007913203909993172\n",
      "epoch:  903, loss: 0.007912359200417995\n",
      "epoch:  904, loss: 0.007911968976259232\n",
      "epoch:  905, loss: 0.007911628112196922\n",
      "epoch:  906, loss: 0.007909255102276802\n",
      "epoch:  907, loss: 0.007908600382506847\n",
      "epoch:  908, loss: 0.007908285595476627\n",
      "epoch:  909, loss: 0.007907185703516006\n",
      "epoch:  910, loss: 0.007905872538685799\n",
      "epoch:  911, loss: 0.007905503734946251\n",
      "epoch:  912, loss: 0.00790527556091547\n",
      "epoch:  913, loss: 0.007903903722763062\n",
      "epoch:  914, loss: 0.00790286436676979\n",
      "epoch:  915, loss: 0.007902457378804684\n",
      "epoch:  916, loss: 0.007902250625193119\n",
      "epoch:  917, loss: 0.007899743504822254\n",
      "epoch:  918, loss: 0.007899235934019089\n",
      "epoch:  919, loss: 0.007898945361375809\n",
      "epoch:  920, loss: 0.007897601462900639\n",
      "epoch:  921, loss: 0.007896476425230503\n",
      "epoch:  922, loss: 0.007896154187619686\n",
      "epoch:  923, loss: 0.007895917631685734\n",
      "epoch:  924, loss: 0.00789419561624527\n",
      "epoch:  925, loss: 0.007893617264926434\n",
      "epoch:  926, loss: 0.007893351837992668\n",
      "epoch:  927, loss: 0.0078926095739007\n",
      "epoch:  928, loss: 0.007891068235039711\n",
      "epoch:  929, loss: 0.007890698499977589\n",
      "epoch:  930, loss: 0.007890472188591957\n",
      "epoch:  931, loss: 0.007888851687312126\n",
      "epoch:  932, loss: 0.007888305932283401\n",
      "epoch:  933, loss: 0.007888026535511017\n",
      "epoch:  934, loss: 0.007886759005486965\n",
      "epoch:  935, loss: 0.007885887287557125\n",
      "epoch:  936, loss: 0.007885588333010674\n",
      "epoch:  937, loss: 0.00788513571023941\n",
      "epoch:  938, loss: 0.007883589714765549\n",
      "epoch:  939, loss: 0.007883220911026001\n",
      "epoch:  940, loss: 0.007882993668317795\n",
      "epoch:  941, loss: 0.007882071658968925\n",
      "epoch:  942, loss: 0.007880950346589088\n",
      "epoch:  943, loss: 0.007880618795752525\n",
      "epoch:  944, loss: 0.007880392484366894\n",
      "epoch:  945, loss: 0.007878679782152176\n",
      "epoch:  946, loss: 0.007878212258219719\n",
      "epoch:  947, loss: 0.007877982221543789\n",
      "epoch:  948, loss: 0.007877577096223831\n",
      "epoch:  949, loss: 0.007875956594944\n",
      "epoch:  950, loss: 0.007875549606978893\n",
      "epoch:  951, loss: 0.007875286042690277\n",
      "epoch:  952, loss: 0.007873916998505592\n",
      "epoch:  953, loss: 0.007872900925576687\n",
      "epoch:  954, loss: 0.00787259265780449\n",
      "epoch:  955, loss: 0.007872363552451134\n",
      "epoch:  956, loss: 0.007870552130043507\n",
      "epoch:  957, loss: 0.00787009671330452\n",
      "epoch:  958, loss: 0.007869846187531948\n",
      "epoch:  959, loss: 0.0078684501349926\n",
      "epoch:  960, loss: 0.007867584004998207\n",
      "epoch:  961, loss: 0.007867260836064816\n",
      "epoch:  962, loss: 0.007867066189646721\n",
      "epoch:  963, loss: 0.007864921353757381\n",
      "epoch:  964, loss: 0.007864385843276978\n",
      "epoch:  965, loss: 0.007864118553698063\n",
      "epoch:  966, loss: 0.007862641476094723\n",
      "epoch:  967, loss: 0.007861683145165443\n",
      "epoch:  968, loss: 0.007861349731683731\n",
      "epoch:  969, loss: 0.007861096411943436\n",
      "epoch:  970, loss: 0.007859405130147934\n",
      "epoch:  971, loss: 0.007858782075345516\n",
      "epoch:  972, loss: 0.00785849615931511\n",
      "epoch:  973, loss: 0.00785806030035019\n",
      "epoch:  974, loss: 0.007856359705328941\n",
      "epoch:  975, loss: 0.007855982519686222\n",
      "epoch:  976, loss: 0.007855748757719994\n",
      "epoch:  977, loss: 0.00785400066524744\n",
      "epoch:  978, loss: 0.007853398099541664\n",
      "epoch:  979, loss: 0.007853117771446705\n",
      "epoch:  980, loss: 0.007852357812225819\n",
      "epoch:  981, loss: 0.007850893773138523\n",
      "epoch:  982, loss: 0.007850483991205692\n",
      "epoch:  983, loss: 0.007850229740142822\n",
      "epoch:  984, loss: 0.007848634384572506\n",
      "epoch:  985, loss: 0.007847967557609081\n",
      "epoch:  986, loss: 0.0078476807102561\n",
      "epoch:  987, loss: 0.00784754753112793\n",
      "epoch:  988, loss: 0.007845645770430565\n",
      "epoch:  989, loss: 0.007845205254852772\n",
      "epoch:  990, loss: 0.00784496869891882\n",
      "epoch:  991, loss: 0.007844334468245506\n",
      "epoch:  992, loss: 0.007842916995286942\n",
      "epoch:  993, loss: 0.007842582650482655\n",
      "epoch:  994, loss: 0.007842350751161575\n",
      "epoch:  995, loss: 0.007841063663363457\n",
      "epoch:  996, loss: 0.007840221747756004\n",
      "epoch:  997, loss: 0.007839912548661232\n",
      "epoch:  998, loss: 0.007839676924049854\n",
      "epoch:  999, loss: 0.007838550955057144\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size=X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# opt = torch_numopt.ConjugateGradient(model=model, lr=1e-4, line_search_method=\"const\", cg_method=\"PR\")\n",
    "opt = torch_numopt.ConjugateGradient(model=model, lr=10, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"armijo\", cg_method=\"PR\")\n",
    "# opt = torch_numopt.ConjugateGradient(model=model, lr=10, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"wolfe\", cg_method=\"PR\")\n",
    "# opt = torch_numopt.ConjugateGradient(model=model, lr=10, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"strong-wolfe\", cg_method=\"PR\")\n",
    "# opt = torch_numopt.ConjugateGradient(model=model, lr=10, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"goldstein\", cg_method=\"PR\")\n",
    "\n",
    "\n",
    "all_loss = {}\n",
    "for epoch in range(1000):\n",
    "    print(\"epoch: \", epoch, end=\"\")\n",
    "    all_loss[epoch + 1] = 0\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y, loss_fn)\n",
    "\n",
    "        all_loss[epoch + 1] += loss\n",
    "    all_loss[epoch + 1] /= len(data_loader)\n",
    "    print(\", loss: {}\".format(all_loss[epoch + 1].detach().numpy().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics: R2 = 0.745688319128039\n",
      "Test metrics:  R2 = 0.6817437390731944\n"
     ]
    }
   ],
   "source": [
    "pred_train = model.forward(X_train).detach()\n",
    "pred_test = model.forward(X_test).detach()\n",
    "print(f\"Train metrics: R2 = {r2_score(pred_train, y_train)}\")\n",
    "print(f\"Test metrics:  R2 = {r2_score(pred_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.5688683986663818\n",
      "epoch:  1, loss: 0.3418840169906616\n",
      "epoch:  2, loss: 0.22045190632343292\n",
      "epoch:  3, loss: 0.1485162377357483\n",
      "epoch:  4, loss: 0.10523427277803421\n",
      "epoch:  5, loss: 0.07901901006698608\n",
      "epoch:  6, loss: 0.06309784948825836\n",
      "epoch:  7, loss: 0.053424522280693054\n",
      "epoch:  8, loss: 0.04755575582385063\n",
      "epoch:  9, loss: 0.044009730219841\n",
      "epoch:  10, loss: 0.041869208216667175\n",
      "epoch:  11, loss: 0.04057271033525467\n",
      "epoch:  12, loss: 0.03978190943598747\n",
      "epoch:  13, loss: 0.03929724171757698\n",
      "epoch:  14, loss: 0.03899863362312317\n",
      "epoch:  15, loss: 0.03881295770406723\n",
      "epoch:  16, loss: 0.03869627043604851\n",
      "epoch:  17, loss: 0.038677435368299484\n",
      "epoch:  18, loss: 0.03857744485139847\n",
      "epoch:  19, loss: 0.038514621555805206\n",
      "epoch:  20, loss: 0.038497600704431534\n",
      "epoch:  21, loss: 0.03843778744339943\n",
      "epoch:  22, loss: 0.03842516243457794\n",
      "epoch:  23, loss: 0.03836779668927193\n",
      "epoch:  24, loss: 0.03836705908179283\n",
      "epoch:  25, loss: 0.03831084817647934\n",
      "epoch:  26, loss: 0.0383097268640995\n",
      "epoch:  27, loss: 0.0382516048848629\n",
      "epoch:  28, loss: 0.03824848309159279\n",
      "epoch:  29, loss: 0.03818922117352486\n",
      "epoch:  30, loss: 0.03818216174840927\n",
      "epoch:  31, loss: 0.038118887692689896\n",
      "epoch:  32, loss: 0.03810492157936096\n",
      "epoch:  33, loss: 0.038037367165088654\n",
      "epoch:  34, loss: 0.038025785237550735\n",
      "epoch:  35, loss: 0.03795066475868225\n",
      "epoch:  36, loss: 0.037943363189697266\n",
      "epoch:  37, loss: 0.03785670921206474\n",
      "epoch:  38, loss: 0.037802066653966904\n",
      "epoch:  39, loss: 0.037757180631160736\n",
      "epoch:  40, loss: 0.03769061714410782\n",
      "epoch:  41, loss: 0.037667278200387955\n",
      "epoch:  42, loss: 0.03758367523550987\n",
      "epoch:  43, loss: 0.037579573690891266\n",
      "epoch:  44, loss: 0.037471044808626175\n",
      "epoch:  45, loss: 0.03740335628390312\n",
      "epoch:  46, loss: 0.037371691316366196\n",
      "epoch:  47, loss: 0.03727691248059273\n",
      "epoch:  48, loss: 0.0372188463807106\n",
      "epoch:  49, loss: 0.037146102637052536\n",
      "epoch:  50, loss: 0.03706992045044899\n",
      "epoch:  51, loss: 0.03704554960131645\n",
      "epoch:  52, loss: 0.036933399736881256\n",
      "epoch:  53, loss: 0.0368647575378418\n",
      "epoch:  54, loss: 0.036814797669649124\n",
      "epoch:  55, loss: 0.03671535849571228\n",
      "epoch:  56, loss: 0.036654531955718994\n",
      "epoch:  57, loss: 0.03658410534262657\n",
      "epoch:  58, loss: 0.03649483621120453\n",
      "epoch:  59, loss: 0.03648517653346062\n",
      "epoch:  60, loss: 0.03634365648031235\n",
      "epoch:  61, loss: 0.03626154735684395\n",
      "epoch:  62, loss: 0.03622464835643768\n",
      "epoch:  63, loss: 0.036095522344112396\n",
      "epoch:  64, loss: 0.036018919199705124\n",
      "epoch:  65, loss: 0.03596155345439911\n",
      "epoch:  66, loss: 0.03583443537354469\n",
      "epoch:  67, loss: 0.03575965017080307\n",
      "epoch:  68, loss: 0.03567403182387352\n",
      "epoch:  69, loss: 0.035554442554712296\n",
      "epoch:  70, loss: 0.03548276424407959\n",
      "epoch:  71, loss: 0.03537873178720474\n",
      "epoch:  72, loss: 0.03525634855031967\n",
      "epoch:  73, loss: 0.0351836197078228\n",
      "epoch:  74, loss: 0.03505636751651764\n",
      "epoch:  75, loss: 0.034935470670461655\n",
      "epoch:  76, loss: 0.034861788153648376\n",
      "epoch:  77, loss: 0.034715842455625534\n",
      "epoch:  78, loss: 0.0345873199403286\n",
      "epoch:  79, loss: 0.03451152890920639\n",
      "epoch:  80, loss: 0.03435235098004341\n",
      "epoch:  81, loss: 0.03421572595834732\n",
      "epoch:  82, loss: 0.03413347154855728\n",
      "epoch:  83, loss: 0.0339665561914444\n",
      "epoch:  84, loss: 0.03381163999438286\n",
      "epoch:  85, loss: 0.03372107818722725\n",
      "epoch:  86, loss: 0.03356277570128441\n",
      "epoch:  87, loss: 0.033381104469299316\n",
      "epoch:  88, loss: 0.03327794373035431\n",
      "epoch:  89, loss: 0.03312631696462631\n",
      "epoch:  90, loss: 0.03290992230176926\n",
      "epoch:  91, loss: 0.03279317915439606\n",
      "epoch:  92, loss: 0.03267116844654083\n",
      "epoch:  93, loss: 0.03240661323070526\n",
      "epoch:  94, loss: 0.03226970508694649\n",
      "epoch:  95, loss: 0.03219958394765854\n",
      "epoch:  96, loss: 0.03186525031924248\n",
      "epoch:  97, loss: 0.03170226886868477\n",
      "epoch:  98, loss: 0.0316992849111557\n",
      "epoch:  99, loss: 0.031276214867830276\n",
      "epoch:  100, loss: 0.031081676483154297\n",
      "epoch:  101, loss: 0.030973346903920174\n",
      "epoch:  102, loss: 0.030639121308922768\n",
      "epoch:  103, loss: 0.030412115156650543\n",
      "epoch:  104, loss: 0.03029152937233448\n",
      "epoch:  105, loss: 0.02999138832092285\n",
      "epoch:  106, loss: 0.029695022851228714\n",
      "epoch:  107, loss: 0.029553573578596115\n",
      "epoch:  108, loss: 0.02927754819393158\n",
      "epoch:  109, loss: 0.028926052153110504\n",
      "epoch:  110, loss: 0.028768759220838547\n",
      "epoch:  111, loss: 0.028537539765238762\n",
      "epoch:  112, loss: 0.02809598296880722\n",
      "epoch:  113, loss: 0.02792101912200451\n",
      "epoch:  114, loss: 0.0276662465184927\n",
      "epoch:  115, loss: 0.027208631858229637\n",
      "epoch:  116, loss: 0.027031058445572853\n",
      "epoch:  117, loss: 0.02681257203221321\n",
      "epoch:  118, loss: 0.026268890127539635\n",
      "epoch:  119, loss: 0.026090750470757484\n",
      "epoch:  120, loss: 0.025794267654418945\n",
      "epoch:  121, loss: 0.02528809942305088\n",
      "epoch:  122, loss: 0.025116223841905594\n",
      "epoch:  123, loss: 0.024770354852080345\n",
      "epoch:  124, loss: 0.02426408790051937\n",
      "epoch:  125, loss: 0.024110613390803337\n",
      "epoch:  126, loss: 0.023639507591724396\n",
      "epoch:  127, loss: 0.023228779435157776\n",
      "epoch:  128, loss: 0.023091910406947136\n",
      "epoch:  129, loss: 0.022482624277472496\n",
      "epoch:  130, loss: 0.022179262712597847\n",
      "epoch:  131, loss: 0.02206728607416153\n",
      "epoch:  132, loss: 0.021344570443034172\n",
      "epoch:  133, loss: 0.021147266030311584\n",
      "epoch:  134, loss: 0.020736875012516975\n",
      "epoch:  135, loss: 0.020256508141756058\n",
      "epoch:  136, loss: 0.020145151764154434\n",
      "epoch:  137, loss: 0.019436998292803764\n",
      "epoch:  138, loss: 0.019264565780758858\n",
      "epoch:  139, loss: 0.01875177212059498\n",
      "epoch:  140, loss: 0.018419988453388214\n",
      "epoch:  141, loss: 0.01826491206884384\n",
      "epoch:  142, loss: 0.017617445439100266\n",
      "epoch:  143, loss: 0.017528025433421135\n",
      "epoch:  144, loss: 0.01639583148062229\n",
      "epoch:  145, loss: 0.012522914446890354\n",
      "epoch:  146, loss: 0.012195843271911144\n",
      "epoch:  147, loss: 0.012136319652199745\n",
      "epoch:  148, loss: 0.011884268373250961\n",
      "epoch:  149, loss: 0.011717945337295532\n",
      "epoch:  150, loss: 0.011679235845804214\n",
      "epoch:  151, loss: 0.011442707851529121\n",
      "epoch:  152, loss: 0.01136059407144785\n",
      "epoch:  153, loss: 0.011331931687891483\n",
      "epoch:  154, loss: 0.011151301674544811\n",
      "epoch:  155, loss: 0.00972920935600996\n",
      "epoch:  156, loss: 0.00958950910717249\n",
      "epoch:  157, loss: 0.009567717090249062\n",
      "epoch:  158, loss: 0.009555310942232609\n",
      "epoch:  159, loss: 0.009463140740990639\n",
      "epoch:  160, loss: 0.009446235373616219\n",
      "epoch:  161, loss: 0.009437544271349907\n",
      "epoch:  162, loss: 0.009384824894368649\n",
      "epoch:  163, loss: 0.00936420913785696\n",
      "epoch:  164, loss: 0.009356659837067127\n",
      "epoch:  165, loss: 0.009347932413220406\n",
      "epoch:  166, loss: 0.009304029867053032\n",
      "epoch:  167, loss: 0.00929509662091732\n",
      "epoch:  168, loss: 0.009290015324950218\n",
      "epoch:  169, loss: 0.00925926398485899\n",
      "epoch:  170, loss: 0.009244641289114952\n",
      "epoch:  171, loss: 0.009239399805665016\n",
      "epoch:  172, loss: 0.009233367629349232\n",
      "epoch:  173, loss: 0.009202355518937111\n",
      "epoch:  174, loss: 0.009195270016789436\n",
      "epoch:  175, loss: 0.009191252291202545\n",
      "epoch:  176, loss: 0.009175609797239304\n",
      "epoch:  177, loss: 0.009157492779195309\n",
      "epoch:  178, loss: 0.009152090176939964\n",
      "epoch:  179, loss: 0.009148719720542431\n",
      "epoch:  180, loss: 0.00914053712040186\n",
      "epoch:  181, loss: 0.009120269678533077\n",
      "epoch:  182, loss: 0.009115108288824558\n",
      "epoch:  183, loss: 0.009112037718296051\n",
      "epoch:  184, loss: 0.009104269556701183\n",
      "epoch:  185, loss: 0.009087502025067806\n",
      "epoch:  186, loss: 0.009082891047000885\n",
      "epoch:  187, loss: 0.009080318734049797\n",
      "epoch:  188, loss: 0.009067606180906296\n",
      "epoch:  189, loss: 0.009058129973709583\n",
      "epoch:  190, loss: 0.009054891765117645\n",
      "epoch:  191, loss: 0.009052718989551067\n",
      "epoch:  192, loss: 0.009038238786160946\n",
      "epoch:  193, loss: 0.009032241068780422\n",
      "epoch:  194, loss: 0.009029822424054146\n",
      "epoch:  195, loss: 0.00902790017426014\n",
      "epoch:  196, loss: 0.009013408794999123\n",
      "epoch:  197, loss: 0.00900921318680048\n",
      "epoch:  198, loss: 0.00900719128549099\n",
      "epoch:  199, loss: 0.009005559608340263\n",
      "epoch:  200, loss: 0.008996988646686077\n",
      "epoch:  201, loss: 0.008990314789116383\n",
      "epoch:  202, loss: 0.008988039568066597\n",
      "epoch:  203, loss: 0.008986540138721466\n",
      "epoch:  204, loss: 0.008979622274637222\n",
      "epoch:  205, loss: 0.008972594514489174\n",
      "epoch:  206, loss: 0.008970458060503006\n",
      "epoch:  207, loss: 0.008969022892415524\n",
      "epoch:  208, loss: 0.008961229585111141\n",
      "epoch:  209, loss: 0.008956103585660458\n",
      "epoch:  210, loss: 0.00895428191870451\n",
      "epoch:  211, loss: 0.008952973410487175\n",
      "epoch:  212, loss: 0.008946636691689491\n",
      "epoch:  213, loss: 0.008941411040723324\n",
      "epoch:  214, loss: 0.008939691819250584\n",
      "epoch:  215, loss: 0.008938513696193695\n",
      "epoch:  216, loss: 0.008930875919759274\n",
      "epoch:  217, loss: 0.008927542716264725\n",
      "epoch:  218, loss: 0.00892618577927351\n",
      "epoch:  219, loss: 0.008925173431634903\n",
      "epoch:  220, loss: 0.008918520994484425\n",
      "epoch:  221, loss: 0.00891544483602047\n",
      "epoch:  222, loss: 0.008914250880479813\n",
      "epoch:  223, loss: 0.008913330733776093\n",
      "epoch:  224, loss: 0.00890651810914278\n",
      "epoch:  225, loss: 0.008904321119189262\n",
      "epoch:  226, loss: 0.008903310634195805\n",
      "epoch:  227, loss: 0.008902481757104397\n",
      "epoch:  228, loss: 0.008896901272237301\n",
      "epoch:  229, loss: 0.008894557133316994\n",
      "epoch:  230, loss: 0.008893552236258984\n",
      "epoch:  231, loss: 0.008892777375876904\n",
      "epoch:  232, loss: 0.008892440237104893\n",
      "epoch:  233, loss: 0.008886468596756458\n",
      "epoch:  234, loss: 0.00888487696647644\n",
      "epoch:  235, loss: 0.00888403132557869\n",
      "epoch:  236, loss: 0.008883364498615265\n",
      "epoch:  237, loss: 0.00887866411358118\n",
      "epoch:  238, loss: 0.008876945823431015\n",
      "epoch:  239, loss: 0.008876120671629906\n",
      "epoch:  240, loss: 0.008875505067408085\n",
      "epoch:  241, loss: 0.00887332297861576\n",
      "epoch:  242, loss: 0.008870082907378674\n",
      "epoch:  243, loss: 0.008868987672030926\n",
      "epoch:  244, loss: 0.008868357166647911\n",
      "epoch:  245, loss: 0.008867815136909485\n",
      "epoch:  246, loss: 0.008864249102771282\n",
      "epoch:  247, loss: 0.008862539194524288\n",
      "epoch:  248, loss: 0.008861848153173923\n",
      "epoch:  249, loss: 0.008861340582370758\n",
      "epoch:  250, loss: 0.008860299363732338\n",
      "epoch:  251, loss: 0.00885701086372137\n",
      "epoch:  252, loss: 0.008855978026986122\n",
      "epoch:  253, loss: 0.008855413645505905\n",
      "epoch:  254, loss: 0.008854945190250874\n",
      "epoch:  255, loss: 0.00885190162807703\n",
      "epoch:  256, loss: 0.008850337006151676\n",
      "epoch:  257, loss: 0.008849691599607468\n",
      "epoch:  258, loss: 0.008849183097481728\n",
      "epoch:  259, loss: 0.008848951198160648\n",
      "epoch:  260, loss: 0.00884515605866909\n",
      "epoch:  261, loss: 0.008844166062772274\n",
      "epoch:  262, loss: 0.008843541145324707\n",
      "epoch:  263, loss: 0.00884307362139225\n",
      "epoch:  264, loss: 0.008842644281685352\n",
      "epoch:  265, loss: 0.008840581402182579\n",
      "epoch:  266, loss: 0.008838963694870472\n",
      "epoch:  267, loss: 0.008838373236358166\n",
      "epoch:  268, loss: 0.008837917819619179\n",
      "epoch:  269, loss: 0.008837537840008736\n",
      "epoch:  270, loss: 0.008835699409246445\n",
      "epoch:  271, loss: 0.008834192529320717\n",
      "epoch:  272, loss: 0.008833564817905426\n",
      "epoch:  273, loss: 0.008833141066133976\n",
      "epoch:  274, loss: 0.008832799270749092\n",
      "epoch:  275, loss: 0.008831638842821121\n",
      "epoch:  276, loss: 0.008829577825963497\n",
      "epoch:  277, loss: 0.008828920312225819\n",
      "epoch:  278, loss: 0.008828475140035152\n",
      "epoch:  279, loss: 0.008828125894069672\n",
      "epoch:  280, loss: 0.008825629949569702\n",
      "epoch:  281, loss: 0.008824771270155907\n",
      "epoch:  282, loss: 0.008824309334158897\n",
      "epoch:  283, loss: 0.008823968470096588\n",
      "epoch:  284, loss: 0.008823874406516552\n",
      "epoch:  285, loss: 0.008821303024888039\n",
      "epoch:  286, loss: 0.00882052257657051\n",
      "epoch:  287, loss: 0.008820105344057083\n",
      "epoch:  288, loss: 0.00881978590041399\n",
      "epoch:  289, loss: 0.008818377740681171\n",
      "epoch:  290, loss: 0.008816950023174286\n",
      "epoch:  291, loss: 0.00881645455956459\n",
      "epoch:  292, loss: 0.00881609320640564\n",
      "epoch:  293, loss: 0.008815797977149487\n",
      "epoch:  294, loss: 0.008815511129796505\n",
      "epoch:  295, loss: 0.008813921362161636\n",
      "epoch:  296, loss: 0.008812726475298405\n",
      "epoch:  297, loss: 0.008812281303107738\n",
      "epoch:  298, loss: 0.008811949752271175\n",
      "epoch:  299, loss: 0.008811651729047298\n",
      "epoch:  300, loss: 0.008810076862573624\n",
      "epoch:  301, loss: 0.008808952756226063\n",
      "epoch:  302, loss: 0.008808467537164688\n",
      "epoch:  303, loss: 0.008808135986328125\n",
      "epoch:  304, loss: 0.008807840757071972\n",
      "epoch:  305, loss: 0.008806012570858002\n",
      "epoch:  306, loss: 0.008804927580058575\n",
      "epoch:  307, loss: 0.00880448892712593\n",
      "epoch:  308, loss: 0.008804161101579666\n",
      "epoch:  309, loss: 0.00880386307835579\n",
      "epoch:  310, loss: 0.008802315220236778\n",
      "epoch:  311, loss: 0.008801073767244816\n",
      "epoch:  312, loss: 0.008800518698990345\n",
      "epoch:  313, loss: 0.008800157345831394\n",
      "epoch:  314, loss: 0.00879983976483345\n",
      "epoch:  315, loss: 0.008797666989266872\n",
      "epoch:  316, loss: 0.008796828798949718\n",
      "epoch:  317, loss: 0.008796441368758678\n",
      "epoch:  318, loss: 0.008796116337180138\n",
      "epoch:  319, loss: 0.008795116096735\n",
      "epoch:  320, loss: 0.008793517015874386\n",
      "epoch:  321, loss: 0.008793005719780922\n",
      "epoch:  322, loss: 0.008792685344815254\n",
      "epoch:  323, loss: 0.008791659027338028\n",
      "epoch:  324, loss: 0.00879027508199215\n",
      "epoch:  325, loss: 0.008789759129285812\n",
      "epoch:  326, loss: 0.008789410814642906\n",
      "epoch:  327, loss: 0.008789125829935074\n",
      "epoch:  328, loss: 0.008787311613559723\n",
      "epoch:  329, loss: 0.008786652237176895\n",
      "epoch:  330, loss: 0.008786354213953018\n",
      "epoch:  331, loss: 0.008786114864051342\n",
      "epoch:  332, loss: 0.008784246630966663\n",
      "epoch:  333, loss: 0.008783775381743908\n",
      "epoch:  334, loss: 0.008783492259681225\n",
      "epoch:  335, loss: 0.00878327339887619\n",
      "epoch:  336, loss: 0.008783061057329178\n",
      "epoch:  337, loss: 0.00878131203353405\n",
      "epoch:  338, loss: 0.008780837059020996\n",
      "epoch:  339, loss: 0.008780525997281075\n",
      "epoch:  340, loss: 0.008780281990766525\n",
      "epoch:  341, loss: 0.008780037984251976\n",
      "epoch:  342, loss: 0.008778559975326061\n",
      "epoch:  343, loss: 0.008777820505201817\n",
      "epoch:  344, loss: 0.008777505718171597\n",
      "epoch:  345, loss: 0.008777270093560219\n",
      "epoch:  346, loss: 0.008777053095400333\n",
      "epoch:  347, loss: 0.00877646915614605\n",
      "epoch:  348, loss: 0.008775094524025917\n",
      "epoch:  349, loss: 0.008774630725383759\n",
      "epoch:  350, loss: 0.008774359710514545\n",
      "epoch:  351, loss: 0.008774111978709698\n",
      "epoch:  352, loss: 0.008772501721978188\n",
      "epoch:  353, loss: 0.008771768771111965\n",
      "epoch:  354, loss: 0.008771427907049656\n",
      "epoch:  355, loss: 0.008771173655986786\n",
      "epoch:  356, loss: 0.008770029991865158\n",
      "epoch:  357, loss: 0.008769098669290543\n",
      "epoch:  358, loss: 0.008768713101744652\n",
      "epoch:  359, loss: 0.008768501691520214\n",
      "epoch:  360, loss: 0.008768308907747269\n",
      "epoch:  361, loss: 0.008767067454755306\n",
      "epoch:  362, loss: 0.008766482584178448\n",
      "epoch:  363, loss: 0.00876620039343834\n",
      "epoch:  364, loss: 0.008766013197600842\n",
      "epoch:  365, loss: 0.008765834383666515\n",
      "epoch:  366, loss: 0.008765666745603085\n",
      "epoch:  367, loss: 0.00876549631357193\n",
      "epoch:  368, loss: 0.008765232749283314\n",
      "epoch:  369, loss: 0.008763925172388554\n",
      "epoch:  370, loss: 0.008763537742197514\n",
      "epoch:  371, loss: 0.008763293735682964\n",
      "epoch:  372, loss: 0.008763113059103489\n",
      "epoch:  373, loss: 0.008762937970459461\n",
      "epoch:  374, loss: 0.008762768469750881\n",
      "epoch:  375, loss: 0.0087626026943326\n",
      "epoch:  376, loss: 0.008762435056269169\n",
      "epoch:  377, loss: 0.00876227393746376\n",
      "epoch:  378, loss: 0.008762103505432606\n",
      "epoch:  379, loss: 0.008761946111917496\n",
      "epoch:  380, loss: 0.008761774748563766\n",
      "epoch:  381, loss: 0.008761042729020119\n",
      "epoch:  382, loss: 0.008760270662605762\n",
      "epoch:  383, loss: 0.008759954944252968\n",
      "epoch:  384, loss: 0.00875974539667368\n",
      "epoch:  385, loss: 0.00875956192612648\n",
      "epoch:  386, loss: 0.008758604526519775\n",
      "epoch:  387, loss: 0.008758035488426685\n",
      "epoch:  388, loss: 0.008757755160331726\n",
      "epoch:  389, loss: 0.00875759031623602\n",
      "epoch:  390, loss: 0.008757435716688633\n",
      "epoch:  391, loss: 0.008756368421018124\n",
      "epoch:  392, loss: 0.008756023831665516\n",
      "epoch:  393, loss: 0.00875579658895731\n",
      "epoch:  394, loss: 0.008755611255764961\n",
      "epoch:  395, loss: 0.00875543337315321\n",
      "epoch:  396, loss: 0.008755261078476906\n",
      "epoch:  397, loss: 0.008754967711865902\n",
      "epoch:  398, loss: 0.008753771893680096\n",
      "epoch:  399, loss: 0.008753390982747078\n",
      "epoch:  400, loss: 0.008753152564167976\n",
      "epoch:  401, loss: 0.008752960711717606\n",
      "epoch:  402, loss: 0.008752772584557533\n",
      "epoch:  403, loss: 0.008752741850912571\n",
      "epoch:  404, loss: 0.008751212619245052\n",
      "epoch:  405, loss: 0.008750761859118938\n",
      "epoch:  406, loss: 0.008750535547733307\n",
      "epoch:  407, loss: 0.008750365115702152\n",
      "epoch:  408, loss: 0.00875012669712305\n",
      "epoch:  409, loss: 0.008749044500291348\n",
      "epoch:  410, loss: 0.008748692460358143\n",
      "epoch:  411, loss: 0.008748496882617474\n",
      "epoch:  412, loss: 0.008748339489102364\n",
      "epoch:  413, loss: 0.008747762069106102\n",
      "epoch:  414, loss: 0.008746919222176075\n",
      "epoch:  415, loss: 0.008746654726564884\n",
      "epoch:  416, loss: 0.008746457286179066\n",
      "epoch:  417, loss: 0.008746305480599403\n",
      "epoch:  418, loss: 0.00874534621834755\n",
      "epoch:  419, loss: 0.00874496903270483\n",
      "epoch:  420, loss: 0.008744734339416027\n",
      "epoch:  421, loss: 0.008744591847062111\n",
      "epoch:  422, loss: 0.008744449354708195\n",
      "epoch:  423, loss: 0.008743993937969208\n",
      "epoch:  424, loss: 0.008743219077587128\n",
      "epoch:  425, loss: 0.008742946200072765\n",
      "epoch:  426, loss: 0.008742760866880417\n",
      "epoch:  427, loss: 0.00874259788542986\n",
      "epoch:  428, loss: 0.00874246098101139\n",
      "epoch:  429, loss: 0.008742318488657475\n",
      "epoch:  430, loss: 0.008742179721593857\n",
      "epoch:  431, loss: 0.008741416968405247\n",
      "epoch:  432, loss: 0.008740839548408985\n",
      "epoch:  433, loss: 0.008740616030991077\n",
      "epoch:  434, loss: 0.008740424178540707\n",
      "epoch:  435, loss: 0.008740279823541641\n",
      "epoch:  436, loss: 0.008740135468542576\n",
      "epoch:  437, loss: 0.008740011602640152\n",
      "epoch:  438, loss: 0.00873987004160881\n",
      "epoch:  439, loss: 0.008739815093576908\n",
      "epoch:  440, loss: 0.008738684467971325\n",
      "epoch:  441, loss: 0.008738343603909016\n",
      "epoch:  442, loss: 0.008738106116652489\n",
      "epoch:  443, loss: 0.008737915195524693\n",
      "epoch:  444, loss: 0.008737732656300068\n",
      "epoch:  445, loss: 0.00873757153749466\n",
      "epoch:  446, loss: 0.008737463504076004\n",
      "epoch:  447, loss: 0.00873617548495531\n",
      "epoch:  448, loss: 0.00873575545847416\n",
      "epoch:  449, loss: 0.008735551498830318\n",
      "epoch:  450, loss: 0.00873538851737976\n",
      "epoch:  451, loss: 0.00873484369367361\n",
      "epoch:  452, loss: 0.008733972907066345\n",
      "epoch:  453, loss: 0.008733700029551983\n",
      "epoch:  454, loss: 0.008733481168746948\n",
      "epoch:  455, loss: 0.008733328431844711\n",
      "epoch:  456, loss: 0.008733194321393967\n",
      "epoch:  457, loss: 0.008732752874493599\n",
      "epoch:  458, loss: 0.008732015267014503\n",
      "epoch:  459, loss: 0.008731785230338573\n",
      "epoch:  460, loss: 0.008731604553759098\n",
      "epoch:  461, loss: 0.008731460198760033\n",
      "epoch:  462, loss: 0.008731319569051266\n",
      "epoch:  463, loss: 0.008730866014957428\n",
      "epoch:  464, loss: 0.00873006321489811\n",
      "epoch:  465, loss: 0.008729768916964531\n",
      "epoch:  466, loss: 0.008729550987482071\n",
      "epoch:  467, loss: 0.00872939545661211\n",
      "epoch:  468, loss: 0.008729284629225731\n",
      "epoch:  469, loss: 0.00872820895165205\n",
      "epoch:  470, loss: 0.00872779730707407\n",
      "epoch:  471, loss: 0.008727594278752804\n",
      "epoch:  472, loss: 0.00872744806110859\n",
      "epoch:  473, loss: 0.008726469241082668\n",
      "epoch:  474, loss: 0.008726098574697971\n",
      "epoch:  475, loss: 0.00872587226331234\n",
      "epoch:  476, loss: 0.00872569065541029\n",
      "epoch:  477, loss: 0.00872496236115694\n",
      "epoch:  478, loss: 0.008724247105419636\n",
      "epoch:  479, loss: 0.008723964914679527\n",
      "epoch:  480, loss: 0.008723794482648373\n",
      "epoch:  481, loss: 0.008723239414393902\n",
      "epoch:  482, loss: 0.008722309023141861\n",
      "epoch:  483, loss: 0.008722028695046902\n",
      "epoch:  484, loss: 0.008721842430531979\n",
      "epoch:  485, loss: 0.008721685037016869\n",
      "epoch:  486, loss: 0.0087211849167943\n",
      "epoch:  487, loss: 0.0087202750146389\n",
      "epoch:  488, loss: 0.00871993601322174\n",
      "epoch:  489, loss: 0.008719739504158497\n",
      "epoch:  490, loss: 0.008719577454030514\n",
      "epoch:  491, loss: 0.008718512021005154\n",
      "epoch:  492, loss: 0.008718093857169151\n",
      "epoch:  493, loss: 0.008717912249267101\n",
      "epoch:  494, loss: 0.008717769756913185\n",
      "epoch:  495, loss: 0.008716822601854801\n",
      "epoch:  496, loss: 0.008716431446373463\n",
      "epoch:  497, loss: 0.008716234005987644\n",
      "epoch:  498, loss: 0.008716095238924026\n",
      "epoch:  499, loss: 0.008715596981346607\n",
      "epoch:  500, loss: 0.008714979514479637\n",
      "epoch:  501, loss: 0.008714710362255573\n",
      "epoch:  502, loss: 0.008714557625353336\n",
      "epoch:  503, loss: 0.008714426308870316\n",
      "epoch:  504, loss: 0.008713562041521072\n",
      "epoch:  505, loss: 0.008713196031749249\n",
      "epoch:  506, loss: 0.008713005110621452\n",
      "epoch:  507, loss: 0.008712870068848133\n",
      "epoch:  508, loss: 0.008712741546332836\n",
      "epoch:  509, loss: 0.008712240494787693\n",
      "epoch:  510, loss: 0.008711599744856358\n",
      "epoch:  511, loss: 0.00871132593601942\n",
      "epoch:  512, loss: 0.008711173199117184\n",
      "epoch:  513, loss: 0.00871103722602129\n",
      "epoch:  514, loss: 0.00871090218424797\n",
      "epoch:  515, loss: 0.008710014633834362\n",
      "epoch:  516, loss: 0.008709775283932686\n",
      "epoch:  517, loss: 0.008709593676030636\n",
      "epoch:  518, loss: 0.008709454908967018\n",
      "epoch:  519, loss: 0.008709338493645191\n",
      "epoch:  520, loss: 0.008709232322871685\n",
      "epoch:  521, loss: 0.008709126152098179\n",
      "epoch:  522, loss: 0.008709016256034374\n",
      "epoch:  523, loss: 0.008708452805876732\n",
      "epoch:  524, loss: 0.008708029054105282\n",
      "epoch:  525, loss: 0.008707824163138866\n",
      "epoch:  526, loss: 0.008707692846655846\n",
      "epoch:  527, loss: 0.008707575500011444\n",
      "epoch:  528, loss: 0.008706994354724884\n",
      "epoch:  529, loss: 0.00870658177882433\n",
      "epoch:  530, loss: 0.008706402964890003\n",
      "epoch:  531, loss: 0.0087062893435359\n",
      "epoch:  532, loss: 0.00870618224143982\n",
      "epoch:  533, loss: 0.0087055005133152\n",
      "epoch:  534, loss: 0.008705240674316883\n",
      "epoch:  535, loss: 0.00870508048683405\n",
      "epoch:  536, loss: 0.00870497152209282\n",
      "epoch:  537, loss: 0.008704865351319313\n",
      "epoch:  538, loss: 0.008704769425094128\n",
      "epoch:  539, loss: 0.008704678155481815\n",
      "epoch:  540, loss: 0.008704578503966331\n",
      "epoch:  541, loss: 0.0087043521925807\n",
      "epoch:  542, loss: 0.008703812956809998\n",
      "epoch:  543, loss: 0.008703548461198807\n",
      "epoch:  544, loss: 0.00870343018323183\n",
      "epoch:  545, loss: 0.008703322149813175\n",
      "epoch:  546, loss: 0.008703220635652542\n",
      "epoch:  547, loss: 0.008702550083398819\n",
      "epoch:  548, loss: 0.008702332153916359\n",
      "epoch:  549, loss: 0.0087021728977561\n",
      "epoch:  550, loss: 0.008702056482434273\n",
      "epoch:  551, loss: 0.008701970800757408\n",
      "epoch:  552, loss: 0.008701863698661327\n",
      "epoch:  553, loss: 0.008701770566403866\n",
      "epoch:  554, loss: 0.008701476268470287\n",
      "epoch:  555, loss: 0.008700959384441376\n",
      "epoch:  556, loss: 0.00870075449347496\n",
      "epoch:  557, loss: 0.008700612001121044\n",
      "epoch:  558, loss: 0.008700508624315262\n",
      "epoch:  559, loss: 0.008700086735188961\n",
      "epoch:  560, loss: 0.008699681609869003\n",
      "epoch:  561, loss: 0.008699481375515461\n",
      "epoch:  562, loss: 0.008699372410774231\n",
      "epoch:  563, loss: 0.008699270896613598\n",
      "epoch:  564, loss: 0.008699174970388412\n",
      "epoch:  565, loss: 0.008698449470102787\n",
      "epoch:  566, loss: 0.008698281832039356\n",
      "epoch:  567, loss: 0.008698144927620888\n",
      "epoch:  568, loss: 0.008698037825524807\n",
      "epoch:  569, loss: 0.00869794748723507\n",
      "epoch:  570, loss: 0.008697855286300182\n",
      "epoch:  571, loss: 0.008697849698364735\n",
      "epoch:  572, loss: 0.00869713630527258\n",
      "epoch:  573, loss: 0.008696882985532284\n",
      "epoch:  574, loss: 0.008696750737726688\n",
      "epoch:  575, loss: 0.008696652948856354\n",
      "epoch:  576, loss: 0.008696558885276318\n",
      "epoch:  577, loss: 0.00869590975344181\n",
      "epoch:  578, loss: 0.008695692755281925\n",
      "epoch:  579, loss: 0.008695565164089203\n",
      "epoch:  580, loss: 0.008695444092154503\n",
      "epoch:  581, loss: 0.00869535468518734\n",
      "epoch:  582, loss: 0.008695261552929878\n",
      "epoch:  583, loss: 0.008695165626704693\n",
      "epoch:  584, loss: 0.008694558404386044\n",
      "epoch:  585, loss: 0.008694300428032875\n",
      "epoch:  586, loss: 0.008694175630807877\n",
      "epoch:  587, loss: 0.008694075047969818\n",
      "epoch:  588, loss: 0.008693980053067207\n",
      "epoch:  589, loss: 0.008693734183907509\n",
      "epoch:  590, loss: 0.008693234995007515\n",
      "epoch:  591, loss: 0.008693025447428226\n",
      "epoch:  592, loss: 0.008692893199622631\n",
      "epoch:  593, loss: 0.008692796342074871\n",
      "epoch:  594, loss: 0.008692706935107708\n",
      "epoch:  595, loss: 0.008692619390785694\n",
      "epoch:  596, loss: 0.008691995404660702\n",
      "epoch:  597, loss: 0.008691773749887943\n",
      "epoch:  598, loss: 0.008691644296050072\n",
      "epoch:  599, loss: 0.008691550232470036\n",
      "epoch:  600, loss: 0.008691460825502872\n",
      "epoch:  601, loss: 0.008690877817571163\n",
      "epoch:  602, loss: 0.008690658956766129\n",
      "epoch:  603, loss: 0.008690513670444489\n",
      "epoch:  604, loss: 0.008690407499670982\n",
      "epoch:  605, loss: 0.008690323680639267\n",
      "epoch:  606, loss: 0.00869022961705923\n",
      "epoch:  607, loss: 0.008689898997545242\n",
      "epoch:  608, loss: 0.008689523674547672\n",
      "epoch:  609, loss: 0.008689324371516705\n",
      "epoch:  610, loss: 0.008689211681485176\n",
      "epoch:  611, loss: 0.008689121343195438\n",
      "epoch:  612, loss: 0.0086890310049057\n",
      "epoch:  613, loss: 0.008688478730618954\n",
      "epoch:  614, loss: 0.008688257075846195\n",
      "epoch:  615, loss: 0.008688113652169704\n",
      "epoch:  616, loss: 0.008688002824783325\n",
      "epoch:  617, loss: 0.00868790689855814\n",
      "epoch:  618, loss: 0.008687816560268402\n",
      "epoch:  619, loss: 0.008687606081366539\n",
      "epoch:  620, loss: 0.008687091991305351\n",
      "epoch:  621, loss: 0.00868686567991972\n",
      "epoch:  622, loss: 0.008686747401952744\n",
      "epoch:  623, loss: 0.00868664775043726\n",
      "epoch:  624, loss: 0.008686559274792671\n",
      "epoch:  625, loss: 0.008686460554599762\n",
      "epoch:  626, loss: 0.008685857988893986\n",
      "epoch:  627, loss: 0.008685622364282608\n",
      "epoch:  628, loss: 0.008685487322509289\n",
      "epoch:  629, loss: 0.008685392327606678\n",
      "epoch:  630, loss: 0.008685292676091194\n",
      "epoch:  631, loss: 0.008684836328029633\n",
      "epoch:  632, loss: 0.008684470318257809\n",
      "epoch:  633, loss: 0.008684255182743073\n",
      "epoch:  634, loss: 0.008684131316840649\n",
      "epoch:  635, loss: 0.008684036321938038\n",
      "epoch:  636, loss: 0.008683944121003151\n",
      "epoch:  637, loss: 0.008683649823069572\n",
      "epoch:  638, loss: 0.008683176711201668\n",
      "epoch:  639, loss: 0.008682954125106335\n",
      "epoch:  640, loss: 0.008682840503752232\n",
      "epoch:  641, loss: 0.00868273712694645\n",
      "epoch:  642, loss: 0.008682642132043839\n",
      "epoch:  643, loss: 0.008681914769113064\n",
      "epoch:  644, loss: 0.008681737817823887\n",
      "epoch:  645, loss: 0.008681613951921463\n",
      "epoch:  646, loss: 0.008681517094373703\n",
      "epoch:  647, loss: 0.00868142582476139\n",
      "epoch:  648, loss: 0.008681339211761951\n",
      "epoch:  649, loss: 0.008681247010827065\n",
      "epoch:  650, loss: 0.008681164123117924\n",
      "epoch:  651, loss: 0.00868107657879591\n",
      "epoch:  652, loss: 0.008680993691086769\n",
      "epoch:  653, loss: 0.008680912666022778\n",
      "epoch:  654, loss: 0.008680819533765316\n",
      "epoch:  655, loss: 0.008680732920765877\n",
      "epoch:  656, loss: 0.008680643513798714\n",
      "epoch:  657, loss: 0.008680298924446106\n",
      "epoch:  658, loss: 0.008679874241352081\n",
      "epoch:  659, loss: 0.008679646998643875\n",
      "epoch:  660, loss: 0.008679529651999474\n",
      "epoch:  661, loss: 0.008679425343871117\n",
      "epoch:  662, loss: 0.008679328486323357\n",
      "epoch:  663, loss: 0.008678617887198925\n",
      "epoch:  664, loss: 0.00867837481200695\n",
      "epoch:  665, loss: 0.008678200654685497\n",
      "epoch:  666, loss: 0.008678095415234566\n",
      "epoch:  667, loss: 0.008677991107106209\n",
      "epoch:  668, loss: 0.008677891455590725\n",
      "epoch:  669, loss: 0.008677799254655838\n",
      "epoch:  670, loss: 0.008677449077367783\n",
      "epoch:  671, loss: 0.008676925674080849\n",
      "epoch:  672, loss: 0.00867673009634018\n",
      "epoch:  673, loss: 0.008676604367792606\n",
      "epoch:  674, loss: 0.008676489815115929\n",
      "epoch:  675, loss: 0.008676272816956043\n",
      "epoch:  676, loss: 0.008675629273056984\n",
      "epoch:  677, loss: 0.00867538433521986\n",
      "epoch:  678, loss: 0.008675222285091877\n",
      "epoch:  679, loss: 0.008675099350512028\n",
      "epoch:  680, loss: 0.008674977347254753\n",
      "epoch:  681, loss: 0.008674132637679577\n",
      "epoch:  682, loss: 0.008673870004713535\n",
      "epoch:  683, loss: 0.008673684671521187\n",
      "epoch:  684, loss: 0.008673564530909061\n",
      "epoch:  685, loss: 0.008673448115587234\n",
      "epoch:  686, loss: 0.008673332631587982\n",
      "epoch:  687, loss: 0.008673306554555893\n",
      "epoch:  688, loss: 0.008672474883496761\n",
      "epoch:  689, loss: 0.008672192692756653\n",
      "epoch:  690, loss: 0.008672049269080162\n",
      "epoch:  691, loss: 0.008671938441693783\n",
      "epoch:  692, loss: 0.008671651594340801\n",
      "epoch:  693, loss: 0.008671012707054615\n",
      "epoch:  694, loss: 0.008670804090797901\n",
      "epoch:  695, loss: 0.008670679293572903\n",
      "epoch:  696, loss: 0.00867057591676712\n",
      "epoch:  697, loss: 0.00867006927728653\n",
      "epoch:  698, loss: 0.008669649250805378\n",
      "epoch:  699, loss: 0.008669453673064709\n",
      "epoch:  700, loss: 0.008669334463775158\n",
      "epoch:  701, loss: 0.008669188246130943\n",
      "epoch:  702, loss: 0.008668498136103153\n",
      "epoch:  703, loss: 0.008668240159749985\n",
      "epoch:  704, loss: 0.008668089285492897\n",
      "epoch:  705, loss: 0.008667979389429092\n",
      "epoch:  706, loss: 0.008667503483593464\n",
      "epoch:  707, loss: 0.008667059242725372\n",
      "epoch:  708, loss: 0.008666859939694405\n",
      "epoch:  709, loss: 0.008666740730404854\n",
      "epoch:  710, loss: 0.00866663083434105\n",
      "epoch:  711, loss: 0.008666206151247025\n",
      "epoch:  712, loss: 0.008665800094604492\n",
      "epoch:  713, loss: 0.008665541186928749\n",
      "epoch:  714, loss: 0.0086654182523489\n",
      "epoch:  715, loss: 0.00866530928760767\n",
      "epoch:  716, loss: 0.008665204048156738\n",
      "epoch:  717, loss: 0.008665110915899277\n",
      "epoch:  718, loss: 0.008665011264383793\n",
      "epoch:  719, loss: 0.008664264343678951\n",
      "epoch:  720, loss: 0.00866406224668026\n",
      "epoch:  721, loss: 0.008663895539939404\n",
      "epoch:  722, loss: 0.008663796819746494\n",
      "epoch:  723, loss: 0.008663691580295563\n",
      "epoch:  724, loss: 0.00866358820348978\n",
      "epoch:  725, loss: 0.008663496002554893\n",
      "epoch:  726, loss: 0.008663399145007133\n",
      "epoch:  727, loss: 0.008662810549139977\n",
      "epoch:  728, loss: 0.008662496693432331\n",
      "epoch:  729, loss: 0.008662300184369087\n",
      "epoch:  730, loss: 0.008662189356982708\n",
      "epoch:  731, loss: 0.0086620869114995\n",
      "epoch:  732, loss: 0.008661985397338867\n",
      "epoch:  733, loss: 0.00866189319640398\n",
      "epoch:  734, loss: 0.008661823347210884\n",
      "epoch:  735, loss: 0.008661065250635147\n",
      "epoch:  736, loss: 0.00866085384041071\n",
      "epoch:  737, loss: 0.008660704828798771\n",
      "epoch:  738, loss: 0.008660583756864071\n",
      "epoch:  739, loss: 0.00866048876196146\n",
      "epoch:  740, loss: 0.008660387247800827\n",
      "epoch:  741, loss: 0.00865982286632061\n",
      "epoch:  742, loss: 0.008659510873258114\n",
      "epoch:  743, loss: 0.00865930039435625\n",
      "epoch:  744, loss: 0.008659178391098976\n",
      "epoch:  745, loss: 0.008659087121486664\n",
      "epoch:  746, loss: 0.008658983744680882\n",
      "epoch:  747, loss: 0.008658512495458126\n",
      "epoch:  748, loss: 0.00865813996642828\n",
      "epoch:  749, loss: 0.008657913655042648\n",
      "epoch:  750, loss: 0.008657787926495075\n",
      "epoch:  751, loss: 0.008657686412334442\n",
      "epoch:  752, loss: 0.008657587692141533\n",
      "epoch:  753, loss: 0.008657499216496944\n",
      "epoch:  754, loss: 0.008657397702336311\n",
      "epoch:  755, loss: 0.008657301776111126\n",
      "epoch:  756, loss: 0.008657204918563366\n",
      "epoch:  757, loss: 0.008657109923660755\n",
      "epoch:  758, loss: 0.00865701399743557\n",
      "epoch:  759, loss: 0.008656895719468594\n",
      "epoch:  760, loss: 0.008656198158860207\n",
      "epoch:  761, loss: 0.008655957877635956\n",
      "epoch:  762, loss: 0.00865582562983036\n",
      "epoch:  763, loss: 0.008655714802443981\n",
      "epoch:  764, loss: 0.008655606769025326\n",
      "epoch:  765, loss: 0.00865490734577179\n",
      "epoch:  766, loss: 0.008654659613966942\n",
      "epoch:  767, loss: 0.008654514327645302\n",
      "epoch:  768, loss: 0.008654397912323475\n",
      "epoch:  769, loss: 0.008649513125419617\n",
      "epoch:  770, loss: 0.008647779934108257\n",
      "epoch:  771, loss: 0.008647252805531025\n",
      "epoch:  772, loss: 0.008646986447274685\n",
      "epoch:  773, loss: 0.008646740578114986\n",
      "epoch:  774, loss: 0.00864540133625269\n",
      "epoch:  775, loss: 0.00864475779235363\n",
      "epoch:  776, loss: 0.00864449143409729\n",
      "epoch:  777, loss: 0.008644300512969494\n",
      "epoch:  778, loss: 0.008643998764455318\n",
      "epoch:  779, loss: 0.008643045090138912\n",
      "epoch:  780, loss: 0.008642736822366714\n",
      "epoch:  781, loss: 0.008642585948109627\n",
      "epoch:  782, loss: 0.008642470464110374\n",
      "epoch:  783, loss: 0.008641637861728668\n",
      "epoch:  784, loss: 0.008641378954052925\n",
      "epoch:  785, loss: 0.008641229011118412\n",
      "epoch:  786, loss: 0.008641119115054607\n",
      "epoch:  787, loss: 0.008641024120151997\n",
      "epoch:  788, loss: 0.008640918880701065\n",
      "epoch:  789, loss: 0.008640832267701626\n",
      "epoch:  790, loss: 0.008640735410153866\n",
      "epoch:  791, loss: 0.00864064134657383\n",
      "epoch:  792, loss: 0.008640549145638943\n",
      "epoch:  793, loss: 0.008640248328447342\n",
      "epoch:  794, loss: 0.008639715611934662\n",
      "epoch:  795, loss: 0.008639511652290821\n",
      "epoch:  796, loss: 0.00863939244300127\n",
      "epoch:  797, loss: 0.00863928534090519\n",
      "epoch:  798, loss: 0.008639180101454258\n",
      "epoch:  799, loss: 0.008639087900519371\n",
      "epoch:  800, loss: 0.008638985455036163\n",
      "epoch:  801, loss: 0.008638418279588223\n",
      "epoch:  802, loss: 0.008638114668428898\n",
      "epoch:  803, loss: 0.00863796379417181\n",
      "epoch:  804, loss: 0.00863785482943058\n",
      "epoch:  805, loss: 0.008637750521302223\n",
      "epoch:  806, loss: 0.008637646213173866\n",
      "epoch:  807, loss: 0.00863749347627163\n",
      "epoch:  808, loss: 0.008636838756501675\n",
      "epoch:  809, loss: 0.008636645041406155\n",
      "epoch:  810, loss: 0.008636511862277985\n",
      "epoch:  811, loss: 0.008636405691504478\n",
      "epoch:  812, loss: 0.008636164478957653\n",
      "epoch:  813, loss: 0.008635560050606728\n",
      "epoch:  814, loss: 0.008635330945253372\n",
      "epoch:  815, loss: 0.00863518938422203\n",
      "epoch:  816, loss: 0.008635071106255054\n",
      "epoch:  817, loss: 0.008634964004158974\n",
      "epoch:  818, loss: 0.008634326979517937\n",
      "epoch:  819, loss: 0.008634032681584358\n",
      "epoch:  820, loss: 0.008633865974843502\n",
      "epoch:  821, loss: 0.008633745834231377\n",
      "epoch:  822, loss: 0.00863363966345787\n",
      "epoch:  823, loss: 0.008633079938590527\n",
      "epoch:  824, loss: 0.008632750250399113\n",
      "epoch:  825, loss: 0.008632589131593704\n",
      "epoch:  826, loss: 0.008632474578917027\n",
      "epoch:  827, loss: 0.00863236840814352\n",
      "epoch:  828, loss: 0.00863226130604744\n",
      "epoch:  829, loss: 0.008632013574242592\n",
      "epoch:  830, loss: 0.008631396107375622\n",
      "epoch:  831, loss: 0.008631205186247826\n",
      "epoch:  832, loss: 0.008631068281829357\n",
      "epoch:  833, loss: 0.008630957454442978\n",
      "epoch:  834, loss: 0.00863052997738123\n",
      "epoch:  835, loss: 0.008630144409835339\n",
      "epoch:  836, loss: 0.008629951626062393\n",
      "epoch:  837, loss: 0.008629824966192245\n",
      "epoch:  838, loss: 0.008629714138805866\n",
      "epoch:  839, loss: 0.008629612624645233\n",
      "epoch:  840, loss: 0.008629235439002514\n",
      "epoch:  841, loss: 0.008628824725747108\n",
      "epoch:  842, loss: 0.008628586307168007\n",
      "epoch:  843, loss: 0.008628465235233307\n",
      "epoch:  844, loss: 0.008628355339169502\n",
      "epoch:  845, loss: 0.00862831436097622\n",
      "epoch:  846, loss: 0.00862763449549675\n",
      "epoch:  847, loss: 0.008627394214272499\n",
      "epoch:  848, loss: 0.008627249859273434\n",
      "epoch:  849, loss: 0.008627132512629032\n",
      "epoch:  850, loss: 0.008627024479210377\n",
      "epoch:  851, loss: 0.008626928552985191\n",
      "epoch:  852, loss: 0.008626830764114857\n",
      "epoch:  853, loss: 0.008626054972410202\n",
      "epoch:  854, loss: 0.00862591527402401\n",
      "epoch:  855, loss: 0.008625810965895653\n",
      "epoch:  856, loss: 0.008625710383057594\n",
      "epoch:  857, loss: 0.00862562283873558\n",
      "epoch:  858, loss: 0.008625519461929798\n",
      "epoch:  859, loss: 0.008625423535704613\n",
      "epoch:  860, loss: 0.008624776266515255\n",
      "epoch:  861, loss: 0.008624549023807049\n",
      "epoch:  862, loss: 0.008624418638646603\n",
      "epoch:  863, loss: 0.00862430315464735\n",
      "epoch:  864, loss: 0.008624204434454441\n",
      "epoch:  865, loss: 0.008624108508229256\n",
      "epoch:  866, loss: 0.008624017238616943\n",
      "epoch:  867, loss: 0.008623925037682056\n",
      "epoch:  868, loss: 0.008623842149972916\n",
      "epoch:  869, loss: 0.008623765781521797\n",
      "epoch:  870, loss: 0.008623670786619186\n",
      "epoch:  871, loss: 0.008623581379652023\n",
      "epoch:  872, loss: 0.00862349197268486\n",
      "epoch:  873, loss: 0.008623402565717697\n",
      "epoch:  874, loss: 0.00862331036478281\n",
      "epoch:  875, loss: 0.008623228408396244\n",
      "epoch:  876, loss: 0.008623145520687103\n",
      "epoch:  877, loss: 0.008623064495623112\n",
      "epoch:  878, loss: 0.008622970432043076\n",
      "epoch:  879, loss: 0.008622882887721062\n",
      "epoch:  880, loss: 0.008622792549431324\n",
      "epoch:  881, loss: 0.008622701279819012\n",
      "epoch:  882, loss: 0.008622617460787296\n",
      "epoch:  883, loss: 0.008622526191174984\n",
      "epoch:  884, loss: 0.008622454479336739\n",
      "epoch:  885, loss: 0.00862237811088562\n",
      "epoch:  886, loss: 0.008622285909950733\n",
      "epoch:  887, loss: 0.008622206747531891\n",
      "epoch:  888, loss: 0.00862212385982275\n",
      "epoch:  889, loss: 0.008622044697403908\n",
      "epoch:  890, loss: 0.008621962741017342\n",
      "epoch:  891, loss: 0.008621889166533947\n",
      "epoch:  892, loss: 0.00862180721014738\n",
      "epoch:  893, loss: 0.008621720597147942\n",
      "epoch:  894, loss: 0.008621631190180779\n",
      "epoch:  895, loss: 0.00862155482172966\n",
      "epoch:  896, loss: 0.008621473796665668\n",
      "epoch:  897, loss: 0.0086213955655694\n",
      "epoch:  898, loss: 0.00862131454050541\n",
      "epoch:  899, loss: 0.008621244691312313\n",
      "epoch:  900, loss: 0.0086211571469903\n",
      "epoch:  901, loss: 0.008621074259281158\n",
      "epoch:  902, loss: 0.008620993234217167\n",
      "epoch:  903, loss: 0.008620907552540302\n",
      "epoch:  904, loss: 0.008620820939540863\n",
      "epoch:  905, loss: 0.00862074550241232\n",
      "epoch:  906, loss: 0.008620663546025753\n",
      "epoch:  907, loss: 0.008620585314929485\n",
      "epoch:  908, loss: 0.008620508015155792\n",
      "epoch:  909, loss: 0.008620435371994972\n",
      "epoch:  910, loss: 0.00862034596502781\n",
      "epoch:  911, loss: 0.008620263077318668\n",
      "epoch:  912, loss: 0.008620182052254677\n",
      "epoch:  913, loss: 0.008620097301900387\n",
      "epoch:  914, loss: 0.008620011620223522\n",
      "epoch:  915, loss: 0.008619933389127254\n",
      "epoch:  916, loss: 0.008619854226708412\n",
      "epoch:  917, loss: 0.008619784377515316\n",
      "epoch:  918, loss: 0.008619699627161026\n",
      "epoch:  919, loss: 0.008619613945484161\n",
      "epoch:  920, loss: 0.008619530126452446\n",
      "epoch:  921, loss: 0.00861944630742073\n",
      "epoch:  922, loss: 0.00861936341971159\n",
      "epoch:  923, loss: 0.008619297295808792\n",
      "epoch:  924, loss: 0.008619203232228756\n",
      "epoch:  925, loss: 0.008619120344519615\n",
      "epoch:  926, loss: 0.008618888445198536\n",
      "epoch:  927, loss: 0.008618321269750595\n",
      "epoch:  928, loss: 0.008618096821010113\n",
      "epoch:  929, loss: 0.008617938496172428\n",
      "epoch:  930, loss: 0.008617819286882877\n",
      "epoch:  931, loss: 0.008617711253464222\n",
      "epoch:  932, loss: 0.008617604151368141\n",
      "epoch:  933, loss: 0.008616957813501358\n",
      "epoch:  934, loss: 0.008616677485406399\n",
      "epoch:  935, loss: 0.008616538718342781\n",
      "epoch:  936, loss: 0.00861642137169838\n",
      "epoch:  937, loss: 0.008616329170763493\n",
      "epoch:  938, loss: 0.008616224862635136\n",
      "epoch:  939, loss: 0.00861599761992693\n",
      "epoch:  940, loss: 0.008615479804575443\n",
      "epoch:  941, loss: 0.008615237660706043\n",
      "epoch:  942, loss: 0.008615097030997276\n",
      "epoch:  943, loss: 0.008614989928901196\n",
      "epoch:  944, loss: 0.008614894933998585\n",
      "epoch:  945, loss: 0.008614668622612953\n",
      "epoch:  946, loss: 0.008614148013293743\n",
      "epoch:  947, loss: 0.00861394964158535\n",
      "epoch:  948, loss: 0.008613822050392628\n",
      "epoch:  949, loss: 0.008613714948296547\n",
      "epoch:  950, loss: 0.00861360877752304\n",
      "epoch:  951, loss: 0.008613240905106068\n",
      "epoch:  952, loss: 0.008612800389528275\n",
      "epoch:  953, loss: 0.00861256755888462\n",
      "epoch:  954, loss: 0.008612430654466152\n",
      "epoch:  955, loss: 0.00861233752220869\n",
      "epoch:  956, loss: 0.008612247183918953\n",
      "epoch:  957, loss: 0.00861215591430664\n",
      "epoch:  958, loss: 0.0086120730265975\n",
      "epoch:  959, loss: 0.008611992932856083\n",
      "epoch:  960, loss: 0.008611895143985748\n",
      "epoch:  961, loss: 0.008611810393631458\n",
      "epoch:  962, loss: 0.008611719124019146\n",
      "epoch:  963, loss: 0.008611631579697132\n",
      "epoch:  964, loss: 0.008611543104052544\n",
      "epoch:  965, loss: 0.008611467666924\n",
      "epoch:  966, loss: 0.00861137080937624\n",
      "epoch:  967, loss: 0.008611286990344524\n",
      "epoch:  968, loss: 0.008611192926764488\n",
      "epoch:  969, loss: 0.008611111901700497\n",
      "epoch:  970, loss: 0.008611021563410759\n",
      "epoch:  971, loss: 0.008610947988927364\n",
      "epoch:  972, loss: 0.008610853925347328\n",
      "epoch:  973, loss: 0.008610760793089867\n",
      "epoch:  974, loss: 0.008610677905380726\n",
      "epoch:  975, loss: 0.008610592223703861\n",
      "epoch:  976, loss: 0.008610514923930168\n",
      "epoch:  977, loss: 0.0086104366928339\n",
      "epoch:  978, loss: 0.008610345423221588\n",
      "epoch:  979, loss: 0.008610259741544724\n",
      "epoch:  980, loss: 0.008610174991190434\n",
      "epoch:  981, loss: 0.008610088378190994\n",
      "epoch:  982, loss: 0.008610006421804428\n",
      "epoch:  983, loss: 0.008609932847321033\n",
      "epoch:  984, loss: 0.008609840646386147\n",
      "epoch:  985, loss: 0.008609759621322155\n",
      "epoch:  986, loss: 0.008609671145677567\n",
      "epoch:  987, loss: 0.008609583601355553\n",
      "epoch:  988, loss: 0.008609496057033539\n",
      "epoch:  989, loss: 0.008609424345195293\n",
      "epoch:  990, loss: 0.008609344251453876\n",
      "epoch:  991, loss: 0.008609255775809288\n",
      "epoch:  992, loss: 0.008609163574874401\n",
      "epoch:  993, loss: 0.008609078824520111\n",
      "epoch:  994, loss: 0.008608993142843246\n",
      "epoch:  995, loss: 0.008608908392488956\n",
      "epoch:  996, loss: 0.008608829230070114\n",
      "epoch:  997, loss: 0.008608752861618996\n",
      "epoch:  998, loss: 0.008608673699200153\n",
      "epoch:  999, loss: 0.008608583360910416\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size=X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# opt = torch_numopt.ConjugateGradient(model=model, lr=1e-4, line_search_method=\"const\", cg_method=\"FR\")\n",
    "opt = torch_numopt.ConjugateGradient(model=model, lr=10, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"armijo\", cg_method=\"FR\")\n",
    "# opt = torch_numopt.ConjugateGradient(model=model, lr=10, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"wolfe\", cg_method=\"FR\")\n",
    "# opt = torch_numopt.ConjugateGradient(model=model, lr=10, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"strong-wolfe\", cg_method=\"FR\")\n",
    "# opt = torch_numopt.ConjugateGradient(model=model, lr=10, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"goldstein\", cg_method=\"FR\")\n",
    "\n",
    "all_loss = {}\n",
    "for epoch in range(1000):\n",
    "    print(\"epoch: \", epoch, end=\"\")\n",
    "    all_loss[epoch + 1] = 0\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y, loss_fn)\n",
    "\n",
    "        all_loss[epoch + 1] += loss\n",
    "    all_loss[epoch + 1] /= len(data_loader)\n",
    "    print(\", loss: {}\".format(all_loss[epoch + 1].detach().numpy().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics: R2 = 0.7134678142158339\n",
      "Test metrics:  R2 = 0.6677058998053428\n"
     ]
    }
   ],
   "source": [
    "pred_train = model.forward(X_train).detach()\n",
    "pred_test = model.forward(X_test).detach()\n",
    "print(f\"Train metrics: R2 = {r2_score(pred_train, y_train)}\")\n",
    "print(f\"Test metrics:  R2 = {r2_score(pred_test, y_test)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
