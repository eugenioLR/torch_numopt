{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch_numopt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import *\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.f1 = nn.Linear(input_size, 10, device=device)\n",
    "        self.f2 = nn.Linear(10, 20, device=device)\n",
    "        self.f3 = nn.Linear(20, 20, device=device)\n",
    "        self.f4 = nn.Linear(20, 10, device=device)\n",
    "        self.f5 = nn.Linear(10, 1, device=device)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        # self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.f1(x))\n",
    "        x = self.activation(self.f2(x))\n",
    "        x = self.activation(self.f3(x))\n",
    "        x = self.activation(self.f4(x))\n",
    "        x = self.f5(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([800, 10]) torch.Size([800, 1])\n",
      "torch.Size([200, 10]) torch.Size([200, 1])\n"
     ]
    }
   ],
   "source": [
    "# X, y = load_diabetes(return_X_y=True, scaled=False)\n",
    "# X, y = make_regression(n_samples=1000, n_features=100)\n",
    "X, y = make_friedman1(n_samples=1000, noise=1e-2)\n",
    "\n",
    "X_scaler = MinMaxScaler()\n",
    "X = X_scaler.fit_transform(X)\n",
    "X = torch.Tensor(X).to(device)\n",
    "\n",
    "y_scaler = MinMaxScaler()\n",
    "y = y_scaler.fit_transform(y.reshape((-1, 1)))\n",
    "y = torch.Tensor(y).to(device)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "torch_data = TensorDataset(X_train, y_train)\n",
    "data_loader = DataLoader(torch_data, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.12925447523593903\n",
      "epoch:  1, loss: 0.08665452897548676\n",
      "epoch:  2, loss: 0.06314680725336075\n",
      "epoch:  3, loss: 0.05012567341327667\n",
      "epoch:  4, loss: 0.04297208786010742\n",
      "epoch:  5, loss: 0.039122775197029114\n",
      "epoch:  6, loss: 0.03706199675798416\n",
      "epoch:  7, loss: 0.03596305474638939\n",
      "epoch:  8, loss: 0.03537771478295326\n",
      "epoch:  9, loss: 0.03506619483232498\n",
      "epoch:  10, loss: 0.0349002443253994\n",
      "epoch:  11, loss: 0.03481151908636093\n",
      "epoch:  12, loss: 0.034763749688863754\n",
      "epoch:  13, loss: 0.034737709909677505\n",
      "epoch:  14, loss: 0.0347231887280941\n",
      "epoch:  15, loss: 0.03471479192376137\n",
      "epoch:  16, loss: 0.03470965474843979\n",
      "epoch:  17, loss: 0.0347028449177742\n",
      "epoch:  18, loss: 0.03469572961330414\n",
      "epoch:  19, loss: 0.034695591777563095\n",
      "epoch:  20, loss: 0.03468509763479233\n",
      "epoch:  21, loss: 0.034678954631090164\n",
      "epoch:  22, loss: 0.03467718884348869\n",
      "epoch:  23, loss: 0.03466811403632164\n",
      "epoch:  24, loss: 0.03466273099184036\n",
      "epoch:  25, loss: 0.034659452736377716\n",
      "epoch:  26, loss: 0.03465160354971886\n",
      "epoch:  27, loss: 0.03464687243103981\n",
      "epoch:  28, loss: 0.03464222699403763\n",
      "epoch:  29, loss: 0.034635402262210846\n",
      "epoch:  30, loss: 0.03463123366236687\n",
      "epoch:  31, loss: 0.03462543338537216\n",
      "epoch:  32, loss: 0.03461954742670059\n",
      "epoch:  33, loss: 0.0346178337931633\n",
      "epoch:  34, loss: 0.0346091203391552\n",
      "epoch:  35, loss: 0.03460397571325302\n",
      "epoch:  36, loss: 0.03460069000720978\n",
      "epoch:  37, loss: 0.03459319472312927\n",
      "epoch:  38, loss: 0.03458867594599724\n",
      "epoch:  39, loss: 0.03458407148718834\n",
      "epoch:  40, loss: 0.03457752987742424\n",
      "epoch:  41, loss: 0.0345735177397728\n",
      "epoch:  42, loss: 0.03456781432032585\n",
      "epoch:  43, loss: 0.03456207737326622\n",
      "epoch:  44, loss: 0.034560248255729675\n",
      "epoch:  45, loss: 0.034551747143268585\n",
      "epoch:  46, loss: 0.03454670310020447\n",
      "epoch:  47, loss: 0.03454332426190376\n",
      "epoch:  48, loss: 0.03453590348362923\n",
      "epoch:  49, loss: 0.03453142195940018\n",
      "epoch:  50, loss: 0.03452667221426964\n",
      "epoch:  51, loss: 0.03452013432979584\n",
      "epoch:  52, loss: 0.03451996669173241\n",
      "epoch:  53, loss: 0.03451009839773178\n",
      "epoch:  54, loss: 0.034504324197769165\n",
      "epoch:  55, loss: 0.03450220078229904\n",
      "epoch:  56, loss: 0.03449355065822601\n",
      "epoch:  57, loss: 0.03448840603232384\n",
      "epoch:  58, loss: 0.034484680742025375\n",
      "epoch:  59, loss: 0.03447706624865532\n",
      "epoch:  60, loss: 0.03447245433926582\n",
      "epoch:  61, loss: 0.03446713835000992\n",
      "epoch:  62, loss: 0.03446037694811821\n",
      "epoch:  63, loss: 0.034459762275218964\n",
      "epoch:  64, loss: 0.03444954380393028\n",
      "epoch:  65, loss: 0.03444354981184006\n",
      "epoch:  66, loss: 0.034440778195858\n",
      "epoch:  67, loss: 0.03443175554275513\n",
      "epoch:  68, loss: 0.03442639857530594\n",
      "epoch:  69, loss: 0.034421734511852264\n",
      "epoch:  70, loss: 0.03441382199525833\n",
      "epoch:  71, loss: 0.03440900892019272\n",
      "epoch:  72, loss: 0.03440272808074951\n",
      "epoch:  73, loss: 0.03439565747976303\n",
      "epoch:  74, loss: 0.03439434990286827\n",
      "epoch:  75, loss: 0.034383587539196014\n",
      "epoch:  76, loss: 0.03437725454568863\n",
      "epoch:  77, loss: 0.03437361121177673\n",
      "epoch:  78, loss: 0.03436402603983879\n",
      "epoch:  79, loss: 0.03435830399394035\n",
      "epoch:  80, loss: 0.03435255587100983\n",
      "epoch:  81, loss: 0.03434404730796814\n",
      "epoch:  82, loss: 0.03433884680271149\n",
      "epoch:  83, loss: 0.03433123975992203\n",
      "epoch:  84, loss: 0.03432357311248779\n",
      "epoch:  85, loss: 0.03432122617959976\n",
      "epoch:  86, loss: 0.0343095138669014\n",
      "epoch:  87, loss: 0.03430260345339775\n",
      "epoch:  88, loss: 0.03429778665304184\n",
      "epoch:  89, loss: 0.03428727015852928\n",
      "epoch:  90, loss: 0.03428097069263458\n",
      "epoch:  91, loss: 0.034273888915777206\n",
      "epoch:  92, loss: 0.0342644527554512\n",
      "epoch:  93, loss: 0.03426409512758255\n",
      "epoch:  94, loss: 0.03424932062625885\n",
      "epoch:  95, loss: 0.03424074500799179\n",
      "epoch:  96, loss: 0.03423723578453064\n",
      "epoch:  97, loss: 0.03422402963042259\n",
      "epoch:  98, loss: 0.03421621397137642\n",
      "epoch:  99, loss: 0.03420990705490112\n",
      "epoch:  100, loss: 0.03419792652130127\n",
      "epoch:  101, loss: 0.03419072553515434\n",
      "epoch:  102, loss: 0.03418184444308281\n",
      "epoch:  103, loss: 0.03417098522186279\n",
      "epoch:  104, loss: 0.03417011350393295\n",
      "epoch:  105, loss: 0.03415292128920555\n",
      "epoch:  106, loss: 0.034142978489398956\n",
      "epoch:  107, loss: 0.034138668328523636\n",
      "epoch:  108, loss: 0.03412315621972084\n",
      "epoch:  109, loss: 0.03411399945616722\n",
      "epoch:  110, loss: 0.034106455743312836\n",
      "epoch:  111, loss: 0.03409223631024361\n",
      "epoch:  112, loss: 0.03408370167016983\n",
      "epoch:  113, loss: 0.03407306969165802\n",
      "epoch:  114, loss: 0.03406001999974251\n",
      "epoch:  115, loss: 0.034059371799230576\n",
      "epoch:  116, loss: 0.03403836116194725\n",
      "epoch:  117, loss: 0.034026265144348145\n",
      "epoch:  118, loss: 0.034021563827991486\n",
      "epoch:  119, loss: 0.034002311527729034\n",
      "epoch:  120, loss: 0.033991020172834396\n",
      "epoch:  121, loss: 0.03398238494992256\n",
      "epoch:  122, loss: 0.03396452218294144\n",
      "epoch:  123, loss: 0.03395388275384903\n",
      "epoch:  124, loss: 0.03394145891070366\n",
      "epoch:  125, loss: 0.033924855291843414\n",
      "epoch:  126, loss: 0.03391476720571518\n",
      "epoch:  127, loss: 0.03389861807227135\n",
      "epoch:  128, loss: 0.033882975578308105\n",
      "epoch:  129, loss: 0.033879078924655914\n",
      "epoch:  130, loss: 0.033853575587272644\n",
      "epoch:  131, loss: 0.03383878991007805\n",
      "epoch:  132, loss: 0.03382999822497368\n",
      "epoch:  133, loss: 0.03380604833364487\n",
      "epoch:  134, loss: 0.03379198536276817\n",
      "epoch:  135, loss: 0.03377848491072655\n",
      "epoch:  136, loss: 0.033755913376808167\n",
      "epoch:  137, loss: 0.03374240919947624\n",
      "epoch:  138, loss: 0.03372403234243393\n",
      "epoch:  139, loss: 0.03370252996683121\n",
      "epoch:  140, loss: 0.03368949145078659\n",
      "epoch:  141, loss: 0.033666785806417465\n",
      "epoch:  142, loss: 0.03364625200629234\n",
      "epoch:  143, loss: 0.03364061191678047\n",
      "epoch:  144, loss: 0.03360610827803612\n",
      "epoch:  145, loss: 0.033586326986551285\n",
      "epoch:  146, loss: 0.033574800938367844\n",
      "epoch:  147, loss: 0.03354184702038765\n",
      "epoch:  148, loss: 0.033522654324769974\n",
      "epoch:  149, loss: 0.033504996448755264\n",
      "epoch:  150, loss: 0.03347310051321983\n",
      "epoch:  151, loss: 0.0334542877972126\n",
      "epoch:  152, loss: 0.03343046456575394\n",
      "epoch:  153, loss: 0.03339947387576103\n",
      "epoch:  154, loss: 0.033380892127752304\n",
      "epoch:  155, loss: 0.033350877463817596\n",
      "epoch:  156, loss: 0.033320482820272446\n",
      "epoch:  157, loss: 0.033319100737571716\n",
      "epoch:  158, loss: 0.03326587751507759\n",
      "epoch:  159, loss: 0.03323594108223915\n",
      "epoch:  160, loss: 0.033226627856492996\n",
      "epoch:  161, loss: 0.03317413106560707\n",
      "epoch:  162, loss: 0.033144447952508926\n",
      "epoch:  163, loss: 0.033127717673778534\n",
      "epoch:  164, loss: 0.033075857907533646\n",
      "epoch:  165, loss: 0.03304615244269371\n",
      "epoch:  166, loss: 0.03302151709794998\n",
      "epoch:  167, loss: 0.03296976163983345\n",
      "epoch:  168, loss: 0.03293992951512337\n",
      "epoch:  169, loss: 0.03290839120745659\n",
      "epoch:  170, loss: 0.032856471836566925\n",
      "epoch:  171, loss: 0.032826248556375504\n",
      "epoch:  172, loss: 0.03278735280036926\n",
      "epoch:  173, loss: 0.032733939588069916\n",
      "epoch:  174, loss: 0.03270301595330238\n",
      "epoch:  175, loss: 0.03265678882598877\n",
      "epoch:  176, loss: 0.03260258585214615\n",
      "epoch:  177, loss: 0.03257076069712639\n",
      "epoch:  178, loss: 0.032518621534109116\n",
      "epoch:  179, loss: 0.03246169909834862\n",
      "epoch:  180, loss: 0.03242889419198036\n",
      "epoch:  181, loss: 0.03236809000372887\n",
      "epoch:  182, loss: 0.03231033310294151\n",
      "epoch:  183, loss: 0.032275717705488205\n",
      "epoch:  184, loss: 0.03221258148550987\n",
      "epoch:  185, loss: 0.03214745596051216\n",
      "epoch:  186, loss: 0.032110001891851425\n",
      "epoch:  187, loss: 0.03204111009836197\n",
      "epoch:  188, loss: 0.03197291120886803\n",
      "epoch:  189, loss: 0.03193255513906479\n",
      "epoch:  190, loss: 0.031864527612924576\n",
      "epoch:  191, loss: 0.0317852683365345\n",
      "epoch:  192, loss: 0.031740423291921616\n",
      "epoch:  193, loss: 0.03166968747973442\n",
      "epoch:  194, loss: 0.031583528965711594\n",
      "epoch:  195, loss: 0.031534045934677124\n",
      "epoch:  196, loss: 0.03146252781152725\n",
      "epoch:  197, loss: 0.03136136010289192\n",
      "epoch:  198, loss: 0.03130695968866348\n",
      "epoch:  199, loss: 0.03124002367258072\n",
      "epoch:  200, loss: 0.03112749010324478\n",
      "epoch:  201, loss: 0.031065134331583977\n",
      "epoch:  202, loss: 0.03100261092185974\n",
      "epoch:  203, loss: 0.030867144465446472\n",
      "epoch:  204, loss: 0.030797643586993217\n",
      "epoch:  205, loss: 0.03074127435684204\n",
      "epoch:  206, loss: 0.030591769143939018\n",
      "epoch:  207, loss: 0.030513040721416473\n",
      "epoch:  208, loss: 0.030480725690722466\n",
      "epoch:  209, loss: 0.03029719367623329\n",
      "epoch:  210, loss: 0.03020709753036499\n",
      "epoch:  211, loss: 0.03019315004348755\n",
      "epoch:  212, loss: 0.02998771145939827\n",
      "epoch:  213, loss: 0.02988601103425026\n",
      "epoch:  214, loss: 0.029827246442437172\n",
      "epoch:  215, loss: 0.02966492623090744\n",
      "epoch:  216, loss: 0.029542645439505577\n",
      "epoch:  217, loss: 0.029476098716259003\n",
      "epoch:  218, loss: 0.029331186786293983\n",
      "epoch:  219, loss: 0.02918374165892601\n",
      "epoch:  220, loss: 0.029106182977557182\n",
      "epoch:  221, loss: 0.028978513553738594\n",
      "epoch:  222, loss: 0.028800195083022118\n",
      "epoch:  223, loss: 0.028711868450045586\n",
      "epoch:  224, loss: 0.028612283989787102\n",
      "epoch:  225, loss: 0.028398780152201653\n",
      "epoch:  226, loss: 0.028296904638409615\n",
      "epoch:  227, loss: 0.028225872665643692\n",
      "epoch:  228, loss: 0.027969174087047577\n",
      "epoch:  229, loss: 0.027853868901729584\n",
      "epoch:  230, loss: 0.027829036116600037\n",
      "epoch:  231, loss: 0.027526943013072014\n",
      "epoch:  232, loss: 0.027397045865654945\n",
      "epoch:  233, loss: 0.027325130999088287\n",
      "epoch:  234, loss: 0.02708289586007595\n",
      "epoch:  235, loss: 0.026919979602098465\n",
      "epoch:  236, loss: 0.026837408542633057\n",
      "epoch:  237, loss: 0.026608863845467567\n",
      "epoch:  238, loss: 0.026419343426823616\n",
      "epoch:  239, loss: 0.0263280738145113\n",
      "epoch:  240, loss: 0.0261297095566988\n",
      "epoch:  241, loss: 0.025906529277563095\n",
      "epoch:  242, loss: 0.025806261226534843\n",
      "epoch:  243, loss: 0.02565179020166397\n",
      "epoch:  244, loss: 0.0253825131803751\n",
      "epoch:  245, loss: 0.025269977748394012\n",
      "epoch:  246, loss: 0.02512477897107601\n",
      "epoch:  247, loss: 0.02483465149998665\n",
      "epoch:  248, loss: 0.0247174259275198\n",
      "epoch:  249, loss: 0.024599412456154823\n",
      "epoch:  250, loss: 0.024269551038742065\n",
      "epoch:  251, loss: 0.024146951735019684\n",
      "epoch:  252, loss: 0.024040846154093742\n",
      "epoch:  253, loss: 0.023690786212682724\n",
      "epoch:  254, loss: 0.023565420880913734\n",
      "epoch:  255, loss: 0.023476531729102135\n",
      "epoch:  256, loss: 0.023100895807147026\n",
      "epoch:  257, loss: 0.022975988686084747\n",
      "epoch:  258, loss: 0.02285338193178177\n",
      "epoch:  259, loss: 0.02249799855053425\n",
      "epoch:  260, loss: 0.022379398345947266\n",
      "epoch:  261, loss: 0.02223142609000206\n",
      "epoch:  262, loss: 0.021886974573135376\n",
      "epoch:  263, loss: 0.021778959780931473\n",
      "epoch:  264, loss: 0.021567868068814278\n",
      "epoch:  265, loss: 0.021275322884321213\n",
      "epoch:  266, loss: 0.021179044619202614\n",
      "epoch:  267, loss: 0.020916642621159554\n",
      "epoch:  268, loss: 0.020666155964136124\n",
      "epoch:  269, loss: 0.02058304287493229\n",
      "epoch:  270, loss: 0.020254403352737427\n",
      "epoch:  271, loss: 0.020062236115336418\n",
      "epoch:  272, loss: 0.01999097689986229\n",
      "epoch:  273, loss: 0.01961277611553669\n",
      "epoch:  274, loss: 0.019465727731585503\n",
      "epoch:  275, loss: 0.019394321367144585\n",
      "epoch:  276, loss: 0.01898164115846157\n",
      "epoch:  277, loss: 0.018883563578128815\n",
      "epoch:  278, loss: 0.01866222359240055\n",
      "epoch:  279, loss: 0.01838255114853382\n",
      "epoch:  280, loss: 0.018309438601136208\n",
      "epoch:  281, loss: 0.017918763682246208\n",
      "epoch:  282, loss: 0.01780121959745884\n",
      "epoch:  283, loss: 0.0176288690418005\n",
      "epoch:  284, loss: 0.017315899953246117\n",
      "epoch:  285, loss: 0.01724514178931713\n",
      "epoch:  286, loss: 0.01684998907148838\n",
      "epoch:  287, loss: 0.016758481040596962\n",
      "epoch:  288, loss: 0.01649620197713375\n",
      "epoch:  289, loss: 0.016294045373797417\n",
      "epoch:  290, loss: 0.016239311546087265\n",
      "epoch:  291, loss: 0.015868594869971275\n",
      "epoch:  292, loss: 0.01578325219452381\n",
      "epoch:  293, loss: 0.015542268753051758\n",
      "epoch:  294, loss: 0.015340658836066723\n",
      "epoch:  295, loss: 0.015292100608348846\n",
      "epoch:  296, loss: 0.014422677457332611\n",
      "epoch:  297, loss: 0.011795871891081333\n",
      "epoch:  298, loss: 0.011726008728146553\n",
      "epoch:  299, loss: 0.011090089567005634\n",
      "epoch:  300, loss: 0.009669674560427666\n",
      "epoch:  301, loss: 0.009416073560714722\n",
      "epoch:  302, loss: 0.008851293474435806\n",
      "epoch:  303, loss: 0.00880415365099907\n",
      "epoch:  304, loss: 0.008600659668445587\n",
      "epoch:  305, loss: 0.008539478294551373\n",
      "epoch:  306, loss: 0.00851074606180191\n",
      "epoch:  307, loss: 0.008387355133891106\n",
      "epoch:  308, loss: 0.008375207893550396\n",
      "epoch:  309, loss: 0.008287772536277771\n",
      "epoch:  310, loss: 0.008278999477624893\n",
      "epoch:  311, loss: 0.008217038586735725\n",
      "epoch:  312, loss: 0.00820744689553976\n",
      "epoch:  313, loss: 0.008162876591086388\n",
      "epoch:  314, loss: 0.008145086467266083\n",
      "epoch:  315, loss: 0.00813700258731842\n",
      "epoch:  316, loss: 0.008088456466794014\n",
      "epoch:  317, loss: 0.00808202289044857\n",
      "epoch:  318, loss: 0.008039325475692749\n",
      "epoch:  319, loss: 0.008028828538954258\n",
      "epoch:  320, loss: 0.008006957359611988\n",
      "epoch:  321, loss: 0.007978345267474651\n",
      "epoch:  322, loss: 0.007973168045282364\n",
      "epoch:  323, loss: 0.00793322455137968\n",
      "epoch:  324, loss: 0.007925689220428467\n",
      "epoch:  325, loss: 0.007903619669377804\n",
      "epoch:  326, loss: 0.007881825789809227\n",
      "epoch:  327, loss: 0.007876868359744549\n",
      "epoch:  328, loss: 0.007841992191970348\n",
      "epoch:  329, loss: 0.007835034281015396\n",
      "epoch:  330, loss: 0.0078096711076796055\n",
      "epoch:  331, loss: 0.007795471232384443\n",
      "epoch:  332, loss: 0.007791609037667513\n",
      "epoch:  333, loss: 0.007762680295854807\n",
      "epoch:  334, loss: 0.007755328435450792\n",
      "epoch:  335, loss: 0.007751897908747196\n",
      "epoch:  336, loss: 0.007687690667808056\n",
      "epoch:  337, loss: 0.00749322259798646\n",
      "epoch:  338, loss: 0.007478427607566118\n",
      "epoch:  339, loss: 0.007475815713405609\n",
      "epoch:  340, loss: 0.007469491567462683\n",
      "epoch:  341, loss: 0.007456513587385416\n",
      "epoch:  342, loss: 0.0074538057669997215\n",
      "epoch:  343, loss: 0.007448929827660322\n",
      "epoch:  344, loss: 0.007435131352394819\n",
      "epoch:  345, loss: 0.00743236904963851\n",
      "epoch:  346, loss: 0.007429483812302351\n",
      "epoch:  347, loss: 0.007414158433675766\n",
      "epoch:  348, loss: 0.007411065045744181\n",
      "epoch:  349, loss: 0.007409169338643551\n",
      "epoch:  350, loss: 0.007399743422865868\n",
      "epoch:  351, loss: 0.007391455117613077\n",
      "epoch:  352, loss: 0.007389097474515438\n",
      "epoch:  353, loss: 0.007383860182017088\n",
      "epoch:  354, loss: 0.007372373249381781\n",
      "epoch:  355, loss: 0.007369885221123695\n",
      "epoch:  356, loss: 0.007369155064225197\n",
      "epoch:  357, loss: 0.007354358211159706\n",
      "epoch:  358, loss: 0.007351321633905172\n",
      "epoch:  359, loss: 0.007349617313593626\n",
      "epoch:  360, loss: 0.007336277049034834\n",
      "epoch:  361, loss: 0.007332928013056517\n",
      "epoch:  362, loss: 0.007331169676035643\n",
      "epoch:  363, loss: 0.007319499272853136\n",
      "epoch:  364, loss: 0.007315103895962238\n",
      "epoch:  365, loss: 0.007313296664506197\n",
      "epoch:  366, loss: 0.007306672167032957\n",
      "epoch:  367, loss: 0.007298970129340887\n",
      "epoch:  368, loss: 0.0072966585867106915\n",
      "epoch:  369, loss: 0.00729501573368907\n",
      "epoch:  370, loss: 0.007283099927008152\n",
      "epoch:  371, loss: 0.007280707824975252\n",
      "epoch:  372, loss: 0.007279267068952322\n",
      "epoch:  373, loss: 0.007267622742801905\n",
      "epoch:  374, loss: 0.007264778017997742\n",
      "epoch:  375, loss: 0.007263294421136379\n",
      "epoch:  376, loss: 0.007253572810441256\n",
      "epoch:  377, loss: 0.007249483838677406\n",
      "epoch:  378, loss: 0.007247671950608492\n",
      "epoch:  379, loss: 0.007245051208883524\n",
      "epoch:  380, loss: 0.007234188262373209\n",
      "epoch:  381, loss: 0.007231883239001036\n",
      "epoch:  382, loss: 0.00723041407763958\n",
      "epoch:  383, loss: 0.007220691069960594\n",
      "epoch:  384, loss: 0.007215879857540131\n",
      "epoch:  385, loss: 0.007214145269244909\n",
      "epoch:  386, loss: 0.007204981986433268\n",
      "epoch:  387, loss: 0.007199520710855722\n",
      "epoch:  388, loss: 0.007197646889835596\n",
      "epoch:  389, loss: 0.007196230813860893\n",
      "epoch:  390, loss: 0.0071851336397230625\n",
      "epoch:  391, loss: 0.007181938737630844\n",
      "epoch:  392, loss: 0.007180375512689352\n",
      "epoch:  393, loss: 0.007171429228037596\n",
      "epoch:  394, loss: 0.007166149560362101\n",
      "epoch:  395, loss: 0.007164356764405966\n",
      "epoch:  396, loss: 0.007159871980547905\n",
      "epoch:  397, loss: 0.007151259575039148\n",
      "epoch:  398, loss: 0.0071487328968942165\n",
      "epoch:  399, loss: 0.00714731914922595\n",
      "epoch:  400, loss: 0.007137620355933905\n",
      "epoch:  401, loss: 0.007133686915040016\n",
      "epoch:  402, loss: 0.0071321213617920876\n",
      "epoch:  403, loss: 0.007126254960894585\n",
      "epoch:  404, loss: 0.007119146641343832\n",
      "epoch:  405, loss: 0.007117357570677996\n",
      "epoch:  406, loss: 0.00711384741589427\n",
      "epoch:  407, loss: 0.007104375399649143\n",
      "epoch:  408, loss: 0.007102242670953274\n",
      "epoch:  409, loss: 0.0071007804945111275\n",
      "epoch:  410, loss: 0.0070897373370826244\n",
      "epoch:  411, loss: 0.007086431607604027\n",
      "epoch:  412, loss: 0.007084909360855818\n",
      "epoch:  413, loss: 0.007082061842083931\n",
      "epoch:  414, loss: 0.0070720817893743515\n",
      "epoch:  415, loss: 0.007069732528179884\n",
      "epoch:  416, loss: 0.0070683774538338184\n",
      "epoch:  417, loss: 0.0070587568916380405\n",
      "epoch:  418, loss: 0.007055304013192654\n",
      "epoch:  419, loss: 0.007053837645798922\n",
      "epoch:  420, loss: 0.007052533328533173\n",
      "epoch:  421, loss: 0.00704191904515028\n",
      "epoch:  422, loss: 0.007039215881377459\n",
      "epoch:  423, loss: 0.007037606090307236\n",
      "epoch:  424, loss: 0.007029015105217695\n",
      "epoch:  425, loss: 0.007024106103926897\n",
      "epoch:  426, loss: 0.007022257894277573\n",
      "epoch:  427, loss: 0.00702165812253952\n",
      "epoch:  428, loss: 0.007010436616837978\n",
      "epoch:  429, loss: 0.007008026819676161\n",
      "epoch:  430, loss: 0.007006601430475712\n",
      "epoch:  431, loss: 0.007000606041401625\n",
      "epoch:  432, loss: 0.006994978524744511\n",
      "epoch:  433, loss: 0.006993222516030073\n",
      "epoch:  434, loss: 0.006991946138441563\n",
      "epoch:  435, loss: 0.006987480912357569\n",
      "epoch:  436, loss: 0.0069806333631277084\n",
      "epoch:  437, loss: 0.006978767924010754\n",
      "epoch:  438, loss: 0.006977498531341553\n",
      "epoch:  439, loss: 0.006969384849071503\n",
      "epoch:  440, loss: 0.006965481676161289\n",
      "epoch:  441, loss: 0.00696377782151103\n",
      "epoch:  442, loss: 0.006962461397051811\n",
      "epoch:  443, loss: 0.006958644837141037\n",
      "epoch:  444, loss: 0.006950393784791231\n",
      "epoch:  445, loss: 0.006948051508516073\n",
      "epoch:  446, loss: 0.006946566980332136\n",
      "epoch:  447, loss: 0.006945323199033737\n",
      "epoch:  448, loss: 0.006938098464161158\n",
      "epoch:  449, loss: 0.0069328900426626205\n",
      "epoch:  450, loss: 0.006931018549948931\n",
      "epoch:  451, loss: 0.006929612718522549\n",
      "epoch:  452, loss: 0.006925093941390514\n",
      "epoch:  453, loss: 0.006917788181453943\n",
      "epoch:  454, loss: 0.006915675941854715\n",
      "epoch:  455, loss: 0.0069143506698310375\n",
      "epoch:  456, loss: 0.006911193020641804\n",
      "epoch:  457, loss: 0.006903709843754768\n",
      "epoch:  458, loss: 0.006901587825268507\n",
      "epoch:  459, loss: 0.006900306791067123\n",
      "epoch:  460, loss: 0.0068929800763726234\n",
      "epoch:  461, loss: 0.006888565141707659\n",
      "epoch:  462, loss: 0.0068868291564285755\n",
      "epoch:  463, loss: 0.006885532755404711\n",
      "epoch:  464, loss: 0.006878998130559921\n",
      "epoch:  465, loss: 0.006874321028590202\n",
      "epoch:  466, loss: 0.006872590631246567\n",
      "epoch:  467, loss: 0.006871397141367197\n",
      "epoch:  468, loss: 0.006863852497190237\n",
      "epoch:  469, loss: 0.006859879475086927\n",
      "epoch:  470, loss: 0.006858198903501034\n",
      "epoch:  471, loss: 0.006856902502477169\n",
      "epoch:  472, loss: 0.00685383565723896\n",
      "epoch:  473, loss: 0.006845634896308184\n",
      "epoch:  474, loss: 0.0068434118293225765\n",
      "epoch:  475, loss: 0.006842011120170355\n",
      "epoch:  476, loss: 0.006835825275629759\n",
      "epoch:  477, loss: 0.006830368656665087\n",
      "epoch:  478, loss: 0.0068285102024674416\n",
      "epoch:  479, loss: 0.006827217061072588\n",
      "epoch:  480, loss: 0.006825501099228859\n",
      "epoch:  481, loss: 0.006816441658884287\n",
      "epoch:  482, loss: 0.006813856773078442\n",
      "epoch:  483, loss: 0.006812383886426687\n",
      "epoch:  484, loss: 0.006810618564486504\n",
      "epoch:  485, loss: 0.0068017542362213135\n",
      "epoch:  486, loss: 0.006799235939979553\n",
      "epoch:  487, loss: 0.006797961890697479\n",
      "epoch:  488, loss: 0.006796830799430609\n",
      "epoch:  489, loss: 0.006792242173105478\n",
      "epoch:  490, loss: 0.006786376237869263\n",
      "epoch:  491, loss: 0.006784556899219751\n",
      "epoch:  492, loss: 0.0067833056673407555\n",
      "epoch:  493, loss: 0.006775164045393467\n",
      "epoch:  494, loss: 0.006772060412913561\n",
      "epoch:  495, loss: 0.00677054887637496\n",
      "epoch:  496, loss: 0.006770505104213953\n",
      "epoch:  497, loss: 0.0067603555507957935\n",
      "epoch:  498, loss: 0.006757827475667\n",
      "epoch:  499, loss: 0.0067564635537564754\n",
      "epoch:  500, loss: 0.0067552742548286915\n",
      "epoch:  501, loss: 0.006749963387846947\n",
      "epoch:  502, loss: 0.00674444530159235\n",
      "epoch:  503, loss: 0.0067424774169921875\n",
      "epoch:  504, loss: 0.0067411987110972404\n",
      "epoch:  505, loss: 0.006738240364938974\n",
      "epoch:  506, loss: 0.006730490364134312\n",
      "epoch:  507, loss: 0.006728300824761391\n",
      "epoch:  508, loss: 0.006726938299834728\n",
      "epoch:  509, loss: 0.006724969483911991\n",
      "epoch:  510, loss: 0.006716655567288399\n",
      "epoch:  511, loss: 0.0067143128253519535\n",
      "epoch:  512, loss: 0.0067128813825547695\n",
      "epoch:  513, loss: 0.006711733061820269\n",
      "epoch:  514, loss: 0.00670288922265172\n",
      "epoch:  515, loss: 0.006700427271425724\n",
      "epoch:  516, loss: 0.006698982790112495\n",
      "epoch:  517, loss: 0.006697856821119785\n",
      "epoch:  518, loss: 0.006688927765935659\n",
      "epoch:  519, loss: 0.0066863857209682465\n",
      "epoch:  520, loss: 0.006684856954962015\n",
      "epoch:  521, loss: 0.00668364018201828\n",
      "epoch:  522, loss: 0.006679077632725239\n",
      "epoch:  523, loss: 0.006672422867268324\n",
      "epoch:  524, loss: 0.006670187693089247\n",
      "epoch:  525, loss: 0.006668833550065756\n",
      "epoch:  526, loss: 0.006667666137218475\n",
      "epoch:  527, loss: 0.006658630445599556\n",
      "epoch:  528, loss: 0.006655906327068806\n",
      "epoch:  529, loss: 0.006654508411884308\n",
      "epoch:  530, loss: 0.006653272546827793\n",
      "epoch:  531, loss: 0.006649031303822994\n",
      "epoch:  532, loss: 0.006642340216785669\n",
      "epoch:  533, loss: 0.006640123203396797\n",
      "epoch:  534, loss: 0.006638763006776571\n",
      "epoch:  535, loss: 0.006637566722929478\n",
      "epoch:  536, loss: 0.006630366202443838\n",
      "epoch:  537, loss: 0.00662605743855238\n",
      "epoch:  538, loss: 0.006624299101531506\n",
      "epoch:  539, loss: 0.0066229915246367455\n",
      "epoch:  540, loss: 0.006621529348194599\n",
      "epoch:  541, loss: 0.006612092722207308\n",
      "epoch:  542, loss: 0.006609429605305195\n",
      "epoch:  543, loss: 0.006607925519347191\n",
      "epoch:  544, loss: 0.00660668732598424\n",
      "epoch:  545, loss: 0.006601087283343077\n",
      "epoch:  546, loss: 0.006595011334866285\n",
      "epoch:  547, loss: 0.006592859979718924\n",
      "epoch:  548, loss: 0.006591456942260265\n",
      "epoch:  549, loss: 0.006582604721188545\n",
      "epoch:  550, loss: 0.006578543223440647\n",
      "epoch:  551, loss: 0.006576779764145613\n",
      "epoch:  552, loss: 0.006575419567525387\n",
      "epoch:  553, loss: 0.00656849704682827\n",
      "epoch:  554, loss: 0.006562838796526194\n",
      "epoch:  555, loss: 0.0065606688149273396\n",
      "epoch:  556, loss: 0.0065591600723564625\n",
      "epoch:  557, loss: 0.00655656261369586\n",
      "epoch:  558, loss: 0.006547821685671806\n",
      "epoch:  559, loss: 0.006544957403093576\n",
      "epoch:  560, loss: 0.006543326657265425\n",
      "epoch:  561, loss: 0.006541995797306299\n",
      "epoch:  562, loss: 0.006533223204314709\n",
      "epoch:  563, loss: 0.006529391277581453\n",
      "epoch:  564, loss: 0.006527569144964218\n",
      "epoch:  565, loss: 0.0065261791460216045\n",
      "epoch:  566, loss: 0.006523673888295889\n",
      "epoch:  567, loss: 0.006514781154692173\n",
      "epoch:  568, loss: 0.006511727347970009\n",
      "epoch:  569, loss: 0.006510027684271336\n",
      "epoch:  570, loss: 0.006508599501103163\n",
      "epoch:  571, loss: 0.006502220407128334\n",
      "epoch:  572, loss: 0.006495374254882336\n",
      "epoch:  573, loss: 0.006493059918284416\n",
      "epoch:  574, loss: 0.006491490639746189\n",
      "epoch:  575, loss: 0.006490480620414019\n",
      "epoch:  576, loss: 0.006479853764176369\n",
      "epoch:  577, loss: 0.00647656200453639\n",
      "epoch:  578, loss: 0.006474797613918781\n",
      "epoch:  579, loss: 0.006473367102444172\n",
      "epoch:  580, loss: 0.006465146318078041\n",
      "epoch:  581, loss: 0.006460057571530342\n",
      "epoch:  582, loss: 0.006457749288529158\n",
      "epoch:  583, loss: 0.006456196308135986\n",
      "epoch:  584, loss: 0.006453059613704681\n",
      "epoch:  585, loss: 0.006443648133426905\n",
      "epoch:  586, loss: 0.006440599914640188\n",
      "epoch:  587, loss: 0.006438845302909613\n",
      "epoch:  588, loss: 0.006437413394451141\n",
      "epoch:  589, loss: 0.006427735090255737\n",
      "epoch:  590, loss: 0.006423600949347019\n",
      "epoch:  591, loss: 0.00642166193574667\n",
      "epoch:  592, loss: 0.006420141085982323\n",
      "epoch:  593, loss: 0.0064164274372160435\n",
      "epoch:  594, loss: 0.006407905835658312\n",
      "epoch:  595, loss: 0.006405020132660866\n",
      "epoch:  596, loss: 0.006403262261301279\n",
      "epoch:  597, loss: 0.0064018419943749905\n",
      "epoch:  598, loss: 0.0063964491710066795\n",
      "epoch:  599, loss: 0.006389734800904989\n",
      "epoch:  600, loss: 0.006386954803019762\n",
      "epoch:  601, loss: 0.006385344546288252\n",
      "epoch:  602, loss: 0.006383988074958324\n",
      "epoch:  603, loss: 0.00637505017220974\n",
      "epoch:  604, loss: 0.006371031980961561\n",
      "epoch:  605, loss: 0.006369045004248619\n",
      "epoch:  606, loss: 0.006367598194628954\n",
      "epoch:  607, loss: 0.006360294297337532\n",
      "epoch:  608, loss: 0.006354562938213348\n",
      "epoch:  609, loss: 0.006352380383759737\n",
      "epoch:  610, loss: 0.006350877229124308\n",
      "epoch:  611, loss: 0.006349533796310425\n",
      "epoch:  612, loss: 0.006339728832244873\n",
      "epoch:  613, loss: 0.006336357444524765\n",
      "epoch:  614, loss: 0.006334348581731319\n",
      "epoch:  615, loss: 0.0063329231925308704\n",
      "epoch:  616, loss: 0.006326902192085981\n",
      "epoch:  617, loss: 0.006320641376078129\n",
      "epoch:  618, loss: 0.006317857187241316\n",
      "epoch:  619, loss: 0.006316258572041988\n",
      "epoch:  620, loss: 0.00631490396335721\n",
      "epoch:  621, loss: 0.006304935086518526\n",
      "epoch:  622, loss: 0.006301357410848141\n",
      "epoch:  623, loss: 0.006299579981714487\n",
      "epoch:  624, loss: 0.0062981415539979935\n",
      "epoch:  625, loss: 0.006292011588811874\n",
      "epoch:  626, loss: 0.006285401992499828\n",
      "epoch:  627, loss: 0.0062828646041452885\n",
      "epoch:  628, loss: 0.0062812864780426025\n",
      "epoch:  629, loss: 0.006279913242906332\n",
      "epoch:  630, loss: 0.006271128077059984\n",
      "epoch:  631, loss: 0.006266911048442125\n",
      "epoch:  632, loss: 0.006264816503971815\n",
      "epoch:  633, loss: 0.006263348273932934\n",
      "epoch:  634, loss: 0.006262024398893118\n",
      "epoch:  635, loss: 0.006253240164369345\n",
      "epoch:  636, loss: 0.006249250844120979\n",
      "epoch:  637, loss: 0.006247288081794977\n",
      "epoch:  638, loss: 0.0062458692118525505\n",
      "epoch:  639, loss: 0.006244287826120853\n",
      "epoch:  640, loss: 0.006234664935618639\n",
      "epoch:  641, loss: 0.006231583654880524\n",
      "epoch:  642, loss: 0.006229718681424856\n",
      "epoch:  643, loss: 0.006228301208466291\n",
      "epoch:  644, loss: 0.00622462248429656\n",
      "epoch:  645, loss: 0.006216647569090128\n",
      "epoch:  646, loss: 0.006213653367012739\n",
      "epoch:  647, loss: 0.006211920641362667\n",
      "epoch:  648, loss: 0.006210523657500744\n",
      "epoch:  649, loss: 0.0062039694748818874\n",
      "epoch:  650, loss: 0.006197859533131123\n",
      "epoch:  651, loss: 0.006195542868226767\n",
      "epoch:  652, loss: 0.006193952634930611\n",
      "epoch:  653, loss: 0.006192608270794153\n",
      "epoch:  654, loss: 0.0061845481395721436\n",
      "epoch:  655, loss: 0.006179793272167444\n",
      "epoch:  656, loss: 0.006177519913762808\n",
      "epoch:  657, loss: 0.006175956688821316\n",
      "epoch:  658, loss: 0.0061745354905724525\n",
      "epoch:  659, loss: 0.006164309103041887\n",
      "epoch:  660, loss: 0.006160297896713018\n",
      "epoch:  661, loss: 0.0061583444476127625\n",
      "epoch:  662, loss: 0.006156847346574068\n",
      "epoch:  663, loss: 0.006149972323328257\n",
      "epoch:  664, loss: 0.006143472157418728\n",
      "epoch:  665, loss: 0.006141048390418291\n",
      "epoch:  666, loss: 0.006139346864074469\n",
      "epoch:  667, loss: 0.00613789539784193\n",
      "epoch:  668, loss: 0.006131775677204132\n",
      "epoch:  669, loss: 0.006124893203377724\n",
      "epoch:  670, loss: 0.00612193439155817\n",
      "epoch:  671, loss: 0.00612025149166584\n",
      "epoch:  672, loss: 0.006118797697126865\n",
      "epoch:  673, loss: 0.006109422538429499\n",
      "epoch:  674, loss: 0.006104901432991028\n",
      "epoch:  675, loss: 0.006102746818214655\n",
      "epoch:  676, loss: 0.006101178005337715\n",
      "epoch:  677, loss: 0.00609940942376852\n",
      "epoch:  678, loss: 0.0060886722058057785\n",
      "epoch:  679, loss: 0.006085117347538471\n",
      "epoch:  680, loss: 0.0060831098817288876\n",
      "epoch:  681, loss: 0.006081541068851948\n",
      "epoch:  682, loss: 0.006075974088162184\n",
      "epoch:  683, loss: 0.006068440619856119\n",
      "epoch:  684, loss: 0.006065179128199816\n",
      "epoch:  685, loss: 0.006063319277018309\n",
      "epoch:  686, loss: 0.006061795633286238\n",
      "epoch:  687, loss: 0.006054365541785955\n",
      "epoch:  688, loss: 0.006048149429261684\n",
      "epoch:  689, loss: 0.006045542657375336\n",
      "epoch:  690, loss: 0.0060438672080636024\n",
      "epoch:  691, loss: 0.006042398512363434\n",
      "epoch:  692, loss: 0.0060372245498001575\n",
      "epoch:  693, loss: 0.006029716692864895\n",
      "epoch:  694, loss: 0.006026581395417452\n",
      "epoch:  695, loss: 0.0060247075743973255\n",
      "epoch:  696, loss: 0.0060232048854231834\n",
      "epoch:  697, loss: 0.006021786946803331\n",
      "epoch:  698, loss: 0.00601457292214036\n",
      "epoch:  699, loss: 0.006008530501276255\n",
      "epoch:  700, loss: 0.006006041541695595\n",
      "epoch:  701, loss: 0.006004316732287407\n",
      "epoch:  702, loss: 0.006002858281135559\n",
      "epoch:  703, loss: 0.005995195358991623\n",
      "epoch:  704, loss: 0.005989416502416134\n",
      "epoch:  705, loss: 0.005986981093883514\n",
      "epoch:  706, loss: 0.005985339637845755\n",
      "epoch:  707, loss: 0.005983913317322731\n",
      "epoch:  708, loss: 0.005976581946015358\n",
      "epoch:  709, loss: 0.005970798432826996\n",
      "epoch:  710, loss: 0.005968344863504171\n",
      "epoch:  711, loss: 0.005966670345515013\n",
      "epoch:  712, loss: 0.005965246818959713\n",
      "epoch:  713, loss: 0.00595894455909729\n",
      "epoch:  714, loss: 0.005952422972768545\n",
      "epoch:  715, loss: 0.005949828308075666\n",
      "epoch:  716, loss: 0.005948121193796396\n",
      "epoch:  717, loss: 0.005946704652160406\n",
      "epoch:  718, loss: 0.00593873206526041\n",
      "epoch:  719, loss: 0.0059334528632462025\n",
      "epoch:  720, loss: 0.005931014195084572\n",
      "epoch:  721, loss: 0.005929375533014536\n",
      "epoch:  722, loss: 0.005927948746830225\n",
      "epoch:  723, loss: 0.005923495162278414\n",
      "epoch:  724, loss: 0.00591551186516881\n",
      "epoch:  725, loss: 0.00591233978047967\n",
      "epoch:  726, loss: 0.0059105572290718555\n",
      "epoch:  727, loss: 0.005909058731049299\n",
      "epoch:  728, loss: 0.0059015038423240185\n",
      "epoch:  729, loss: 0.005895465612411499\n",
      "epoch:  730, loss: 0.005892568733543158\n",
      "epoch:  731, loss: 0.0058907316997647285\n",
      "epoch:  732, loss: 0.005889162886887789\n",
      "epoch:  733, loss: 0.0058808824978768826\n",
      "epoch:  734, loss: 0.005874376744031906\n",
      "epoch:  735, loss: 0.005871498491615057\n",
      "epoch:  736, loss: 0.00586948124691844\n",
      "epoch:  737, loss: 0.0058677648194134235\n",
      "epoch:  738, loss: 0.005863658152520657\n",
      "epoch:  739, loss: 0.005853898357599974\n",
      "epoch:  740, loss: 0.005849895533174276\n",
      "epoch:  741, loss: 0.005847573280334473\n",
      "epoch:  742, loss: 0.005845817271620035\n",
      "epoch:  743, loss: 0.00584417674690485\n",
      "epoch:  744, loss: 0.00583814550191164\n",
      "epoch:  745, loss: 0.005830115173012018\n",
      "epoch:  746, loss: 0.005826595705002546\n",
      "epoch:  747, loss: 0.005824430380016565\n",
      "epoch:  748, loss: 0.0058227223344147205\n",
      "epoch:  749, loss: 0.005822350736707449\n",
      "epoch:  750, loss: 0.005810123402625322\n",
      "epoch:  751, loss: 0.005806006025522947\n",
      "epoch:  752, loss: 0.005803502164781094\n",
      "epoch:  753, loss: 0.005801759660243988\n",
      "epoch:  754, loss: 0.005800195969641209\n",
      "epoch:  755, loss: 0.005791064817458391\n",
      "epoch:  756, loss: 0.005785377696156502\n",
      "epoch:  757, loss: 0.005782661959528923\n",
      "epoch:  758, loss: 0.005780769512057304\n",
      "epoch:  759, loss: 0.005779116414487362\n",
      "epoch:  760, loss: 0.005775884259492159\n",
      "epoch:  761, loss: 0.0057657198049128056\n",
      "epoch:  762, loss: 0.005761684849858284\n",
      "epoch:  763, loss: 0.005759238265454769\n",
      "epoch:  764, loss: 0.005757381208240986\n",
      "epoch:  765, loss: 0.005755607504397631\n",
      "epoch:  766, loss: 0.005754091776907444\n",
      "epoch:  767, loss: 0.005742826499044895\n",
      "epoch:  768, loss: 0.005737769417464733\n",
      "epoch:  769, loss: 0.005735208746045828\n",
      "epoch:  770, loss: 0.005733160767704248\n",
      "epoch:  771, loss: 0.005731442477554083\n",
      "epoch:  772, loss: 0.005724803078919649\n",
      "epoch:  773, loss: 0.005716604646295309\n",
      "epoch:  774, loss: 0.005712982267141342\n",
      "epoch:  775, loss: 0.005710617173463106\n",
      "epoch:  776, loss: 0.00570874335244298\n",
      "epoch:  777, loss: 0.0057070497423410416\n",
      "epoch:  778, loss: 0.005705380812287331\n",
      "epoch:  779, loss: 0.005694447550922632\n",
      "epoch:  780, loss: 0.005689737852662802\n",
      "epoch:  781, loss: 0.005686895921826363\n",
      "epoch:  782, loss: 0.005684857256710529\n",
      "epoch:  783, loss: 0.005683109629899263\n",
      "epoch:  784, loss: 0.005681406240910292\n",
      "epoch:  785, loss: 0.00567977549508214\n",
      "epoch:  786, loss: 0.005669014994055033\n",
      "epoch:  787, loss: 0.005664458032697439\n",
      "epoch:  788, loss: 0.005661734379827976\n",
      "epoch:  789, loss: 0.005659772083163261\n",
      "epoch:  790, loss: 0.005658113397657871\n",
      "epoch:  791, loss: 0.005656516645103693\n",
      "epoch:  792, loss: 0.0056474171578884125\n",
      "epoch:  793, loss: 0.005641676485538483\n",
      "epoch:  794, loss: 0.005638757720589638\n",
      "epoch:  795, loss: 0.005636780057102442\n",
      "epoch:  796, loss: 0.005635021720081568\n",
      "epoch:  797, loss: 0.005633392371237278\n",
      "epoch:  798, loss: 0.005631800275295973\n",
      "epoch:  799, loss: 0.005626136902719736\n",
      "epoch:  800, loss: 0.005618180613964796\n",
      "epoch:  801, loss: 0.0056144786067306995\n",
      "epoch:  802, loss: 0.0056120590306818485\n",
      "epoch:  803, loss: 0.005610159132629633\n",
      "epoch:  804, loss: 0.005608473904430866\n",
      "epoch:  805, loss: 0.00560559518635273\n",
      "epoch:  806, loss: 0.005595772061496973\n",
      "epoch:  807, loss: 0.005591079592704773\n",
      "epoch:  808, loss: 0.00558845791965723\n",
      "epoch:  809, loss: 0.005586409475654364\n",
      "epoch:  810, loss: 0.005584640894085169\n",
      "epoch:  811, loss: 0.0055829607881605625\n",
      "epoch:  812, loss: 0.005576691590249538\n",
      "epoch:  813, loss: 0.005568979308009148\n",
      "epoch:  814, loss: 0.005564958788454533\n",
      "epoch:  815, loss: 0.005562546197324991\n",
      "epoch:  816, loss: 0.005560576915740967\n",
      "epoch:  817, loss: 0.00555882602930069\n",
      "epoch:  818, loss: 0.005557155702263117\n",
      "epoch:  819, loss: 0.005547426175326109\n",
      "epoch:  820, loss: 0.00554202776402235\n",
      "epoch:  821, loss: 0.005538902245461941\n",
      "epoch:  822, loss: 0.00553679745644331\n",
      "epoch:  823, loss: 0.005534985102713108\n",
      "epoch:  824, loss: 0.005533311981707811\n",
      "epoch:  825, loss: 0.005532725714147091\n",
      "epoch:  826, loss: 0.0055215731263160706\n",
      "epoch:  827, loss: 0.005516532342880964\n",
      "epoch:  828, loss: 0.005513545125722885\n",
      "epoch:  829, loss: 0.005511418450623751\n",
      "epoch:  830, loss: 0.005509598180651665\n",
      "epoch:  831, loss: 0.0055079152807593346\n",
      "epoch:  832, loss: 0.005506266839802265\n",
      "epoch:  833, loss: 0.005502116400748491\n",
      "epoch:  834, loss: 0.005493339151144028\n",
      "epoch:  835, loss: 0.005488983355462551\n",
      "epoch:  836, loss: 0.0054862103424966335\n",
      "epoch:  837, loss: 0.005484245717525482\n",
      "epoch:  838, loss: 0.005482433829456568\n",
      "epoch:  839, loss: 0.0054808109998703\n",
      "epoch:  840, loss: 0.005479821469634771\n",
      "epoch:  841, loss: 0.005469431169331074\n",
      "epoch:  842, loss: 0.0054642208851873875\n",
      "epoch:  843, loss: 0.005461348220705986\n",
      "epoch:  844, loss: 0.005459187552332878\n",
      "epoch:  845, loss: 0.005457412917166948\n",
      "epoch:  846, loss: 0.005455798003822565\n",
      "epoch:  847, loss: 0.005450587719678879\n",
      "epoch:  848, loss: 0.005442871246486902\n",
      "epoch:  849, loss: 0.005438927095383406\n",
      "epoch:  850, loss: 0.005436406470835209\n",
      "epoch:  851, loss: 0.005434493999928236\n",
      "epoch:  852, loss: 0.005432823672890663\n",
      "epoch:  853, loss: 0.005431243684142828\n",
      "epoch:  854, loss: 0.0054223621264100075\n",
      "epoch:  855, loss: 0.005416956264525652\n",
      "epoch:  856, loss: 0.005413886625319719\n",
      "epoch:  857, loss: 0.005411782301962376\n",
      "epoch:  858, loss: 0.005410048644989729\n",
      "epoch:  859, loss: 0.0054084537550807\n",
      "epoch:  860, loss: 0.005404158495366573\n",
      "epoch:  861, loss: 0.005395897664129734\n",
      "epoch:  862, loss: 0.005391885992139578\n",
      "epoch:  863, loss: 0.005389326252043247\n",
      "epoch:  864, loss: 0.005387422163039446\n",
      "epoch:  865, loss: 0.005385773256421089\n",
      "epoch:  866, loss: 0.005384175572544336\n",
      "epoch:  867, loss: 0.005374111235141754\n",
      "epoch:  868, loss: 0.005369353573769331\n",
      "epoch:  869, loss: 0.005366487428545952\n",
      "epoch:  870, loss: 0.0053643835708498955\n",
      "epoch:  871, loss: 0.005362662021070719\n",
      "epoch:  872, loss: 0.005361026618629694\n",
      "epoch:  873, loss: 0.005352750886231661\n",
      "epoch:  874, loss: 0.005346762016415596\n",
      "epoch:  875, loss: 0.005343313794583082\n",
      "epoch:  876, loss: 0.005340988747775555\n",
      "epoch:  877, loss: 0.00533908000215888\n",
      "epoch:  878, loss: 0.005337377544492483\n",
      "epoch:  879, loss: 0.005336156580597162\n",
      "epoch:  880, loss: 0.0053261760622262955\n",
      "epoch:  881, loss: 0.005321073811501265\n",
      "epoch:  882, loss: 0.005317913834005594\n",
      "epoch:  883, loss: 0.005315717309713364\n",
      "epoch:  884, loss: 0.005313832778483629\n",
      "epoch:  885, loss: 0.005312130320817232\n",
      "epoch:  886, loss: 0.005310485139489174\n",
      "epoch:  887, loss: 0.0053045907989144325\n",
      "epoch:  888, loss: 0.0052976482547819614\n",
      "epoch:  889, loss: 0.005293467082083225\n",
      "epoch:  890, loss: 0.005290716886520386\n",
      "epoch:  891, loss: 0.005288572050631046\n",
      "epoch:  892, loss: 0.005286761559545994\n",
      "epoch:  893, loss: 0.005285046063363552\n",
      "epoch:  894, loss: 0.00528340321034193\n",
      "epoch:  895, loss: 0.0052759586833417416\n",
      "epoch:  896, loss: 0.005269763991236687\n",
      "epoch:  897, loss: 0.005266090389341116\n",
      "epoch:  898, loss: 0.0052635204046964645\n",
      "epoch:  899, loss: 0.005261520855128765\n",
      "epoch:  900, loss: 0.0052597662433981895\n",
      "epoch:  901, loss: 0.005258121993392706\n",
      "epoch:  902, loss: 0.005256575997918844\n",
      "epoch:  903, loss: 0.005247003864496946\n",
      "epoch:  904, loss: 0.005242313724011183\n",
      "epoch:  905, loss: 0.005239120684564114\n",
      "epoch:  906, loss: 0.005236919969320297\n",
      "epoch:  907, loss: 0.005234978161752224\n",
      "epoch:  908, loss: 0.005233253352344036\n",
      "epoch:  909, loss: 0.005231591872870922\n",
      "epoch:  910, loss: 0.005229981616139412\n",
      "epoch:  911, loss: 0.005224849097430706\n",
      "epoch:  912, loss: 0.005217798054218292\n",
      "epoch:  913, loss: 0.005213849246501923\n",
      "epoch:  914, loss: 0.005210972856730223\n",
      "epoch:  915, loss: 0.005208768881857395\n",
      "epoch:  916, loss: 0.005206943489611149\n",
      "epoch:  917, loss: 0.005205274559557438\n",
      "epoch:  918, loss: 0.005203683394938707\n",
      "epoch:  919, loss: 0.005199577193707228\n",
      "epoch:  920, loss: 0.005192001815885305\n",
      "epoch:  921, loss: 0.00518769258633256\n",
      "epoch:  922, loss: 0.005184889305382967\n",
      "epoch:  923, loss: 0.0051827155984938145\n",
      "epoch:  924, loss: 0.0051808846183121204\n",
      "epoch:  925, loss: 0.005179224070161581\n",
      "epoch:  926, loss: 0.0051766387186944485\n",
      "epoch:  927, loss: 0.005168262403458357\n",
      "epoch:  928, loss: 0.005163621623069048\n",
      "epoch:  929, loss: 0.005160519853234291\n",
      "epoch:  930, loss: 0.0051582143642008305\n",
      "epoch:  931, loss: 0.005156394559890032\n",
      "epoch:  932, loss: 0.00515466183423996\n",
      "epoch:  933, loss: 0.005153024569153786\n",
      "epoch:  934, loss: 0.0051514580845832825\n",
      "epoch:  935, loss: 0.005149890668690205\n",
      "epoch:  936, loss: 0.005144281312823296\n",
      "epoch:  937, loss: 0.005137600935995579\n",
      "epoch:  938, loss: 0.005133855622261763\n",
      "epoch:  939, loss: 0.005131186917424202\n",
      "epoch:  940, loss: 0.005129062570631504\n",
      "epoch:  941, loss: 0.00512723671272397\n",
      "epoch:  942, loss: 0.005125586874783039\n",
      "epoch:  943, loss: 0.005124003626406193\n",
      "epoch:  944, loss: 0.00512212049216032\n",
      "epoch:  945, loss: 0.00511394627392292\n",
      "epoch:  946, loss: 0.005109448917210102\n",
      "epoch:  947, loss: 0.005106092430651188\n",
      "epoch:  948, loss: 0.005103779956698418\n",
      "epoch:  949, loss: 0.005101807415485382\n",
      "epoch:  950, loss: 0.005100077483803034\n",
      "epoch:  951, loss: 0.005098475608974695\n",
      "epoch:  952, loss: 0.00509693892672658\n",
      "epoch:  953, loss: 0.005093391053378582\n",
      "epoch:  954, loss: 0.005086268298327923\n",
      "epoch:  955, loss: 0.005082123912870884\n",
      "epoch:  956, loss: 0.00507914274930954\n",
      "epoch:  957, loss: 0.005076932720839977\n",
      "epoch:  958, loss: 0.005075139459222555\n",
      "epoch:  959, loss: 0.005073492880910635\n",
      "epoch:  960, loss: 0.0050719366408884525\n",
      "epoch:  961, loss: 0.005070432089269161\n",
      "epoch:  962, loss: 0.005061786621809006\n",
      "epoch:  963, loss: 0.005057270638644695\n",
      "epoch:  964, loss: 0.005054363515228033\n",
      "epoch:  965, loss: 0.005052092019468546\n",
      "epoch:  966, loss: 0.005050247069448233\n",
      "epoch:  967, loss: 0.005048547871410847\n",
      "epoch:  968, loss: 0.005046979058533907\n",
      "epoch:  969, loss: 0.00504547031596303\n",
      "epoch:  970, loss: 0.005041805561631918\n",
      "epoch:  971, loss: 0.005034848116338253\n",
      "epoch:  972, loss: 0.005030943546444178\n",
      "epoch:  973, loss: 0.005028080195188522\n",
      "epoch:  974, loss: 0.0050259968265891075\n",
      "epoch:  975, loss: 0.00502417329698801\n",
      "epoch:  976, loss: 0.0050225332379341125\n",
      "epoch:  977, loss: 0.005020981188863516\n",
      "epoch:  978, loss: 0.00501949293538928\n",
      "epoch:  979, loss: 0.005018602591007948\n",
      "epoch:  980, loss: 0.00501056294888258\n",
      "epoch:  981, loss: 0.005005815997719765\n",
      "epoch:  982, loss: 0.005002671852707863\n",
      "epoch:  983, loss: 0.005000314209610224\n",
      "epoch:  984, loss: 0.004998454358428717\n",
      "epoch:  985, loss: 0.004996796604245901\n",
      "epoch:  986, loss: 0.004995257593691349\n",
      "epoch:  987, loss: 0.0049937814474105835\n",
      "epoch:  988, loss: 0.004993117414414883\n",
      "epoch:  989, loss: 0.004984815139323473\n",
      "epoch:  990, loss: 0.00498009379953146\n",
      "epoch:  991, loss: 0.0049767992459237576\n",
      "epoch:  992, loss: 0.004974487237632275\n",
      "epoch:  993, loss: 0.0049725002609193325\n",
      "epoch:  994, loss: 0.004970873706042767\n",
      "epoch:  995, loss: 0.004969255067408085\n",
      "epoch:  996, loss: 0.004967707209289074\n",
      "epoch:  997, loss: 0.004966189619153738\n",
      "epoch:  998, loss: 0.004964700900018215\n",
      "epoch:  999, loss: 0.004959926940500736\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size=X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# opt = torch_numopt.ConjugateGradient(model=model, lr=1e-4, line_search_method=\"const\", cg_method=\"PR\")\n",
    "opt = torch_numopt.ConjugateGradient(model=model, lr=10, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"armijo\", cg_method=\"PR\")\n",
    "# opt = torch_numopt.ConjugateGradient(model=model, lr=10, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"wolfe\", cg_method=\"PR\")\n",
    "# opt = torch_numopt.ConjugateGradient(model=model, lr=10, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"strong-wolfe\", cg_method=\"PR\")\n",
    "# opt = torch_numopt.ConjugateGradient(model=model, lr=10, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"goldstein\", cg_method=\"PR\")\n",
    "\n",
    "\n",
    "all_loss = {}\n",
    "for epoch in range(1000):\n",
    "    print(\"epoch: \", epoch, end=\"\")\n",
    "    all_loss[epoch + 1] = 0\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y, loss_fn)\n",
    "\n",
    "        all_loss[epoch + 1] += loss\n",
    "    all_loss[epoch + 1] /= len(data_loader)\n",
    "    print(\", loss: {}\".format(all_loss[epoch + 1].cpu().detach().numpy().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics: R2 = 0.8295341648094239\n",
      "Test metrics:  R2 = 0.8281815633024978\n"
     ]
    }
   ],
   "source": [
    "pred_train = model.forward(X_train).cpu().detach()\n",
    "pred_test = model.forward(X_test).cpu().detach()\n",
    "print(f\"Train metrics: R2 = {r2_score(pred_train, y_train.cpu())}\")\n",
    "print(f\"Test metrics:  R2 = {r2_score(pred_test, y_test.cpu())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.1158432587981224\n",
      "epoch:  1, loss: 0.07378571480512619\n",
      "epoch:  2, loss: 0.05309215933084488\n",
      "epoch:  3, loss: 0.043004605919122696\n",
      "epoch:  4, loss: 0.03814656287431717\n",
      "epoch:  5, loss: 0.03583122789859772\n",
      "epoch:  6, loss: 0.03473467007279396\n",
      "epoch:  7, loss: 0.03421512246131897\n",
      "epoch:  8, loss: 0.0339660719037056\n",
      "epoch:  9, loss: 0.03384292125701904\n",
      "epoch:  10, loss: 0.03377803787589073\n",
      "epoch:  11, loss: 0.03373997285962105\n",
      "epoch:  12, loss: 0.03366062417626381\n",
      "epoch:  13, loss: 0.03358980268239975\n",
      "epoch:  14, loss: 0.03358781710267067\n",
      "epoch:  15, loss: 0.03344167768955231\n",
      "epoch:  16, loss: 0.033360887318849564\n",
      "epoch:  17, loss: 0.033277180045843124\n",
      "epoch:  18, loss: 0.03311292082071304\n",
      "epoch:  19, loss: 0.03302168473601341\n",
      "epoch:  20, loss: 0.03295879065990448\n",
      "epoch:  21, loss: 0.032783523201942444\n",
      "epoch:  22, loss: 0.03269325941801071\n",
      "epoch:  23, loss: 0.03264087811112404\n",
      "epoch:  24, loss: 0.03253110870718956\n",
      "epoch:  25, loss: 0.032434042543172836\n",
      "epoch:  26, loss: 0.03237979859113693\n",
      "epoch:  27, loss: 0.03228910267353058\n",
      "epoch:  28, loss: 0.0321819968521595\n",
      "epoch:  29, loss: 0.03212304040789604\n",
      "epoch:  30, loss: 0.03203967586159706\n",
      "epoch:  31, loss: 0.031917791813611984\n",
      "epoch:  32, loss: 0.03185256943106651\n",
      "epoch:  33, loss: 0.03176938742399216\n",
      "epoch:  34, loss: 0.03163348138332367\n",
      "epoch:  35, loss: 0.03156110271811485\n",
      "epoch:  36, loss: 0.0314844474196434\n",
      "epoch:  37, loss: 0.031326714903116226\n",
      "epoch:  38, loss: 0.031245507299900055\n",
      "epoch:  39, loss: 0.031171301379799843\n",
      "epoch:  40, loss: 0.030992653220891953\n",
      "epoch:  41, loss: 0.030901124700903893\n",
      "epoch:  42, loss: 0.03084259293973446\n",
      "epoch:  43, loss: 0.030629657208919525\n",
      "epoch:  44, loss: 0.030525336042046547\n",
      "epoch:  45, loss: 0.030477847903966904\n",
      "epoch:  46, loss: 0.030234135687351227\n",
      "epoch:  47, loss: 0.0301155187189579\n",
      "epoch:  48, loss: 0.030104540288448334\n",
      "epoch:  49, loss: 0.029806988313794136\n",
      "epoch:  50, loss: 0.02967073768377304\n",
      "epoch:  51, loss: 0.029594354331493378\n",
      "epoch:  52, loss: 0.029345031827688217\n",
      "epoch:  53, loss: 0.02918635867536068\n",
      "epoch:  54, loss: 0.02909903973340988\n",
      "epoch:  55, loss: 0.02885388396680355\n",
      "epoch:  56, loss: 0.028658375144004822\n",
      "epoch:  57, loss: 0.02855881303548813\n",
      "epoch:  58, loss: 0.028308019042015076\n",
      "epoch:  59, loss: 0.028081614524126053\n",
      "epoch:  60, loss: 0.027968240901827812\n",
      "epoch:  61, loss: 0.02773463726043701\n",
      "epoch:  62, loss: 0.02745407447218895\n",
      "epoch:  63, loss: 0.02732396125793457\n",
      "epoch:  64, loss: 0.027090653777122498\n",
      "epoch:  65, loss: 0.026764869689941406\n",
      "epoch:  66, loss: 0.026616668328642845\n",
      "epoch:  67, loss: 0.026403244584798813\n",
      "epoch:  68, loss: 0.025988701730966568\n",
      "epoch:  69, loss: 0.025814272463321686\n",
      "epoch:  70, loss: 0.025452647358179092\n",
      "epoch:  71, loss: 0.024953817948698997\n",
      "epoch:  72, loss: 0.024733522906899452\n",
      "epoch:  73, loss: 0.024368256330490112\n",
      "epoch:  74, loss: 0.0236525796353817\n",
      "epoch:  75, loss: 0.02342178486287594\n",
      "epoch:  76, loss: 0.023091156035661697\n",
      "epoch:  77, loss: 0.02246563322842121\n",
      "epoch:  78, loss: 0.02227805368602276\n",
      "epoch:  79, loss: 0.022002434358000755\n",
      "epoch:  80, loss: 0.021439118310809135\n",
      "epoch:  81, loss: 0.02128194272518158\n",
      "epoch:  82, loss: 0.02080734446644783\n",
      "epoch:  83, loss: 0.02040926367044449\n",
      "epoch:  84, loss: 0.020282430574297905\n",
      "epoch:  85, loss: 0.019696611911058426\n",
      "epoch:  86, loss: 0.019413961097598076\n",
      "epoch:  87, loss: 0.019313404336571693\n",
      "epoch:  88, loss: 0.018631065264344215\n",
      "epoch:  89, loss: 0.01847277767956257\n",
      "epoch:  90, loss: 0.01807047612965107\n",
      "epoch:  91, loss: 0.017689984291791916\n",
      "epoch:  92, loss: 0.01759706623852253\n",
      "epoch:  93, loss: 0.01698184199631214\n",
      "epoch:  94, loss: 0.01684805378317833\n",
      "epoch:  95, loss: 0.016398750245571136\n",
      "epoch:  96, loss: 0.0161551833152771\n",
      "epoch:  97, loss: 0.015968626365065575\n",
      "epoch:  98, loss: 0.015501315705478191\n",
      "epoch:  99, loss: 0.015429665334522724\n",
      "epoch:  100, loss: 0.01490039937198162\n",
      "epoch:  101, loss: 0.0148181626573205\n",
      "epoch:  102, loss: 0.01469344086945057\n",
      "epoch:  103, loss: 0.01090069767087698\n",
      "epoch:  104, loss: 0.010816715657711029\n",
      "epoch:  105, loss: 0.01058109663426876\n",
      "epoch:  106, loss: 0.010459695011377335\n",
      "epoch:  107, loss: 0.010411386378109455\n",
      "epoch:  108, loss: 0.010194240137934685\n",
      "epoch:  109, loss: 0.010170144960284233\n",
      "epoch:  110, loss: 0.009550816379487514\n",
      "epoch:  111, loss: 0.008846270851790905\n",
      "epoch:  112, loss: 0.008815799839794636\n",
      "epoch:  113, loss: 0.008780458942055702\n",
      "epoch:  114, loss: 0.008705580607056618\n",
      "epoch:  115, loss: 0.008695457130670547\n",
      "epoch:  116, loss: 0.00864493753761053\n",
      "epoch:  117, loss: 0.008626268245279789\n",
      "epoch:  118, loss: 0.008620616979897022\n",
      "epoch:  119, loss: 0.008578719571232796\n",
      "epoch:  120, loss: 0.008570839650928974\n",
      "epoch:  121, loss: 0.00856421235948801\n",
      "epoch:  122, loss: 0.00853234063833952\n",
      "epoch:  123, loss: 0.008527454920113087\n",
      "epoch:  124, loss: 0.00851456355303526\n",
      "epoch:  125, loss: 0.008494908921420574\n",
      "epoch:  126, loss: 0.008490925654768944\n",
      "epoch:  127, loss: 0.008476976305246353\n",
      "epoch:  128, loss: 0.008462551981210709\n",
      "epoch:  129, loss: 0.008459365926682949\n",
      "epoch:  130, loss: 0.008446062915027142\n",
      "epoch:  131, loss: 0.008434622548520565\n",
      "epoch:  132, loss: 0.008431751281023026\n",
      "epoch:  133, loss: 0.008419835940003395\n",
      "epoch:  134, loss: 0.00840991735458374\n",
      "epoch:  135, loss: 0.008407424204051495\n",
      "epoch:  136, loss: 0.008398953825235367\n",
      "epoch:  137, loss: 0.008388294838368893\n",
      "epoch:  138, loss: 0.008385838940739632\n",
      "epoch:  139, loss: 0.008380304090678692\n",
      "epoch:  140, loss: 0.008368886075913906\n",
      "epoch:  141, loss: 0.008366631343960762\n",
      "epoch:  142, loss: 0.008365835063159466\n",
      "epoch:  143, loss: 0.008351876400411129\n",
      "epoch:  144, loss: 0.00834946520626545\n",
      "epoch:  145, loss: 0.008348003029823303\n",
      "epoch:  146, loss: 0.008337119594216347\n",
      "epoch:  147, loss: 0.008333982899785042\n",
      "epoch:  148, loss: 0.008332590572535992\n",
      "epoch:  149, loss: 0.008323515765368938\n",
      "epoch:  150, loss: 0.008319911547005177\n",
      "epoch:  151, loss: 0.008318581618368626\n",
      "epoch:  152, loss: 0.008311153389513493\n",
      "epoch:  153, loss: 0.008307041600346565\n",
      "epoch:  154, loss: 0.00830573309212923\n",
      "epoch:  155, loss: 0.008299458771944046\n",
      "epoch:  156, loss: 0.008295287378132343\n",
      "epoch:  157, loss: 0.008294104598462582\n",
      "epoch:  158, loss: 0.008290058001875877\n",
      "epoch:  159, loss: 0.00828487891703844\n",
      "epoch:  160, loss: 0.008283603936433792\n",
      "epoch:  161, loss: 0.008280790410935879\n",
      "epoch:  162, loss: 0.008274939842522144\n",
      "epoch:  163, loss: 0.00827365554869175\n",
      "epoch:  164, loss: 0.008272026665508747\n",
      "epoch:  165, loss: 0.00826527364552021\n",
      "epoch:  166, loss: 0.008263892494142056\n",
      "epoch:  167, loss: 0.00826308224350214\n",
      "epoch:  168, loss: 0.008256754837930202\n",
      "epoch:  169, loss: 0.008255071006715298\n",
      "epoch:  170, loss: 0.008254267275333405\n",
      "epoch:  171, loss: 0.008249201811850071\n",
      "epoch:  172, loss: 0.008246798999607563\n",
      "epoch:  173, loss: 0.008245996199548244\n",
      "epoch:  174, loss: 0.008240890689194202\n",
      "epoch:  175, loss: 0.008238674141466618\n",
      "epoch:  176, loss: 0.008237834088504314\n",
      "epoch:  177, loss: 0.008233231492340565\n",
      "epoch:  178, loss: 0.008230460807681084\n",
      "epoch:  179, loss: 0.008229553699493408\n",
      "epoch:  180, loss: 0.00822508241981268\n",
      "epoch:  181, loss: 0.008221802301704884\n",
      "epoch:  182, loss: 0.008220842108130455\n",
      "epoch:  183, loss: 0.008217276073992252\n",
      "epoch:  184, loss: 0.008213560096919537\n",
      "epoch:  185, loss: 0.008212651126086712\n",
      "epoch:  186, loss: 0.008210483938455582\n",
      "epoch:  187, loss: 0.008206062018871307\n",
      "epoch:  188, loss: 0.008205100893974304\n",
      "epoch:  189, loss: 0.00820406898856163\n",
      "epoch:  190, loss: 0.008199012838304043\n",
      "epoch:  191, loss: 0.008198033086955547\n",
      "epoch:  192, loss: 0.008197426795959473\n",
      "epoch:  193, loss: 0.008192739449441433\n",
      "epoch:  194, loss: 0.008191419765353203\n",
      "epoch:  195, loss: 0.008190813474357128\n",
      "epoch:  196, loss: 0.008187128230929375\n",
      "epoch:  197, loss: 0.008185243234038353\n",
      "epoch:  198, loss: 0.008184586651623249\n",
      "epoch:  199, loss: 0.008182375691831112\n",
      "epoch:  200, loss: 0.008179358206689358\n",
      "epoch:  201, loss: 0.008178613148629665\n",
      "epoch:  202, loss: 0.008178508840501308\n",
      "epoch:  203, loss: 0.008173544891178608\n",
      "epoch:  204, loss: 0.00817258469760418\n",
      "epoch:  205, loss: 0.008172057569026947\n",
      "epoch:  206, loss: 0.008168106898665428\n",
      "epoch:  207, loss: 0.008166826330125332\n",
      "epoch:  208, loss: 0.008166264742612839\n",
      "epoch:  209, loss: 0.008163303136825562\n",
      "epoch:  210, loss: 0.008161240257322788\n",
      "epoch:  211, loss: 0.008160619996488094\n",
      "epoch:  212, loss: 0.008159300312399864\n",
      "epoch:  213, loss: 0.00815590750426054\n",
      "epoch:  214, loss: 0.008155147545039654\n",
      "epoch:  215, loss: 0.008154673501849174\n",
      "epoch:  216, loss: 0.008150839246809483\n",
      "epoch:  217, loss: 0.008149963803589344\n",
      "epoch:  218, loss: 0.008149493485689163\n",
      "epoch:  219, loss: 0.008146398700773716\n",
      "epoch:  220, loss: 0.008145025931298733\n",
      "epoch:  221, loss: 0.008144521154463291\n",
      "epoch:  222, loss: 0.008142604492604733\n",
      "epoch:  223, loss: 0.00814048945903778\n",
      "epoch:  224, loss: 0.008139930665493011\n",
      "epoch:  225, loss: 0.008139483630657196\n",
      "epoch:  226, loss: 0.008136075921356678\n",
      "epoch:  227, loss: 0.008135407231748104\n",
      "epoch:  228, loss: 0.008134997449815273\n",
      "epoch:  229, loss: 0.008131950162351131\n",
      "epoch:  230, loss: 0.008130976930260658\n",
      "epoch:  231, loss: 0.00813054759055376\n",
      "epoch:  232, loss: 0.008127703331410885\n",
      "epoch:  233, loss: 0.008126587606966496\n",
      "epoch:  234, loss: 0.00812613870948553\n",
      "epoch:  235, loss: 0.008123853243887424\n",
      "epoch:  236, loss: 0.008122319355607033\n",
      "epoch:  237, loss: 0.00812183041125536\n",
      "epoch:  238, loss: 0.00812128558754921\n",
      "epoch:  239, loss: 0.008118152618408203\n",
      "epoch:  240, loss: 0.00811749417334795\n",
      "epoch:  241, loss: 0.008117103017866611\n",
      "epoch:  242, loss: 0.008114011026918888\n",
      "epoch:  243, loss: 0.008113249205052853\n",
      "epoch:  244, loss: 0.008112849667668343\n",
      "epoch:  245, loss: 0.008110147900879383\n",
      "epoch:  246, loss: 0.008109043352305889\n",
      "epoch:  247, loss: 0.008108633570373058\n",
      "epoch:  248, loss: 0.00810655951499939\n",
      "epoch:  249, loss: 0.008104857988655567\n",
      "epoch:  250, loss: 0.0081043541431427\n",
      "epoch:  251, loss: 0.00810293573886156\n",
      "epoch:  252, loss: 0.008100613951683044\n",
      "epoch:  253, loss: 0.008100084029138088\n",
      "epoch:  254, loss: 0.00809970311820507\n",
      "epoch:  255, loss: 0.008096447214484215\n",
      "epoch:  256, loss: 0.0080957543104887\n",
      "epoch:  257, loss: 0.008095365948975086\n",
      "epoch:  258, loss: 0.008092286065220833\n",
      "epoch:  259, loss: 0.00809144601225853\n",
      "epoch:  260, loss: 0.008091028779745102\n",
      "epoch:  261, loss: 0.00808805599808693\n",
      "epoch:  262, loss: 0.008086983114480972\n",
      "epoch:  263, loss: 0.008086529560387135\n",
      "epoch:  264, loss: 0.008083569817245007\n",
      "epoch:  265, loss: 0.008082234300673008\n",
      "epoch:  266, loss: 0.008081731386482716\n",
      "epoch:  267, loss: 0.008078462444245815\n",
      "epoch:  268, loss: 0.008077346719801426\n",
      "epoch:  269, loss: 0.008076890371739864\n",
      "epoch:  270, loss: 0.00807417742908001\n",
      "epoch:  271, loss: 0.008072735741734505\n",
      "epoch:  272, loss: 0.008072244003415108\n",
      "epoch:  273, loss: 0.008070115931332111\n",
      "epoch:  274, loss: 0.00806819461286068\n",
      "epoch:  275, loss: 0.00806767214089632\n",
      "epoch:  276, loss: 0.008066697046160698\n",
      "epoch:  277, loss: 0.00806381180882454\n",
      "epoch:  278, loss: 0.008063171990215778\n",
      "epoch:  279, loss: 0.008062334731221199\n",
      "epoch:  280, loss: 0.008059393614530563\n",
      "epoch:  281, loss: 0.008058752864599228\n",
      "epoch:  282, loss: 0.008058348670601845\n",
      "epoch:  283, loss: 0.008054976351559162\n",
      "epoch:  284, loss: 0.008054211735725403\n",
      "epoch:  285, loss: 0.008053798228502274\n",
      "epoch:  286, loss: 0.008050590753555298\n",
      "epoch:  287, loss: 0.008049719966948032\n",
      "epoch:  288, loss: 0.008049297146499157\n",
      "epoch:  289, loss: 0.008046475239098072\n",
      "epoch:  290, loss: 0.008045293390750885\n",
      "epoch:  291, loss: 0.008044857531785965\n",
      "epoch:  292, loss: 0.008042443543672562\n",
      "epoch:  293, loss: 0.00804105680435896\n",
      "epoch:  294, loss: 0.008040589280426502\n",
      "epoch:  295, loss: 0.008039259351789951\n",
      "epoch:  296, loss: 0.00803698506206274\n",
      "epoch:  297, loss: 0.008036507293581963\n",
      "epoch:  298, loss: 0.008034593425691128\n",
      "epoch:  299, loss: 0.008032881654798985\n",
      "epoch:  300, loss: 0.008032416924834251\n",
      "epoch:  301, loss: 0.008030593395233154\n",
      "epoch:  302, loss: 0.008028671145439148\n",
      "epoch:  303, loss: 0.008028200827538967\n",
      "epoch:  304, loss: 0.008027251809835434\n",
      "epoch:  305, loss: 0.008024553768336773\n",
      "epoch:  306, loss: 0.008023962378501892\n",
      "epoch:  307, loss: 0.008023564703762531\n",
      "epoch:  308, loss: 0.008020392619073391\n",
      "epoch:  309, loss: 0.008019541390240192\n",
      "epoch:  310, loss: 0.008019120432436466\n",
      "epoch:  311, loss: 0.008016352541744709\n",
      "epoch:  312, loss: 0.00801516231149435\n",
      "epoch:  313, loss: 0.008014710620045662\n",
      "epoch:  314, loss: 0.008014285005629063\n",
      "epoch:  315, loss: 0.008011256344616413\n",
      "epoch:  316, loss: 0.00801057182252407\n",
      "epoch:  317, loss: 0.008010182529687881\n",
      "epoch:  318, loss: 0.008007099851965904\n",
      "epoch:  319, loss: 0.008006303571164608\n",
      "epoch:  320, loss: 0.008005893789231777\n",
      "epoch:  321, loss: 0.0080031119287014\n",
      "epoch:  322, loss: 0.008002063259482384\n",
      "epoch:  323, loss: 0.008001646026968956\n",
      "epoch:  324, loss: 0.007999536581337452\n",
      "epoch:  325, loss: 0.007998009212315083\n",
      "epoch:  326, loss: 0.007997545413672924\n",
      "epoch:  327, loss: 0.007996009662747383\n",
      "epoch:  328, loss: 0.007993980310857296\n",
      "epoch:  329, loss: 0.007993469946086407\n",
      "epoch:  330, loss: 0.00799329113215208\n",
      "epoch:  331, loss: 0.00799001194536686\n",
      "epoch:  332, loss: 0.00798934418708086\n",
      "epoch:  333, loss: 0.007988971658051014\n",
      "epoch:  334, loss: 0.007986274547874928\n",
      "epoch:  335, loss: 0.007985321804881096\n",
      "epoch:  336, loss: 0.007984908297657967\n",
      "epoch:  337, loss: 0.007982851937413216\n",
      "epoch:  338, loss: 0.007981262169778347\n",
      "epoch:  339, loss: 0.007980802096426487\n",
      "epoch:  340, loss: 0.007980229333043098\n",
      "epoch:  341, loss: 0.00797724537551403\n",
      "epoch:  342, loss: 0.007976611144840717\n",
      "epoch:  343, loss: 0.007976247929036617\n",
      "epoch:  344, loss: 0.007973314262926579\n",
      "epoch:  345, loss: 0.007972607389092445\n",
      "epoch:  346, loss: 0.007972230203449726\n",
      "epoch:  347, loss: 0.007969907484948635\n",
      "epoch:  348, loss: 0.007968719117343426\n",
      "epoch:  349, loss: 0.00796831026673317\n",
      "epoch:  350, loss: 0.007966331206262112\n",
      "epoch:  351, loss: 0.007964841090142727\n",
      "epoch:  352, loss: 0.007964369840919971\n",
      "epoch:  353, loss: 0.007962852716445923\n",
      "epoch:  354, loss: 0.00796101801097393\n",
      "epoch:  355, loss: 0.00796054769307375\n",
      "epoch:  356, loss: 0.007960116490721703\n",
      "epoch:  357, loss: 0.007957455702126026\n",
      "epoch:  358, loss: 0.007956892251968384\n",
      "epoch:  359, loss: 0.00795656442642212\n",
      "epoch:  360, loss: 0.007954143919050694\n",
      "epoch:  361, loss: 0.007953423075377941\n",
      "epoch:  362, loss: 0.007953069172799587\n",
      "epoch:  363, loss: 0.007951857522130013\n",
      "epoch:  364, loss: 0.007950042374432087\n",
      "epoch:  365, loss: 0.007949605584144592\n",
      "epoch:  366, loss: 0.007949285209178925\n",
      "epoch:  367, loss: 0.007946661673486233\n",
      "epoch:  368, loss: 0.007945999503135681\n",
      "epoch:  369, loss: 0.007945636287331581\n",
      "epoch:  370, loss: 0.007943716831505299\n",
      "epoch:  371, loss: 0.007942348718643188\n",
      "epoch:  372, loss: 0.007941906340420246\n",
      "epoch:  373, loss: 0.007941586896777153\n",
      "epoch:  374, loss: 0.007939286530017853\n",
      "epoch:  375, loss: 0.007938394322991371\n",
      "epoch:  376, loss: 0.007938042283058167\n",
      "epoch:  377, loss: 0.00793749000877142\n",
      "epoch:  378, loss: 0.007935001514852047\n",
      "epoch:  379, loss: 0.007934482768177986\n",
      "epoch:  380, loss: 0.0079341446980834\n",
      "epoch:  381, loss: 0.007931540720164776\n",
      "epoch:  382, loss: 0.007930823601782322\n",
      "epoch:  383, loss: 0.007930461317300797\n",
      "epoch:  384, loss: 0.007929209619760513\n",
      "epoch:  385, loss: 0.007927307859063148\n",
      "epoch:  386, loss: 0.00792680960148573\n",
      "epoch:  387, loss: 0.007926484569907188\n",
      "epoch:  388, loss: 0.00792459025979042\n",
      "epoch:  389, loss: 0.007923374883830547\n",
      "epoch:  390, loss: 0.007922985590994358\n",
      "epoch:  391, loss: 0.007922684773802757\n",
      "epoch:  392, loss: 0.007920162752270699\n",
      "epoch:  393, loss: 0.007919670082628727\n",
      "epoch:  394, loss: 0.007919363677501678\n",
      "epoch:  395, loss: 0.007917353883385658\n",
      "epoch:  396, loss: 0.007916437461972237\n",
      "epoch:  397, loss: 0.007916090078651905\n",
      "epoch:  398, loss: 0.007915091700851917\n",
      "epoch:  399, loss: 0.007913314737379551\n",
      "epoch:  400, loss: 0.007912848144769669\n",
      "epoch:  401, loss: 0.007912558503448963\n",
      "epoch:  402, loss: 0.007910387590527534\n",
      "epoch:  403, loss: 0.0079096844419837\n",
      "epoch:  404, loss: 0.007909359410405159\n",
      "epoch:  405, loss: 0.007907981052994728\n",
      "epoch:  406, loss: 0.00790661945939064\n",
      "epoch:  407, loss: 0.007906223647296429\n",
      "epoch:  408, loss: 0.007905947975814342\n",
      "epoch:  409, loss: 0.007903849706053734\n",
      "epoch:  410, loss: 0.007903183810412884\n",
      "epoch:  411, loss: 0.007902877405285835\n",
      "epoch:  412, loss: 0.007901303470134735\n",
      "epoch:  413, loss: 0.007900184020400047\n",
      "epoch:  414, loss: 0.007899814285337925\n",
      "epoch:  415, loss: 0.007899574004113674\n",
      "epoch:  416, loss: 0.007897259667515755\n",
      "epoch:  417, loss: 0.007896767929196358\n",
      "epoch:  418, loss: 0.0078964838758111\n",
      "epoch:  419, loss: 0.007894417271018028\n",
      "epoch:  420, loss: 0.00789372157305479\n",
      "epoch:  421, loss: 0.00789341889321804\n",
      "epoch:  422, loss: 0.007892042398452759\n",
      "epoch:  423, loss: 0.007890750654041767\n",
      "epoch:  424, loss: 0.007890364155173302\n",
      "epoch:  425, loss: 0.007890203967690468\n",
      "epoch:  426, loss: 0.007887803018093109\n",
      "epoch:  427, loss: 0.007887335494160652\n",
      "epoch:  428, loss: 0.007887049578130245\n",
      "epoch:  429, loss: 0.007884972728788853\n",
      "epoch:  430, loss: 0.007884308695793152\n",
      "epoch:  431, loss: 0.007884003221988678\n",
      "epoch:  432, loss: 0.007882474921643734\n",
      "epoch:  433, loss: 0.007881307043135166\n",
      "epoch:  434, loss: 0.007880942896008492\n",
      "epoch:  435, loss: 0.007880429737269878\n",
      "epoch:  436, loss: 0.007878346368670464\n",
      "epoch:  437, loss: 0.007877901196479797\n",
      "epoch:  438, loss: 0.007877624593675137\n",
      "epoch:  439, loss: 0.007875482551753521\n",
      "epoch:  440, loss: 0.007874879986047745\n",
      "epoch:  441, loss: 0.007874583825469017\n",
      "epoch:  442, loss: 0.007873055525124073\n",
      "epoch:  443, loss: 0.007871906273066998\n",
      "epoch:  444, loss: 0.007871544919908047\n",
      "epoch:  445, loss: 0.007871521636843681\n",
      "epoch:  446, loss: 0.007868998683989048\n",
      "epoch:  447, loss: 0.00786852277815342\n",
      "epoch:  448, loss: 0.007868237793445587\n",
      "epoch:  449, loss: 0.007866361178457737\n",
      "epoch:  450, loss: 0.007865515537559986\n",
      "epoch:  451, loss: 0.00786518957465887\n",
      "epoch:  452, loss: 0.007864465937018394\n",
      "epoch:  453, loss: 0.007862582802772522\n",
      "epoch:  454, loss: 0.007862118072807789\n",
      "epoch:  455, loss: 0.007861836813390255\n",
      "epoch:  456, loss: 0.007859806530177593\n",
      "epoch:  457, loss: 0.00785907357931137\n",
      "epoch:  458, loss: 0.0078587606549263\n",
      "epoch:  459, loss: 0.007857480086386204\n",
      "epoch:  460, loss: 0.007856062613427639\n",
      "epoch:  461, loss: 0.007855688221752644\n",
      "epoch:  462, loss: 0.007855415344238281\n",
      "epoch:  463, loss: 0.007853271439671516\n",
      "epoch:  464, loss: 0.007852645590901375\n",
      "epoch:  465, loss: 0.007852342911064625\n",
      "epoch:  466, loss: 0.007850817404687405\n",
      "epoch:  467, loss: 0.007849635556340218\n",
      "epoch:  468, loss: 0.007849273271858692\n",
      "epoch:  469, loss: 0.007848819717764854\n",
      "epoch:  470, loss: 0.00784667395055294\n",
      "epoch:  471, loss: 0.007846219465136528\n",
      "epoch:  472, loss: 0.007845942862331867\n",
      "epoch:  473, loss: 0.007843815721571445\n",
      "epoch:  474, loss: 0.007843181490898132\n",
      "epoch:  475, loss: 0.007842889986932278\n",
      "epoch:  476, loss: 0.007840990088880062\n",
      "epoch:  477, loss: 0.007840151898562908\n",
      "epoch:  478, loss: 0.007839828729629517\n",
      "epoch:  479, loss: 0.00783822126686573\n",
      "epoch:  480, loss: 0.00783709529787302\n",
      "epoch:  481, loss: 0.007836737670004368\n",
      "epoch:  482, loss: 0.007835670374333858\n",
      "epoch:  483, loss: 0.007834026589989662\n",
      "epoch:  484, loss: 0.007833634503185749\n",
      "epoch:  485, loss: 0.007833356969058514\n",
      "epoch:  486, loss: 0.007831049151718616\n",
      "epoch:  487, loss: 0.007830542512238026\n",
      "epoch:  488, loss: 0.007830256596207619\n",
      "epoch:  489, loss: 0.007828324101865292\n",
      "epoch:  490, loss: 0.007827465422451496\n",
      "epoch:  491, loss: 0.00782714318484068\n",
      "epoch:  492, loss: 0.00782645121216774\n",
      "epoch:  493, loss: 0.007824530825018883\n",
      "epoch:  494, loss: 0.007824094034731388\n",
      "epoch:  495, loss: 0.007823812775313854\n",
      "epoch:  496, loss: 0.007821773178875446\n",
      "epoch:  497, loss: 0.007821057923138142\n",
      "epoch:  498, loss: 0.00782074499875307\n",
      "epoch:  499, loss: 0.007819033227860928\n",
      "epoch:  500, loss: 0.00781797245144844\n",
      "epoch:  501, loss: 0.007817630656063557\n",
      "epoch:  502, loss: 0.007816778495907784\n",
      "epoch:  503, loss: 0.007814940065145493\n",
      "epoch:  504, loss: 0.007814495824277401\n",
      "epoch:  505, loss: 0.007814200595021248\n",
      "epoch:  506, loss: 0.007811854127794504\n",
      "epoch:  507, loss: 0.007811262272298336\n",
      "epoch:  508, loss: 0.007810954935848713\n",
      "epoch:  509, loss: 0.007808932103216648\n",
      "epoch:  510, loss: 0.007808038964867592\n",
      "epoch:  511, loss: 0.007807705085724592\n",
      "epoch:  512, loss: 0.0078061590902507305\n",
      "epoch:  513, loss: 0.007804811932146549\n",
      "epoch:  514, loss: 0.00780440354719758\n",
      "epoch:  515, loss: 0.007803564425557852\n",
      "epoch:  516, loss: 0.007801531348377466\n",
      "epoch:  517, loss: 0.0078010293655097485\n",
      "epoch:  518, loss: 0.007800717372447252\n",
      "epoch:  519, loss: 0.007798368111252785\n",
      "epoch:  520, loss: 0.007797641679644585\n",
      "epoch:  521, loss: 0.0077973161824047565\n",
      "epoch:  522, loss: 0.007796168327331543\n",
      "epoch:  523, loss: 0.0077944425866007805\n",
      "epoch:  524, loss: 0.007793963886797428\n",
      "epoch:  525, loss: 0.007793656550347805\n",
      "epoch:  526, loss: 0.00779199693351984\n",
      "epoch:  527, loss: 0.007790666539222002\n",
      "epoch:  528, loss: 0.007790227420628071\n",
      "epoch:  529, loss: 0.0077899182215332985\n",
      "epoch:  530, loss: 0.007787926122546196\n",
      "epoch:  531, loss: 0.007786870002746582\n",
      "epoch:  532, loss: 0.007786478381603956\n",
      "epoch:  533, loss: 0.007786174770444632\n",
      "epoch:  534, loss: 0.007784584071487188\n",
      "epoch:  535, loss: 0.007783187553286552\n",
      "epoch:  536, loss: 0.00778274517506361\n",
      "epoch:  537, loss: 0.007782439235597849\n",
      "epoch:  538, loss: 0.007781085092574358\n",
      "epoch:  539, loss: 0.007779479026794434\n",
      "epoch:  540, loss: 0.00777901615947485\n",
      "epoch:  541, loss: 0.007778695784509182\n",
      "epoch:  542, loss: 0.007777406834065914\n",
      "epoch:  543, loss: 0.007775735110044479\n",
      "epoch:  544, loss: 0.007775268517434597\n",
      "epoch:  545, loss: 0.0077749514020979404\n",
      "epoch:  546, loss: 0.007773747202008963\n",
      "epoch:  547, loss: 0.007772000972181559\n",
      "epoch:  548, loss: 0.007771529257297516\n",
      "epoch:  549, loss: 0.007771212141960859\n",
      "epoch:  550, loss: 0.007770130410790443\n",
      "epoch:  551, loss: 0.007768281735479832\n",
      "epoch:  552, loss: 0.007767786737531424\n",
      "epoch:  553, loss: 0.007767457980662584\n",
      "epoch:  554, loss: 0.007766207680106163\n",
      "epoch:  555, loss: 0.007764442358165979\n",
      "epoch:  556, loss: 0.007763948291540146\n",
      "epoch:  557, loss: 0.0077636209316551685\n",
      "epoch:  558, loss: 0.007761883549392223\n",
      "epoch:  559, loss: 0.0077605401165783405\n",
      "epoch:  560, loss: 0.007760106585919857\n",
      "epoch:  561, loss: 0.007759780623018742\n",
      "epoch:  562, loss: 0.007758248131722212\n",
      "epoch:  563, loss: 0.007756739854812622\n",
      "epoch:  564, loss: 0.007756280712783337\n",
      "epoch:  565, loss: 0.0077559491619467735\n",
      "epoch:  566, loss: 0.007753551006317139\n",
      "epoch:  567, loss: 0.007752783130854368\n",
      "epoch:  568, loss: 0.007752411067485809\n",
      "epoch:  569, loss: 0.007751182187348604\n",
      "epoch:  570, loss: 0.0077494061551988125\n",
      "epoch:  571, loss: 0.007748904637992382\n",
      "epoch:  572, loss: 0.0077485875226557255\n",
      "epoch:  573, loss: 0.007747276220470667\n",
      "epoch:  574, loss: 0.007745616137981415\n",
      "epoch:  575, loss: 0.007745158392935991\n",
      "epoch:  576, loss: 0.0077448454685509205\n",
      "epoch:  577, loss: 0.007742492016404867\n",
      "epoch:  578, loss: 0.007741743233054876\n",
      "epoch:  579, loss: 0.00774137768894434\n",
      "epoch:  580, loss: 0.007740516681224108\n",
      "epoch:  581, loss: 0.0077384887263178825\n",
      "epoch:  582, loss: 0.007737928070127964\n",
      "epoch:  583, loss: 0.007737600244581699\n",
      "epoch:  584, loss: 0.0077363853342831135\n",
      "epoch:  585, loss: 0.007734606042504311\n",
      "epoch:  586, loss: 0.007734120357781649\n",
      "epoch:  587, loss: 0.007733803708106279\n",
      "epoch:  588, loss: 0.007731669582426548\n",
      "epoch:  589, loss: 0.007730681914836168\n",
      "epoch:  590, loss: 0.007730289362370968\n",
      "epoch:  591, loss: 0.007730255369096994\n",
      "epoch:  592, loss: 0.007727223448455334\n",
      "epoch:  593, loss: 0.007726561278104782\n",
      "epoch:  594, loss: 0.007726198062300682\n",
      "epoch:  595, loss: 0.007724249269813299\n",
      "epoch:  596, loss: 0.007722885813564062\n",
      "epoch:  597, loss: 0.007722429931163788\n",
      "epoch:  598, loss: 0.007722093723714352\n",
      "epoch:  599, loss: 0.007719306740909815\n",
      "epoch:  600, loss: 0.0077186571434140205\n",
      "epoch:  601, loss: 0.007718278560787439\n",
      "epoch:  602, loss: 0.007716545835137367\n",
      "epoch:  603, loss: 0.007714956998825073\n",
      "epoch:  604, loss: 0.007714463863521814\n",
      "epoch:  605, loss: 0.007714115083217621\n",
      "epoch:  606, loss: 0.007711592596024275\n",
      "epoch:  607, loss: 0.007710680831223726\n",
      "epoch:  608, loss: 0.007710281293839216\n",
      "epoch:  609, loss: 0.007710260804742575\n",
      "epoch:  610, loss: 0.007707106880843639\n",
      "epoch:  611, loss: 0.0077064260840415955\n",
      "epoch:  612, loss: 0.007706054486334324\n",
      "epoch:  613, loss: 0.007704385556280613\n",
      "epoch:  614, loss: 0.007702600210905075\n",
      "epoch:  615, loss: 0.007702080998569727\n",
      "epoch:  616, loss: 0.007701727095991373\n",
      "epoch:  617, loss: 0.007699522655457258\n",
      "epoch:  618, loss: 0.007698198314756155\n",
      "epoch:  619, loss: 0.007697732653468847\n",
      "epoch:  620, loss: 0.007697037421166897\n",
      "epoch:  621, loss: 0.007694378960877657\n",
      "epoch:  622, loss: 0.0076937624253332615\n",
      "epoch:  623, loss: 0.007693383377045393\n",
      "epoch:  624, loss: 0.007690983824431896\n",
      "epoch:  625, loss: 0.007689898367971182\n",
      "epoch:  626, loss: 0.00768947834149003\n",
      "epoch:  627, loss: 0.007689131423830986\n",
      "epoch:  628, loss: 0.007686272729188204\n",
      "epoch:  629, loss: 0.007685601711273193\n",
      "epoch:  630, loss: 0.00768523383885622\n",
      "epoch:  631, loss: 0.007683923002332449\n",
      "epoch:  632, loss: 0.007681825198233128\n",
      "epoch:  633, loss: 0.007681233808398247\n",
      "epoch:  634, loss: 0.007680819369852543\n",
      "epoch:  635, loss: 0.007678452413529158\n",
      "epoch:  636, loss: 0.00767672061920166\n",
      "epoch:  637, loss: 0.00767617067322135\n",
      "epoch:  638, loss: 0.00767575204372406\n",
      "epoch:  639, loss: 0.007672697305679321\n",
      "epoch:  640, loss: 0.007671365048736334\n",
      "epoch:  641, loss: 0.007670825347304344\n",
      "epoch:  642, loss: 0.007670050021260977\n",
      "epoch:  643, loss: 0.007666463498026133\n",
      "epoch:  644, loss: 0.007665662094950676\n",
      "epoch:  645, loss: 0.007665181998163462\n",
      "epoch:  646, loss: 0.007661973126232624\n",
      "epoch:  647, loss: 0.0076606618240475655\n",
      "epoch:  648, loss: 0.007660143077373505\n",
      "epoch:  649, loss: 0.007659792900085449\n",
      "epoch:  650, loss: 0.007655954919755459\n",
      "epoch:  651, loss: 0.00765506736934185\n",
      "epoch:  652, loss: 0.007654562592506409\n",
      "epoch:  653, loss: 0.007652753964066505\n",
      "epoch:  654, loss: 0.007650166749954224\n",
      "epoch:  655, loss: 0.007649482227861881\n",
      "epoch:  656, loss: 0.007649044971913099\n",
      "epoch:  657, loss: 0.007646676618605852\n",
      "epoch:  658, loss: 0.007644929923117161\n",
      "epoch:  659, loss: 0.0076443529687821865\n",
      "epoch:  660, loss: 0.007643932476639748\n",
      "epoch:  661, loss: 0.007641516625881195\n",
      "epoch:  662, loss: 0.007639752235263586\n",
      "epoch:  663, loss: 0.00763918599113822\n",
      "epoch:  664, loss: 0.007638752926141024\n",
      "epoch:  665, loss: 0.007635657675564289\n",
      "epoch:  666, loss: 0.007634553126990795\n",
      "epoch:  667, loss: 0.007634044159203768\n",
      "epoch:  668, loss: 0.00763248186558485\n",
      "epoch:  669, loss: 0.007630038075149059\n",
      "epoch:  670, loss: 0.00762935308739543\n",
      "epoch:  671, loss: 0.007628906983882189\n",
      "epoch:  672, loss: 0.007625884842127562\n",
      "epoch:  673, loss: 0.007624696474522352\n",
      "epoch:  674, loss: 0.0076241884380578995\n",
      "epoch:  675, loss: 0.007622730452567339\n",
      "epoch:  676, loss: 0.007620255928486586\n",
      "epoch:  677, loss: 0.007619556505233049\n",
      "epoch:  678, loss: 0.00761911366134882\n",
      "epoch:  679, loss: 0.007615996524691582\n",
      "epoch:  680, loss: 0.007614770904183388\n",
      "epoch:  681, loss: 0.007614241447299719\n",
      "epoch:  682, loss: 0.0076130046509206295\n",
      "epoch:  683, loss: 0.00761020602658391\n",
      "epoch:  684, loss: 0.007609465625137091\n",
      "epoch:  685, loss: 0.007609025575220585\n",
      "epoch:  686, loss: 0.007607487495988607\n",
      "epoch:  687, loss: 0.0076050544157624245\n",
      "epoch:  688, loss: 0.007604410406202078\n",
      "epoch:  689, loss: 0.007603962905704975\n",
      "epoch:  690, loss: 0.007601448334753513\n",
      "epoch:  691, loss: 0.007599817123264074\n",
      "epoch:  692, loss: 0.007599264848977327\n",
      "epoch:  693, loss: 0.007598835043609142\n",
      "epoch:  694, loss: 0.007595679722726345\n",
      "epoch:  695, loss: 0.007594605907797813\n",
      "epoch:  696, loss: 0.007594094146043062\n",
      "epoch:  697, loss: 0.007593680638819933\n",
      "epoch:  698, loss: 0.007590707391500473\n",
      "epoch:  699, loss: 0.007589404471218586\n",
      "epoch:  700, loss: 0.007588896434754133\n",
      "epoch:  701, loss: 0.007587474305182695\n",
      "epoch:  702, loss: 0.007584808859974146\n",
      "epoch:  703, loss: 0.007584042381495237\n",
      "epoch:  704, loss: 0.007583565544337034\n",
      "epoch:  705, loss: 0.00758036132901907\n",
      "epoch:  706, loss: 0.007578761782497168\n",
      "epoch:  707, loss: 0.007578058633953333\n",
      "epoch:  708, loss: 0.007577479816973209\n",
      "epoch:  709, loss: 0.0075729042291641235\n",
      "epoch:  710, loss: 0.00757187232375145\n",
      "epoch:  711, loss: 0.007571293506771326\n",
      "epoch:  712, loss: 0.007568700239062309\n",
      "epoch:  713, loss: 0.007566227577626705\n",
      "epoch:  714, loss: 0.00756548997014761\n",
      "epoch:  715, loss: 0.007564958184957504\n",
      "epoch:  716, loss: 0.007561128120869398\n",
      "epoch:  717, loss: 0.0075597562827169895\n",
      "epoch:  718, loss: 0.007559170946478844\n",
      "epoch:  719, loss: 0.007558494806289673\n",
      "epoch:  720, loss: 0.00755462609231472\n",
      "epoch:  721, loss: 0.007553671486675739\n",
      "epoch:  722, loss: 0.007553130388259888\n",
      "epoch:  723, loss: 0.007551042828708887\n",
      "epoch:  724, loss: 0.007548310793936253\n",
      "epoch:  725, loss: 0.007547547109425068\n",
      "epoch:  726, loss: 0.007547031622380018\n",
      "epoch:  727, loss: 0.007543492130935192\n",
      "epoch:  728, loss: 0.00754208629950881\n",
      "epoch:  729, loss: 0.007541470229625702\n",
      "epoch:  730, loss: 0.00754020269960165\n",
      "epoch:  731, loss: 0.007536804303526878\n",
      "epoch:  732, loss: 0.007535931188613176\n",
      "epoch:  733, loss: 0.0075354003347456455\n",
      "epoch:  734, loss: 0.0075334045104682446\n",
      "epoch:  735, loss: 0.007530491333454847\n",
      "epoch:  736, loss: 0.007529688999056816\n",
      "epoch:  737, loss: 0.007529132068157196\n",
      "epoch:  738, loss: 0.007525533437728882\n",
      "epoch:  739, loss: 0.007523856125771999\n",
      "epoch:  740, loss: 0.0075231813825666904\n",
      "epoch:  741, loss: 0.0075226714834570885\n",
      "epoch:  742, loss: 0.007519596256315708\n",
      "epoch:  743, loss: 0.007517717778682709\n",
      "epoch:  744, loss: 0.0075170849449932575\n",
      "epoch:  745, loss: 0.007516586221754551\n",
      "epoch:  746, loss: 0.007512953598052263\n",
      "epoch:  747, loss: 0.007511706091463566\n",
      "epoch:  748, loss: 0.007511122152209282\n",
      "epoch:  749, loss: 0.007510646246373653\n",
      "epoch:  750, loss: 0.007507018744945526\n",
      "epoch:  751, loss: 0.007505851797759533\n",
      "epoch:  752, loss: 0.0075053139589726925\n",
      "epoch:  753, loss: 0.007505015004426241\n",
      "epoch:  754, loss: 0.007501116022467613\n",
      "epoch:  755, loss: 0.007500144653022289\n",
      "epoch:  756, loss: 0.007499619387090206\n",
      "epoch:  757, loss: 0.007498149760067463\n",
      "epoch:  758, loss: 0.007495288737118244\n",
      "epoch:  759, loss: 0.007494504097849131\n",
      "epoch:  760, loss: 0.00749402167275548\n",
      "epoch:  761, loss: 0.007492355071008205\n",
      "epoch:  762, loss: 0.007489808835089207\n",
      "epoch:  763, loss: 0.007489034440368414\n",
      "epoch:  764, loss: 0.007488564122468233\n",
      "epoch:  765, loss: 0.007485554087907076\n",
      "epoch:  766, loss: 0.007484113797545433\n",
      "epoch:  767, loss: 0.007483539637178183\n",
      "epoch:  768, loss: 0.007483084686100483\n",
      "epoch:  769, loss: 0.007479872088879347\n",
      "epoch:  770, loss: 0.00747858639806509\n",
      "epoch:  771, loss: 0.0074780285358428955\n",
      "epoch:  772, loss: 0.007477574050426483\n",
      "epoch:  773, loss: 0.007474182173609734\n",
      "epoch:  774, loss: 0.007472998928278685\n",
      "epoch:  775, loss: 0.007472445722669363\n",
      "epoch:  776, loss: 0.0074719879776239395\n",
      "epoch:  777, loss: 0.007468426134437323\n",
      "epoch:  778, loss: 0.0074673364870250225\n",
      "epoch:  779, loss: 0.00746679725125432\n",
      "epoch:  780, loss: 0.007466533686965704\n",
      "epoch:  781, loss: 0.00746264448389411\n",
      "epoch:  782, loss: 0.0074617392383515835\n",
      "epoch:  783, loss: 0.007461236324161291\n",
      "epoch:  784, loss: 0.007460115011781454\n",
      "epoch:  785, loss: 0.007457031402736902\n",
      "epoch:  786, loss: 0.0074562132358551025\n",
      "epoch:  787, loss: 0.007455718237906694\n",
      "epoch:  788, loss: 0.007453263737261295\n",
      "epoch:  789, loss: 0.00745118223130703\n",
      "epoch:  790, loss: 0.007450494449585676\n",
      "epoch:  791, loss: 0.00745002506300807\n",
      "epoch:  792, loss: 0.007446518167853355\n",
      "epoch:  793, loss: 0.0074452790431678295\n",
      "epoch:  794, loss: 0.0074447039514780045\n",
      "epoch:  795, loss: 0.0074439081363379955\n",
      "epoch:  796, loss: 0.007440248969942331\n",
      "epoch:  797, loss: 0.007439285982400179\n",
      "epoch:  798, loss: 0.007438726723194122\n",
      "epoch:  799, loss: 0.007436361163854599\n",
      "epoch:  800, loss: 0.007433978375047445\n",
      "epoch:  801, loss: 0.007433176506310701\n",
      "epoch:  802, loss: 0.007432654500007629\n",
      "epoch:  803, loss: 0.007429651916027069\n",
      "epoch:  804, loss: 0.0074278139509260654\n",
      "epoch:  805, loss: 0.007427116855978966\n",
      "epoch:  806, loss: 0.007426599506288767\n",
      "epoch:  807, loss: 0.007422998547554016\n",
      "epoch:  808, loss: 0.007421547546982765\n",
      "epoch:  809, loss: 0.007420902606099844\n",
      "epoch:  810, loss: 0.007420383393764496\n",
      "epoch:  811, loss: 0.007416673935949802\n",
      "epoch:  812, loss: 0.007415271829813719\n",
      "epoch:  813, loss: 0.0074146282859146595\n",
      "epoch:  814, loss: 0.007414121646434069\n",
      "epoch:  815, loss: 0.007411312777549028\n",
      "epoch:  816, loss: 0.007409194950014353\n",
      "epoch:  817, loss: 0.007408434990793467\n",
      "epoch:  818, loss: 0.007407895289361477\n",
      "epoch:  819, loss: 0.007406753022223711\n",
      "epoch:  820, loss: 0.007403170224279165\n",
      "epoch:  821, loss: 0.0074022249318659306\n",
      "epoch:  822, loss: 0.007401653565466404\n",
      "epoch:  823, loss: 0.007399430498480797\n",
      "epoch:  824, loss: 0.00739673338830471\n",
      "epoch:  825, loss: 0.007395856082439423\n",
      "epoch:  826, loss: 0.007395301479846239\n",
      "epoch:  827, loss: 0.007394393440335989\n",
      "epoch:  828, loss: 0.007390410639345646\n",
      "epoch:  829, loss: 0.007389366626739502\n",
      "epoch:  830, loss: 0.007388728205114603\n",
      "epoch:  831, loss: 0.007386515382677317\n",
      "epoch:  832, loss: 0.007383562624454498\n",
      "epoch:  833, loss: 0.0073826187290251255\n",
      "epoch:  834, loss: 0.007382013835012913\n",
      "epoch:  835, loss: 0.007380630820989609\n",
      "epoch:  836, loss: 0.007376810070127249\n",
      "epoch:  837, loss: 0.007375754415988922\n",
      "epoch:  838, loss: 0.007375125307589769\n",
      "epoch:  839, loss: 0.007373154163360596\n",
      "epoch:  840, loss: 0.007369884289801121\n",
      "epoch:  841, loss: 0.007368909660726786\n",
      "epoch:  842, loss: 0.0073682949878275394\n",
      "epoch:  843, loss: 0.0073660290800035\n",
      "epoch:  844, loss: 0.007362897042185068\n",
      "epoch:  845, loss: 0.007361910305917263\n",
      "epoch:  846, loss: 0.007361265830695629\n",
      "epoch:  847, loss: 0.007357847411185503\n",
      "epoch:  848, loss: 0.007355416193604469\n",
      "epoch:  849, loss: 0.0073545086197555065\n",
      "epoch:  850, loss: 0.0073538911528885365\n",
      "epoch:  851, loss: 0.007349531631916761\n",
      "epoch:  852, loss: 0.007347741164267063\n",
      "epoch:  853, loss: 0.007346920669078827\n",
      "epoch:  854, loss: 0.007346281781792641\n",
      "epoch:  855, loss: 0.007341302931308746\n",
      "epoch:  856, loss: 0.0073397159576416016\n",
      "epoch:  857, loss: 0.007338970899581909\n",
      "epoch:  858, loss: 0.007338245864957571\n",
      "epoch:  859, loss: 0.007333272602409124\n",
      "epoch:  860, loss: 0.007331972010433674\n",
      "epoch:  861, loss: 0.007331244647502899\n",
      "epoch:  862, loss: 0.007328915409743786\n",
      "epoch:  863, loss: 0.007325117476284504\n",
      "epoch:  864, loss: 0.007323948200792074\n",
      "epoch:  865, loss: 0.007323178928345442\n",
      "epoch:  866, loss: 0.00732007110491395\n",
      "epoch:  867, loss: 0.007316553499549627\n",
      "epoch:  868, loss: 0.007315449416637421\n",
      "epoch:  869, loss: 0.007314745802432299\n",
      "epoch:  870, loss: 0.007311838213354349\n",
      "epoch:  871, loss: 0.007308437488973141\n",
      "epoch:  872, loss: 0.007307325955480337\n",
      "epoch:  873, loss: 0.007306633051484823\n",
      "epoch:  874, loss: 0.007303526625037193\n",
      "epoch:  875, loss: 0.00730022182688117\n",
      "epoch:  876, loss: 0.007299067918211222\n",
      "epoch:  877, loss: 0.007298331707715988\n",
      "epoch:  878, loss: 0.007295230403542519\n",
      "epoch:  879, loss: 0.007291777525097132\n",
      "epoch:  880, loss: 0.007290629204362631\n",
      "epoch:  881, loss: 0.007289934437721968\n",
      "epoch:  882, loss: 0.007287129759788513\n",
      "epoch:  883, loss: 0.007283684331923723\n",
      "epoch:  884, loss: 0.007282520178705454\n",
      "epoch:  885, loss: 0.007281803991645575\n",
      "epoch:  886, loss: 0.0072782947681844234\n",
      "epoch:  887, loss: 0.007275281008332968\n",
      "epoch:  888, loss: 0.007274194620549679\n",
      "epoch:  889, loss: 0.007273485884070396\n",
      "epoch:  890, loss: 0.007271284237504005\n",
      "epoch:  891, loss: 0.007267216220498085\n",
      "epoch:  892, loss: 0.007265947759151459\n",
      "epoch:  893, loss: 0.0072652059607207775\n",
      "epoch:  894, loss: 0.007263365667313337\n",
      "epoch:  895, loss: 0.007258974947035313\n",
      "epoch:  896, loss: 0.007257640361785889\n",
      "epoch:  897, loss: 0.007256878539919853\n",
      "epoch:  898, loss: 0.0072548771277070045\n",
      "epoch:  899, loss: 0.007250630762428045\n",
      "epoch:  900, loss: 0.007249306421726942\n",
      "epoch:  901, loss: 0.0072485399432480335\n",
      "epoch:  902, loss: 0.007247436325997114\n",
      "epoch:  903, loss: 0.007242343854159117\n",
      "epoch:  904, loss: 0.007240787614136934\n",
      "epoch:  905, loss: 0.007239938247948885\n",
      "epoch:  906, loss: 0.007238205522298813\n",
      "epoch:  907, loss: 0.007233305834233761\n",
      "epoch:  908, loss: 0.007231809198856354\n",
      "epoch:  909, loss: 0.007230983581393957\n",
      "epoch:  910, loss: 0.00722923269495368\n",
      "epoch:  911, loss: 0.0072242822498083115\n",
      "epoch:  912, loss: 0.007222861982882023\n",
      "epoch:  913, loss: 0.0072220503352582455\n",
      "epoch:  914, loss: 0.007220707833766937\n",
      "epoch:  915, loss: 0.007215649355202913\n",
      "epoch:  916, loss: 0.007214212790131569\n",
      "epoch:  917, loss: 0.007213423028588295\n",
      "epoch:  918, loss: 0.007212717551738024\n",
      "epoch:  919, loss: 0.0072072041220963\n",
      "epoch:  920, loss: 0.0072055719792842865\n",
      "epoch:  921, loss: 0.0072047351859509945\n",
      "epoch:  922, loss: 0.007204021792858839\n",
      "epoch:  923, loss: 0.00719934469088912\n",
      "epoch:  924, loss: 0.007197035010904074\n",
      "epoch:  925, loss: 0.007196059450507164\n",
      "epoch:  926, loss: 0.007195330690592527\n",
      "epoch:  927, loss: 0.007191183976829052\n",
      "epoch:  928, loss: 0.007188417017459869\n",
      "epoch:  929, loss: 0.007187388837337494\n",
      "epoch:  930, loss: 0.007186664268374443\n",
      "epoch:  931, loss: 0.00718300323933363\n",
      "epoch:  932, loss: 0.0071799252182245255\n",
      "epoch:  933, loss: 0.007178839761763811\n",
      "epoch:  934, loss: 0.007178102154284716\n",
      "epoch:  935, loss: 0.007174743339419365\n",
      "epoch:  936, loss: 0.007171246223151684\n",
      "epoch:  937, loss: 0.007170070894062519\n",
      "epoch:  938, loss: 0.007169298827648163\n",
      "epoch:  939, loss: 0.007165686693042517\n",
      "epoch:  940, loss: 0.007162441965192556\n",
      "epoch:  941, loss: 0.007161272689700127\n",
      "epoch:  942, loss: 0.007160503882914782\n",
      "epoch:  943, loss: 0.007156168110668659\n",
      "epoch:  944, loss: 0.007153566461056471\n",
      "epoch:  945, loss: 0.0071524749509990215\n",
      "epoch:  946, loss: 0.007151693571358919\n",
      "epoch:  947, loss: 0.007147527765482664\n",
      "epoch:  948, loss: 0.007144743576645851\n",
      "epoch:  949, loss: 0.007143653463572264\n",
      "epoch:  950, loss: 0.007142886985093355\n",
      "epoch:  951, loss: 0.007138862274587154\n",
      "epoch:  952, loss: 0.007135876454412937\n",
      "epoch:  953, loss: 0.007134799845516682\n",
      "epoch:  954, loss: 0.007134057115763426\n",
      "epoch:  955, loss: 0.0071304175071418285\n",
      "epoch:  956, loss: 0.007127197925001383\n",
      "epoch:  957, loss: 0.007126089185476303\n",
      "epoch:  958, loss: 0.007125333417207003\n",
      "epoch:  959, loss: 0.007121990900486708\n",
      "epoch:  960, loss: 0.007118514738976955\n",
      "epoch:  961, loss: 0.007117352914065123\n",
      "epoch:  962, loss: 0.007116577588021755\n",
      "epoch:  963, loss: 0.007113763131201267\n",
      "epoch:  964, loss: 0.007109804078936577\n",
      "epoch:  965, loss: 0.007108559366315603\n",
      "epoch:  966, loss: 0.007107775658369064\n",
      "epoch:  967, loss: 0.007104555610567331\n",
      "epoch:  968, loss: 0.007100969552993774\n",
      "epoch:  969, loss: 0.0070997485890984535\n",
      "epoch:  970, loss: 0.007098964415490627\n",
      "epoch:  971, loss: 0.007097143214195967\n",
      "epoch:  972, loss: 0.00709238275885582\n",
      "epoch:  973, loss: 0.00709096435457468\n",
      "epoch:  974, loss: 0.007090140599757433\n",
      "epoch:  975, loss: 0.007089441176503897\n",
      "epoch:  976, loss: 0.007084900047630072\n",
      "epoch:  977, loss: 0.007082520518451929\n",
      "epoch:  978, loss: 0.007081508636474609\n",
      "epoch:  979, loss: 0.0070807114243507385\n",
      "epoch:  980, loss: 0.00707687484100461\n",
      "epoch:  981, loss: 0.007074045483022928\n",
      "epoch:  982, loss: 0.007072917185723782\n",
      "epoch:  983, loss: 0.007072082720696926\n",
      "epoch:  984, loss: 0.007067786529660225\n",
      "epoch:  985, loss: 0.007065118290483952\n",
      "epoch:  986, loss: 0.007064050063490868\n",
      "epoch:  987, loss: 0.0070632342249155045\n",
      "epoch:  988, loss: 0.0070589161477983\n",
      "epoch:  989, loss: 0.007056016009300947\n",
      "epoch:  990, loss: 0.007054838817566633\n",
      "epoch:  991, loss: 0.007054018322378397\n",
      "epoch:  992, loss: 0.007049581967294216\n",
      "epoch:  993, loss: 0.007046734448522329\n",
      "epoch:  994, loss: 0.007045639678835869\n",
      "epoch:  995, loss: 0.007044844329357147\n",
      "epoch:  996, loss: 0.007040082011371851\n",
      "epoch:  997, loss: 0.007037370000034571\n",
      "epoch:  998, loss: 0.007036286406219006\n",
      "epoch:  999, loss: 0.007035489659756422\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size=X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# opt = torch_numopt.ConjugateGradient(model=model, lr=1e-4, line_search_method=\"const\", cg_method=\"FR\")\n",
    "opt = torch_numopt.ConjugateGradient(model=model, lr=10, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"armijo\", cg_method=\"FR\")\n",
    "# opt = torch_numopt.ConjugateGradient(model=model, lr=10, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"wolfe\", cg_method=\"FR\")\n",
    "# opt = torch_numopt.ConjugateGradient(model=model, lr=10, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"strong-wolfe\", cg_method=\"FR\")\n",
    "# opt = torch_numopt.ConjugateGradient(model=model, lr=10, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"goldstein\", cg_method=\"FR\")\n",
    "\n",
    "all_loss = {}\n",
    "for epoch in range(1000):\n",
    "    print(\"epoch: \", epoch, end=\"\")\n",
    "    all_loss[epoch + 1] = 0\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y, loss_fn)\n",
    "\n",
    "        all_loss[epoch + 1] += loss\n",
    "    all_loss[epoch + 1] /= len(data_loader)\n",
    "    print(\", loss: {}\".format(all_loss[epoch + 1].cpu().detach().numpy().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics: R2 = 0.7428101024457374\n",
      "Test metrics:  R2 = 0.7435365749712606\n"
     ]
    }
   ],
   "source": [
    "pred_train = model.forward(X_train).cpu().detach()\n",
    "pred_test = model.forward(X_test).cpu().detach()\n",
    "print(f\"Train metrics: R2 = {r2_score(pred_train, y_train.cpu())}\")\n",
    "print(f\"Test metrics:  R2 = {r2_score(pred_test, y_test.cpu())}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_numopt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
