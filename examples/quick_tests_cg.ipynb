{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch_numopt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import *\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.f1 = nn.Linear(input_size, 10, device=device)\n",
    "        self.f2 = nn.Linear(10, 20, device=device)\n",
    "        self.f3 = nn.Linear(20, 20, device=device)\n",
    "        self.f4 = nn.Linear(20, 10, device=device)\n",
    "        self.f5 = nn.Linear(10, 1, device=device)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        # self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.f1(x))\n",
    "        x = self.activation(self.f2(x))\n",
    "        x = self.activation(self.f3(x))\n",
    "        x = self.activation(self.f4(x))\n",
    "        x = self.f5(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([800, 10]) torch.Size([800, 1])\n",
      "torch.Size([200, 10]) torch.Size([200, 1])\n"
     ]
    }
   ],
   "source": [
    "# X, y = load_diabetes(return_X_y=True, scaled=False)\n",
    "# X, y = make_regression(n_samples=1000, n_features=100)\n",
    "X, y = make_friedman1(n_samples=1000, noise=1e-2)\n",
    "\n",
    "X_scaler = MinMaxScaler()\n",
    "X = X_scaler.fit_transform(X)\n",
    "X = torch.Tensor(X).to(device)\n",
    "\n",
    "y_scaler = MinMaxScaler()\n",
    "y = y_scaler.fit_transform(y.reshape((-1, 1)))\n",
    "y = torch.Tensor(y).to(device)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "torch_data = TensorDataset(X_train, y_train)\n",
    "data_loader = DataLoader(torch_data, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.40786683559417725\n",
      "epoch:  1, loss: 0.23815593123435974\n",
      "epoch:  2, loss: 0.23815593123435974\n",
      "epoch:  3, loss: 0.04747726395726204\n",
      "epoch:  4, loss: 0.04747726395726204\n",
      "epoch:  5, loss: 0.03552425280213356\n",
      "epoch:  6, loss: 0.03432055562734604\n",
      "epoch:  7, loss: 0.03406721353530884\n",
      "epoch:  8, loss: 0.03406721353530884\n",
      "epoch:  9, loss: 0.03391239419579506\n",
      "epoch:  10, loss: 0.033842816948890686\n",
      "epoch:  11, loss: 0.033842816948890686\n",
      "epoch:  12, loss: 0.033614709973335266\n",
      "epoch:  13, loss: 0.03319547325372696\n",
      "epoch:  14, loss: 0.03319384902715683\n",
      "epoch:  15, loss: 0.032414402812719345\n",
      "epoch:  16, loss: 0.032264236360788345\n",
      "epoch:  17, loss: 0.032042406499385834\n",
      "epoch:  18, loss: 0.03204220533370972\n",
      "epoch:  19, loss: 0.03195972740650177\n",
      "epoch:  20, loss: 0.031941007822752\n",
      "epoch:  21, loss: 0.031478844583034515\n",
      "epoch:  22, loss: 0.031478844583034515\n",
      "epoch:  23, loss: 0.03147903457283974\n",
      "epoch:  24, loss: 0.03143925964832306\n",
      "epoch:  25, loss: 0.03090745396912098\n",
      "epoch:  26, loss: 0.030849551782011986\n",
      "epoch:  27, loss: 0.030397847294807434\n",
      "epoch:  28, loss: 0.030345819890499115\n",
      "epoch:  29, loss: 0.030345819890499115\n",
      "epoch:  30, loss: 0.03033704124391079\n",
      "epoch:  31, loss: 0.02645658515393734\n",
      "epoch:  32, loss: 0.026057491078972816\n",
      "epoch:  33, loss: 0.025972213596105576\n",
      "epoch:  34, loss: 0.024798059836030006\n",
      "epoch:  35, loss: 0.023655040189623833\n",
      "epoch:  36, loss: 0.023655040189623833\n",
      "epoch:  37, loss: 0.023655042052268982\n",
      "epoch:  38, loss: 0.02358315885066986\n",
      "epoch:  39, loss: 0.02346637472510338\n",
      "epoch:  40, loss: 0.02346637472510338\n",
      "epoch:  41, loss: 0.02346639707684517\n",
      "epoch:  42, loss: 0.02344362996518612\n",
      "epoch:  43, loss: 0.02310195006430149\n",
      "epoch:  44, loss: 0.022202126681804657\n",
      "epoch:  45, loss: 0.019567009061574936\n",
      "epoch:  46, loss: 0.01856171153485775\n",
      "epoch:  47, loss: 0.017245950177311897\n",
      "epoch:  48, loss: 0.01716807670891285\n",
      "epoch:  49, loss: 0.01716807670891285\n",
      "epoch:  50, loss: 0.01716807670891285\n",
      "epoch:  51, loss: 0.0171680748462677\n",
      "epoch:  52, loss: 0.0171680748462677\n",
      "epoch:  53, loss: 0.01716436631977558\n",
      "epoch:  54, loss: 0.017006753012537956\n",
      "epoch:  55, loss: 0.017006753012537956\n",
      "epoch:  56, loss: 0.017008699476718903\n",
      "epoch:  57, loss: 0.012049160897731781\n",
      "epoch:  58, loss: 0.012049160897731781\n",
      "epoch:  59, loss: 0.011491093784570694\n",
      "epoch:  60, loss: 0.011394449509680271\n",
      "epoch:  61, loss: 0.011394449509680271\n",
      "epoch:  62, loss: 0.010837798938155174\n",
      "epoch:  63, loss: 0.009446302428841591\n",
      "epoch:  64, loss: 0.009275713935494423\n",
      "epoch:  65, loss: 0.009258326143026352\n",
      "epoch:  66, loss: 0.009258326143026352\n",
      "epoch:  67, loss: 0.009258326143026352\n",
      "epoch:  68, loss: 0.009257047437131405\n",
      "epoch:  69, loss: 0.009099133312702179\n",
      "epoch:  70, loss: 0.009075051173567772\n",
      "epoch:  71, loss: 0.009027109481394291\n",
      "epoch:  72, loss: 0.00898678693920374\n",
      "epoch:  73, loss: 0.008986632339656353\n",
      "epoch:  74, loss: 0.00898220855742693\n",
      "epoch:  75, loss: 0.008955809287726879\n",
      "epoch:  76, loss: 0.008955809287726879\n",
      "epoch:  77, loss: 0.008947333320975304\n",
      "epoch:  78, loss: 0.008912067860364914\n",
      "epoch:  79, loss: 0.008864928968250751\n",
      "epoch:  80, loss: 0.008851159363985062\n",
      "epoch:  81, loss: 0.008849225006997585\n",
      "epoch:  82, loss: 0.008849225006997585\n",
      "epoch:  83, loss: 0.008849225006997585\n",
      "epoch:  84, loss: 0.008849225006997585\n",
      "epoch:  85, loss: 0.00884922593832016\n",
      "epoch:  86, loss: 0.008849225006997585\n",
      "epoch:  87, loss: 0.008848574012517929\n",
      "epoch:  88, loss: 0.008841879665851593\n",
      "epoch:  89, loss: 0.008650559931993484\n",
      "epoch:  90, loss: 0.008513257838785648\n",
      "epoch:  91, loss: 0.008510190062224865\n",
      "epoch:  92, loss: 0.008510190062224865\n",
      "epoch:  93, loss: 0.00850928109139204\n",
      "epoch:  94, loss: 0.008504660800099373\n",
      "epoch:  95, loss: 0.008504660800099373\n",
      "epoch:  96, loss: 0.008503871038556099\n",
      "epoch:  97, loss: 0.00846029445528984\n",
      "epoch:  98, loss: 0.008304969407618046\n",
      "epoch:  99, loss: 0.008300255984067917\n",
      "epoch:  100, loss: 0.008299313485622406\n",
      "epoch:  101, loss: 0.008299313485622406\n",
      "epoch:  102, loss: 0.008292842656373978\n",
      "epoch:  103, loss: 0.008286518976092339\n",
      "epoch:  104, loss: 0.008285286836326122\n",
      "epoch:  105, loss: 0.008285286836326122\n",
      "epoch:  106, loss: 0.008285286836326122\n",
      "epoch:  107, loss: 0.008285286836326122\n",
      "epoch:  108, loss: 0.008285286836326122\n",
      "epoch:  109, loss: 0.008276230655610561\n",
      "epoch:  110, loss: 0.008276230655610561\n",
      "epoch:  111, loss: 0.00827314518392086\n",
      "epoch:  112, loss: 0.00827314518392086\n",
      "epoch:  113, loss: 0.00827314518392086\n",
      "epoch:  114, loss: 0.0082718376070261\n",
      "epoch:  115, loss: 0.0082718376070261\n",
      "epoch:  116, loss: 0.00826983992010355\n",
      "epoch:  117, loss: 0.008266502991318703\n",
      "epoch:  118, loss: 0.008266502991318703\n",
      "epoch:  119, loss: 0.00822882167994976\n",
      "epoch:  120, loss: 0.008226200006902218\n",
      "epoch:  121, loss: 0.008225683122873306\n",
      "epoch:  122, loss: 0.008225683122873306\n",
      "epoch:  123, loss: 0.008225683122873306\n",
      "epoch:  124, loss: 0.008225683122873306\n",
      "epoch:  125, loss: 0.008225683122873306\n",
      "epoch:  126, loss: 0.008225683122873306\n",
      "epoch:  127, loss: 0.008213656023144722\n",
      "epoch:  128, loss: 0.00821211189031601\n",
      "epoch:  129, loss: 0.00821211189031601\n",
      "epoch:  130, loss: 0.008212112821638584\n",
      "epoch:  131, loss: 0.008211635053157806\n",
      "epoch:  132, loss: 0.008211635053157806\n",
      "epoch:  133, loss: 0.008196739479899406\n",
      "epoch:  134, loss: 0.00819583609700203\n",
      "epoch:  135, loss: 0.00819583609700203\n",
      "epoch:  136, loss: 0.00819583609700203\n",
      "epoch:  137, loss: 0.008195824921131134\n",
      "epoch:  138, loss: 0.008195824921131134\n",
      "epoch:  139, loss: 0.00819490198045969\n",
      "epoch:  140, loss: 0.008194100111722946\n",
      "epoch:  141, loss: 0.008194100111722946\n",
      "epoch:  142, loss: 0.008194100111722946\n",
      "epoch:  143, loss: 0.008194100111722946\n",
      "epoch:  144, loss: 0.008190355263650417\n",
      "epoch:  145, loss: 0.008189491927623749\n",
      "epoch:  146, loss: 0.00816420279443264\n",
      "epoch:  147, loss: 0.008147770538926125\n",
      "epoch:  148, loss: 0.008141856640577316\n",
      "epoch:  149, loss: 0.00814118143171072\n",
      "epoch:  150, loss: 0.00814118143171072\n",
      "epoch:  151, loss: 0.008141182363033295\n",
      "epoch:  152, loss: 0.008141066879034042\n",
      "epoch:  153, loss: 0.00812529120594263\n",
      "epoch:  154, loss: 0.008122557774186134\n",
      "epoch:  155, loss: 0.008119127713143826\n",
      "epoch:  156, loss: 0.00811862014234066\n",
      "epoch:  157, loss: 0.00811862014234066\n",
      "epoch:  158, loss: 0.00811862014234066\n",
      "epoch:  159, loss: 0.00811862014234066\n",
      "epoch:  160, loss: 0.008118697442114353\n",
      "epoch:  161, loss: 0.008118394762277603\n",
      "epoch:  162, loss: 0.008118394762277603\n",
      "epoch:  163, loss: 0.0081081697717309\n",
      "epoch:  164, loss: 0.008103935047984123\n",
      "epoch:  165, loss: 0.00810334738343954\n",
      "epoch:  166, loss: 0.008103292435407639\n",
      "epoch:  167, loss: 0.0080747464671731\n",
      "epoch:  168, loss: 0.008068950846791267\n",
      "epoch:  169, loss: 0.008068322204053402\n",
      "epoch:  170, loss: 0.008068322204053402\n",
      "epoch:  171, loss: 0.008065648376941681\n",
      "epoch:  172, loss: 0.008064720779657364\n",
      "epoch:  173, loss: 0.008064720779657364\n",
      "epoch:  174, loss: 0.008064722642302513\n",
      "epoch:  175, loss: 0.008064186200499535\n",
      "epoch:  176, loss: 0.008064186200499535\n",
      "epoch:  177, loss: 0.008064188994467258\n",
      "epoch:  178, loss: 0.008060774765908718\n",
      "epoch:  179, loss: 0.008060774765908718\n",
      "epoch:  180, loss: 0.008060774765908718\n",
      "epoch:  181, loss: 0.008059728890657425\n",
      "epoch:  182, loss: 0.00805913656949997\n",
      "epoch:  183, loss: 0.00805913656949997\n",
      "epoch:  184, loss: 0.008051044307649136\n",
      "epoch:  185, loss: 0.008049894124269485\n",
      "epoch:  186, loss: 0.008049797266721725\n",
      "epoch:  187, loss: 0.008047593757510185\n",
      "epoch:  188, loss: 0.008047593757510185\n",
      "epoch:  189, loss: 0.008045091293752193\n",
      "epoch:  190, loss: 0.008039740845561028\n",
      "epoch:  191, loss: 0.008038743399083614\n",
      "epoch:  192, loss: 0.008038743399083614\n",
      "epoch:  193, loss: 0.008038743399083614\n",
      "epoch:  194, loss: 0.008038744330406189\n",
      "epoch:  195, loss: 0.008038744330406189\n",
      "epoch:  196, loss: 0.008010565303266048\n",
      "epoch:  197, loss: 0.00800289772450924\n",
      "epoch:  198, loss: 0.007998358458280563\n",
      "epoch:  199, loss: 0.007997548207640648\n",
      "epoch:  200, loss: 0.00799751840531826\n",
      "epoch:  201, loss: 0.00799751840531826\n",
      "epoch:  202, loss: 0.00799751840531826\n",
      "epoch:  203, loss: 0.007988879457116127\n",
      "epoch:  204, loss: 0.007988291792571545\n",
      "epoch:  205, loss: 0.007988289929926395\n",
      "epoch:  206, loss: 0.007988289929926395\n",
      "epoch:  207, loss: 0.007988289929926395\n",
      "epoch:  208, loss: 0.00798744335770607\n",
      "epoch:  209, loss: 0.00798724964261055\n",
      "epoch:  210, loss: 0.00798724964261055\n",
      "epoch:  211, loss: 0.00798724964261055\n",
      "epoch:  212, loss: 0.007949694991111755\n",
      "epoch:  213, loss: 0.007948894053697586\n",
      "epoch:  214, loss: 0.007948300801217556\n",
      "epoch:  215, loss: 0.007899715565145016\n",
      "epoch:  216, loss: 0.007899715565145016\n",
      "epoch:  217, loss: 0.007899715565145016\n",
      "epoch:  218, loss: 0.007899715565145016\n",
      "epoch:  219, loss: 0.007899715565145016\n",
      "epoch:  220, loss: 0.007899715565145016\n",
      "epoch:  221, loss: 0.007896981202065945\n",
      "epoch:  222, loss: 0.007896349765360355\n",
      "epoch:  223, loss: 0.007896349765360355\n",
      "epoch:  224, loss: 0.007895682007074356\n",
      "epoch:  225, loss: 0.007893801666796207\n",
      "epoch:  226, loss: 0.007893702946603298\n",
      "epoch:  227, loss: 0.007853245362639427\n",
      "epoch:  228, loss: 0.00785263255238533\n",
      "epoch:  229, loss: 0.00785263255238533\n",
      "epoch:  230, loss: 0.007846181280910969\n",
      "epoch:  231, loss: 0.007845607586205006\n",
      "epoch:  232, loss: 0.007845607586205006\n",
      "epoch:  233, loss: 0.007843103259801865\n",
      "epoch:  234, loss: 0.007842541672289371\n",
      "epoch:  235, loss: 0.007842541672289371\n",
      "epoch:  236, loss: 0.00783010944724083\n",
      "epoch:  237, loss: 0.007791645359247923\n",
      "epoch:  238, loss: 0.007791025098413229\n",
      "epoch:  239, loss: 0.007790051866322756\n",
      "epoch:  240, loss: 0.007790051866322756\n",
      "epoch:  241, loss: 0.007790056057274342\n",
      "epoch:  242, loss: 0.007789603900164366\n",
      "epoch:  243, loss: 0.007789603900164366\n",
      "epoch:  244, loss: 0.007777833379805088\n",
      "epoch:  245, loss: 0.007777833379805088\n",
      "epoch:  246, loss: 0.00777479587122798\n",
      "epoch:  247, loss: 0.007774221710860729\n",
      "epoch:  248, loss: 0.007774221245199442\n",
      "epoch:  249, loss: 0.007771101780235767\n",
      "epoch:  250, loss: 0.007770376745611429\n",
      "epoch:  251, loss: 0.007770376745611429\n",
      "epoch:  252, loss: 0.0077659147791564465\n",
      "epoch:  253, loss: 0.007764068432152271\n",
      "epoch:  254, loss: 0.007763742934912443\n",
      "epoch:  255, loss: 0.007763742934912443\n",
      "epoch:  256, loss: 0.007765388581901789\n",
      "epoch:  257, loss: 0.007745953742414713\n",
      "epoch:  258, loss: 0.007736706640571356\n",
      "epoch:  259, loss: 0.0077302525751292706\n",
      "epoch:  260, loss: 0.007729609031230211\n",
      "epoch:  261, loss: 0.007729609031230211\n",
      "epoch:  262, loss: 0.007587014231830835\n",
      "epoch:  263, loss: 0.007578346878290176\n",
      "epoch:  264, loss: 0.00757621368393302\n",
      "epoch:  265, loss: 0.00757621368393302\n",
      "epoch:  266, loss: 0.007534537464380264\n",
      "epoch:  267, loss: 0.007530702743679285\n",
      "epoch:  268, loss: 0.007530702743679285\n",
      "epoch:  269, loss: 0.007530759554356337\n",
      "epoch:  270, loss: 0.007525677792727947\n",
      "epoch:  271, loss: 0.007524403277784586\n",
      "epoch:  272, loss: 0.007524402812123299\n",
      "epoch:  273, loss: 0.007516085635870695\n",
      "epoch:  274, loss: 0.007512244395911694\n",
      "epoch:  275, loss: 0.007511540781706572\n",
      "epoch:  276, loss: 0.007511539850383997\n",
      "epoch:  277, loss: 0.007385811302810907\n",
      "epoch:  278, loss: 0.0073769246228039265\n",
      "epoch:  279, loss: 0.0073746321722865105\n",
      "epoch:  280, loss: 0.007374631706625223\n",
      "epoch:  281, loss: 0.007374631706625223\n",
      "epoch:  282, loss: 0.00736818416044116\n",
      "epoch:  283, loss: 0.007364618591964245\n",
      "epoch:  284, loss: 0.007360728457570076\n",
      "epoch:  285, loss: 0.007359194569289684\n",
      "epoch:  286, loss: 0.007359194569289684\n",
      "epoch:  287, loss: 0.007359195966273546\n",
      "epoch:  288, loss: 0.007274864241480827\n",
      "epoch:  289, loss: 0.007274863310158253\n",
      "epoch:  290, loss: 0.007274863310158253\n",
      "epoch:  291, loss: 0.007273667026311159\n",
      "epoch:  292, loss: 0.00727221742272377\n",
      "epoch:  293, loss: 0.00725123705342412\n",
      "epoch:  294, loss: 0.007243081461638212\n",
      "epoch:  295, loss: 0.007242381572723389\n",
      "epoch:  296, loss: 0.007242381572723389\n",
      "epoch:  297, loss: 0.007242381572723389\n",
      "epoch:  298, loss: 0.007242381572723389\n",
      "epoch:  299, loss: 0.007234138902276754\n",
      "epoch:  300, loss: 0.007234138436615467\n",
      "epoch:  301, loss: 0.007234138436615467\n",
      "epoch:  302, loss: 0.007233227603137493\n",
      "epoch:  303, loss: 0.007233227603137493\n",
      "epoch:  304, loss: 0.007233230862766504\n",
      "epoch:  305, loss: 0.007233199663460255\n",
      "epoch:  306, loss: 0.007233199663460255\n",
      "epoch:  307, loss: 0.007232224568724632\n",
      "epoch:  308, loss: 0.007232224568724632\n",
      "epoch:  309, loss: 0.007230677176266909\n",
      "epoch:  310, loss: 0.00722968764603138\n",
      "epoch:  311, loss: 0.00722968764603138\n",
      "epoch:  312, loss: 0.00722968764603138\n",
      "epoch:  313, loss: 0.00722968764603138\n",
      "epoch:  314, loss: 0.00722968764603138\n",
      "epoch:  315, loss: 0.00722500728443265\n",
      "epoch:  316, loss: 0.00722500728443265\n",
      "epoch:  317, loss: 0.00722500728443265\n",
      "epoch:  318, loss: 0.00722500728443265\n",
      "epoch:  319, loss: 0.00722500728443265\n",
      "epoch:  320, loss: 0.007220529485493898\n",
      "epoch:  321, loss: 0.007219753228127956\n",
      "epoch:  322, loss: 0.007219753228127956\n",
      "epoch:  323, loss: 0.007219752762466669\n",
      "epoch:  324, loss: 0.00685938959941268\n",
      "epoch:  325, loss: 0.00685938959941268\n",
      "epoch:  326, loss: 0.006811116822063923\n",
      "epoch:  327, loss: 0.006737611722201109\n",
      "epoch:  328, loss: 0.006725594867020845\n",
      "epoch:  329, loss: 0.006723459810018539\n",
      "epoch:  330, loss: 0.0066923159174621105\n",
      "epoch:  331, loss: 0.006689871661365032\n",
      "epoch:  332, loss: 0.006689871661365032\n",
      "epoch:  333, loss: 0.006689872592687607\n",
      "epoch:  334, loss: 0.006689872592687607\n",
      "epoch:  335, loss: 0.006682796869426966\n",
      "epoch:  336, loss: 0.006677227094769478\n",
      "epoch:  337, loss: 0.006677227094769478\n",
      "epoch:  338, loss: 0.006677233148366213\n",
      "epoch:  339, loss: 0.00667354604229331\n",
      "epoch:  340, loss: 0.006673544645309448\n",
      "epoch:  341, loss: 0.006630482152104378\n",
      "epoch:  342, loss: 0.006620330270379782\n",
      "epoch:  343, loss: 0.0066176350228488445\n",
      "epoch:  344, loss: 0.0066176350228488445\n",
      "epoch:  345, loss: 0.006617712322622538\n",
      "epoch:  346, loss: 0.006592137739062309\n",
      "epoch:  347, loss: 0.006591447163373232\n",
      "epoch:  348, loss: 0.006586430594325066\n",
      "epoch:  349, loss: 0.006585231516510248\n",
      "epoch:  350, loss: 0.006585231050848961\n",
      "epoch:  351, loss: 0.006554603110998869\n",
      "epoch:  352, loss: 0.006540057715028524\n",
      "epoch:  353, loss: 0.006536708679050207\n",
      "epoch:  354, loss: 0.006536708679050207\n",
      "epoch:  355, loss: 0.0064504812471568584\n",
      "epoch:  356, loss: 0.00644075870513916\n",
      "epoch:  357, loss: 0.006437718402594328\n",
      "epoch:  358, loss: 0.006437718402594328\n",
      "epoch:  359, loss: 0.006425065454095602\n",
      "epoch:  360, loss: 0.00642248522490263\n",
      "epoch:  361, loss: 0.006409381050616503\n",
      "epoch:  362, loss: 0.006409381050616503\n",
      "epoch:  363, loss: 0.006400624290108681\n",
      "epoch:  364, loss: 0.006374899763613939\n",
      "epoch:  365, loss: 0.006369596812874079\n",
      "epoch:  366, loss: 0.006369239185005426\n",
      "epoch:  367, loss: 0.006238183472305536\n",
      "epoch:  368, loss: 0.006238183472305536\n",
      "epoch:  369, loss: 0.0062384698539972305\n",
      "epoch:  370, loss: 0.006165077909827232\n",
      "epoch:  371, loss: 0.006135658826678991\n",
      "epoch:  372, loss: 0.006135658826678991\n",
      "epoch:  373, loss: 0.0061328173615038395\n",
      "epoch:  374, loss: 0.006124638486653566\n",
      "epoch:  375, loss: 0.006122410763055086\n",
      "epoch:  376, loss: 0.006122410763055086\n",
      "epoch:  377, loss: 0.006122412160038948\n",
      "epoch:  378, loss: 0.0060752881690859795\n",
      "epoch:  379, loss: 0.0060752881690859795\n",
      "epoch:  380, loss: 0.006053285673260689\n",
      "epoch:  381, loss: 0.006050643976777792\n",
      "epoch:  382, loss: 0.006050643976777792\n",
      "epoch:  383, loss: 0.5712977647781372\n",
      "epoch:  384, loss: 0.5712977647781372\n",
      "epoch:  385, loss: 0.1567314714193344\n",
      "epoch:  386, loss: 0.03621673956513405\n",
      "epoch:  387, loss: 0.021108422428369522\n",
      "epoch:  388, loss: 0.021108422428369522\n",
      "epoch:  389, loss: 0.014926067553460598\n",
      "epoch:  390, loss: 0.014921263791620731\n",
      "epoch:  391, loss: 0.014921263791620731\n",
      "epoch:  392, loss: 0.012256625108420849\n",
      "epoch:  393, loss: 0.009114393964409828\n",
      "epoch:  394, loss: 0.009114393964409828\n",
      "epoch:  395, loss: 0.008871502242982388\n",
      "epoch:  396, loss: 0.008871502242982388\n",
      "epoch:  397, loss: 0.014447375200688839\n",
      "epoch:  398, loss: 0.01252509094774723\n",
      "epoch:  399, loss: 0.010444510728120804\n",
      "epoch:  400, loss: 0.010444510728120804\n",
      "epoch:  401, loss: 0.009248764254152775\n",
      "epoch:  402, loss: 0.008935059420764446\n",
      "epoch:  403, loss: 0.008935059420764446\n",
      "epoch:  404, loss: 0.008711326867341995\n",
      "epoch:  405, loss: 0.00866215955466032\n",
      "epoch:  406, loss: 0.008526738733053207\n",
      "epoch:  407, loss: 0.008526738733053207\n",
      "epoch:  408, loss: 0.008527214638888836\n",
      "epoch:  409, loss: 0.00846076849848032\n",
      "epoch:  410, loss: 0.00846076849848032\n",
      "epoch:  411, loss: 0.00844787061214447\n",
      "epoch:  412, loss: 0.008422222919762135\n",
      "epoch:  413, loss: 0.008422222919762135\n",
      "epoch:  414, loss: 0.008405577391386032\n",
      "epoch:  415, loss: 0.00832462403923273\n",
      "epoch:  416, loss: 0.00832462403923273\n",
      "epoch:  417, loss: 0.008300593122839928\n",
      "epoch:  418, loss: 0.008300593122839928\n",
      "epoch:  419, loss: 0.00828805472701788\n",
      "epoch:  420, loss: 0.00827563926577568\n",
      "epoch:  421, loss: 0.00827563926577568\n",
      "epoch:  422, loss: 0.008275753818452358\n",
      "epoch:  423, loss: 0.008256970904767513\n",
      "epoch:  424, loss: 0.008256970904767513\n",
      "epoch:  425, loss: 0.008258838206529617\n",
      "epoch:  426, loss: 0.00825619138777256\n",
      "epoch:  427, loss: 0.00816416647285223\n",
      "epoch:  428, loss: 0.00816416647285223\n",
      "epoch:  429, loss: 0.008101403713226318\n",
      "epoch:  430, loss: 0.008083295077085495\n",
      "epoch:  431, loss: 0.008058564737439156\n",
      "epoch:  432, loss: 0.007629741914570332\n",
      "epoch:  433, loss: 0.007629741914570332\n",
      "epoch:  434, loss: 0.007631026674062014\n",
      "epoch:  435, loss: 0.007314739748835564\n",
      "epoch:  436, loss: 0.007086530327796936\n",
      "epoch:  437, loss: 0.007086530327796936\n",
      "epoch:  438, loss: 0.006844648160040379\n",
      "epoch:  439, loss: 0.006811854895204306\n",
      "epoch:  440, loss: 0.006811854895204306\n",
      "epoch:  441, loss: 0.006771951448172331\n",
      "epoch:  442, loss: 0.006769901607185602\n",
      "epoch:  443, loss: 0.006741208955645561\n",
      "epoch:  444, loss: 0.00671132979914546\n",
      "epoch:  445, loss: 0.006703730206936598\n",
      "epoch:  446, loss: 0.006700258236378431\n",
      "epoch:  447, loss: 0.00668311258777976\n",
      "epoch:  448, loss: 0.00666961632668972\n",
      "epoch:  449, loss: 0.00666961632668972\n",
      "epoch:  450, loss: 0.006662838626652956\n",
      "epoch:  451, loss: 0.006659884471446276\n",
      "epoch:  452, loss: 0.006659884471446276\n",
      "epoch:  453, loss: 0.006659886334091425\n",
      "epoch:  454, loss: 0.006656969897449017\n",
      "epoch:  455, loss: 0.0066494871862232685\n",
      "epoch:  456, loss: 0.006387930363416672\n",
      "epoch:  457, loss: 0.006387930363416672\n",
      "epoch:  458, loss: 0.006376249250024557\n",
      "epoch:  459, loss: 0.006371744908392429\n",
      "epoch:  460, loss: 0.006371744908392429\n",
      "epoch:  461, loss: 0.006369916256517172\n",
      "epoch:  462, loss: 0.006362931802868843\n",
      "epoch:  463, loss: 0.006362930405884981\n",
      "epoch:  464, loss: 0.006360670551657677\n",
      "epoch:  465, loss: 0.006348440423607826\n",
      "epoch:  466, loss: 0.006345158442854881\n",
      "epoch:  467, loss: 0.006345158442854881\n",
      "epoch:  468, loss: 0.006343611050397158\n",
      "epoch:  469, loss: 0.0062148692086339\n",
      "epoch:  470, loss: 0.006122133694589138\n",
      "epoch:  471, loss: 0.0061109596863389015\n",
      "epoch:  472, loss: 0.0061109596863389015\n",
      "epoch:  473, loss: 0.006110984832048416\n",
      "epoch:  474, loss: 0.006102616898715496\n",
      "epoch:  475, loss: 0.006102616898715496\n",
      "epoch:  476, loss: 0.006072496064007282\n",
      "epoch:  477, loss: 0.006072496064007282\n",
      "epoch:  478, loss: 0.0060729412361979485\n",
      "epoch:  479, loss: 0.006061940919607878\n",
      "epoch:  480, loss: 0.006047331262379885\n",
      "epoch:  481, loss: 0.00603526784107089\n",
      "epoch:  482, loss: 0.0059948875568807125\n",
      "epoch:  483, loss: 0.005990203004330397\n",
      "epoch:  484, loss: 0.005932665895670652\n",
      "epoch:  485, loss: 0.00591603759676218\n",
      "epoch:  486, loss: 0.005909676663577557\n",
      "epoch:  487, loss: 0.0059092091396451\n",
      "epoch:  488, loss: 0.0059092091396451\n",
      "epoch:  489, loss: 0.005902932491153479\n",
      "epoch:  490, loss: 0.005902932491153479\n",
      "epoch:  491, loss: 0.005900999531149864\n",
      "epoch:  492, loss: 0.005897932220250368\n",
      "epoch:  493, loss: 0.005897932220250368\n",
      "epoch:  494, loss: 0.00589793361723423\n",
      "epoch:  495, loss: 0.005894133821129799\n",
      "epoch:  496, loss: 0.005894133355468512\n",
      "epoch:  497, loss: 0.005894143600016832\n",
      "epoch:  498, loss: 0.005886211525648832\n",
      "epoch:  499, loss: 0.005886211525648832\n",
      "epoch:  500, loss: 0.0058826575987041\n",
      "epoch:  501, loss: 0.005833239760249853\n",
      "epoch:  502, loss: 0.005810835398733616\n",
      "epoch:  503, loss: 0.005784152075648308\n",
      "epoch:  504, loss: 0.005780684296041727\n",
      "epoch:  505, loss: 0.00578068383038044\n",
      "epoch:  506, loss: 0.005780691280961037\n",
      "epoch:  507, loss: 0.00577777624130249\n",
      "epoch:  508, loss: 0.00577777624130249\n",
      "epoch:  509, loss: 0.005776798352599144\n",
      "epoch:  510, loss: 0.00577210308983922\n",
      "epoch:  511, loss: 0.00577210308983922\n",
      "epoch:  512, loss: 0.005772102624177933\n",
      "epoch:  513, loss: 0.005772102624177933\n",
      "epoch:  514, loss: 0.005770205054432154\n",
      "epoch:  515, loss: 0.005770205054432154\n",
      "epoch:  516, loss: 0.005769564304500818\n",
      "epoch:  517, loss: 0.005715389270335436\n",
      "epoch:  518, loss: 0.005715389270335436\n",
      "epoch:  519, loss: 0.0057154023088514805\n",
      "epoch:  520, loss: 0.005715401377528906\n",
      "epoch:  521, loss: 0.005603461991995573\n",
      "epoch:  522, loss: 0.005556174553930759\n",
      "epoch:  523, loss: 0.005549423862248659\n",
      "epoch:  524, loss: 0.005549423862248659\n",
      "epoch:  525, loss: 0.005549424793571234\n",
      "epoch:  526, loss: 0.005542668513953686\n",
      "epoch:  527, loss: 0.005542668513953686\n",
      "epoch:  528, loss: 0.00554268341511488\n",
      "epoch:  529, loss: 0.0055401925928890705\n",
      "epoch:  530, loss: 0.0055401925928890705\n",
      "epoch:  531, loss: 0.00553929153829813\n",
      "epoch:  532, loss: 0.00553929153829813\n",
      "epoch:  533, loss: 0.00553929153829813\n",
      "epoch:  534, loss: 0.005515808239579201\n",
      "epoch:  535, loss: 0.005515339784324169\n",
      "epoch:  536, loss: 0.005515339784324169\n",
      "epoch:  537, loss: 0.005514801014214754\n",
      "epoch:  538, loss: 0.005514801014214754\n",
      "epoch:  539, loss: 0.005514801014214754\n",
      "epoch:  540, loss: 0.005437295883893967\n",
      "epoch:  541, loss: 0.005427428521215916\n",
      "epoch:  542, loss: 0.00541658652946353\n",
      "epoch:  543, loss: 0.005401656497269869\n",
      "epoch:  544, loss: 0.005400872323662043\n",
      "epoch:  545, loss: 0.005400872323662043\n",
      "epoch:  546, loss: 0.005400874186307192\n",
      "epoch:  547, loss: 0.005400615744292736\n",
      "epoch:  548, loss: 0.005400615744292736\n",
      "epoch:  549, loss: 0.005400615744292736\n",
      "epoch:  550, loss: 0.005400615744292736\n",
      "epoch:  551, loss: 0.005400617141276598\n",
      "epoch:  552, loss: 0.00539889233186841\n",
      "epoch:  553, loss: 0.00539889233186841\n",
      "epoch:  554, loss: 0.005397634115070105\n",
      "epoch:  555, loss: 0.005397160071879625\n",
      "epoch:  556, loss: 0.005397160071879625\n",
      "epoch:  557, loss: 0.0053956168703734875\n",
      "epoch:  558, loss: 0.0053956168703734875\n",
      "epoch:  559, loss: 0.0053956164047122\n",
      "epoch:  560, loss: 0.0053956164047122\n",
      "epoch:  561, loss: 0.0053945137187838554\n",
      "epoch:  562, loss: 0.0053945137187838554\n",
      "epoch:  563, loss: 0.00539451465010643\n",
      "epoch:  564, loss: 0.005394048523157835\n",
      "epoch:  565, loss: 0.005393156781792641\n",
      "epoch:  566, loss: 0.005393087863922119\n",
      "epoch:  567, loss: 0.005390650127083063\n",
      "epoch:  568, loss: 0.005390613805502653\n",
      "epoch:  569, loss: 0.0052264779806137085\n",
      "epoch:  570, loss: 0.005223323591053486\n",
      "epoch:  571, loss: 0.005223323591053486\n",
      "epoch:  572, loss: 0.005223372019827366\n",
      "epoch:  573, loss: 0.005221230443567038\n",
      "epoch:  574, loss: 0.005221230443567038\n",
      "epoch:  575, loss: 0.005221230443567038\n",
      "epoch:  576, loss: 0.005201755091547966\n",
      "epoch:  577, loss: 0.0051992954686284065\n",
      "epoch:  578, loss: 0.005199294537305832\n",
      "epoch:  579, loss: 0.005199294537305832\n",
      "epoch:  580, loss: 0.005198269616812468\n",
      "epoch:  581, loss: 0.0051736594177782536\n",
      "epoch:  582, loss: 0.005173658952116966\n",
      "epoch:  583, loss: 0.0051695783622562885\n",
      "epoch:  584, loss: 0.005160176660865545\n",
      "epoch:  585, loss: 0.0051600150763988495\n",
      "epoch:  586, loss: 0.005159543827176094\n",
      "epoch:  587, loss: 0.005116689018905163\n",
      "epoch:  588, loss: 0.005110510624945164\n",
      "epoch:  589, loss: 0.005110510624945164\n",
      "epoch:  590, loss: 0.005110510624945164\n",
      "epoch:  591, loss: 0.005110510624945164\n",
      "epoch:  592, loss: 0.005110510624945164\n",
      "epoch:  593, loss: 0.005110510159283876\n",
      "epoch:  594, loss: 0.005110510159283876\n",
      "epoch:  595, loss: 0.005110510159283876\n",
      "epoch:  596, loss: 0.0051088701002299786\n",
      "epoch:  597, loss: 0.00510796345770359\n",
      "epoch:  598, loss: 0.00510796345770359\n",
      "epoch:  599, loss: 0.005107981152832508\n",
      "epoch:  600, loss: 0.0051075126975774765\n",
      "epoch:  601, loss: 0.0051075126975774765\n",
      "epoch:  602, loss: 0.005142654292285442\n",
      "epoch:  603, loss: 0.005080474074929953\n",
      "epoch:  604, loss: 0.005077776964753866\n",
      "epoch:  605, loss: 0.005077776964753866\n",
      "epoch:  606, loss: 0.005077295936644077\n",
      "epoch:  607, loss: 0.005077295936644077\n",
      "epoch:  608, loss: 0.005077080335468054\n",
      "epoch:  609, loss: 0.004948250483721495\n",
      "epoch:  610, loss: 0.004948250018060207\n",
      "epoch:  611, loss: 0.004948250018060207\n",
      "epoch:  612, loss: 0.004948250018060207\n",
      "epoch:  613, loss: 0.004948251415044069\n",
      "epoch:  614, loss: 0.004945375490933657\n",
      "epoch:  615, loss: 0.004945375490933657\n",
      "epoch:  616, loss: 0.0049452451057732105\n",
      "epoch:  617, loss: 0.004944781772792339\n",
      "epoch:  618, loss: 0.004944781772792339\n",
      "epoch:  619, loss: 0.004936852026730776\n",
      "epoch:  620, loss: 0.004936851095408201\n",
      "epoch:  621, loss: 0.004936852492392063\n",
      "epoch:  622, loss: 0.004837306682020426\n",
      "epoch:  623, loss: 0.004837306682020426\n",
      "epoch:  624, loss: 0.0048378449864685535\n",
      "epoch:  625, loss: 0.004828150384128094\n",
      "epoch:  626, loss: 0.004827720578759909\n",
      "epoch:  627, loss: 0.004827720113098621\n",
      "epoch:  628, loss: 0.0048277233727276325\n",
      "epoch:  629, loss: 0.004823125898838043\n",
      "epoch:  630, loss: 0.004823125433176756\n",
      "epoch:  631, loss: 0.004823125433176756\n",
      "epoch:  632, loss: 0.004823125433176756\n",
      "epoch:  633, loss: 0.004823125433176756\n",
      "epoch:  634, loss: 0.0048217796720564365\n",
      "epoch:  635, loss: 0.0048217796720564365\n",
      "epoch:  636, loss: 0.004821780603379011\n",
      "epoch:  637, loss: 0.00482134148478508\n",
      "epoch:  638, loss: 0.00482134148478508\n",
      "epoch:  639, loss: 0.004821390844881535\n",
      "epoch:  640, loss: 0.004821103997528553\n",
      "epoch:  641, loss: 0.004821103997528553\n",
      "epoch:  642, loss: 0.00482110446318984\n",
      "epoch:  643, loss: 0.004819916561245918\n",
      "epoch:  644, loss: 0.004819916561245918\n",
      "epoch:  645, loss: 0.004819916561245918\n",
      "epoch:  646, loss: 0.004819916561245918\n",
      "epoch:  647, loss: 0.004819635767489672\n",
      "epoch:  648, loss: 0.004819236695766449\n",
      "epoch:  649, loss: 0.004819236695766449\n",
      "epoch:  650, loss: 0.004819282330572605\n",
      "epoch:  651, loss: 0.004818790592253208\n",
      "epoch:  652, loss: 0.004818776156753302\n",
      "epoch:  653, loss: 0.004818776156753302\n",
      "epoch:  654, loss: 0.004818784072995186\n",
      "epoch:  655, loss: 0.004817317705601454\n",
      "epoch:  656, loss: 0.0048173172399401665\n",
      "epoch:  657, loss: 0.0048173172399401665\n",
      "epoch:  658, loss: 0.0048172627575695515\n",
      "epoch:  659, loss: 0.004816753324121237\n",
      "epoch:  660, loss: 0.004816753324121237\n",
      "epoch:  661, loss: 0.004816508386284113\n",
      "epoch:  662, loss: 0.004805274773389101\n",
      "epoch:  663, loss: 0.004805274773389101\n",
      "epoch:  664, loss: 0.004697100725024939\n",
      "epoch:  665, loss: 0.004681560210883617\n",
      "epoch:  666, loss: 0.004681560210883617\n",
      "epoch:  667, loss: 0.004681560210883617\n",
      "epoch:  668, loss: 0.004664914216846228\n",
      "epoch:  669, loss: 0.004662633873522282\n",
      "epoch:  670, loss: 0.0046281893737614155\n",
      "epoch:  671, loss: 0.00456515047699213\n",
      "epoch:  672, loss: 0.004560084082186222\n",
      "epoch:  673, loss: 0.004519359674304724\n",
      "epoch:  674, loss: 0.004519359674304724\n",
      "epoch:  675, loss: 0.004502528812736273\n",
      "epoch:  676, loss: 0.004502528812736273\n",
      "epoch:  677, loss: 0.004502528812736273\n",
      "epoch:  678, loss: 0.004502528812736273\n",
      "epoch:  679, loss: 0.004502549767494202\n",
      "epoch:  680, loss: 0.004496659617871046\n",
      "epoch:  681, loss: 0.004496659617871046\n",
      "epoch:  682, loss: 0.0044903215020895\n",
      "epoch:  683, loss: 0.004406204912811518\n",
      "epoch:  684, loss: 0.004405902698636055\n",
      "epoch:  685, loss: 0.004405902232974768\n",
      "epoch:  686, loss: 0.004405902698636055\n",
      "epoch:  687, loss: 0.004368266556411982\n",
      "epoch:  688, loss: 0.004366649780422449\n",
      "epoch:  689, loss: 0.004366649314761162\n",
      "epoch:  690, loss: 0.004366649314761162\n",
      "epoch:  691, loss: 0.004348427522927523\n",
      "epoch:  692, loss: 0.004331513307988644\n",
      "epoch:  693, loss: 0.004331512842327356\n",
      "epoch:  694, loss: 0.004331512842327356\n",
      "epoch:  695, loss: 0.004329896066337824\n",
      "epoch:  696, loss: 0.004329896066337824\n",
      "epoch:  697, loss: 0.004329902585595846\n",
      "epoch:  698, loss: 0.00432917196303606\n",
      "epoch:  699, loss: 0.00432917196303606\n",
      "epoch:  700, loss: 0.004329171031713486\n",
      "epoch:  701, loss: 0.004326668567955494\n",
      "epoch:  702, loss: 0.004326668567955494\n",
      "epoch:  703, loss: 0.004326668567955494\n",
      "epoch:  704, loss: 0.004325614310801029\n",
      "epoch:  705, loss: 0.004324301145970821\n",
      "epoch:  706, loss: 0.004324301145970821\n",
      "epoch:  707, loss: 0.004324303939938545\n",
      "epoch:  708, loss: 0.004323696251958609\n",
      "epoch:  709, loss: 0.004323696251958609\n",
      "epoch:  710, loss: 0.0043236613273620605\n",
      "epoch:  711, loss: 0.004321395885199308\n",
      "epoch:  712, loss: 0.004321395885199308\n",
      "epoch:  713, loss: 0.0043213991448283195\n",
      "epoch:  714, loss: 0.0043204291723668575\n",
      "epoch:  715, loss: 0.0043204291723668575\n",
      "epoch:  716, loss: 0.0043204291723668575\n",
      "epoch:  717, loss: 0.0043204291723668575\n",
      "epoch:  718, loss: 0.004319994244724512\n",
      "epoch:  719, loss: 0.004317267332226038\n",
      "epoch:  720, loss: 0.004317188635468483\n",
      "epoch:  721, loss: 0.004317188169807196\n",
      "epoch:  722, loss: 0.004316679667681456\n",
      "epoch:  723, loss: 0.00431404821574688\n",
      "epoch:  724, loss: 0.004313844721764326\n",
      "epoch:  725, loss: 0.004304526839405298\n",
      "epoch:  726, loss: 0.0043039508163928986\n",
      "epoch:  727, loss: 0.004303950350731611\n",
      "epoch:  728, loss: 0.004303343128412962\n",
      "epoch:  729, loss: 0.004303235560655594\n",
      "epoch:  730, loss: 0.0043032350949943066\n",
      "epoch:  731, loss: 0.0043032350949943066\n",
      "epoch:  732, loss: 0.0043020485900342464\n",
      "epoch:  733, loss: 0.0043020485900342464\n",
      "epoch:  734, loss: 0.004301425069570541\n",
      "epoch:  735, loss: 0.0043001179583370686\n",
      "epoch:  736, loss: 0.004295475780963898\n",
      "epoch:  737, loss: 0.004295125603675842\n",
      "epoch:  738, loss: 0.004295125603675842\n",
      "epoch:  739, loss: 0.004295117687433958\n",
      "epoch:  740, loss: 0.004293960984796286\n",
      "epoch:  741, loss: 0.004293960984796286\n",
      "epoch:  742, loss: 0.0042935991659760475\n",
      "epoch:  743, loss: 0.0042935991659760475\n",
      "epoch:  744, loss: 0.0042935991659760475\n",
      "epoch:  745, loss: 0.0042885299772024155\n",
      "epoch:  746, loss: 0.004288064781576395\n",
      "epoch:  747, loss: 0.004288064781576395\n",
      "epoch:  748, loss: 0.00428806571289897\n",
      "epoch:  749, loss: 0.004287777468562126\n",
      "epoch:  750, loss: 0.004287777468562126\n",
      "epoch:  751, loss: 0.004287778399884701\n",
      "epoch:  752, loss: 0.004286906216293573\n",
      "epoch:  753, loss: 0.004286906216293573\n",
      "epoch:  754, loss: 0.004286906216293573\n",
      "epoch:  755, loss: 0.004285791423171759\n",
      "epoch:  756, loss: 0.004285791423171759\n",
      "epoch:  757, loss: 0.004285183735191822\n",
      "epoch:  758, loss: 0.00428397161886096\n",
      "epoch:  759, loss: 0.00428397161886096\n",
      "epoch:  760, loss: 0.004283972084522247\n",
      "epoch:  761, loss: 0.004283882211893797\n",
      "epoch:  762, loss: 0.004280576016753912\n",
      "epoch:  763, loss: 0.004280576016753912\n",
      "epoch:  764, loss: 0.0042787217535078526\n",
      "epoch:  765, loss: 0.004278465639799833\n",
      "epoch:  766, loss: 0.004278465639799833\n",
      "epoch:  767, loss: 0.004278466105461121\n",
      "epoch:  768, loss: 0.0042762975208461285\n",
      "epoch:  769, loss: 0.0042762975208461285\n",
      "epoch:  770, loss: 0.004275585990399122\n",
      "epoch:  771, loss: 0.004275585990399122\n",
      "epoch:  772, loss: 0.004275181796401739\n",
      "epoch:  773, loss: 0.004268404096364975\n",
      "epoch:  774, loss: 0.00425969110801816\n",
      "epoch:  775, loss: 0.0042589628137648106\n",
      "epoch:  776, loss: 0.004246750846505165\n",
      "epoch:  777, loss: 0.004246750846505165\n",
      "epoch:  778, loss: 0.00423765042796731\n",
      "epoch:  779, loss: 0.004235739819705486\n",
      "epoch:  780, loss: 0.004235695116221905\n",
      "epoch:  781, loss: 0.0042356946505606174\n",
      "epoch:  782, loss: 0.0042356946505606174\n",
      "epoch:  783, loss: 0.0042356946505606174\n",
      "epoch:  784, loss: 0.004235145635902882\n",
      "epoch:  785, loss: 0.004235145635902882\n",
      "epoch:  786, loss: 0.004235146567225456\n",
      "epoch:  787, loss: 0.004234897438436747\n",
      "epoch:  788, loss: 0.004234897438436747\n",
      "epoch:  789, loss: 0.0042344615794718266\n",
      "epoch:  790, loss: 0.00423399917781353\n",
      "epoch:  791, loss: 0.00423399917781353\n",
      "epoch:  792, loss: 0.00423399917781353\n",
      "epoch:  793, loss: 0.00423399917781353\n",
      "epoch:  794, loss: 0.00423399917781353\n",
      "epoch:  795, loss: 0.00423399917781353\n",
      "epoch:  796, loss: 0.004233177751302719\n",
      "epoch:  797, loss: 0.004233177751302719\n",
      "epoch:  798, loss: 0.004232896026223898\n",
      "epoch:  799, loss: 0.004232843406498432\n",
      "epoch:  800, loss: 0.004232843406498432\n",
      "epoch:  801, loss: 0.004231753759086132\n",
      "epoch:  802, loss: 0.004231753759086132\n",
      "epoch:  803, loss: 0.004231322556734085\n",
      "epoch:  804, loss: 0.0042310128919780254\n",
      "epoch:  805, loss: 0.0042310128919780254\n",
      "epoch:  806, loss: 0.004231012426316738\n",
      "epoch:  807, loss: 0.004230961203575134\n",
      "epoch:  808, loss: 0.004215241875499487\n",
      "epoch:  809, loss: 0.004215241875499487\n",
      "epoch:  810, loss: 0.004195295739918947\n",
      "epoch:  811, loss: 0.004177783150225878\n",
      "epoch:  812, loss: 0.004177406430244446\n",
      "epoch:  813, loss: 0.004177406430244446\n",
      "epoch:  814, loss: 0.004177406430244446\n",
      "epoch:  815, loss: 0.004175982903689146\n",
      "epoch:  816, loss: 0.00417243130505085\n",
      "epoch:  817, loss: 0.00417243130505085\n",
      "epoch:  818, loss: 0.004171388223767281\n",
      "epoch:  819, loss: 0.0041643292643129826\n",
      "epoch:  820, loss: 0.004132217727601528\n",
      "epoch:  821, loss: 0.004122626036405563\n",
      "epoch:  822, loss: 0.004122231621295214\n",
      "epoch:  823, loss: 0.004122231621295214\n",
      "epoch:  824, loss: 0.004122232086956501\n",
      "epoch:  825, loss: 0.00412148330360651\n",
      "epoch:  826, loss: 0.00412148330360651\n",
      "epoch:  827, loss: 0.004121158737689257\n",
      "epoch:  828, loss: 0.0041208695620298386\n",
      "epoch:  829, loss: 0.0041208695620298386\n",
      "epoch:  830, loss: 0.0041205850429832935\n",
      "epoch:  831, loss: 0.004120339639484882\n",
      "epoch:  832, loss: 0.004120339639484882\n",
      "epoch:  833, loss: 0.004120339639484882\n",
      "epoch:  834, loss: 0.0041199494153261185\n",
      "epoch:  835, loss: 0.0041199494153261185\n",
      "epoch:  836, loss: 0.00411835266277194\n",
      "epoch:  837, loss: 0.004116031341254711\n",
      "epoch:  838, loss: 0.004115409683436155\n",
      "epoch:  839, loss: 0.004098822828382254\n",
      "epoch:  840, loss: 0.00409669429063797\n",
      "epoch:  841, loss: 0.004087358247488737\n",
      "epoch:  842, loss: 0.004085902124643326\n",
      "epoch:  843, loss: 0.004084599204361439\n",
      "epoch:  844, loss: 0.0040845987387001514\n",
      "epoch:  845, loss: 0.004084598273038864\n",
      "epoch:  846, loss: 0.004084598273038864\n",
      "epoch:  847, loss: 0.004084600601345301\n",
      "epoch:  848, loss: 0.004084200132638216\n",
      "epoch:  849, loss: 0.004084200132638216\n",
      "epoch:  850, loss: 0.004106398671865463\n",
      "epoch:  851, loss: 0.004093711264431477\n",
      "epoch:  852, loss: 0.0040924339555203915\n",
      "epoch:  853, loss: 0.0040924339555203915\n",
      "epoch:  854, loss: 0.004088037181645632\n",
      "epoch:  855, loss: 0.004086473491042852\n",
      "epoch:  856, loss: 0.004085930529981852\n",
      "epoch:  857, loss: 0.004085930064320564\n",
      "epoch:  858, loss: 0.004085930064320564\n",
      "epoch:  859, loss: 0.004085930064320564\n",
      "epoch:  860, loss: 0.004085850901901722\n",
      "epoch:  861, loss: 0.004085060674697161\n",
      "epoch:  862, loss: 0.004085060674697161\n",
      "epoch:  863, loss: 0.0040846941992640495\n",
      "epoch:  864, loss: 0.00408454705029726\n",
      "epoch:  865, loss: 0.00408454705029726\n",
      "epoch:  866, loss: 0.004084347747266293\n",
      "epoch:  867, loss: 0.004084183368831873\n",
      "epoch:  868, loss: 0.004084183368831873\n",
      "epoch:  869, loss: 0.004084183368831873\n",
      "epoch:  870, loss: 0.004084183368831873\n",
      "epoch:  871, loss: 0.004083606414496899\n",
      "epoch:  872, loss: 0.004083606414496899\n",
      "epoch:  873, loss: 0.004083606414496899\n",
      "epoch:  874, loss: 0.004083398263901472\n",
      "epoch:  875, loss: 0.004083392210304737\n",
      "epoch:  876, loss: 0.004083392210304737\n",
      "epoch:  877, loss: 0.004083245526999235\n",
      "epoch:  878, loss: 0.004083245526999235\n",
      "epoch:  879, loss: 0.004083101637661457\n",
      "epoch:  880, loss: 0.004082344006747007\n",
      "epoch:  881, loss: 0.0040823123417794704\n",
      "epoch:  882, loss: 0.004082204774022102\n",
      "epoch:  883, loss: 0.004082204774022102\n",
      "epoch:  884, loss: 0.004082031082361937\n",
      "epoch:  885, loss: 0.004081582650542259\n",
      "epoch:  886, loss: 0.004081582650542259\n",
      "epoch:  887, loss: 0.004081582650542259\n",
      "epoch:  888, loss: 0.004081582650542259\n",
      "epoch:  889, loss: 0.004081582650542259\n",
      "epoch:  890, loss: 0.004079814068973064\n",
      "epoch:  891, loss: 0.004079156555235386\n",
      "epoch:  892, loss: 0.004078967031091452\n",
      "epoch:  893, loss: 0.004078698810189962\n",
      "epoch:  894, loss: 0.004078551661223173\n",
      "epoch:  895, loss: 0.004077081568539143\n",
      "epoch:  896, loss: 0.004077081568539143\n",
      "epoch:  897, loss: 0.004062378313392401\n",
      "epoch:  898, loss: 0.004057207610458136\n",
      "epoch:  899, loss: 0.004057163838297129\n",
      "epoch:  900, loss: 0.004057163838297129\n",
      "epoch:  901, loss: 0.004057049751281738\n",
      "epoch:  902, loss: 0.004057049285620451\n",
      "epoch:  903, loss: 0.004055642522871494\n",
      "epoch:  904, loss: 0.0040537649765610695\n",
      "epoch:  905, loss: 0.0040537649765610695\n",
      "epoch:  906, loss: 0.0040530557744205\n",
      "epoch:  907, loss: 0.0040530557744205\n",
      "epoch:  908, loss: 0.004052333999425173\n",
      "epoch:  909, loss: 0.004051656927913427\n",
      "epoch:  910, loss: 0.004051656927913427\n",
      "epoch:  911, loss: 0.004050175193697214\n",
      "epoch:  912, loss: 0.004050175193697214\n",
      "epoch:  913, loss: 0.004049687180668116\n",
      "epoch:  914, loss: 0.004049612674862146\n",
      "epoch:  915, loss: 0.004049612674862146\n",
      "epoch:  916, loss: 0.0040496233850717545\n",
      "epoch:  917, loss: 0.00404781149700284\n",
      "epoch:  918, loss: 0.00404781149700284\n",
      "epoch:  919, loss: 0.00404781149700284\n",
      "epoch:  920, loss: 0.004043114371597767\n",
      "epoch:  921, loss: 0.004041410516947508\n",
      "epoch:  922, loss: 0.004036820959299803\n",
      "epoch:  923, loss: 0.004036576021462679\n",
      "epoch:  924, loss: 0.004036576021462679\n",
      "epoch:  925, loss: 0.004036576021462679\n",
      "epoch:  926, loss: 0.004036479163914919\n",
      "epoch:  927, loss: 0.004036479163914919\n",
      "epoch:  928, loss: 0.004036479163914919\n",
      "epoch:  929, loss: 0.004035943653434515\n",
      "epoch:  930, loss: 0.004035861697047949\n",
      "epoch:  931, loss: 0.004035861697047949\n",
      "epoch:  932, loss: 0.004035511519759893\n",
      "epoch:  933, loss: 0.004035511519759893\n",
      "epoch:  934, loss: 0.004035511519759893\n",
      "epoch:  935, loss: 0.004035511054098606\n",
      "epoch:  936, loss: 0.00403345562517643\n",
      "epoch:  937, loss: 0.0040327198803424835\n",
      "epoch:  938, loss: 0.0040327198803424835\n",
      "epoch:  939, loss: 0.004032339435070753\n",
      "epoch:  940, loss: 0.004032338969409466\n",
      "epoch:  941, loss: 0.004032338969409466\n",
      "epoch:  942, loss: 0.00403221370652318\n",
      "epoch:  943, loss: 0.00403221370652318\n",
      "epoch:  944, loss: 0.004032063763588667\n",
      "epoch:  945, loss: 0.004026964772492647\n",
      "epoch:  946, loss: 0.004023679997771978\n",
      "epoch:  947, loss: 0.0040230159647762775\n",
      "epoch:  948, loss: 0.0040230159647762775\n",
      "epoch:  949, loss: 0.0040230159647762775\n",
      "epoch:  950, loss: 0.0040230159647762775\n",
      "epoch:  951, loss: 0.0040230159647762775\n",
      "epoch:  952, loss: 0.0040230159647762775\n",
      "epoch:  953, loss: 0.0040228404104709625\n",
      "epoch:  954, loss: 0.00402224762365222\n",
      "epoch:  955, loss: 0.004022246226668358\n",
      "epoch:  956, loss: 0.004022158682346344\n",
      "epoch:  957, loss: 0.004022158682346344\n",
      "epoch:  958, loss: 0.004022158682346344\n",
      "epoch:  959, loss: 0.0040214392356574535\n",
      "epoch:  960, loss: 0.004021438769996166\n",
      "epoch:  961, loss: 0.004019342828541994\n",
      "epoch:  962, loss: 0.0040189349092543125\n",
      "epoch:  963, loss: 0.004018006846308708\n",
      "epoch:  964, loss: 0.004018006846308708\n",
      "epoch:  965, loss: 0.004016797989606857\n",
      "epoch:  966, loss: 0.004016585182398558\n",
      "epoch:  967, loss: 0.004016585182398558\n",
      "epoch:  968, loss: 0.0040162065997719765\n",
      "epoch:  969, loss: 0.004011901095509529\n",
      "epoch:  970, loss: 0.004010406788438559\n",
      "epoch:  971, loss: 0.00400929618626833\n",
      "epoch:  972, loss: 0.004008892923593521\n",
      "epoch:  973, loss: 0.004008892923593521\n",
      "epoch:  974, loss: 0.004008892923593521\n",
      "epoch:  975, loss: 0.004008701536804438\n",
      "epoch:  976, loss: 0.004008701536804438\n",
      "epoch:  977, loss: 0.004008702002465725\n",
      "epoch:  978, loss: 0.004008673131465912\n",
      "epoch:  979, loss: 0.004001397639513016\n",
      "epoch:  980, loss: 0.004001397639513016\n",
      "epoch:  981, loss: 0.003999894019216299\n",
      "epoch:  982, loss: 0.003999649081379175\n",
      "epoch:  983, loss: 0.003999649081379175\n",
      "epoch:  984, loss: 0.003999649081379175\n",
      "epoch:  985, loss: 0.003999537322670221\n",
      "epoch:  986, loss: 0.003999537322670221\n",
      "epoch:  987, loss: 0.003999537322670221\n",
      "epoch:  988, loss: 0.003999537322670221\n",
      "epoch:  989, loss: 0.003999537322670221\n",
      "epoch:  990, loss: 0.003999174106866121\n",
      "epoch:  991, loss: 0.003999174106866121\n",
      "epoch:  992, loss: 0.003999174106866121\n",
      "epoch:  993, loss: 0.003999174106866121\n",
      "epoch:  994, loss: 0.003999174106866121\n",
      "epoch:  995, loss: 0.003999174106866121\n",
      "epoch:  996, loss: 0.003999073524028063\n",
      "epoch:  997, loss: 0.00399817805737257\n",
      "epoch:  998, loss: 0.003998150583356619\n",
      "epoch:  999, loss: 0.003998150117695332\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size=X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# opt = torch_numopt.ConjugateGradient(model=model, lr=1e-4, line_search_method=\"const\", cg_method=\"FR\")\n",
    "opt = torch_numopt.ConjugateGradient(model=model, lr=10, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"armijo\", cg_method=\"DY\")\n",
    "# opt = torch_numopt.ConjugateGradient(model=model, lr=10, c1=0.1, c2=0.9, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"wolfe\", cg_method=\"FR\")\n",
    "# opt = torch_numopt.ConjugateGradient(model=model, lr=10, c1=0.1, c2=0.9, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"strong-wolfe\", cg_method=\"FR\")\n",
    "# opt = torch_numopt.ConjugateGradient(model=model, lr=10, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"goldstein\", cg_method=\"FR\")\n",
    "\n",
    "all_loss = {}\n",
    "for epoch in range(1000):\n",
    "    print(\"epoch: \", epoch, end=\"\")\n",
    "    all_loss[epoch + 1] = 0\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y, loss_fn)\n",
    "\n",
    "        all_loss[epoch + 1] += loss\n",
    "    all_loss[epoch + 1] /= len(data_loader)\n",
    "    print(\", loss: {}\".format(all_loss[epoch + 1].detach().numpy().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics: R2 = 0.8762005858713022\n",
      "Test metrics:  R2 = 0.8832225145901158\n"
     ]
    }
   ],
   "source": [
    "pred_train = model.forward(X_train).detach()\n",
    "pred_test = model.forward(X_test).detach()\n",
    "print(f\"Train metrics: R2 = {r2_score(pred_train, y_train)}\")\n",
    "print(f\"Test metrics:  R2 = {r2_score(pred_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.4871537685394287\n",
      "epoch:  1, loss: 0.2800534665584564\n",
      "epoch:  2, loss: 0.14550399780273438\n",
      "epoch:  3, loss: 0.0787799134850502\n",
      "epoch:  4, loss: 0.05023331567645073\n",
      "epoch:  5, loss: 0.03950706124305725\n",
      "epoch:  6, loss: 0.035870056599378586\n",
      "epoch:  7, loss: 0.03470418602228165\n",
      "epoch:  8, loss: 0.03432386741042137\n",
      "epoch:  9, loss: 0.03418225049972534\n",
      "epoch:  10, loss: 0.034110717475414276\n",
      "epoch:  11, loss: 0.033823247998952866\n",
      "epoch:  12, loss: 0.033823247998952866\n",
      "epoch:  13, loss: 0.0337141789495945\n",
      "epoch:  14, loss: 0.03364672511816025\n",
      "epoch:  15, loss: 0.03348459303379059\n",
      "epoch:  16, loss: 0.03348459303379059\n",
      "epoch:  17, loss: 0.03333776071667671\n",
      "epoch:  18, loss: 0.03325455263257027\n",
      "epoch:  19, loss: 0.03315816819667816\n",
      "epoch:  20, loss: 0.03315816819667816\n",
      "epoch:  21, loss: 0.03295530378818512\n",
      "epoch:  22, loss: 0.0328504778444767\n",
      "epoch:  23, loss: 0.03284872695803642\n",
      "epoch:  24, loss: 0.03284872695803642\n",
      "epoch:  25, loss: 0.0325714647769928\n",
      "epoch:  26, loss: 0.03243338689208031\n",
      "epoch:  27, loss: 0.03237002715468407\n",
      "epoch:  28, loss: 0.03209789842367172\n",
      "epoch:  29, loss: 0.03209789842367172\n",
      "epoch:  30, loss: 0.0319792665541172\n",
      "epoch:  31, loss: 0.03190752491354942\n",
      "epoch:  32, loss: 0.0317218042910099\n",
      "epoch:  33, loss: 0.0317218042910099\n",
      "epoch:  34, loss: 0.031538866460323334\n",
      "epoch:  35, loss: 0.03143823519349098\n",
      "epoch:  36, loss: 0.031230663880705833\n",
      "epoch:  37, loss: 0.031230663880705833\n",
      "epoch:  38, loss: 0.030988091602921486\n",
      "epoch:  39, loss: 0.030867528170347214\n",
      "epoch:  40, loss: 0.03075585886836052\n",
      "epoch:  41, loss: 0.03075585886836052\n",
      "epoch:  42, loss: 0.03042817860841751\n",
      "epoch:  43, loss: 0.03027154505252838\n",
      "epoch:  44, loss: 0.030157461762428284\n",
      "epoch:  45, loss: 0.030157461762428284\n",
      "epoch:  46, loss: 0.029732082039117813\n",
      "epoch:  47, loss: 0.02955031394958496\n",
      "epoch:  48, loss: 0.029519708827137947\n",
      "epoch:  49, loss: 0.029519708827137947\n",
      "epoch:  50, loss: 0.029014967381954193\n",
      "epoch:  51, loss: 0.028799457475543022\n",
      "epoch:  52, loss: 0.028705382719635963\n",
      "epoch:  53, loss: 0.028705382719635963\n",
      "epoch:  54, loss: 0.028108803555369377\n",
      "epoch:  55, loss: 0.027884770184755325\n",
      "epoch:  56, loss: 0.027836861088871956\n",
      "epoch:  57, loss: 0.027836861088871956\n",
      "epoch:  58, loss: 0.02720785327255726\n",
      "epoch:  59, loss: 0.02696230635046959\n",
      "epoch:  60, loss: 0.026716062799096107\n",
      "epoch:  61, loss: 0.026716062799096107\n",
      "epoch:  62, loss: 0.02606840617954731\n",
      "epoch:  63, loss: 0.02584383450448513\n",
      "epoch:  64, loss: 0.02577929012477398\n",
      "epoch:  65, loss: 0.02577929012477398\n",
      "epoch:  66, loss: 0.02507012151181698\n",
      "epoch:  67, loss: 0.024819266051054\n",
      "epoch:  68, loss: 0.02439172938466072\n",
      "epoch:  69, loss: 0.02439173497259617\n",
      "epoch:  70, loss: 0.02374895289540291\n",
      "epoch:  71, loss: 0.023540111258625984\n",
      "epoch:  72, loss: 0.023468174040317535\n",
      "epoch:  73, loss: 0.01514646876603365\n",
      "epoch:  74, loss: 0.01514646876603365\n",
      "epoch:  75, loss: 0.014764399267733097\n",
      "epoch:  76, loss: 0.014730081893503666\n",
      "epoch:  77, loss: 0.01473008282482624\n",
      "epoch:  78, loss: 0.013644100166857243\n",
      "epoch:  79, loss: 0.013551278039813042\n",
      "epoch:  80, loss: 0.013049919158220291\n",
      "epoch:  81, loss: 0.013049919158220291\n",
      "epoch:  82, loss: 0.012850534170866013\n",
      "epoch:  83, loss: 0.012726569548249245\n",
      "epoch:  84, loss: 0.012726569548249245\n",
      "epoch:  85, loss: 0.012094518169760704\n",
      "epoch:  86, loss: 0.011647646315395832\n",
      "epoch:  87, loss: 0.011647646315395832\n",
      "epoch:  88, loss: 0.011485440656542778\n",
      "epoch:  89, loss: 0.011282902210950851\n",
      "epoch:  90, loss: 0.011282902210950851\n",
      "epoch:  91, loss: 0.010965495370328426\n",
      "epoch:  92, loss: 0.008787660859525204\n",
      "epoch:  93, loss: 0.008683296851813793\n",
      "epoch:  94, loss: 0.008683296851813793\n",
      "epoch:  95, loss: 0.008644250221550465\n",
      "epoch:  96, loss: 0.008635913021862507\n",
      "epoch:  97, loss: 0.008575646206736565\n",
      "epoch:  98, loss: 0.008575646206736565\n",
      "epoch:  99, loss: 0.00851825624704361\n",
      "epoch:  100, loss: 0.008510305546224117\n",
      "epoch:  101, loss: 0.008441068232059479\n",
      "epoch:  102, loss: 0.008441068232059479\n",
      "epoch:  103, loss: 0.008411730639636517\n",
      "epoch:  104, loss: 0.008406505919992924\n",
      "epoch:  105, loss: 0.008346525020897388\n",
      "epoch:  106, loss: 0.008346525020897388\n",
      "epoch:  107, loss: 0.00833086296916008\n",
      "epoch:  108, loss: 0.008328569121658802\n",
      "epoch:  109, loss: 0.008328569121658802\n",
      "epoch:  110, loss: 0.008293421007692814\n",
      "epoch:  111, loss: 0.00828868430107832\n",
      "epoch:  112, loss: 0.008253481239080429\n",
      "epoch:  113, loss: 0.008253481239080429\n",
      "epoch:  114, loss: 0.0082319937646389\n",
      "epoch:  115, loss: 0.008228485472500324\n",
      "epoch:  116, loss: 0.00819894764572382\n",
      "epoch:  117, loss: 0.00819894764572382\n",
      "epoch:  118, loss: 0.00818142294883728\n",
      "epoch:  119, loss: 0.00817834585905075\n",
      "epoch:  120, loss: 0.008155030198395252\n",
      "epoch:  121, loss: 0.008155030198395252\n",
      "epoch:  122, loss: 0.0081382030621171\n",
      "epoch:  123, loss: 0.00813543051481247\n",
      "epoch:  124, loss: 0.008117202669382095\n",
      "epoch:  125, loss: 0.008117202669382095\n",
      "epoch:  126, loss: 0.008101238869130611\n",
      "epoch:  127, loss: 0.008098709397017956\n",
      "epoch:  128, loss: 0.008084511384367943\n",
      "epoch:  129, loss: 0.008084511384367943\n",
      "epoch:  130, loss: 0.008068458177149296\n",
      "epoch:  131, loss: 0.008066095411777496\n",
      "epoch:  132, loss: 0.008053065277636051\n",
      "epoch:  133, loss: 0.008053065277636051\n",
      "epoch:  134, loss: 0.008039427921175957\n",
      "epoch:  135, loss: 0.008037369698286057\n",
      "epoch:  136, loss: 0.008026034571230412\n",
      "epoch:  137, loss: 0.008026034571230412\n",
      "epoch:  138, loss: 0.008013934828341007\n",
      "epoch:  139, loss: 0.008012104779481888\n",
      "epoch:  140, loss: 0.008002317510545254\n",
      "epoch:  141, loss: 0.008002317510545254\n",
      "epoch:  142, loss: 0.007991478778421879\n",
      "epoch:  143, loss: 0.007989839650690556\n",
      "epoch:  144, loss: 0.007981650531291962\n",
      "epoch:  145, loss: 0.007981650531291962\n",
      "epoch:  146, loss: 0.007971575483679771\n",
      "epoch:  147, loss: 0.007970089092850685\n",
      "epoch:  148, loss: 0.007962573319673538\n",
      "epoch:  149, loss: 0.007962573319673538\n",
      "epoch:  150, loss: 0.007953806780278683\n",
      "epoch:  151, loss: 0.007952454499900341\n",
      "epoch:  152, loss: 0.007945064455270767\n",
      "epoch:  153, loss: 0.007945063523948193\n",
      "epoch:  154, loss: 0.007937705144286156\n",
      "epoch:  155, loss: 0.007936433888971806\n",
      "epoch:  156, loss: 0.007931294851005077\n",
      "epoch:  157, loss: 0.007931294851005077\n",
      "epoch:  158, loss: 0.007922820746898651\n",
      "epoch:  159, loss: 0.007921491749584675\n",
      "epoch:  160, loss: 0.007917435839772224\n",
      "epoch:  161, loss: 0.007917435839772224\n",
      "epoch:  162, loss: 0.00790881086140871\n",
      "epoch:  163, loss: 0.007907498627901077\n",
      "epoch:  164, loss: 0.007904301397502422\n",
      "epoch:  165, loss: 0.007904301397502422\n",
      "epoch:  166, loss: 0.00789559818804264\n",
      "epoch:  167, loss: 0.007894273847341537\n",
      "epoch:  168, loss: 0.007891871966421604\n",
      "epoch:  169, loss: 0.007891871966421604\n",
      "epoch:  170, loss: 0.007883187383413315\n",
      "epoch:  171, loss: 0.00788191333413124\n",
      "epoch:  172, loss: 0.007881466299295425\n",
      "epoch:  173, loss: 0.00788146536797285\n",
      "epoch:  174, loss: 0.007871882058680058\n",
      "epoch:  175, loss: 0.007870535366237164\n",
      "epoch:  176, loss: 0.007869538851082325\n",
      "epoch:  177, loss: 0.007833311334252357\n",
      "epoch:  178, loss: 0.007833311334252357\n",
      "epoch:  179, loss: 0.007811020128428936\n",
      "epoch:  180, loss: 0.0078087556175887585\n",
      "epoch:  181, loss: 0.0078077977523207664\n",
      "epoch:  182, loss: 0.0078077977523207664\n",
      "epoch:  183, loss: 0.007805647328495979\n",
      "epoch:  184, loss: 0.007805647328495979\n",
      "epoch:  185, loss: 0.007800456136465073\n",
      "epoch:  186, loss: 0.007799721322953701\n",
      "epoch:  187, loss: 0.0077990335412323475\n",
      "epoch:  188, loss: 0.007798203267157078\n",
      "epoch:  189, loss: 0.007798203267157078\n",
      "epoch:  190, loss: 0.007795591372996569\n",
      "epoch:  191, loss: 0.007794393692165613\n",
      "epoch:  192, loss: 0.007794393692165613\n",
      "epoch:  193, loss: 0.007791626267135143\n",
      "epoch:  194, loss: 0.00779055617749691\n",
      "epoch:  195, loss: 0.00779055617749691\n",
      "epoch:  196, loss: 0.007787234615534544\n",
      "epoch:  197, loss: 0.007786685135215521\n",
      "epoch:  198, loss: 0.007786010392010212\n",
      "epoch:  199, loss: 0.0077857645228505135\n",
      "epoch:  200, loss: 0.0077857645228505135\n",
      "epoch:  201, loss: 0.007782997563481331\n",
      "epoch:  202, loss: 0.007780628278851509\n",
      "epoch:  203, loss: 0.007780628278851509\n",
      "epoch:  204, loss: 0.007778281345963478\n",
      "epoch:  205, loss: 0.007775564212352037\n",
      "epoch:  206, loss: 0.007775564212352037\n",
      "epoch:  207, loss: 0.007773904595524073\n",
      "epoch:  208, loss: 0.007733801379799843\n",
      "epoch:  209, loss: 0.007733129896223545\n",
      "epoch:  210, loss: 0.007732578553259373\n",
      "epoch:  211, loss: 0.007719395682215691\n",
      "epoch:  212, loss: 0.007719395682215691\n",
      "epoch:  213, loss: 0.007695485837757587\n",
      "epoch:  214, loss: 0.007692794781178236\n",
      "epoch:  215, loss: 0.007692234590649605\n",
      "epoch:  216, loss: 0.007674043066799641\n",
      "epoch:  217, loss: 0.007674043066799641\n",
      "epoch:  218, loss: 0.007672461681067944\n",
      "epoch:  219, loss: 0.007671795319765806\n",
      "epoch:  220, loss: 0.007671603932976723\n",
      "epoch:  221, loss: 0.007671603932976723\n",
      "epoch:  222, loss: 0.007666218094527721\n",
      "epoch:  223, loss: 0.0076655237935483456\n",
      "epoch:  224, loss: 0.007664943113923073\n",
      "epoch:  225, loss: 0.0076649426482617855\n",
      "epoch:  226, loss: 0.007664455100893974\n",
      "epoch:  227, loss: 0.007635485380887985\n",
      "epoch:  228, loss: 0.007635485380887985\n",
      "epoch:  229, loss: 0.0076329088769853115\n",
      "epoch:  230, loss: 0.007632415276020765\n",
      "epoch:  231, loss: 0.007632027845829725\n",
      "epoch:  232, loss: 0.00760941905900836\n",
      "epoch:  233, loss: 0.00760941905900836\n",
      "epoch:  234, loss: 0.007605753373354673\n",
      "epoch:  235, loss: 0.007605196442455053\n",
      "epoch:  236, loss: 0.007604761049151421\n",
      "epoch:  237, loss: 0.007604596205055714\n",
      "epoch:  238, loss: 0.007604596205055714\n",
      "epoch:  239, loss: 0.00760413846001029\n",
      "epoch:  240, loss: 0.007591430563479662\n",
      "epoch:  241, loss: 0.007591431960463524\n",
      "epoch:  242, loss: 0.007579274009913206\n",
      "epoch:  243, loss: 0.007577686104923487\n",
      "epoch:  244, loss: 0.007577050477266312\n",
      "epoch:  245, loss: 0.007574333343654871\n",
      "epoch:  246, loss: 0.007574333343654871\n",
      "epoch:  247, loss: 0.007573233917355537\n",
      "epoch:  248, loss: 0.007572926115244627\n",
      "epoch:  249, loss: 0.0075690471567213535\n",
      "epoch:  250, loss: 0.0075690471567213535\n",
      "epoch:  251, loss: 0.007567357271909714\n",
      "epoch:  252, loss: 0.007566992659121752\n",
      "epoch:  253, loss: 0.00756268622353673\n",
      "epoch:  254, loss: 0.007562686689198017\n",
      "epoch:  255, loss: 0.0075605399906635284\n",
      "epoch:  256, loss: 0.007560142315924168\n",
      "epoch:  257, loss: 0.0075571853667497635\n",
      "epoch:  258, loss: 0.007557184901088476\n",
      "epoch:  259, loss: 0.0075537473894655704\n",
      "epoch:  260, loss: 0.00755322678014636\n",
      "epoch:  261, loss: 0.007551832590252161\n",
      "epoch:  262, loss: 0.007551832590252161\n",
      "epoch:  263, loss: 0.007546184118837118\n",
      "epoch:  264, loss: 0.007545407395809889\n",
      "epoch:  265, loss: 0.007544639054685831\n",
      "epoch:  266, loss: 0.007544639054685831\n",
      "epoch:  267, loss: 0.007542019709944725\n",
      "epoch:  268, loss: 0.007542019709944725\n",
      "epoch:  269, loss: 0.00753787811845541\n",
      "epoch:  270, loss: 0.007537064608186483\n",
      "epoch:  271, loss: 0.0075363232754170895\n",
      "epoch:  272, loss: 0.007531451992690563\n",
      "epoch:  273, loss: 0.007531451992690563\n",
      "epoch:  274, loss: 0.007530352100729942\n",
      "epoch:  275, loss: 0.007528600748628378\n",
      "epoch:  276, loss: 0.007528600748628378\n",
      "epoch:  277, loss: 0.00752654392272234\n",
      "epoch:  278, loss: 0.007526203989982605\n",
      "epoch:  279, loss: 0.007524332031607628\n",
      "epoch:  280, loss: 0.007524332031607628\n",
      "epoch:  281, loss: 0.007520026061683893\n",
      "epoch:  282, loss: 0.007519357372075319\n",
      "epoch:  283, loss: 0.007518718019127846\n",
      "epoch:  284, loss: 0.007518555037677288\n",
      "epoch:  285, loss: 0.007518555037677288\n",
      "epoch:  286, loss: 0.007517821621149778\n",
      "epoch:  287, loss: 0.007514843251556158\n",
      "epoch:  288, loss: 0.007514843251556158\n",
      "epoch:  289, loss: 0.007513956632465124\n",
      "epoch:  290, loss: 0.007512454874813557\n",
      "epoch:  291, loss: 0.007512454874813557\n",
      "epoch:  292, loss: 0.007510824128985405\n",
      "epoch:  293, loss: 0.007510442286729813\n",
      "epoch:  294, loss: 0.007508411537855864\n",
      "epoch:  295, loss: 0.007508411537855864\n",
      "epoch:  296, loss: 0.007505587302148342\n",
      "epoch:  297, loss: 0.0075050657615065575\n",
      "epoch:  298, loss: 0.007504582870751619\n",
      "epoch:  299, loss: 0.007470680400729179\n",
      "epoch:  300, loss: 0.007470680400729179\n",
      "epoch:  301, loss: 0.00746868597343564\n",
      "epoch:  302, loss: 0.007468271069228649\n",
      "epoch:  303, loss: 0.0074652438051998615\n",
      "epoch:  304, loss: 0.0074652438051998615\n",
      "epoch:  305, loss: 0.007463456597179174\n",
      "epoch:  306, loss: 0.007463045883923769\n",
      "epoch:  307, loss: 0.0074607874266803265\n",
      "epoch:  308, loss: 0.0074607874266803265\n",
      "epoch:  309, loss: 0.007458625826984644\n",
      "epoch:  310, loss: 0.007458174601197243\n",
      "epoch:  311, loss: 0.007456200197339058\n",
      "epoch:  312, loss: 0.007456200197339058\n",
      "epoch:  313, loss: 0.007453767582774162\n",
      "epoch:  314, loss: 0.00745329400524497\n",
      "epoch:  315, loss: 0.007451873272657394\n",
      "epoch:  316, loss: 0.007451873272657394\n",
      "epoch:  317, loss: 0.00744893541559577\n",
      "epoch:  318, loss: 0.007448408752679825\n",
      "epoch:  319, loss: 0.007447623647749424\n",
      "epoch:  320, loss: 0.007447623647749424\n",
      "epoch:  321, loss: 0.007444088347256184\n",
      "epoch:  322, loss: 0.007443496026098728\n",
      "epoch:  323, loss: 0.0074430485256016254\n",
      "epoch:  324, loss: 0.007429680787026882\n",
      "epoch:  325, loss: 0.007429680787026882\n",
      "epoch:  326, loss: 0.007411004509776831\n",
      "epoch:  327, loss: 0.007408554665744305\n",
      "epoch:  328, loss: 0.0074079157784581184\n",
      "epoch:  329, loss: 0.007407325319945812\n",
      "epoch:  330, loss: 0.007407325319945812\n",
      "epoch:  331, loss: 0.007405736483633518\n",
      "epoch:  332, loss: 0.007405340671539307\n",
      "epoch:  333, loss: 0.007404908537864685\n",
      "epoch:  334, loss: 0.007377161178737879\n",
      "epoch:  335, loss: 0.007377161178737879\n",
      "epoch:  336, loss: 0.007371464278548956\n",
      "epoch:  337, loss: 0.007370544597506523\n",
      "epoch:  338, loss: 0.007369896396994591\n",
      "epoch:  339, loss: 0.007369639351963997\n",
      "epoch:  340, loss: 0.0073694586753845215\n",
      "epoch:  341, loss: 0.007368785794824362\n",
      "epoch:  342, loss: 0.007368173450231552\n",
      "epoch:  343, loss: 0.007359768263995647\n",
      "epoch:  344, loss: 0.007359768263995647\n",
      "epoch:  345, loss: 0.007340652402490377\n",
      "epoch:  346, loss: 0.007338168099522591\n",
      "epoch:  347, loss: 0.007337496150285006\n",
      "epoch:  348, loss: 0.007337495684623718\n",
      "epoch:  349, loss: 0.007336953654885292\n",
      "epoch:  350, loss: 0.0073061054572463036\n",
      "epoch:  351, loss: 0.0073061054572463036\n",
      "epoch:  352, loss: 0.007298226468265057\n",
      "epoch:  353, loss: 0.007297051139175892\n",
      "epoch:  354, loss: 0.007296471390873194\n",
      "epoch:  355, loss: 0.007294051814824343\n",
      "epoch:  356, loss: 0.007294051814824343\n",
      "epoch:  357, loss: 0.007292812690138817\n",
      "epoch:  358, loss: 0.007292426191270351\n",
      "epoch:  359, loss: 0.007289916276931763\n",
      "epoch:  360, loss: 0.007289916276931763\n",
      "epoch:  361, loss: 0.007287602871656418\n",
      "epoch:  362, loss: 0.007287093438208103\n",
      "epoch:  363, loss: 0.007286572828888893\n",
      "epoch:  364, loss: 0.0072863237001001835\n",
      "epoch:  365, loss: 0.007286320440471172\n",
      "epoch:  366, loss: 0.007285758387297392\n",
      "epoch:  367, loss: 0.007277714554220438\n",
      "epoch:  368, loss: 0.007277714554220438\n",
      "epoch:  369, loss: 0.007249488960951567\n",
      "epoch:  370, loss: 0.007245494052767754\n",
      "epoch:  371, loss: 0.007244597654789686\n",
      "epoch:  372, loss: 0.007230839226394892\n",
      "epoch:  373, loss: 0.007230839226394892\n",
      "epoch:  374, loss: 0.007225967478007078\n",
      "epoch:  375, loss: 0.007225170731544495\n",
      "epoch:  376, loss: 0.007224255241453648\n",
      "epoch:  377, loss: 0.007224255241453648\n",
      "epoch:  378, loss: 0.007222809828817844\n",
      "epoch:  379, loss: 0.007222809363156557\n",
      "epoch:  380, loss: 0.007216619327664375\n",
      "epoch:  381, loss: 0.007215582299977541\n",
      "epoch:  382, loss: 0.007214736193418503\n",
      "epoch:  383, loss: 0.007210495416074991\n",
      "epoch:  384, loss: 0.007210494950413704\n",
      "epoch:  385, loss: 0.007209294009953737\n",
      "epoch:  386, loss: 0.007208141498267651\n",
      "epoch:  387, loss: 0.007208141498267651\n",
      "epoch:  388, loss: 0.007205825299024582\n",
      "epoch:  389, loss: 0.00720535684376955\n",
      "epoch:  390, loss: 0.007204693742096424\n",
      "epoch:  391, loss: 0.007204693742096424\n",
      "epoch:  392, loss: 0.0072008417919278145\n",
      "epoch:  393, loss: 0.007200841326266527\n",
      "epoch:  394, loss: 0.007198595907539129\n",
      "epoch:  395, loss: 0.007198010105639696\n",
      "epoch:  396, loss: 0.0071973674930632114\n",
      "epoch:  397, loss: 0.00715756556019187\n",
      "epoch:  398, loss: 0.00715756556019187\n",
      "epoch:  399, loss: 0.007149925455451012\n",
      "epoch:  400, loss: 0.007148692384362221\n",
      "epoch:  401, loss: 0.007147674448788166\n",
      "epoch:  402, loss: 0.007147610653191805\n",
      "epoch:  403, loss: 0.007147610187530518\n",
      "epoch:  404, loss: 0.007146466989070177\n",
      "epoch:  405, loss: 0.007141502108424902\n",
      "epoch:  406, loss: 0.007141502108424902\n",
      "epoch:  407, loss: 0.007139815017580986\n",
      "epoch:  408, loss: 0.007139283232390881\n",
      "epoch:  409, loss: 0.007135644555091858\n",
      "epoch:  410, loss: 0.007135644555091858\n",
      "epoch:  411, loss: 0.007131970487535\n",
      "epoch:  412, loss: 0.007131256628781557\n",
      "epoch:  413, loss: 0.007127323187887669\n",
      "epoch:  414, loss: 0.007127323187887669\n",
      "epoch:  415, loss: 0.007123603019863367\n",
      "epoch:  416, loss: 0.007122877053916454\n",
      "epoch:  417, loss: 0.007119366899132729\n",
      "epoch:  418, loss: 0.007119366899132729\n",
      "epoch:  419, loss: 0.007115636952221394\n",
      "epoch:  420, loss: 0.007114921230822802\n",
      "epoch:  421, loss: 0.007112931460142136\n",
      "epoch:  422, loss: 0.007112931460142136\n",
      "epoch:  423, loss: 0.007108082063496113\n",
      "epoch:  424, loss: 0.007107229437679052\n",
      "epoch:  425, loss: 0.007106511387974024\n",
      "epoch:  426, loss: 0.007105435710400343\n",
      "epoch:  427, loss: 0.007105435710400343\n",
      "epoch:  428, loss: 0.007101709954440594\n",
      "epoch:  429, loss: 0.0071009863168001175\n",
      "epoch:  430, loss: 0.007100265473127365\n",
      "epoch:  431, loss: 0.007097914349287748\n",
      "epoch:  432, loss: 0.007097914349287748\n",
      "epoch:  433, loss: 0.00709516229107976\n",
      "epoch:  434, loss: 0.00709453085437417\n",
      "epoch:  435, loss: 0.007093752734363079\n",
      "epoch:  436, loss: 0.007093668449670076\n",
      "epoch:  437, loss: 0.007093668449670076\n",
      "epoch:  438, loss: 0.0070884739980101585\n",
      "epoch:  439, loss: 0.0070875161327421665\n",
      "epoch:  440, loss: 0.007086588069796562\n",
      "epoch:  441, loss: 0.007086587604135275\n",
      "epoch:  442, loss: 0.007085910998284817\n",
      "epoch:  443, loss: 0.007085910998284817\n",
      "epoch:  444, loss: 0.00707975821569562\n",
      "epoch:  445, loss: 0.007078573573380709\n",
      "epoch:  446, loss: 0.0070777577348053455\n",
      "epoch:  447, loss: 0.007073566783219576\n",
      "epoch:  448, loss: 0.007073566783219576\n",
      "epoch:  449, loss: 0.007072356529533863\n",
      "epoch:  450, loss: 0.0070711467415094376\n",
      "epoch:  451, loss: 0.0070711467415094376\n",
      "epoch:  452, loss: 0.007067532744258642\n",
      "epoch:  453, loss: 0.007066788151860237\n",
      "epoch:  454, loss: 0.007066050078719854\n",
      "epoch:  455, loss: 0.0070638651959598064\n",
      "epoch:  456, loss: 0.0070638651959598064\n",
      "epoch:  457, loss: 0.007060798350721598\n",
      "epoch:  458, loss: 0.007060122676193714\n",
      "epoch:  459, loss: 0.007059346418827772\n",
      "epoch:  460, loss: 0.007056843023747206\n",
      "epoch:  461, loss: 0.007056843023747206\n",
      "epoch:  462, loss: 0.007053871173411608\n",
      "epoch:  463, loss: 0.0070532215759158134\n",
      "epoch:  464, loss: 0.007052402477711439\n",
      "epoch:  465, loss: 0.007052293047308922\n",
      "epoch:  466, loss: 0.007052293047308922\n",
      "epoch:  467, loss: 0.007051343098282814\n",
      "epoch:  468, loss: 0.007047936785966158\n",
      "epoch:  469, loss: 0.007047936785966158\n",
      "epoch:  470, loss: 0.007046190556138754\n",
      "epoch:  471, loss: 0.007045645732432604\n",
      "epoch:  472, loss: 0.0070436252281069756\n",
      "epoch:  473, loss: 0.0070436252281069756\n",
      "epoch:  474, loss: 0.0070386650040745735\n",
      "epoch:  475, loss: 0.007037755101919174\n",
      "epoch:  476, loss: 0.007036967668682337\n",
      "epoch:  477, loss: 0.0070363241247832775\n",
      "epoch:  478, loss: 0.0070363241247832775\n",
      "epoch:  479, loss: 0.007032562978565693\n",
      "epoch:  480, loss: 0.007031792309135199\n",
      "epoch:  481, loss: 0.007030942477285862\n",
      "epoch:  482, loss: 0.007030797656625509\n",
      "epoch:  483, loss: 0.007030797656625509\n",
      "epoch:  484, loss: 0.007029855623841286\n",
      "epoch:  485, loss: 0.007025301456451416\n",
      "epoch:  486, loss: 0.007025301456451416\n",
      "epoch:  487, loss: 0.007023874670267105\n",
      "epoch:  488, loss: 0.00702375965192914\n",
      "epoch:  489, loss: 0.00702375965192914\n",
      "epoch:  490, loss: 0.007019504439085722\n",
      "epoch:  491, loss: 0.007018699776381254\n",
      "epoch:  492, loss: 0.007017932366579771\n",
      "epoch:  493, loss: 0.007017589174211025\n",
      "epoch:  494, loss: 0.0070133209228515625\n",
      "epoch:  495, loss: 0.0070133209228515625\n",
      "epoch:  496, loss: 0.007011812180280685\n",
      "epoch:  497, loss: 0.007011336740106344\n",
      "epoch:  498, loss: 0.007009027525782585\n",
      "epoch:  499, loss: 0.007009027060121298\n",
      "epoch:  500, loss: 0.007004556246101856\n",
      "epoch:  501, loss: 0.007003716193139553\n",
      "epoch:  502, loss: 0.0070029390044510365\n",
      "epoch:  503, loss: 0.007002923637628555\n",
      "epoch:  504, loss: 0.007001585327088833\n",
      "epoch:  505, loss: 0.0070015848614275455\n",
      "epoch:  506, loss: 0.006996680982410908\n",
      "epoch:  507, loss: 0.00699567561969161\n",
      "epoch:  508, loss: 0.006994878873229027\n",
      "epoch:  509, loss: 0.006990804802626371\n",
      "epoch:  510, loss: 0.006990804802626371\n",
      "epoch:  511, loss: 0.006989292334765196\n",
      "epoch:  512, loss: 0.00698878662660718\n",
      "epoch:  513, loss: 0.006987183820456266\n",
      "epoch:  514, loss: 0.006987183820456266\n",
      "epoch:  515, loss: 0.006981635000556707\n",
      "epoch:  516, loss: 0.006980620790272951\n",
      "epoch:  517, loss: 0.006979791913181543\n",
      "epoch:  518, loss: 0.006979658734053373\n",
      "epoch:  519, loss: 0.006979657802730799\n",
      "epoch:  520, loss: 0.0069787283428013325\n",
      "epoch:  521, loss: 0.006975887808948755\n",
      "epoch:  522, loss: 0.006975887808948755\n",
      "epoch:  523, loss: 0.006973681040108204\n",
      "epoch:  524, loss: 0.00697310222312808\n",
      "epoch:  525, loss: 0.006972325965762138\n",
      "epoch:  526, loss: 0.006970645394176245\n",
      "epoch:  527, loss: 0.006970645394176245\n",
      "epoch:  528, loss: 0.006967220455408096\n",
      "epoch:  529, loss: 0.006966506130993366\n",
      "epoch:  530, loss: 0.006965667009353638\n",
      "epoch:  531, loss: 0.006965590175241232\n",
      "epoch:  532, loss: 0.006965590175241232\n",
      "epoch:  533, loss: 0.006964570377022028\n",
      "epoch:  534, loss: 0.006961111910641193\n",
      "epoch:  535, loss: 0.006961111444979906\n",
      "epoch:  536, loss: 0.006959018763154745\n",
      "epoch:  537, loss: 0.006958442740142345\n",
      "epoch:  538, loss: 0.006957648321986198\n",
      "epoch:  539, loss: 0.006957278121262789\n",
      "epoch:  540, loss: 0.006957271136343479\n",
      "epoch:  541, loss: 0.0069563863798975945\n",
      "epoch:  542, loss: 0.0069512175396084785\n",
      "epoch:  543, loss: 0.0069512175396084785\n",
      "epoch:  544, loss: 0.006949877366423607\n",
      "epoch:  545, loss: 0.006949189119040966\n",
      "epoch:  546, loss: 0.006949189119040966\n",
      "epoch:  547, loss: 0.006945414002984762\n",
      "epoch:  548, loss: 0.006944569293409586\n",
      "epoch:  549, loss: 0.00694374879822135\n",
      "epoch:  550, loss: 0.00694370036944747\n",
      "epoch:  551, loss: 0.00694370036944747\n",
      "epoch:  552, loss: 0.006942742504179478\n",
      "epoch:  553, loss: 0.006939248647540808\n",
      "epoch:  554, loss: 0.006939248647540808\n",
      "epoch:  555, loss: 0.006937329191714525\n",
      "epoch:  556, loss: 0.006936746649444103\n",
      "epoch:  557, loss: 0.0069356681779026985\n",
      "epoch:  558, loss: 0.006935667246580124\n",
      "epoch:  559, loss: 0.0069294762797653675\n",
      "epoch:  560, loss: 0.00692828418686986\n",
      "epoch:  561, loss: 0.006927400827407837\n",
      "epoch:  562, loss: 0.006927126552909613\n",
      "epoch:  563, loss: 0.006922758184373379\n",
      "epoch:  564, loss: 0.006922758184373379\n",
      "epoch:  565, loss: 0.006921195890754461\n",
      "epoch:  566, loss: 0.006920684594660997\n",
      "epoch:  567, loss: 0.006918123923242092\n",
      "epoch:  568, loss: 0.006918123923242092\n",
      "epoch:  569, loss: 0.006913142278790474\n",
      "epoch:  570, loss: 0.0069121625274419785\n",
      "epoch:  571, loss: 0.006911309901624918\n",
      "epoch:  572, loss: 0.0069112516939640045\n",
      "epoch:  573, loss: 0.0069112516939640045\n",
      "epoch:  574, loss: 0.006910237483680248\n",
      "epoch:  575, loss: 0.006907371338456869\n",
      "epoch:  576, loss: 0.006907371338456869\n",
      "epoch:  577, loss: 0.006904676090925932\n",
      "epoch:  578, loss: 0.006904003210365772\n",
      "epoch:  579, loss: 0.006903199478983879\n",
      "epoch:  580, loss: 0.006901992484927177\n",
      "epoch:  581, loss: 0.006901992484927177\n",
      "epoch:  582, loss: 0.006897698622196913\n",
      "epoch:  583, loss: 0.006896851118654013\n",
      "epoch:  584, loss: 0.00689596775919199\n",
      "epoch:  585, loss: 0.006895950995385647\n",
      "epoch:  586, loss: 0.006894391030073166\n",
      "epoch:  587, loss: 0.006894391030073166\n",
      "epoch:  588, loss: 0.0068888175301253796\n",
      "epoch:  589, loss: 0.006887659430503845\n",
      "epoch:  590, loss: 0.006886759307235479\n",
      "epoch:  591, loss: 0.006882485002279282\n",
      "epoch:  592, loss: 0.006882485002279282\n",
      "epoch:  593, loss: 0.006880990229547024\n",
      "epoch:  594, loss: 0.006880400702357292\n",
      "epoch:  595, loss: 0.006877156440168619\n",
      "epoch:  596, loss: 0.006877156440168619\n",
      "epoch:  597, loss: 0.006872524041682482\n",
      "epoch:  598, loss: 0.006871544290333986\n",
      "epoch:  599, loss: 0.006870649289339781\n",
      "epoch:  600, loss: 0.006870553363114595\n",
      "epoch:  601, loss: 0.006870553363114595\n",
      "epoch:  602, loss: 0.006869524717330933\n",
      "epoch:  603, loss: 0.006866378244012594\n",
      "epoch:  604, loss: 0.006866377778351307\n",
      "epoch:  605, loss: 0.006863685790449381\n",
      "epoch:  606, loss: 0.006862947251647711\n",
      "epoch:  607, loss: 0.006862068548798561\n",
      "epoch:  608, loss: 0.006861628964543343\n",
      "epoch:  609, loss: 0.0068575358018279076\n",
      "epoch:  610, loss: 0.0068575358018279076\n",
      "epoch:  611, loss: 0.006855107378214598\n",
      "epoch:  612, loss: 0.006854468956589699\n",
      "epoch:  613, loss: 0.006853524595499039\n",
      "epoch:  614, loss: 0.006853489670902491\n",
      "epoch:  615, loss: 0.006848597899079323\n",
      "epoch:  616, loss: 0.006848597899079323\n",
      "epoch:  617, loss: 0.006845191121101379\n",
      "epoch:  618, loss: 0.0068442863412201405\n",
      "epoch:  619, loss: 0.006843379698693752\n",
      "epoch:  620, loss: 0.006838471163064241\n",
      "epoch:  621, loss: 0.006838471163064241\n",
      "epoch:  622, loss: 0.006836941931396723\n",
      "epoch:  623, loss: 0.006836320273578167\n",
      "epoch:  624, loss: 0.006836320273578167\n",
      "epoch:  625, loss: 0.006831401493400335\n",
      "epoch:  626, loss: 0.00683036632835865\n",
      "epoch:  627, loss: 0.00682940473780036\n",
      "epoch:  628, loss: 0.0068290578201413155\n",
      "epoch:  629, loss: 0.006828694138675928\n",
      "epoch:  630, loss: 0.006827580276876688\n",
      "epoch:  631, loss: 0.006821383256465197\n",
      "epoch:  632, loss: 0.006821383256465197\n",
      "epoch:  633, loss: 0.00681873457506299\n",
      "epoch:  634, loss: 0.006817912682890892\n",
      "epoch:  635, loss: 0.006816968787461519\n",
      "epoch:  636, loss: 0.006813179235905409\n",
      "epoch:  637, loss: 0.006813179235905409\n",
      "epoch:  638, loss: 0.006751543376594782\n",
      "epoch:  639, loss: 0.0067415498197078705\n",
      "epoch:  640, loss: 0.006739735137671232\n",
      "epoch:  641, loss: 0.006739735137671232\n",
      "epoch:  642, loss: 0.006738089490681887\n",
      "epoch:  643, loss: 0.006737185176461935\n",
      "epoch:  644, loss: 0.006737185176461935\n",
      "epoch:  645, loss: 0.006731884088367224\n",
      "epoch:  646, loss: 0.006730733439326286\n",
      "epoch:  647, loss: 0.006729606539011002\n",
      "epoch:  648, loss: 0.006729606539011002\n",
      "epoch:  649, loss: 0.006728627718985081\n",
      "epoch:  650, loss: 0.006721368990838528\n",
      "epoch:  651, loss: 0.00672136852517724\n",
      "epoch:  652, loss: 0.006719747558236122\n",
      "epoch:  653, loss: 0.006718914024531841\n",
      "epoch:  654, loss: 0.006718914024531841\n",
      "epoch:  655, loss: 0.00671426672488451\n",
      "epoch:  656, loss: 0.006713179871439934\n",
      "epoch:  657, loss: 0.006712077651172876\n",
      "epoch:  658, loss: 0.006711889524012804\n",
      "epoch:  659, loss: 0.006711876485496759\n",
      "epoch:  660, loss: 0.006710618268698454\n",
      "epoch:  661, loss: 0.006705915555357933\n",
      "epoch:  662, loss: 0.006705915555357933\n",
      "epoch:  663, loss: 0.00670292554423213\n",
      "epoch:  664, loss: 0.006702053360641003\n",
      "epoch:  665, loss: 0.006700970698148012\n",
      "epoch:  666, loss: 0.006700956262648106\n",
      "epoch:  667, loss: 0.006699961610138416\n",
      "epoch:  668, loss: 0.0066732075065374374\n",
      "epoch:  669, loss: 0.0066732075065374374\n",
      "epoch:  670, loss: 0.006621780339628458\n",
      "epoch:  671, loss: 0.006612605415284634\n",
      "epoch:  672, loss: 0.006610362324863672\n",
      "epoch:  673, loss: 0.006610362324863672\n",
      "epoch:  674, loss: 0.0066083925776183605\n",
      "epoch:  675, loss: 0.006605169735848904\n",
      "epoch:  676, loss: 0.006605169735848904\n",
      "epoch:  677, loss: 0.006600400432944298\n",
      "epoch:  678, loss: 0.006599168758839369\n",
      "epoch:  679, loss: 0.00659784022718668\n",
      "epoch:  680, loss: 0.006592738442122936\n",
      "epoch:  681, loss: 0.006592737510800362\n",
      "epoch:  682, loss: 0.006589906755834818\n",
      "epoch:  683, loss: 0.006588966120034456\n",
      "epoch:  684, loss: 0.006587733514606953\n",
      "epoch:  685, loss: 0.0065827155485749245\n",
      "epoch:  686, loss: 0.0065827155485749245\n",
      "epoch:  687, loss: 0.006579086184501648\n",
      "epoch:  688, loss: 0.006578055210411549\n",
      "epoch:  689, loss: 0.006576757412403822\n",
      "epoch:  690, loss: 0.006570375058799982\n",
      "epoch:  691, loss: 0.006570375058799982\n",
      "epoch:  692, loss: 0.006567591801285744\n",
      "epoch:  693, loss: 0.006567067466676235\n",
      "epoch:  694, loss: 0.006567067466676235\n",
      "epoch:  695, loss: 0.006559781264513731\n",
      "epoch:  696, loss: 0.006558216642588377\n",
      "epoch:  697, loss: 0.00655669579282403\n",
      "epoch:  698, loss: 0.006556692533195019\n",
      "epoch:  699, loss: 0.006555357947945595\n",
      "epoch:  700, loss: 0.006447472609579563\n",
      "epoch:  701, loss: 0.006447472609579563\n",
      "epoch:  702, loss: 0.006443755701184273\n",
      "epoch:  703, loss: 0.006442557089030743\n",
      "epoch:  704, loss: 0.006441112142056227\n",
      "epoch:  705, loss: 0.006431604735553265\n",
      "epoch:  706, loss: 0.006431604735553265\n",
      "epoch:  707, loss: 0.0064293635077774525\n",
      "epoch:  708, loss: 0.006428856868296862\n",
      "epoch:  709, loss: 0.006428856868296862\n",
      "epoch:  710, loss: 0.006421378813683987\n",
      "epoch:  711, loss: 0.006419673096388578\n",
      "epoch:  712, loss: 0.0064181555062532425\n",
      "epoch:  713, loss: 0.0064168451353907585\n",
      "epoch:  714, loss: 0.0064168451353907585\n",
      "epoch:  715, loss: 0.006410563364624977\n",
      "epoch:  716, loss: 0.006409067660570145\n",
      "epoch:  717, loss: 0.006407533772289753\n",
      "epoch:  718, loss: 0.006407533772289753\n",
      "epoch:  719, loss: 0.006406183820217848\n",
      "epoch:  720, loss: 0.006405012682080269\n",
      "epoch:  721, loss: 0.006405012682080269\n",
      "epoch:  722, loss: 0.006313619203865528\n",
      "epoch:  723, loss: 0.006295989733189344\n",
      "epoch:  724, loss: 0.006292133126407862\n",
      "epoch:  725, loss: 0.006292133126407862\n",
      "epoch:  726, loss: 0.006287866737693548\n",
      "epoch:  727, loss: 0.00628651212900877\n",
      "epoch:  728, loss: 0.0062848343513906\n",
      "epoch:  729, loss: 0.006198224611580372\n",
      "epoch:  730, loss: 0.006198224611580372\n",
      "epoch:  731, loss: 0.006195820402354002\n",
      "epoch:  732, loss: 0.006195820402354002\n",
      "epoch:  733, loss: 0.006193929817527533\n",
      "epoch:  734, loss: 0.0061799306422472\n",
      "epoch:  735, loss: 0.0061799306422472\n",
      "epoch:  736, loss: 0.006177525036036968\n",
      "epoch:  737, loss: 0.006172769237309694\n",
      "epoch:  738, loss: 0.006172769237309694\n",
      "epoch:  739, loss: 0.006168073508888483\n",
      "epoch:  740, loss: 0.006166939157992601\n",
      "epoch:  741, loss: 0.006165091879665852\n",
      "epoch:  742, loss: 0.006165091879665852\n",
      "epoch:  743, loss: 0.006163974292576313\n",
      "epoch:  744, loss: 0.006163974292576313\n",
      "epoch:  745, loss: 0.006150918081402779\n",
      "epoch:  746, loss: 0.006148216780275106\n",
      "epoch:  747, loss: 0.006146274972707033\n",
      "epoch:  748, loss: 0.00611380347982049\n",
      "epoch:  749, loss: 0.00611380347982049\n",
      "epoch:  750, loss: 0.006095824297517538\n",
      "epoch:  751, loss: 0.006092790514230728\n",
      "epoch:  752, loss: 0.006090323906391859\n",
      "epoch:  753, loss: 0.0060895769856870174\n",
      "epoch:  754, loss: 0.00608957652002573\n",
      "epoch:  755, loss: 0.0060826377011835575\n",
      "epoch:  756, loss: 0.006081589497625828\n",
      "epoch:  757, loss: 0.0060792420990765095\n",
      "epoch:  758, loss: 0.006076186895370483\n",
      "epoch:  759, loss: 0.006076186895370483\n",
      "epoch:  760, loss: 0.006069868803024292\n",
      "epoch:  761, loss: 0.006068887654691935\n",
      "epoch:  762, loss: 0.0060666399076581\n",
      "epoch:  763, loss: 0.006063074339181185\n",
      "epoch:  764, loss: 0.006063074339181185\n",
      "epoch:  765, loss: 0.0060563599690794945\n",
      "epoch:  766, loss: 0.006055314559489489\n",
      "epoch:  767, loss: 0.006052983459085226\n",
      "epoch:  768, loss: 0.006050451193004847\n",
      "epoch:  769, loss: 0.006050451193004847\n",
      "epoch:  770, loss: 0.006043027620762587\n",
      "epoch:  771, loss: 0.006041908171027899\n",
      "epoch:  772, loss: 0.006039544474333525\n",
      "epoch:  773, loss: 0.006039106287062168\n",
      "epoch:  774, loss: 0.006037172861397266\n",
      "epoch:  775, loss: 0.006035512313246727\n",
      "epoch:  776, loss: 0.006024150177836418\n",
      "epoch:  777, loss: 0.006024149712175131\n",
      "epoch:  778, loss: 0.006020365282893181\n",
      "epoch:  779, loss: 0.006020240020006895\n",
      "epoch:  780, loss: 0.006018037907779217\n",
      "epoch:  781, loss: 0.006011185701936483\n",
      "epoch:  782, loss: 0.006011185701936483\n",
      "epoch:  783, loss: 0.006002675276249647\n",
      "epoch:  784, loss: 0.006001514382660389\n",
      "epoch:  785, loss: 0.0059991017915308475\n",
      "epoch:  786, loss: 0.005999081768095493\n",
      "epoch:  787, loss: 0.005997014231979847\n",
      "epoch:  788, loss: 0.005956008564680815\n",
      "epoch:  789, loss: 0.005956008564680815\n",
      "epoch:  790, loss: 0.005919043906033039\n",
      "epoch:  791, loss: 0.005913840606808662\n",
      "epoch:  792, loss: 0.005907605402171612\n",
      "epoch:  793, loss: 0.005907605402171612\n",
      "epoch:  794, loss: 0.005902337841689587\n",
      "epoch:  795, loss: 0.005878054536879063\n",
      "epoch:  796, loss: 0.005878054536879063\n",
      "epoch:  797, loss: 0.005874605383723974\n",
      "epoch:  798, loss: 0.005874605383723974\n",
      "epoch:  799, loss: 0.0058718957006931305\n",
      "epoch:  800, loss: 0.005805672612041235\n",
      "epoch:  801, loss: 0.005805672612041235\n",
      "epoch:  802, loss: 0.005785468965768814\n",
      "epoch:  803, loss: 0.005785468965768814\n",
      "epoch:  804, loss: 0.0057744914665818214\n",
      "epoch:  805, loss: 0.0057744914665818214\n",
      "epoch:  806, loss: 0.005765637848526239\n",
      "epoch:  807, loss: 0.005731045734137297\n",
      "epoch:  808, loss: 0.005731045734137297\n",
      "epoch:  809, loss: 0.005723142996430397\n",
      "epoch:  810, loss: 0.005723088514059782\n",
      "epoch:  811, loss: 0.005723088514059782\n",
      "epoch:  812, loss: 0.005716731771826744\n",
      "epoch:  813, loss: 0.005710033234208822\n",
      "epoch:  814, loss: 0.005710032768547535\n",
      "epoch:  815, loss: 0.0057043214328587055\n",
      "epoch:  816, loss: 0.005695512983947992\n",
      "epoch:  817, loss: 0.005695512983947992\n",
      "epoch:  818, loss: 0.005689120851457119\n",
      "epoch:  819, loss: 0.005687995348125696\n",
      "epoch:  820, loss: 0.005687995348125696\n",
      "epoch:  821, loss: 0.005681671667844057\n",
      "epoch:  822, loss: 0.005679867696017027\n",
      "epoch:  823, loss: 0.005679867696017027\n",
      "epoch:  824, loss: 0.005674293264746666\n",
      "epoch:  825, loss: 0.005669610109180212\n",
      "epoch:  826, loss: 0.005669610109180212\n",
      "epoch:  827, loss: 0.005665136501193047\n",
      "epoch:  828, loss: 0.005654340609908104\n",
      "epoch:  829, loss: 0.005654340609908104\n",
      "epoch:  830, loss: 0.005650517996400595\n",
      "epoch:  831, loss: 0.005638069473206997\n",
      "epoch:  832, loss: 0.005638069473206997\n",
      "epoch:  833, loss: 0.005634702276438475\n",
      "epoch:  834, loss: 0.0055606006644666195\n",
      "epoch:  835, loss: 0.005560600198805332\n",
      "epoch:  836, loss: 0.00554419681429863\n",
      "epoch:  837, loss: 0.005543329752981663\n",
      "epoch:  838, loss: 0.005537774413824081\n",
      "epoch:  839, loss: 0.005537774413824081\n",
      "epoch:  840, loss: 0.005533548537641764\n",
      "epoch:  841, loss: 0.005533548537641764\n",
      "epoch:  842, loss: 0.005530391354113817\n",
      "epoch:  843, loss: 0.005530366208404303\n",
      "epoch:  844, loss: 0.005530366208404303\n",
      "epoch:  845, loss: 0.005526734050363302\n",
      "epoch:  846, loss: 0.005526734050363302\n",
      "epoch:  847, loss: 0.005523852072656155\n",
      "epoch:  848, loss: 0.005523352418094873\n",
      "epoch:  849, loss: 0.005523352418094873\n",
      "epoch:  850, loss: 0.005520164966583252\n",
      "epoch:  851, loss: 0.005520164966583252\n",
      "epoch:  852, loss: 0.005517486482858658\n",
      "epoch:  853, loss: 0.0055165402591228485\n",
      "epoch:  854, loss: 0.0055165402591228485\n",
      "epoch:  855, loss: 0.005513707175850868\n",
      "epoch:  856, loss: 0.005513689015060663\n",
      "epoch:  857, loss: 0.005511174909770489\n",
      "epoch:  858, loss: 0.005498331040143967\n",
      "epoch:  859, loss: 0.005498330574482679\n",
      "epoch:  860, loss: 0.005492580123245716\n",
      "epoch:  861, loss: 0.005488573107868433\n",
      "epoch:  862, loss: 0.005488573107868433\n",
      "epoch:  863, loss: 0.005467046983540058\n",
      "epoch:  864, loss: 0.005467046983540058\n",
      "epoch:  865, loss: 0.005457471590489149\n",
      "epoch:  866, loss: 0.005457471590489149\n",
      "epoch:  867, loss: 0.005451247561722994\n",
      "epoch:  868, loss: 0.005451247561722994\n",
      "epoch:  869, loss: 0.005446142051368952\n",
      "epoch:  870, loss: 0.005446142051368952\n",
      "epoch:  871, loss: 0.0054369582794606686\n",
      "epoch:  872, loss: 0.0054369582794606686\n",
      "epoch:  873, loss: 0.005410776939243078\n",
      "epoch:  874, loss: 0.00540862837806344\n",
      "epoch:  875, loss: 0.005403454881161451\n",
      "epoch:  876, loss: 0.005403454881161451\n",
      "epoch:  877, loss: 0.0053985100239515305\n",
      "epoch:  878, loss: 0.0053985100239515305\n",
      "epoch:  879, loss: 0.00539516843855381\n",
      "epoch:  880, loss: 0.00539516843855381\n",
      "epoch:  881, loss: 0.005392264109104872\n",
      "epoch:  882, loss: 0.005391150247305632\n",
      "epoch:  883, loss: 0.005390556994825602\n",
      "epoch:  884, loss: 0.005390556994825602\n",
      "epoch:  885, loss: 0.005272907670587301\n",
      "epoch:  886, loss: 0.005258955992758274\n",
      "epoch:  887, loss: 0.005258955992758274\n",
      "epoch:  888, loss: 0.0052319918759167194\n",
      "epoch:  889, loss: 0.005226677283644676\n",
      "epoch:  890, loss: 0.0051987930200994015\n",
      "epoch:  891, loss: 0.0051987930200994015\n",
      "epoch:  892, loss: 0.005186619237065315\n",
      "epoch:  893, loss: 0.0051851654425263405\n",
      "epoch:  894, loss: 0.0051851654425263405\n",
      "epoch:  895, loss: 0.005179641302675009\n",
      "epoch:  896, loss: 0.0051275636069476604\n",
      "epoch:  897, loss: 0.00512027507647872\n",
      "epoch:  898, loss: 0.00512027507647872\n",
      "epoch:  899, loss: 0.005101894028484821\n",
      "epoch:  900, loss: 0.005101894028484821\n",
      "epoch:  901, loss: 0.005093383137136698\n",
      "epoch:  902, loss: 0.005093383137136698\n",
      "epoch:  903, loss: 0.005088516511023045\n",
      "epoch:  904, loss: 0.005088516511023045\n",
      "epoch:  905, loss: 0.005084944888949394\n",
      "epoch:  906, loss: 0.005084944888949394\n",
      "epoch:  907, loss: 0.005081913433969021\n",
      "epoch:  908, loss: 0.0050814347341656685\n",
      "epoch:  909, loss: 0.0050814347341656685\n",
      "epoch:  910, loss: 0.005078336223959923\n",
      "epoch:  911, loss: 0.005078336223959923\n",
      "epoch:  912, loss: 0.005075495690107346\n",
      "epoch:  913, loss: 0.005005447193980217\n",
      "epoch:  914, loss: 0.005005447193980217\n",
      "epoch:  915, loss: 0.004952947609126568\n",
      "epoch:  916, loss: 0.004944449290633202\n",
      "epoch:  917, loss: 0.004937943071126938\n",
      "epoch:  918, loss: 0.004937943071126938\n",
      "epoch:  919, loss: 0.004926849156618118\n",
      "epoch:  920, loss: 0.004926849156618118\n",
      "epoch:  921, loss: 0.00492108054459095\n",
      "epoch:  922, loss: 0.00492108054459095\n",
      "epoch:  923, loss: 0.004917461890727282\n",
      "epoch:  924, loss: 0.004917461890727282\n",
      "epoch:  925, loss: 0.0049145217053592205\n",
      "epoch:  926, loss: 0.004905941430479288\n",
      "epoch:  927, loss: 0.004905941430479288\n",
      "epoch:  928, loss: 0.00490180728957057\n",
      "epoch:  929, loss: 0.004899158608168364\n",
      "epoch:  930, loss: 0.004899158608168364\n",
      "epoch:  931, loss: 0.004896324127912521\n",
      "epoch:  932, loss: 0.004888605326414108\n",
      "epoch:  933, loss: 0.004888605326414108\n",
      "epoch:  934, loss: 0.004884479567408562\n",
      "epoch:  935, loss: 0.004883563611656427\n",
      "epoch:  936, loss: 0.004872351419180632\n",
      "epoch:  937, loss: 0.004872351419180632\n",
      "epoch:  938, loss: 0.004863191395998001\n",
      "epoch:  939, loss: 0.004863191395998001\n",
      "epoch:  940, loss: 0.004858375992625952\n",
      "epoch:  941, loss: 0.004858375992625952\n",
      "epoch:  942, loss: 0.0048551615327596664\n",
      "epoch:  943, loss: 0.004851516801863909\n",
      "epoch:  944, loss: 0.004851516801863909\n",
      "epoch:  945, loss: 0.00484909862279892\n",
      "epoch:  946, loss: 0.004838315770030022\n",
      "epoch:  947, loss: 0.004838315770030022\n",
      "epoch:  948, loss: 0.004833997692912817\n",
      "epoch:  949, loss: 0.004833996761590242\n",
      "epoch:  950, loss: 0.004830947145819664\n",
      "epoch:  951, loss: 0.004830947145819664\n",
      "epoch:  952, loss: 0.00482845026999712\n",
      "epoch:  953, loss: 0.004821194335818291\n",
      "epoch:  954, loss: 0.004821194335818291\n",
      "epoch:  955, loss: 0.0048180436715483665\n",
      "epoch:  956, loss: 0.0048142289742827415\n",
      "epoch:  957, loss: 0.004798290319740772\n",
      "epoch:  958, loss: 0.004798290319740772\n",
      "epoch:  959, loss: 0.004794187378138304\n",
      "epoch:  960, loss: 0.004793232772499323\n",
      "epoch:  961, loss: 0.004790710750967264\n",
      "epoch:  962, loss: 0.004790710750967264\n",
      "epoch:  963, loss: 0.004788299091160297\n",
      "epoch:  964, loss: 0.004777594469487667\n",
      "epoch:  965, loss: 0.004777594469487667\n",
      "epoch:  966, loss: 0.004774624947458506\n",
      "epoch:  967, loss: 0.004769730847328901\n",
      "epoch:  968, loss: 0.004769730847328901\n",
      "epoch:  969, loss: 0.0047674886882305145\n",
      "epoch:  970, loss: 0.004754430148750544\n",
      "epoch:  971, loss: 0.004754430148750544\n",
      "epoch:  972, loss: 0.004750570748001337\n",
      "epoch:  973, loss: 0.004750479012727737\n",
      "epoch:  974, loss: 0.0047477008774876595\n",
      "epoch:  975, loss: 0.0047477008774876595\n",
      "epoch:  976, loss: 0.004745284095406532\n",
      "epoch:  977, loss: 0.004735729191452265\n",
      "epoch:  978, loss: 0.004735729191452265\n",
      "epoch:  979, loss: 0.004732990171760321\n",
      "epoch:  980, loss: 0.004726266488432884\n",
      "epoch:  981, loss: 0.004726266488432884\n",
      "epoch:  982, loss: 0.004723952617496252\n",
      "epoch:  983, loss: 0.0047122035175561905\n",
      "epoch:  984, loss: 0.0047122035175561905\n",
      "epoch:  985, loss: 0.004708793479949236\n",
      "epoch:  986, loss: 0.004706653766334057\n",
      "epoch:  987, loss: 0.004704267252236605\n",
      "epoch:  988, loss: 0.004704267252236605\n",
      "epoch:  989, loss: 0.004702016711235046\n",
      "epoch:  990, loss: 0.004689478315412998\n",
      "epoch:  991, loss: 0.004689477849751711\n",
      "epoch:  992, loss: 0.004685788881033659\n",
      "epoch:  993, loss: 0.004685470834374428\n",
      "epoch:  994, loss: 0.0046827299520373344\n",
      "epoch:  995, loss: 0.0046827299520373344\n",
      "epoch:  996, loss: 0.004680307116359472\n",
      "epoch:  997, loss: 0.004670899361371994\n",
      "epoch:  998, loss: 0.004670899361371994\n",
      "epoch:  999, loss: 0.004668043460696936\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size=X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# opt = torch_numopt.ConjugateGradient(model=model, lr=1e-4, line_search_method=\"const\", cg_method=\"PR\")\n",
    "opt = torch_numopt.ConjugateGradient(model=model, lr=10, c1=0.01, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"armijo\", cg_method=\"PR\")\n",
    "# opt = torch_numopt.ConjugateGradient(model=model, lr=10, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"wolfe\", cg_method=\"PR\")\n",
    "# opt = torch_numopt.ConjugateGradient(model=model, lr=10, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"strong-wolfe\", cg_method=\"PR\")\n",
    "# opt = torch_numopt.ConjugateGradient(model=model, lr=10, c1=1e-6, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"goldstein\", cg_method=\"PR\")\n",
    "\n",
    "\n",
    "all_loss = {}\n",
    "for epoch in range(1000):\n",
    "    print(\"epoch: \", epoch, end=\"\")\n",
    "    all_loss[epoch + 1] = 0\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y, loss_fn)\n",
    "\n",
    "        all_loss[epoch + 1] += loss\n",
    "    all_loss[epoch + 1] /= len(data_loader)\n",
    "    print(\", loss: {}\".format(all_loss[epoch + 1].detach().numpy().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics: R2 = 0.8501156290473114\n",
      "Test metrics:  R2 = 0.7606448171390976\n"
     ]
    }
   ],
   "source": [
    "pred_train = model.forward(X_train).detach()\n",
    "pred_test = model.forward(X_test).detach()\n",
    "print(f\"Train metrics: R2 = {r2_score(pred_train, y_train)}\")\n",
    "print(f\"Test metrics:  R2 = {r2_score(pred_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.3715151250362396\n",
      "epoch:  1, loss: 0.22022412717342377\n",
      "epoch:  2, loss: 0.1412205547094345\n",
      "epoch:  3, loss: 0.09496459364891052\n",
      "epoch:  4, loss: 0.06842473149299622\n",
      "epoch:  5, loss: 0.053537026047706604\n",
      "epoch:  6, loss: 0.04536965861916542\n",
      "epoch:  7, loss: 0.040975283831357956\n",
      "epoch:  8, loss: 0.038647860288619995\n",
      "epoch:  9, loss: 0.03742722421884537\n",
      "epoch:  10, loss: 0.036788277328014374\n",
      "epoch:  11, loss: 0.03644932806491852\n",
      "epoch:  12, loss: 0.03626001253724098\n",
      "epoch:  13, loss: 0.03624358028173447\n",
      "epoch:  14, loss: 0.03624358028173447\n",
      "epoch:  15, loss: 0.035934075713157654\n",
      "epoch:  16, loss: 0.03578561916947365\n",
      "epoch:  17, loss: 0.03570716455578804\n",
      "epoch:  18, loss: 0.03566036373376846\n",
      "epoch:  19, loss: 0.035545866936445236\n",
      "epoch:  20, loss: 0.035545866936445236\n",
      "epoch:  21, loss: 0.035459887236356735\n",
      "epoch:  22, loss: 0.03540756180882454\n",
      "epoch:  23, loss: 0.03528168797492981\n",
      "epoch:  24, loss: 0.03528168797492981\n",
      "epoch:  25, loss: 0.03518339991569519\n",
      "epoch:  26, loss: 0.03512776643037796\n",
      "epoch:  27, loss: 0.035004980862140656\n",
      "epoch:  28, loss: 0.035004980862140656\n",
      "epoch:  29, loss: 0.03489845618605614\n",
      "epoch:  30, loss: 0.03483670577406883\n",
      "epoch:  31, loss: 0.034718360751867294\n",
      "epoch:  32, loss: 0.034718360751867294\n",
      "epoch:  33, loss: 0.03459174185991287\n",
      "epoch:  34, loss: 0.03452510014176369\n",
      "epoch:  35, loss: 0.034416500478982925\n",
      "epoch:  36, loss: 0.034416500478982925\n",
      "epoch:  37, loss: 0.03427919000387192\n",
      "epoch:  38, loss: 0.03420300781726837\n",
      "epoch:  39, loss: 0.03409649431705475\n",
      "epoch:  40, loss: 0.03409649431705475\n",
      "epoch:  41, loss: 0.03392728418111801\n",
      "epoch:  42, loss: 0.03384179249405861\n",
      "epoch:  43, loss: 0.03374941274523735\n",
      "epoch:  44, loss: 0.03374941274523735\n",
      "epoch:  45, loss: 0.03356295824050903\n",
      "epoch:  46, loss: 0.03346633538603783\n",
      "epoch:  47, loss: 0.03338004648685455\n",
      "epoch:  48, loss: 0.03338004648685455\n",
      "epoch:  49, loss: 0.03314501419663429\n",
      "epoch:  50, loss: 0.033035535365343094\n",
      "epoch:  51, loss: 0.0329684317111969\n",
      "epoch:  52, loss: 0.0329684317111969\n",
      "epoch:  53, loss: 0.03271201252937317\n",
      "epoch:  54, loss: 0.032584842294454575\n",
      "epoch:  55, loss: 0.03255196288228035\n",
      "epoch:  56, loss: 0.03255196288228035\n",
      "epoch:  57, loss: 0.032212402671575546\n",
      "epoch:  58, loss: 0.032067328691482544\n",
      "epoch:  59, loss: 0.032070234417915344\n",
      "epoch:  60, loss: 0.032070234417915344\n",
      "epoch:  61, loss: 0.031699880957603455\n",
      "epoch:  62, loss: 0.03152920678257942\n",
      "epoch:  63, loss: 0.031433816999197006\n",
      "epoch:  64, loss: 0.031139329075813293\n",
      "epoch:  65, loss: 0.031139329075813293\n",
      "epoch:  66, loss: 0.030911030247807503\n",
      "epoch:  67, loss: 0.030802853405475616\n",
      "epoch:  68, loss: 0.0305356215685606\n",
      "epoch:  69, loss: 0.0305356215685606\n",
      "epoch:  70, loss: 0.030279476195573807\n",
      "epoch:  71, loss: 0.030151110142469406\n",
      "epoch:  72, loss: 0.02989191561937332\n",
      "epoch:  73, loss: 0.02989191561937332\n",
      "epoch:  74, loss: 0.029540078714489937\n",
      "epoch:  75, loss: 0.02939189225435257\n",
      "epoch:  76, loss: 0.029187196865677834\n",
      "epoch:  77, loss: 0.029187196865677834\n",
      "epoch:  78, loss: 0.02878584899008274\n",
      "epoch:  79, loss: 0.02861017733812332\n",
      "epoch:  80, loss: 0.028430122882127762\n",
      "epoch:  81, loss: 0.028430122882127762\n",
      "epoch:  82, loss: 0.027890902012586594\n",
      "epoch:  83, loss: 0.027697129175066948\n",
      "epoch:  84, loss: 0.027524309232831\n",
      "epoch:  85, loss: 0.027524309232831\n",
      "epoch:  86, loss: 0.026967039331793785\n",
      "epoch:  87, loss: 0.026752054691314697\n",
      "epoch:  88, loss: 0.02655806951224804\n",
      "epoch:  89, loss: 0.02655806951224804\n",
      "epoch:  90, loss: 0.025854701176285744\n",
      "epoch:  91, loss: 0.025631913915276527\n",
      "epoch:  92, loss: 0.025350462645292282\n",
      "epoch:  93, loss: 0.025350462645292282\n",
      "epoch:  94, loss: 0.024703746661543846\n",
      "epoch:  95, loss: 0.024478446692228317\n",
      "epoch:  96, loss: 0.024117467924952507\n",
      "epoch:  97, loss: 0.024117467924952507\n",
      "epoch:  98, loss: 0.023389602079987526\n",
      "epoch:  99, loss: 0.023184986785054207\n",
      "epoch:  100, loss: 0.022793063893914223\n",
      "epoch:  101, loss: 0.022793063893914223\n",
      "epoch:  102, loss: 0.02219919115304947\n",
      "epoch:  103, loss: 0.022011814638972282\n",
      "epoch:  104, loss: 0.021911032497882843\n",
      "epoch:  105, loss: 0.021908827126026154\n",
      "epoch:  106, loss: 0.01218687929213047\n",
      "epoch:  107, loss: 0.01218687929213047\n",
      "epoch:  108, loss: 0.012009704485535622\n",
      "epoch:  109, loss: 0.011692552827298641\n",
      "epoch:  110, loss: 0.011692552827298641\n",
      "epoch:  111, loss: 0.011494552716612816\n",
      "epoch:  112, loss: 0.00925714336335659\n",
      "epoch:  113, loss: 0.00925714336335659\n",
      "epoch:  114, loss: 0.008997999131679535\n",
      "epoch:  115, loss: 0.008965429849922657\n",
      "epoch:  116, loss: 0.00895067397505045\n",
      "epoch:  117, loss: 0.008925016969442368\n",
      "epoch:  118, loss: 0.008925016969442368\n",
      "epoch:  119, loss: 0.008840236812829971\n",
      "epoch:  120, loss: 0.00882637407630682\n",
      "epoch:  121, loss: 0.008763360790908337\n",
      "epoch:  122, loss: 0.008763360790908337\n",
      "epoch:  123, loss: 0.008705335669219494\n",
      "epoch:  124, loss: 0.00869474932551384\n",
      "epoch:  125, loss: 0.008634999394416809\n",
      "epoch:  126, loss: 0.008634999394416809\n",
      "epoch:  127, loss: 0.008594783022999763\n",
      "epoch:  128, loss: 0.008586712181568146\n",
      "epoch:  129, loss: 0.008526977151632309\n",
      "epoch:  130, loss: 0.008526977151632309\n",
      "epoch:  131, loss: 0.008503030985593796\n",
      "epoch:  132, loss: 0.008496900089085102\n",
      "epoch:  133, loss: 0.008435367606580257\n",
      "epoch:  134, loss: 0.008435367606580257\n",
      "epoch:  135, loss: 0.008425160311162472\n",
      "epoch:  136, loss: 0.00840737670660019\n",
      "epoch:  137, loss: 0.008407375775277615\n",
      "epoch:  138, loss: 0.008375545032322407\n",
      "epoch:  139, loss: 0.008369824849069118\n",
      "epoch:  140, loss: 0.008332324214279652\n",
      "epoch:  141, loss: 0.008332324214279652\n",
      "epoch:  142, loss: 0.008316149935126305\n",
      "epoch:  143, loss: 0.008312002755701542\n",
      "epoch:  144, loss: 0.00827485229820013\n",
      "epoch:  145, loss: 0.00827485229820013\n",
      "epoch:  146, loss: 0.008265442214906216\n",
      "epoch:  147, loss: 0.0082620307803154\n",
      "epoch:  148, loss: 0.008228461258113384\n",
      "epoch:  149, loss: 0.008228462189435959\n",
      "epoch:  150, loss: 0.00822176318615675\n",
      "epoch:  151, loss: 0.00822124257683754\n",
      "epoch:  152, loss: 0.00822124257683754\n",
      "epoch:  153, loss: 0.008196357637643814\n",
      "epoch:  154, loss: 0.0081923333927989\n",
      "epoch:  155, loss: 0.008183960802853107\n",
      "epoch:  156, loss: 0.008183960802853107\n",
      "epoch:  157, loss: 0.008162610232830048\n",
      "epoch:  158, loss: 0.008159138262271881\n",
      "epoch:  159, loss: 0.008147881366312504\n",
      "epoch:  160, loss: 0.008147881366312504\n",
      "epoch:  161, loss: 0.008132750168442726\n",
      "epoch:  162, loss: 0.008130062371492386\n",
      "epoch:  163, loss: 0.008118192665278912\n",
      "epoch:  164, loss: 0.008118192665278912\n",
      "epoch:  165, loss: 0.008106797002255917\n",
      "epoch:  166, loss: 0.008104664273560047\n",
      "epoch:  167, loss: 0.008090587332844734\n",
      "epoch:  168, loss: 0.008090587332844734\n",
      "epoch:  169, loss: 0.008084068074822426\n",
      "epoch:  170, loss: 0.00808241218328476\n",
      "epoch:  171, loss: 0.008068135008215904\n",
      "epoch:  172, loss: 0.008068135008215904\n",
      "epoch:  173, loss: 0.008064227178692818\n",
      "epoch:  174, loss: 0.008062861859798431\n",
      "epoch:  175, loss: 0.008018894121050835\n",
      "epoch:  176, loss: 0.008018894121050835\n",
      "epoch:  177, loss: 0.007962001487612724\n",
      "epoch:  178, loss: 0.007954875938594341\n",
      "epoch:  179, loss: 0.007953892461955547\n",
      "epoch:  180, loss: 0.007951280102133751\n",
      "epoch:  181, loss: 0.007951280102133751\n",
      "epoch:  182, loss: 0.007949751801788807\n",
      "epoch:  183, loss: 0.007949340157210827\n",
      "epoch:  184, loss: 0.007948932237923145\n",
      "epoch:  185, loss: 0.007946154102683067\n",
      "epoch:  186, loss: 0.007946154102683067\n",
      "epoch:  187, loss: 0.007945481687784195\n",
      "epoch:  188, loss: 0.007945412769913673\n",
      "epoch:  189, loss: 0.007945412769913673\n",
      "epoch:  190, loss: 0.00794306956231594\n",
      "epoch:  191, loss: 0.007942683063447475\n",
      "epoch:  192, loss: 0.007941270247101784\n",
      "epoch:  193, loss: 0.007941270247101784\n",
      "epoch:  194, loss: 0.007939551025629044\n",
      "epoch:  195, loss: 0.007939223200082779\n",
      "epoch:  196, loss: 0.00793803483247757\n",
      "epoch:  197, loss: 0.007938033901154995\n",
      "epoch:  198, loss: 0.007936370559036732\n",
      "epoch:  199, loss: 0.00793605949729681\n",
      "epoch:  200, loss: 0.007935111410915852\n",
      "epoch:  201, loss: 0.007935111410915852\n",
      "epoch:  202, loss: 0.007933419197797775\n",
      "epoch:  203, loss: 0.007933112792670727\n",
      "epoch:  204, loss: 0.007932332344353199\n",
      "epoch:  205, loss: 0.007932332344353199\n",
      "epoch:  206, loss: 0.007930632680654526\n",
      "epoch:  207, loss: 0.007930333726108074\n",
      "epoch:  208, loss: 0.007929682731628418\n",
      "epoch:  209, loss: 0.007929682731628418\n",
      "epoch:  210, loss: 0.007927986793220043\n",
      "epoch:  211, loss: 0.007927692495286465\n",
      "epoch:  212, loss: 0.007927197962999344\n",
      "epoch:  213, loss: 0.007927197962999344\n",
      "epoch:  214, loss: 0.0079254861921072\n",
      "epoch:  215, loss: 0.007925194688141346\n",
      "epoch:  216, loss: 0.007924871519207954\n",
      "epoch:  217, loss: 0.00792487058788538\n",
      "epoch:  218, loss: 0.00792311504483223\n",
      "epoch:  219, loss: 0.00792282447218895\n",
      "epoch:  220, loss: 0.007922681979835033\n",
      "epoch:  221, loss: 0.007922681979835033\n",
      "epoch:  222, loss: 0.007920851930975914\n",
      "epoch:  223, loss: 0.007920556701719761\n",
      "epoch:  224, loss: 0.007920355536043644\n",
      "epoch:  225, loss: 0.007911914959549904\n",
      "epoch:  226, loss: 0.007911914959549904\n",
      "epoch:  227, loss: 0.007904681377112865\n",
      "epoch:  228, loss: 0.007903818041086197\n",
      "epoch:  229, loss: 0.00790360663086176\n",
      "epoch:  230, loss: 0.007902318611741066\n",
      "epoch:  231, loss: 0.007902318611741066\n",
      "epoch:  232, loss: 0.007902010343968868\n",
      "epoch:  233, loss: 0.007901495322585106\n",
      "epoch:  234, loss: 0.007901495322585106\n",
      "epoch:  235, loss: 0.007900667376816273\n",
      "epoch:  236, loss: 0.007900494150817394\n",
      "epoch:  237, loss: 0.007899517193436623\n",
      "epoch:  238, loss: 0.007899517193436623\n",
      "epoch:  239, loss: 0.007898827083408833\n",
      "epoch:  240, loss: 0.007898672483861446\n",
      "epoch:  241, loss: 0.007897621020674706\n",
      "epoch:  242, loss: 0.007897621020674706\n",
      "epoch:  243, loss: 0.007897112518548965\n",
      "epoch:  244, loss: 0.007896972820162773\n",
      "epoch:  245, loss: 0.007896081544458866\n",
      "epoch:  246, loss: 0.007896081544458866\n",
      "epoch:  247, loss: 0.00789544079452753\n",
      "epoch:  248, loss: 0.007895289920270443\n",
      "epoch:  249, loss: 0.007894536480307579\n",
      "epoch:  250, loss: 0.007894536480307579\n",
      "epoch:  251, loss: 0.007893762551248074\n",
      "epoch:  252, loss: 0.007893597707152367\n",
      "epoch:  253, loss: 0.00789291225373745\n",
      "epoch:  254, loss: 0.00789291225373745\n",
      "epoch:  255, loss: 0.007892049849033356\n",
      "epoch:  256, loss: 0.007891875691711903\n",
      "epoch:  257, loss: 0.007891328074038029\n",
      "epoch:  258, loss: 0.007891328074038029\n",
      "epoch:  259, loss: 0.007890332490205765\n",
      "epoch:  260, loss: 0.007890142500400543\n",
      "epoch:  261, loss: 0.007889256812632084\n",
      "epoch:  262, loss: 0.007889256812632084\n",
      "epoch:  263, loss: 0.00788853969424963\n",
      "epoch:  264, loss: 0.0078883720561862\n",
      "epoch:  265, loss: 0.007887604646384716\n",
      "epoch:  266, loss: 0.007887604646384716\n",
      "epoch:  267, loss: 0.007886746898293495\n",
      "epoch:  268, loss: 0.00788656435906887\n",
      "epoch:  269, loss: 0.007885988801717758\n",
      "epoch:  270, loss: 0.007885988801717758\n",
      "epoch:  271, loss: 0.00788493175059557\n",
      "epoch:  272, loss: 0.007884723134338856\n",
      "epoch:  273, loss: 0.007884184829890728\n",
      "epoch:  274, loss: 0.007884184829890728\n",
      "epoch:  275, loss: 0.007882993668317795\n",
      "epoch:  276, loss: 0.007882770150899887\n",
      "epoch:  277, loss: 0.007881849072873592\n",
      "epoch:  278, loss: 0.007881849072873592\n",
      "epoch:  279, loss: 0.007880990393459797\n",
      "epoch:  280, loss: 0.007880797609686852\n",
      "epoch:  281, loss: 0.007879987359046936\n",
      "epoch:  282, loss: 0.007879987359046936\n",
      "epoch:  283, loss: 0.007878984324634075\n",
      "epoch:  284, loss: 0.007878771051764488\n",
      "epoch:  285, loss: 0.00787804089486599\n",
      "epoch:  286, loss: 0.00787804089486599\n",
      "epoch:  287, loss: 0.007876912131905556\n",
      "epoch:  288, loss: 0.007876688614487648\n",
      "epoch:  289, loss: 0.007876201532781124\n",
      "epoch:  290, loss: 0.007876201532781124\n",
      "epoch:  291, loss: 0.007874901406466961\n",
      "epoch:  292, loss: 0.007874671369791031\n",
      "epoch:  293, loss: 0.007873903959989548\n",
      "epoch:  294, loss: 0.007873903959989548\n",
      "epoch:  295, loss: 0.007872922345995903\n",
      "epoch:  296, loss: 0.007872719317674637\n",
      "epoch:  297, loss: 0.007872163318097591\n",
      "epoch:  298, loss: 0.007872163318097591\n",
      "epoch:  299, loss: 0.007870996370911598\n",
      "epoch:  300, loss: 0.007870770990848541\n",
      "epoch:  301, loss: 0.007870389148592949\n",
      "epoch:  302, loss: 0.007870389148592949\n",
      "epoch:  303, loss: 0.00786901917308569\n",
      "epoch:  304, loss: 0.007868784479796886\n",
      "epoch:  305, loss: 0.007868113927543163\n",
      "epoch:  306, loss: 0.007868113927543163\n",
      "epoch:  307, loss: 0.007867008447647095\n",
      "epoch:  308, loss: 0.00786678772419691\n",
      "epoch:  309, loss: 0.007866375148296356\n",
      "epoch:  310, loss: 0.007866374216973782\n",
      "epoch:  311, loss: 0.00786503404378891\n",
      "epoch:  312, loss: 0.007864794693887234\n",
      "epoch:  313, loss: 0.007863998413085938\n",
      "epoch:  314, loss: 0.007863998413085938\n",
      "epoch:  315, loss: 0.007862988859415054\n",
      "epoch:  316, loss: 0.007862781174480915\n",
      "epoch:  317, loss: 0.007862219586968422\n",
      "epoch:  318, loss: 0.007862219586968422\n",
      "epoch:  319, loss: 0.00786103680729866\n",
      "epoch:  320, loss: 0.007860817015171051\n",
      "epoch:  321, loss: 0.007860414683818817\n",
      "epoch:  322, loss: 0.007860414683818817\n",
      "epoch:  323, loss: 0.0078591238707304\n",
      "epoch:  324, loss: 0.007858864963054657\n",
      "epoch:  325, loss: 0.007858672179281712\n",
      "epoch:  326, loss: 0.007858612574636936\n",
      "epoch:  327, loss: 0.007858636789023876\n",
      "epoch:  328, loss: 0.00784405879676342\n",
      "epoch:  329, loss: 0.007842679508030415\n",
      "epoch:  330, loss: 0.007842425256967545\n",
      "epoch:  331, loss: 0.007827142253518105\n",
      "epoch:  332, loss: 0.007827142253518105\n",
      "epoch:  333, loss: 0.007826643995940685\n",
      "epoch:  334, loss: 0.007826493121683598\n",
      "epoch:  335, loss: 0.007825398817658424\n",
      "epoch:  336, loss: 0.007825398817658424\n",
      "epoch:  337, loss: 0.007824760861694813\n",
      "epoch:  338, loss: 0.007824597880244255\n",
      "epoch:  339, loss: 0.00782361626625061\n",
      "epoch:  340, loss: 0.00782361626625061\n",
      "epoch:  341, loss: 0.007822929881513119\n",
      "epoch:  342, loss: 0.007822766900062561\n",
      "epoch:  343, loss: 0.007821784354746342\n",
      "epoch:  344, loss: 0.007821783423423767\n",
      "epoch:  345, loss: 0.007821116596460342\n",
      "epoch:  346, loss: 0.00782095454633236\n",
      "epoch:  347, loss: 0.007819964550435543\n",
      "epoch:  348, loss: 0.007819963619112968\n",
      "epoch:  349, loss: 0.007819315418601036\n",
      "epoch:  350, loss: 0.007819169200956821\n",
      "epoch:  351, loss: 0.00781828723847866\n",
      "epoch:  352, loss: 0.00781828723847866\n",
      "epoch:  353, loss: 0.007817546837031841\n",
      "epoch:  354, loss: 0.007817375473678112\n",
      "epoch:  355, loss: 0.00781659409403801\n",
      "epoch:  356, loss: 0.00781659409403801\n",
      "epoch:  357, loss: 0.007815738208591938\n",
      "epoch:  358, loss: 0.007815548218786716\n",
      "epoch:  359, loss: 0.007815382443368435\n",
      "epoch:  360, loss: 0.007814371958374977\n",
      "epoch:  361, loss: 0.007814371958374977\n",
      "epoch:  362, loss: 0.00781394261866808\n",
      "epoch:  363, loss: 0.007813816890120506\n",
      "epoch:  364, loss: 0.007813169620931149\n",
      "epoch:  365, loss: 0.007813169620931149\n",
      "epoch:  366, loss: 0.007812228053808212\n",
      "epoch:  367, loss: 0.007812042720615864\n",
      "epoch:  368, loss: 0.007811880204826593\n",
      "epoch:  369, loss: 0.007798556704074144\n",
      "epoch:  370, loss: 0.007797505706548691\n",
      "epoch:  371, loss: 0.007797505706548691\n",
      "epoch:  372, loss: 0.0077971844002604485\n",
      "epoch:  373, loss: 0.0077970754355192184\n",
      "epoch:  374, loss: 0.007795693818479776\n",
      "epoch:  375, loss: 0.007795693818479776\n",
      "epoch:  376, loss: 0.007795444689691067\n",
      "epoch:  377, loss: 0.007795161101967096\n",
      "epoch:  378, loss: 0.007795161101967096\n",
      "epoch:  379, loss: 0.007794408593326807\n",
      "epoch:  380, loss: 0.0077942474745213985\n",
      "epoch:  381, loss: 0.007794094737619162\n",
      "epoch:  382, loss: 0.007784126792103052\n",
      "epoch:  383, loss: 0.007784126792103052\n",
      "epoch:  384, loss: 0.007782577071338892\n",
      "epoch:  385, loss: 0.007782303728163242\n",
      "epoch:  386, loss: 0.007782110013067722\n",
      "epoch:  387, loss: 0.007782110013067722\n",
      "epoch:  388, loss: 0.0077817440032958984\n",
      "epoch:  389, loss: 0.0077817440032958984\n",
      "epoch:  390, loss: 0.007780720014125109\n",
      "epoch:  391, loss: 0.007780530955642462\n",
      "epoch:  392, loss: 0.007780364714562893\n",
      "epoch:  393, loss: 0.007780364714562893\n",
      "epoch:  394, loss: 0.007779212668538094\n",
      "epoch:  395, loss: 0.0077789900824427605\n",
      "epoch:  396, loss: 0.0077788447961211205\n",
      "epoch:  397, loss: 0.00777786411345005\n",
      "epoch:  398, loss: 0.00777786411345005\n",
      "epoch:  399, loss: 0.007777586113661528\n",
      "epoch:  400, loss: 0.0077774845995008945\n",
      "epoch:  401, loss: 0.007775661535561085\n",
      "epoch:  402, loss: 0.007775662001222372\n",
      "epoch:  403, loss: 0.007766013033688068\n",
      "epoch:  404, loss: 0.007764763198792934\n",
      "epoch:  405, loss: 0.007764552719891071\n",
      "epoch:  406, loss: 0.007763956673443317\n",
      "epoch:  407, loss: 0.007763956673443317\n",
      "epoch:  408, loss: 0.007763465866446495\n",
      "epoch:  409, loss: 0.0077633424662053585\n",
      "epoch:  410, loss: 0.0077626146376132965\n",
      "epoch:  411, loss: 0.0077626146376132965\n",
      "epoch:  412, loss: 0.007762082852423191\n",
      "epoch:  413, loss: 0.007761957589536905\n",
      "epoch:  414, loss: 0.00776118878275156\n",
      "epoch:  415, loss: 0.00776118878275156\n",
      "epoch:  416, loss: 0.0077607035636901855\n",
      "epoch:  417, loss: 0.0077605783008039\n",
      "epoch:  418, loss: 0.007759717758744955\n",
      "epoch:  419, loss: 0.007759717758744955\n",
      "epoch:  420, loss: 0.007759236264973879\n",
      "epoch:  421, loss: 0.007759107276797295\n",
      "epoch:  422, loss: 0.0077581931836903095\n",
      "epoch:  423, loss: 0.0077581931836903095\n",
      "epoch:  424, loss: 0.007757747080177069\n",
      "epoch:  425, loss: 0.007757620420306921\n",
      "epoch:  426, loss: 0.00775672122836113\n",
      "epoch:  427, loss: 0.00775672122836113\n",
      "epoch:  428, loss: 0.007756243925541639\n",
      "epoch:  429, loss: 0.007756110280752182\n",
      "epoch:  430, loss: 0.007755219470709562\n",
      "epoch:  431, loss: 0.007755218539386988\n",
      "epoch:  432, loss: 0.007754694204777479\n",
      "epoch:  433, loss: 0.007754558231681585\n",
      "epoch:  434, loss: 0.007753963582217693\n",
      "epoch:  435, loss: 0.007753963582217693\n",
      "epoch:  436, loss: 0.007753238547593355\n",
      "epoch:  437, loss: 0.007753065787255764\n",
      "epoch:  438, loss: 0.007752920966595411\n",
      "epoch:  439, loss: 0.0077518438920378685\n",
      "epoch:  440, loss: 0.0077518438920378685\n",
      "epoch:  441, loss: 0.007751584518700838\n",
      "epoch:  442, loss: 0.007751462049782276\n",
      "epoch:  443, loss: 0.007751462049782276\n",
      "epoch:  444, loss: 0.007750596385449171\n",
      "epoch:  445, loss: 0.007750434800982475\n",
      "epoch:  446, loss: 0.007749857846647501\n",
      "epoch:  447, loss: 0.007749857846647501\n",
      "epoch:  448, loss: 0.007749030366539955\n",
      "epoch:  449, loss: 0.00774887390434742\n",
      "epoch:  450, loss: 0.007748328614979982\n",
      "epoch:  451, loss: 0.007748328614979982\n",
      "epoch:  452, loss: 0.00774749182164669\n",
      "epoch:  453, loss: 0.0077473316341638565\n",
      "epoch:  454, loss: 0.007746740709990263\n",
      "epoch:  455, loss: 0.007746740709990263\n",
      "epoch:  456, loss: 0.007745936047285795\n",
      "epoch:  457, loss: 0.007745778653770685\n",
      "epoch:  458, loss: 0.007745449431240559\n",
      "epoch:  459, loss: 0.007745448965579271\n",
      "epoch:  460, loss: 0.007744463626295328\n",
      "epoch:  461, loss: 0.007744288071990013\n",
      "epoch:  462, loss: 0.007744108326733112\n",
      "epoch:  463, loss: 0.007744108326733112\n",
      "epoch:  464, loss: 0.007743011228740215\n",
      "epoch:  465, loss: 0.00774282356724143\n",
      "epoch:  466, loss: 0.007742705289274454\n",
      "epoch:  467, loss: 0.007742705289274454\n",
      "epoch:  468, loss: 0.007741571869701147\n",
      "epoch:  469, loss: 0.007741376291960478\n",
      "epoch:  470, loss: 0.007741237990558147\n",
      "epoch:  471, loss: 0.007730423007160425\n",
      "epoch:  472, loss: 0.007730423007160425\n",
      "epoch:  473, loss: 0.007730204612016678\n",
      "epoch:  474, loss: 0.007729636039584875\n",
      "epoch:  475, loss: 0.007729636039584875\n",
      "epoch:  476, loss: 0.007729180622845888\n",
      "epoch:  477, loss: 0.007729049772024155\n",
      "epoch:  478, loss: 0.007728912401944399\n",
      "epoch:  479, loss: 0.007722613401710987\n",
      "epoch:  480, loss: 0.007722613401710987\n",
      "epoch:  481, loss: 0.007718586828559637\n",
      "epoch:  482, loss: 0.007717965170741081\n",
      "epoch:  483, loss: 0.007717744912952185\n",
      "epoch:  484, loss: 0.007708802353590727\n",
      "epoch:  485, loss: 0.007708802353590727\n",
      "epoch:  486, loss: 0.00770771037787199\n",
      "epoch:  487, loss: 0.007707500830292702\n",
      "epoch:  488, loss: 0.007707316428422928\n",
      "epoch:  489, loss: 0.007697767578065395\n",
      "epoch:  490, loss: 0.007697767578065395\n",
      "epoch:  491, loss: 0.007695139851421118\n",
      "epoch:  492, loss: 0.007694720756262541\n",
      "epoch:  493, loss: 0.007694523315876722\n",
      "epoch:  494, loss: 0.007693490479141474\n",
      "epoch:  495, loss: 0.007693490479141474\n",
      "epoch:  496, loss: 0.007693106308579445\n",
      "epoch:  497, loss: 0.007692990358918905\n",
      "epoch:  498, loss: 0.0076916893012821674\n",
      "epoch:  499, loss: 0.0076916893012821674\n",
      "epoch:  500, loss: 0.0076913596130907536\n",
      "epoch:  501, loss: 0.007691245526075363\n",
      "epoch:  502, loss: 0.007689871825277805\n",
      "epoch:  503, loss: 0.007689871825277805\n",
      "epoch:  504, loss: 0.007689609657973051\n",
      "epoch:  505, loss: 0.007689048536121845\n",
      "epoch:  506, loss: 0.007689048070460558\n",
      "epoch:  507, loss: 0.007688464596867561\n",
      "epoch:  508, loss: 0.007688326295465231\n",
      "epoch:  509, loss: 0.007688148412853479\n",
      "epoch:  510, loss: 0.007688148412853479\n",
      "epoch:  511, loss: 0.00768681988120079\n",
      "epoch:  512, loss: 0.00768681988120079\n",
      "epoch:  513, loss: 0.007686577271670103\n",
      "epoch:  514, loss: 0.007685725577175617\n",
      "epoch:  515, loss: 0.007685725577175617\n",
      "epoch:  516, loss: 0.007685291580855846\n",
      "epoch:  517, loss: 0.007685155142098665\n",
      "epoch:  518, loss: 0.007684339303523302\n",
      "epoch:  519, loss: 0.007684339303523302\n",
      "epoch:  520, loss: 0.007683712989091873\n",
      "epoch:  521, loss: 0.0076835560612380505\n",
      "epoch:  522, loss: 0.00768326036632061\n",
      "epoch:  523, loss: 0.00768326036632061\n",
      "epoch:  524, loss: 0.007682216819375753\n",
      "epoch:  525, loss: 0.007681997027248144\n",
      "epoch:  526, loss: 0.007681841962039471\n",
      "epoch:  527, loss: 0.007680865470319986\n",
      "epoch:  528, loss: 0.007680865470319986\n",
      "epoch:  529, loss: 0.007680556271225214\n",
      "epoch:  530, loss: 0.007680450100451708\n",
      "epoch:  531, loss: 0.007679339032620192\n",
      "epoch:  532, loss: 0.007679339032620192\n",
      "epoch:  533, loss: 0.007679030764847994\n",
      "epoch:  534, loss: 0.00767892599105835\n",
      "epoch:  535, loss: 0.00767782935872674\n",
      "epoch:  536, loss: 0.007677828893065453\n",
      "epoch:  537, loss: 0.007677509915083647\n",
      "epoch:  538, loss: 0.007677404675632715\n",
      "epoch:  539, loss: 0.007676328998059034\n",
      "epoch:  540, loss: 0.007676328532397747\n",
      "epoch:  541, loss: 0.007675995118916035\n",
      "epoch:  542, loss: 0.007675887551158667\n",
      "epoch:  543, loss: 0.0076748039573431015\n",
      "epoch:  544, loss: 0.0076748039573431015\n",
      "epoch:  545, loss: 0.00767446169629693\n",
      "epoch:  546, loss: 0.007674363907426596\n",
      "epoch:  547, loss: 0.007673528045415878\n",
      "epoch:  548, loss: 0.007673528045415878\n",
      "epoch:  549, loss: 0.007672979962080717\n",
      "epoch:  550, loss: 0.007672853302210569\n",
      "epoch:  551, loss: 0.007672321982681751\n",
      "epoch:  552, loss: 0.007672321051359177\n",
      "epoch:  553, loss: 0.0076715038157999516\n",
      "epoch:  554, loss: 0.007671339903026819\n",
      "epoch:  555, loss: 0.007671200204640627\n",
      "epoch:  556, loss: 0.00767058739438653\n",
      "epoch:  557, loss: 0.00767058739438653\n",
      "epoch:  558, loss: 0.007670077029615641\n",
      "epoch:  559, loss: 0.00766995782032609\n",
      "epoch:  560, loss: 0.007669803686439991\n",
      "epoch:  561, loss: 0.007669735699892044\n",
      "epoch:  562, loss: 0.007669735699892044\n",
      "epoch:  563, loss: 0.007669556885957718\n",
      "epoch:  564, loss: 0.007668852340430021\n",
      "epoch:  565, loss: 0.007668852340430021\n",
      "epoch:  566, loss: 0.007668405305594206\n",
      "epoch:  567, loss: 0.007668288890272379\n",
      "epoch:  568, loss: 0.007667479570955038\n",
      "epoch:  569, loss: 0.007667479570955038\n",
      "epoch:  570, loss: 0.007666862104088068\n",
      "epoch:  571, loss: 0.00766673032194376\n",
      "epoch:  572, loss: 0.007666099816560745\n",
      "epoch:  573, loss: 0.007666099816560745\n",
      "epoch:  574, loss: 0.007665323559194803\n",
      "epoch:  575, loss: 0.0076651726849377155\n",
      "epoch:  576, loss: 0.0076651014387607574\n",
      "epoch:  577, loss: 0.0076651014387607574\n",
      "epoch:  578, loss: 0.007663837634027004\n",
      "epoch:  579, loss: 0.007663613185286522\n",
      "epoch:  580, loss: 0.007663464639335871\n",
      "epoch:  581, loss: 0.007650918792933226\n",
      "epoch:  582, loss: 0.007650918792933226\n",
      "epoch:  583, loss: 0.007650716695934534\n",
      "epoch:  584, loss: 0.007639952935278416\n",
      "epoch:  585, loss: 0.007639952935278416\n",
      "epoch:  586, loss: 0.00763945234939456\n",
      "epoch:  587, loss: 0.007639334071427584\n",
      "epoch:  588, loss: 0.007638986222445965\n",
      "epoch:  589, loss: 0.007638986222445965\n",
      "epoch:  590, loss: 0.0076374756172299385\n",
      "epoch:  591, loss: 0.0076372623443603516\n",
      "epoch:  592, loss: 0.0076365903951227665\n",
      "epoch:  593, loss: 0.0076365903951227665\n",
      "epoch:  594, loss: 0.007635486777871847\n",
      "epoch:  595, loss: 0.0076353102922439575\n",
      "epoch:  596, loss: 0.007634275127202272\n",
      "epoch:  597, loss: 0.007634275127202272\n",
      "epoch:  598, loss: 0.007633547764271498\n",
      "epoch:  599, loss: 0.007633408531546593\n",
      "epoch:  600, loss: 0.0076321568340063095\n",
      "epoch:  601, loss: 0.0076321568340063095\n",
      "epoch:  602, loss: 0.007631698157638311\n",
      "epoch:  603, loss: 0.007631576154381037\n",
      "epoch:  604, loss: 0.007628816179931164\n",
      "epoch:  605, loss: 0.007628816179931164\n",
      "epoch:  606, loss: 0.0076185124926269054\n",
      "epoch:  607, loss: 0.007617218419909477\n",
      "epoch:  608, loss: 0.007616976276040077\n",
      "epoch:  609, loss: 0.007615991402417421\n",
      "epoch:  610, loss: 0.007615990471094847\n",
      "epoch:  611, loss: 0.007615718990564346\n",
      "epoch:  612, loss: 0.0076149832457304\n",
      "epoch:  613, loss: 0.0076149832457304\n",
      "epoch:  614, loss: 0.007614250294864178\n",
      "epoch:  615, loss: 0.007614085450768471\n",
      "epoch:  616, loss: 0.007602447643876076\n",
      "epoch:  617, loss: 0.007602447643876076\n",
      "epoch:  618, loss: 0.007599038537591696\n",
      "epoch:  619, loss: 0.007598421536386013\n",
      "epoch:  620, loss: 0.00759816775098443\n",
      "epoch:  621, loss: 0.007597618270665407\n",
      "epoch:  622, loss: 0.0075976173393428326\n",
      "epoch:  623, loss: 0.007597003597766161\n",
      "epoch:  624, loss: 0.007596834097057581\n",
      "epoch:  625, loss: 0.0075966231524944305\n",
      "epoch:  626, loss: 0.007584583945572376\n",
      "epoch:  627, loss: 0.007584583945572376\n",
      "epoch:  628, loss: 0.007583539932966232\n",
      "epoch:  629, loss: 0.00758332060649991\n",
      "epoch:  630, loss: 0.007583077065646648\n",
      "epoch:  631, loss: 0.007575139869004488\n",
      "epoch:  632, loss: 0.007575139869004488\n",
      "epoch:  633, loss: 0.007569598965346813\n",
      "epoch:  634, loss: 0.007568744011223316\n",
      "epoch:  635, loss: 0.007568340748548508\n",
      "epoch:  636, loss: 0.00756650697439909\n",
      "epoch:  637, loss: 0.00756650697439909\n",
      "epoch:  638, loss: 0.007566043175756931\n",
      "epoch:  639, loss: 0.007565404288470745\n",
      "epoch:  640, loss: 0.007565404288470745\n",
      "epoch:  641, loss: 0.007564457133412361\n",
      "epoch:  642, loss: 0.007564219180494547\n",
      "epoch:  643, loss: 0.0075639779679477215\n",
      "epoch:  644, loss: 0.007555331569164991\n",
      "epoch:  645, loss: 0.007555331569164991\n",
      "epoch:  646, loss: 0.0075476705096662045\n",
      "epoch:  647, loss: 0.007546675857156515\n",
      "epoch:  648, loss: 0.007546379696577787\n",
      "epoch:  649, loss: 0.007544931955635548\n",
      "epoch:  650, loss: 0.007544931955635548\n",
      "epoch:  651, loss: 0.0075443401001393795\n",
      "epoch:  652, loss: 0.007544161751866341\n",
      "epoch:  653, loss: 0.007542404346168041\n",
      "epoch:  654, loss: 0.007542404346168041\n",
      "epoch:  655, loss: 0.007541856728494167\n",
      "epoch:  656, loss: 0.0075416844338178635\n",
      "epoch:  657, loss: 0.007540980353951454\n",
      "epoch:  658, loss: 0.007540980353951454\n",
      "epoch:  659, loss: 0.007539631333202124\n",
      "epoch:  660, loss: 0.007539363577961922\n",
      "epoch:  661, loss: 0.007539142854511738\n",
      "epoch:  662, loss: 0.007526485715061426\n",
      "epoch:  663, loss: 0.007526485715061426\n",
      "epoch:  664, loss: 0.007521273102611303\n",
      "epoch:  665, loss: 0.007520490325987339\n",
      "epoch:  666, loss: 0.0075202095322310925\n",
      "epoch:  667, loss: 0.00751879159361124\n",
      "epoch:  668, loss: 0.00751879159361124\n",
      "epoch:  669, loss: 0.007518223952502012\n",
      "epoch:  670, loss: 0.007518033962696791\n",
      "epoch:  671, loss: 0.007517597638070583\n",
      "epoch:  672, loss: 0.007517597638070583\n",
      "epoch:  673, loss: 0.007515871897339821\n",
      "epoch:  674, loss: 0.007515543140470982\n",
      "epoch:  675, loss: 0.007515305653214455\n",
      "epoch:  676, loss: 0.007501453161239624\n",
      "epoch:  677, loss: 0.007501453161239624\n",
      "epoch:  678, loss: 0.007496681064367294\n",
      "epoch:  679, loss: 0.0074959308840334415\n",
      "epoch:  680, loss: 0.007495627272874117\n",
      "epoch:  681, loss: 0.007494031917303801\n",
      "epoch:  682, loss: 0.007494030985981226\n",
      "epoch:  683, loss: 0.007493525743484497\n",
      "epoch:  684, loss: 0.007493338547646999\n",
      "epoch:  685, loss: 0.007491725496947765\n",
      "epoch:  686, loss: 0.007491725496947765\n",
      "epoch:  687, loss: 0.007490783929824829\n",
      "epoch:  688, loss: 0.007490554358810186\n",
      "epoch:  689, loss: 0.007489896845072508\n",
      "epoch:  690, loss: 0.0074898963794112206\n",
      "epoch:  691, loss: 0.007488076109439135\n",
      "epoch:  692, loss: 0.007487737573683262\n",
      "epoch:  693, loss: 0.007487470284104347\n",
      "epoch:  694, loss: 0.007466710638254881\n",
      "epoch:  695, loss: 0.007466710638254881\n",
      "epoch:  696, loss: 0.007466237060725689\n",
      "epoch:  697, loss: 0.007462164852768183\n",
      "epoch:  698, loss: 0.007462164852768183\n",
      "epoch:  699, loss: 0.007449866738170385\n",
      "epoch:  700, loss: 0.007448094896972179\n",
      "epoch:  701, loss: 0.007447600364685059\n",
      "epoch:  702, loss: 0.007435045670717955\n",
      "epoch:  703, loss: 0.007435045670717955\n",
      "epoch:  704, loss: 0.0074330721981823444\n",
      "epoch:  705, loss: 0.007432783953845501\n",
      "epoch:  706, loss: 0.007432324346154928\n",
      "epoch:  707, loss: 0.007432324346154928\n",
      "epoch:  708, loss: 0.0074293301440775394\n",
      "epoch:  709, loss: 0.0074293301440775394\n",
      "epoch:  710, loss: 0.0074281636625528336\n",
      "epoch:  711, loss: 0.007428084500133991\n",
      "epoch:  712, loss: 0.007428084500133991\n",
      "epoch:  713, loss: 0.007426268421113491\n",
      "epoch:  714, loss: 0.007425916846841574\n",
      "epoch:  715, loss: 0.007425521034747362\n",
      "epoch:  716, loss: 0.007425014860928059\n",
      "epoch:  717, loss: 0.007425014860928059\n",
      "epoch:  718, loss: 0.007423738949000835\n",
      "epoch:  719, loss: 0.007423480041325092\n",
      "epoch:  720, loss: 0.007423085160553455\n",
      "epoch:  721, loss: 0.007422716822475195\n",
      "epoch:  722, loss: 0.007422716822475195\n",
      "epoch:  723, loss: 0.007421247195452452\n",
      "epoch:  724, loss: 0.007420957554131746\n",
      "epoch:  725, loss: 0.00742054358124733\n",
      "epoch:  726, loss: 0.007420274429023266\n",
      "epoch:  727, loss: 0.0074202739633619785\n",
      "epoch:  728, loss: 0.007418663706630468\n",
      "epoch:  729, loss: 0.0074183521792292595\n",
      "epoch:  730, loss: 0.007417942397296429\n",
      "epoch:  731, loss: 0.007416205946356058\n",
      "epoch:  732, loss: 0.007416205946356058\n",
      "epoch:  733, loss: 0.007415449246764183\n",
      "epoch:  734, loss: 0.007409284822642803\n",
      "epoch:  735, loss: 0.007409283891320229\n",
      "epoch:  736, loss: 0.0073995147831737995\n",
      "epoch:  737, loss: 0.007398111745715141\n",
      "epoch:  738, loss: 0.007397590670734644\n",
      "epoch:  739, loss: 0.007396617904305458\n",
      "epoch:  740, loss: 0.007396617438644171\n",
      "epoch:  741, loss: 0.007395785301923752\n",
      "epoch:  742, loss: 0.007382535375654697\n",
      "epoch:  743, loss: 0.007382535375654697\n",
      "epoch:  744, loss: 0.007376940455287695\n",
      "epoch:  745, loss: 0.007376226596534252\n",
      "epoch:  746, loss: 0.007375634275376797\n",
      "epoch:  747, loss: 0.007375633809715509\n",
      "epoch:  748, loss: 0.007371782790869474\n",
      "epoch:  749, loss: 0.007371782790869474\n",
      "epoch:  750, loss: 0.0073702456429600716\n",
      "epoch:  751, loss: 0.007368730381131172\n",
      "epoch:  752, loss: 0.007368730381131172\n",
      "epoch:  753, loss: 0.00736733665689826\n",
      "epoch:  754, loss: 0.007365336641669273\n",
      "epoch:  755, loss: 0.007365335710346699\n",
      "epoch:  756, loss: 0.007364400662481785\n",
      "epoch:  757, loss: 0.007362608797848225\n",
      "epoch:  758, loss: 0.007362608797848225\n",
      "epoch:  759, loss: 0.007361690513789654\n",
      "epoch:  760, loss: 0.007360096089541912\n",
      "epoch:  761, loss: 0.007360096089541912\n",
      "epoch:  762, loss: 0.0073595428839325905\n",
      "epoch:  763, loss: 0.007357786409556866\n",
      "epoch:  764, loss: 0.007357786409556866\n",
      "epoch:  765, loss: 0.007356899790465832\n",
      "epoch:  766, loss: 0.007355961017310619\n",
      "epoch:  767, loss: 0.007355961017310619\n",
      "epoch:  768, loss: 0.007354214787483215\n",
      "epoch:  769, loss: 0.007353929337114096\n",
      "epoch:  770, loss: 0.007353403605520725\n",
      "epoch:  771, loss: 0.007353403605520725\n",
      "epoch:  772, loss: 0.00734980683773756\n",
      "epoch:  773, loss: 0.00734980683773756\n",
      "epoch:  774, loss: 0.007348444312810898\n",
      "epoch:  775, loss: 0.00734707759693265\n",
      "epoch:  776, loss: 0.00734707759693265\n",
      "epoch:  777, loss: 0.0073454962112009525\n",
      "epoch:  778, loss: 0.007345305290073156\n",
      "epoch:  779, loss: 0.007345305290073156\n",
      "epoch:  780, loss: 0.007342739496380091\n",
      "epoch:  781, loss: 0.007342268247157335\n",
      "epoch:  782, loss: 0.0073417979292571545\n",
      "epoch:  783, loss: 0.007314229384064674\n",
      "epoch:  784, loss: 0.007314229384064674\n",
      "epoch:  785, loss: 0.007309683598577976\n",
      "epoch:  786, loss: 0.0073089599609375\n",
      "epoch:  787, loss: 0.007308475207537413\n",
      "epoch:  788, loss: 0.007305707782506943\n",
      "epoch:  789, loss: 0.0073057073168456554\n",
      "epoch:  790, loss: 0.00730484863743186\n",
      "epoch:  791, loss: 0.007302377838641405\n",
      "epoch:  792, loss: 0.007302377372980118\n",
      "epoch:  793, loss: 0.007301379926502705\n",
      "epoch:  794, loss: 0.007299844175577164\n",
      "epoch:  795, loss: 0.007299844175577164\n",
      "epoch:  796, loss: 0.007298129610717297\n",
      "epoch:  797, loss: 0.007297702599316835\n",
      "epoch:  798, loss: 0.00729618314653635\n",
      "epoch:  799, loss: 0.00729618314653635\n",
      "epoch:  800, loss: 0.007292976602911949\n",
      "epoch:  801, loss: 0.007292400114238262\n",
      "epoch:  802, loss: 0.007291930727660656\n",
      "epoch:  803, loss: 0.007291021291166544\n",
      "epoch:  804, loss: 0.007291021291166544\n",
      "epoch:  805, loss: 0.007261235266923904\n",
      "epoch:  806, loss: 0.007257325574755669\n",
      "epoch:  807, loss: 0.007256379816681147\n",
      "epoch:  808, loss: 0.007255896460264921\n",
      "epoch:  809, loss: 0.007255896460264921\n",
      "epoch:  810, loss: 0.007253584917634726\n",
      "epoch:  811, loss: 0.007253312971442938\n",
      "epoch:  812, loss: 0.0072533125057816505\n",
      "epoch:  813, loss: 0.00725007988512516\n",
      "epoch:  814, loss: 0.00724948663264513\n",
      "epoch:  815, loss: 0.007248778361827135\n",
      "epoch:  816, loss: 0.007246558554470539\n",
      "epoch:  817, loss: 0.007246558554470539\n",
      "epoch:  818, loss: 0.007244572509080172\n",
      "epoch:  819, loss: 0.00724335853010416\n",
      "epoch:  820, loss: 0.00724335853010416\n",
      "epoch:  821, loss: 0.007240775041282177\n",
      "epoch:  822, loss: 0.0072402446530759335\n",
      "epoch:  823, loss: 0.00723956897854805\n",
      "epoch:  824, loss: 0.007236164063215256\n",
      "epoch:  825, loss: 0.007236164063215256\n",
      "epoch:  826, loss: 0.0072347866371273994\n",
      "epoch:  827, loss: 0.007231584750115871\n",
      "epoch:  828, loss: 0.007231584750115871\n",
      "epoch:  829, loss: 0.0072305286303162575\n",
      "epoch:  830, loss: 0.0071961418725550175\n",
      "epoch:  831, loss: 0.0071961418725550175\n",
      "epoch:  832, loss: 0.007192558143287897\n",
      "epoch:  833, loss: 0.007191846147179604\n",
      "epoch:  834, loss: 0.007191154174506664\n",
      "epoch:  835, loss: 0.007154030725359917\n",
      "epoch:  836, loss: 0.007154030725359917\n",
      "epoch:  837, loss: 0.007145336363464594\n",
      "epoch:  838, loss: 0.007144029717892408\n",
      "epoch:  839, loss: 0.007143253460526466\n",
      "epoch:  840, loss: 0.007139744237065315\n",
      "epoch:  841, loss: 0.007139744237065315\n",
      "epoch:  842, loss: 0.007137781009078026\n",
      "epoch:  843, loss: 0.0071372706443071365\n",
      "epoch:  844, loss: 0.0071347784250974655\n",
      "epoch:  845, loss: 0.0071347784250974655\n",
      "epoch:  846, loss: 0.007130850106477737\n",
      "epoch:  847, loss: 0.007130146492272615\n",
      "epoch:  848, loss: 0.007129705511033535\n",
      "epoch:  849, loss: 0.007129705511033535\n",
      "epoch:  850, loss: 0.007123948540538549\n",
      "epoch:  851, loss: 0.007123044691979885\n",
      "epoch:  852, loss: 0.007122371345758438\n",
      "epoch:  853, loss: 0.007078466471284628\n",
      "epoch:  854, loss: 0.007078466005623341\n",
      "epoch:  855, loss: 0.0070701283402740955\n",
      "epoch:  856, loss: 0.0070688254199922085\n",
      "epoch:  857, loss: 0.0070680053904652596\n",
      "epoch:  858, loss: 0.007067907601594925\n",
      "epoch:  859, loss: 0.00706790667027235\n",
      "epoch:  860, loss: 0.007063963916152716\n",
      "epoch:  861, loss: 0.007063203025609255\n",
      "epoch:  862, loss: 0.007062377408146858\n",
      "epoch:  863, loss: 0.007041669450700283\n",
      "epoch:  864, loss: 0.007041669450700283\n",
      "epoch:  865, loss: 0.0070109800435602665\n",
      "epoch:  866, loss: 0.007007467094808817\n",
      "epoch:  867, loss: 0.007006459403783083\n",
      "epoch:  868, loss: 0.007006330415606499\n",
      "epoch:  869, loss: 0.007006330415606499\n",
      "epoch:  870, loss: 0.007005331106483936\n",
      "epoch:  871, loss: 0.007002934813499451\n",
      "epoch:  872, loss: 0.007002934813499451\n",
      "epoch:  873, loss: 0.006999235134571791\n",
      "epoch:  874, loss: 0.006998532451689243\n",
      "epoch:  875, loss: 0.00699766818434\n",
      "epoch:  876, loss: 0.006997665856033564\n",
      "epoch:  877, loss: 0.006990801077336073\n",
      "epoch:  878, loss: 0.006990801077336073\n",
      "epoch:  879, loss: 0.00698951818048954\n",
      "epoch:  880, loss: 0.00695015536621213\n",
      "epoch:  881, loss: 0.00695015536621213\n",
      "epoch:  882, loss: 0.006939524784684181\n",
      "epoch:  883, loss: 0.006937789265066385\n",
      "epoch:  884, loss: 0.006936689838767052\n",
      "epoch:  885, loss: 0.006932320538908243\n",
      "epoch:  886, loss: 0.006932320538908243\n",
      "epoch:  887, loss: 0.006930396426469088\n",
      "epoch:  888, loss: 0.006926826201379299\n",
      "epoch:  889, loss: 0.006926826201379299\n",
      "epoch:  890, loss: 0.006924684625118971\n",
      "epoch:  891, loss: 0.006922908592969179\n",
      "epoch:  892, loss: 0.006922908592969179\n",
      "epoch:  893, loss: 0.0069197360426187515\n",
      "epoch:  894, loss: 0.006919031962752342\n",
      "epoch:  895, loss: 0.006918177008628845\n",
      "epoch:  896, loss: 0.006914077792316675\n",
      "epoch:  897, loss: 0.006914077326655388\n",
      "epoch:  898, loss: 0.006912454962730408\n",
      "epoch:  899, loss: 0.00690759951248765\n",
      "epoch:  900, loss: 0.006907599046826363\n",
      "epoch:  901, loss: 0.006906534545123577\n",
      "epoch:  902, loss: 0.006904544308781624\n",
      "epoch:  903, loss: 0.006904544308781624\n",
      "epoch:  904, loss: 0.0069008092395961285\n",
      "epoch:  905, loss: 0.0069001163356006145\n",
      "epoch:  906, loss: 0.006899223197251558\n",
      "epoch:  907, loss: 0.006899223197251558\n",
      "epoch:  908, loss: 0.00689452001824975\n",
      "epoch:  909, loss: 0.00689452001824975\n",
      "epoch:  910, loss: 0.006890910677611828\n",
      "epoch:  911, loss: 0.006890162825584412\n",
      "epoch:  912, loss: 0.006889339070767164\n",
      "epoch:  913, loss: 0.00688916863873601\n",
      "epoch:  914, loss: 0.00688916863873601\n",
      "epoch:  915, loss: 0.006888241972774267\n",
      "epoch:  916, loss: 0.006887965369969606\n",
      "epoch:  917, loss: 0.006887791212648153\n",
      "epoch:  918, loss: 0.006886701099574566\n",
      "epoch:  919, loss: 0.0068614850752055645\n",
      "epoch:  920, loss: 0.0068614850752055645\n",
      "epoch:  921, loss: 0.006825506687164307\n",
      "epoch:  922, loss: 0.006820132955908775\n",
      "epoch:  923, loss: 0.006818862631917\n",
      "epoch:  924, loss: 0.006783818826079369\n",
      "epoch:  925, loss: 0.006783818826079369\n",
      "epoch:  926, loss: 0.006781835108995438\n",
      "epoch:  927, loss: 0.006781814154237509\n",
      "epoch:  928, loss: 0.006770864129066467\n",
      "epoch:  929, loss: 0.00677086366340518\n",
      "epoch:  930, loss: 0.006765520665794611\n",
      "epoch:  931, loss: 0.006764639634639025\n",
      "epoch:  932, loss: 0.006763339042663574\n",
      "epoch:  933, loss: 0.006756939459592104\n",
      "epoch:  934, loss: 0.006756939459592104\n",
      "epoch:  935, loss: 0.006754955742508173\n",
      "epoch:  936, loss: 0.006752177607268095\n",
      "epoch:  937, loss: 0.006752177607268095\n",
      "epoch:  938, loss: 0.00674996804445982\n",
      "epoch:  939, loss: 0.006749967113137245\n",
      "epoch:  940, loss: 0.006748732179403305\n",
      "epoch:  941, loss: 0.006748732179403305\n",
      "epoch:  942, loss: 0.006744914222508669\n",
      "epoch:  943, loss: 0.006744914222508669\n",
      "epoch:  944, loss: 0.0067385099828243256\n",
      "epoch:  945, loss: 0.0067374613136053085\n",
      "epoch:  946, loss: 0.0067364429123699665\n",
      "epoch:  947, loss: 0.006736251525580883\n",
      "epoch:  948, loss: 0.006736251525580883\n",
      "epoch:  949, loss: 0.006735188886523247\n",
      "epoch:  950, loss: 0.006734901107847691\n",
      "epoch:  951, loss: 0.006731453351676464\n",
      "epoch:  952, loss: 0.006731453351676464\n",
      "epoch:  953, loss: 0.006729329004883766\n",
      "epoch:  954, loss: 0.0067261988297104836\n",
      "epoch:  955, loss: 0.0067261988297104836\n",
      "epoch:  956, loss: 0.006715725641697645\n",
      "epoch:  957, loss: 0.006712082773447037\n",
      "epoch:  958, loss: 0.006712082773447037\n",
      "epoch:  959, loss: 0.006705609615892172\n",
      "epoch:  960, loss: 0.006660374812781811\n",
      "epoch:  961, loss: 0.006660374812781811\n",
      "epoch:  962, loss: 0.006648738868534565\n",
      "epoch:  963, loss: 0.00664715189486742\n",
      "epoch:  964, loss: 0.0066456603817641735\n",
      "epoch:  965, loss: 0.006644973531365395\n",
      "epoch:  966, loss: 0.006644973531365395\n",
      "epoch:  967, loss: 0.006639197468757629\n",
      "epoch:  968, loss: 0.006637099198997021\n",
      "epoch:  969, loss: 0.006637099198997021\n",
      "epoch:  970, loss: 0.006631876341998577\n",
      "epoch:  971, loss: 0.006628123577684164\n",
      "epoch:  972, loss: 0.006628123577684164\n",
      "epoch:  973, loss: 0.0066246031783521175\n",
      "epoch:  974, loss: 0.006587064824998379\n",
      "epoch:  975, loss: 0.0065870643593370914\n",
      "epoch:  976, loss: 0.006570776924490929\n",
      "epoch:  977, loss: 0.006568919867277145\n",
      "epoch:  978, loss: 0.006566750351339579\n",
      "epoch:  979, loss: 0.006566750351339579\n",
      "epoch:  980, loss: 0.00655584828928113\n",
      "epoch:  981, loss: 0.00655584828928113\n",
      "epoch:  982, loss: 0.006548421457409859\n",
      "epoch:  983, loss: 0.006546856835484505\n",
      "epoch:  984, loss: 0.006546856835484505\n",
      "epoch:  985, loss: 0.006540560629218817\n",
      "epoch:  986, loss: 0.0065350825898349285\n",
      "epoch:  987, loss: 0.0065350825898349285\n",
      "epoch:  988, loss: 0.006532433442771435\n",
      "epoch:  989, loss: 0.006532433442771435\n",
      "epoch:  990, loss: 0.006530691869556904\n",
      "epoch:  991, loss: 0.00649996055290103\n",
      "epoch:  992, loss: 0.00649996055290103\n",
      "epoch:  993, loss: 0.006445457227528095\n",
      "epoch:  994, loss: 0.006437439937144518\n",
      "epoch:  995, loss: 0.006435500457882881\n",
      "epoch:  996, loss: 0.006431179121136665\n",
      "epoch:  997, loss: 0.006431179121136665\n",
      "epoch:  998, loss: 0.006428724620491266\n",
      "epoch:  999, loss: 0.006423832383006811\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size=X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# opt = torch_numopt.ConjugateGradient(model=model, lr=1e-4, line_search_method=\"const\", cg_method=\"PR\")\n",
    "opt = torch_numopt.ConjugateGradient(model=model, lr=10, c1=0.01, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"armijo\", cg_method=\"PRP+\")\n",
    "# opt = torch_numopt.ConjugateGradient(model=model, lr=10, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"wolfe\", cg_method=\"PRP+\")\n",
    "# opt = torch_numopt.ConjugateGradient(model=model, lr=10, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"strong-wolfe\", cg_method=\"PRP+\")\n",
    "# opt = torch_numopt.ConjugateGradient(model=model, lr=10, c1=1e-6, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"goldstein\", cg_method=\"PRP\")\n",
    "\n",
    "\n",
    "all_loss = {}\n",
    "for epoch in range(1000):\n",
    "    print(\"epoch: \", epoch, end=\"\")\n",
    "    all_loss[epoch + 1] = 0\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y, loss_fn)\n",
    "\n",
    "        all_loss[epoch + 1] += loss\n",
    "    all_loss[epoch + 1] /= len(data_loader)\n",
    "    print(\", loss: {}\".format(all_loss[epoch + 1].detach().numpy().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics: R2 = 0.782798339195211\n",
      "Test metrics:  R2 = 0.6721940797246311\n"
     ]
    }
   ],
   "source": [
    "pred_train = model.forward(X_train).detach()\n",
    "pred_test = model.forward(X_test).detach()\n",
    "print(f\"Train metrics: R2 = {r2_score(pred_train, y_train)}\")\n",
    "print(f\"Test metrics:  R2 = {r2_score(pred_test, y_test)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
