{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch_numopt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import *\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.f1 = nn.Linear(input_size, 10, device=device)\n",
    "        self.f2 = nn.Linear(10, 20, device=device)\n",
    "        self.f3 = nn.Linear(20, 20, device=device)\n",
    "        self.f4 = nn.Linear(20, 10, device=device)\n",
    "        self.f5 = nn.Linear(10, 1, device=device)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        # self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.f1(x))\n",
    "        x = self.activation(self.f2(x))\n",
    "        x = self.activation(self.f3(x))\n",
    "        x = self.activation(self.f4(x))\n",
    "        x = self.f5(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([800, 10]) torch.Size([800, 1])\n",
      "torch.Size([200, 10]) torch.Size([200, 1])\n"
     ]
    }
   ],
   "source": [
    "# X, y = load_diabetes(return_X_y=True, scaled=False)\n",
    "# X, y = make_regression(n_samples=1000, n_features=100)\n",
    "X, y = make_friedman1(n_samples=1000, noise=1e-2)\n",
    "\n",
    "X_scaler = MinMaxScaler()\n",
    "X = X_scaler.fit_transform(X)\n",
    "X = torch.Tensor(X).to(device)\n",
    "\n",
    "y_scaler = MinMaxScaler()\n",
    "y = y_scaler.fit_transform(y.reshape((-1, 1)))\n",
    "y = torch.Tensor(y).to(device)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "torch_data = TensorDataset(X_train, y_train)\n",
    "data_loader = DataLoader(torch_data, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.21060918271541595\n",
      "epoch:  1, loss: 0.12482902407646179\n",
      "epoch:  2, loss: 0.07619329541921616\n",
      "epoch:  3, loss: 0.05481067672371864\n",
      "epoch:  4, loss: 0.04528016597032547\n",
      "epoch:  5, loss: 0.040952809154987335\n",
      "epoch:  6, loss: 0.038965556770563126\n",
      "epoch:  7, loss: 0.038045287132263184\n",
      "epoch:  8, loss: 0.03761456534266472\n",
      "epoch:  9, loss: 0.037409476935863495\n",
      "epoch:  10, loss: 0.037308596074581146\n",
      "epoch:  11, loss: 0.03725593164563179\n",
      "epoch:  12, loss: 0.037225689738988876\n",
      "epoch:  13, loss: 0.0371784046292305\n",
      "epoch:  14, loss: 0.0371784046292305\n",
      "epoch:  15, loss: 0.037133198231458664\n",
      "epoch:  16, loss: 0.03710085153579712\n",
      "epoch:  17, loss: 0.03707220405340195\n",
      "epoch:  18, loss: 0.03707220405340195\n",
      "epoch:  19, loss: 0.037019193172454834\n",
      "epoch:  20, loss: 0.036981821060180664\n",
      "epoch:  21, loss: 0.03696424514055252\n",
      "epoch:  22, loss: 0.03696424514055252\n",
      "epoch:  23, loss: 0.03690384328365326\n",
      "epoch:  24, loss: 0.03686201944947243\n",
      "epoch:  25, loss: 0.03683687746524811\n",
      "epoch:  26, loss: 0.03678961843252182\n",
      "epoch:  27, loss: 0.03678961843252182\n",
      "epoch:  28, loss: 0.03675113618373871\n",
      "epoch:  29, loss: 0.03672315925359726\n",
      "epoch:  30, loss: 0.03668389841914177\n",
      "epoch:  31, loss: 0.03668389841914177\n",
      "epoch:  32, loss: 0.036640606820583344\n",
      "epoch:  33, loss: 0.03660985082387924\n",
      "epoch:  34, loss: 0.03659176453948021\n",
      "epoch:  35, loss: 0.03659176453948021\n",
      "epoch:  36, loss: 0.03654158115386963\n",
      "epoch:  37, loss: 0.03650658205151558\n",
      "epoch:  38, loss: 0.03648531809449196\n",
      "epoch:  39, loss: 0.03644072636961937\n",
      "epoch:  40, loss: 0.03644072636961937\n",
      "epoch:  41, loss: 0.036409612745046616\n",
      "epoch:  42, loss: 0.03638694807887077\n",
      "epoch:  43, loss: 0.036357756704092026\n",
      "epoch:  44, loss: 0.036357756704092026\n",
      "epoch:  45, loss: 0.03632143512368202\n",
      "epoch:  46, loss: 0.036295514553785324\n",
      "epoch:  47, loss: 0.03627387434244156\n",
      "epoch:  48, loss: 0.03627387434244156\n",
      "epoch:  49, loss: 0.03623172268271446\n",
      "epoch:  50, loss: 0.03620246797800064\n",
      "epoch:  51, loss: 0.03619639202952385\n",
      "epoch:  52, loss: 0.03619639202952385\n",
      "epoch:  53, loss: 0.03614622354507446\n",
      "epoch:  54, loss: 0.036111705005168915\n",
      "epoch:  55, loss: 0.036091022193431854\n",
      "epoch:  56, loss: 0.03605388104915619\n",
      "epoch:  57, loss: 0.03605388104915619\n",
      "epoch:  58, loss: 0.036020856350660324\n",
      "epoch:  59, loss: 0.03599730506539345\n",
      "epoch:  60, loss: 0.03597209230065346\n",
      "epoch:  61, loss: 0.03597209230065346\n",
      "epoch:  62, loss: 0.03593125566840172\n",
      "epoch:  63, loss: 0.03590267524123192\n",
      "epoch:  64, loss: 0.03588997945189476\n",
      "epoch:  65, loss: 0.03588997945189476\n",
      "epoch:  66, loss: 0.03583806753158569\n",
      "epoch:  67, loss: 0.035802874714136124\n",
      "epoch:  68, loss: 0.035782333463430405\n",
      "epoch:  69, loss: 0.035739559680223465\n",
      "epoch:  70, loss: 0.035739559680223465\n",
      "epoch:  71, loss: 0.03570512682199478\n",
      "epoch:  72, loss: 0.035680338740348816\n",
      "epoch:  73, loss: 0.035642776638269424\n",
      "epoch:  74, loss: 0.035642776638269424\n",
      "epoch:  75, loss: 0.03559957817196846\n",
      "epoch:  76, loss: 0.03556983917951584\n",
      "epoch:  77, loss: 0.035548873245716095\n",
      "epoch:  78, loss: 0.035548873245716095\n",
      "epoch:  79, loss: 0.035494402050971985\n",
      "epoch:  80, loss: 0.03545686975121498\n",
      "epoch:  81, loss: 0.03545048087835312\n",
      "epoch:  82, loss: 0.03545048087835312\n",
      "epoch:  83, loss: 0.03538007661700249\n",
      "epoch:  84, loss: 0.03533283248543739\n",
      "epoch:  85, loss: 0.035305704921483994\n",
      "epoch:  86, loss: 0.03524906933307648\n",
      "epoch:  87, loss: 0.03524906933307648\n",
      "epoch:  88, loss: 0.03520366549491882\n",
      "epoch:  89, loss: 0.035171762108802795\n",
      "epoch:  90, loss: 0.03511366993188858\n",
      "epoch:  91, loss: 0.03511366993188858\n",
      "epoch:  92, loss: 0.03505820408463478\n",
      "epoch:  93, loss: 0.03501937538385391\n",
      "epoch:  94, loss: 0.03498344495892525\n",
      "epoch:  95, loss: 0.03498344495892525\n",
      "epoch:  96, loss: 0.034909673035144806\n",
      "epoch:  97, loss: 0.03485981747508049\n",
      "epoch:  98, loss: 0.03484050929546356\n",
      "epoch:  99, loss: 0.03484050929546356\n",
      "epoch:  100, loss: 0.03474301099777222\n",
      "epoch:  101, loss: 0.03467848151922226\n",
      "epoch:  102, loss: 0.03464295342564583\n",
      "epoch:  103, loss: 0.03455154225230217\n",
      "epoch:  104, loss: 0.03455154225230217\n",
      "epoch:  105, loss: 0.03449056297540665\n",
      "epoch:  106, loss: 0.034447282552719116\n",
      "epoch:  107, loss: 0.03436460345983505\n",
      "epoch:  108, loss: 0.03436460345983505\n",
      "epoch:  109, loss: 0.03428064286708832\n",
      "epoch:  110, loss: 0.034224532544612885\n",
      "epoch:  111, loss: 0.0341767780482769\n",
      "epoch:  112, loss: 0.0341767780482769\n",
      "epoch:  113, loss: 0.03406332805752754\n",
      "epoch:  114, loss: 0.03398803621530533\n",
      "epoch:  115, loss: 0.03394366055727005\n",
      "epoch:  116, loss: 0.03379922732710838\n",
      "epoch:  117, loss: 0.03379922732710838\n",
      "epoch:  118, loss: 0.03371993824839592\n",
      "epoch:  119, loss: 0.033667296171188354\n",
      "epoch:  120, loss: 0.03354376181960106\n",
      "epoch:  121, loss: 0.03354376181960106\n",
      "epoch:  122, loss: 0.03344181552529335\n",
      "epoch:  123, loss: 0.03337342292070389\n",
      "epoch:  124, loss: 0.03326347842812538\n",
      "epoch:  125, loss: 0.03326347842812538\n",
      "epoch:  126, loss: 0.03311547636985779\n",
      "epoch:  127, loss: 0.03302616998553276\n",
      "epoch:  128, loss: 0.03297700732946396\n",
      "epoch:  129, loss: 0.03297700732946396\n",
      "epoch:  130, loss: 0.0327821709215641\n",
      "epoch:  131, loss: 0.032659754157066345\n",
      "epoch:  132, loss: 0.032593075186014175\n",
      "epoch:  133, loss: 0.03238373622298241\n",
      "epoch:  134, loss: 0.03238373622298241\n",
      "epoch:  135, loss: 0.0322476327419281\n",
      "epoch:  136, loss: 0.03216070681810379\n",
      "epoch:  137, loss: 0.03201165050268173\n",
      "epoch:  138, loss: 0.03201165050268173\n",
      "epoch:  139, loss: 0.031820427626371384\n",
      "epoch:  140, loss: 0.03170059248805046\n",
      "epoch:  141, loss: 0.03163420781493187\n",
      "epoch:  142, loss: 0.03163420781493187\n",
      "epoch:  143, loss: 0.031316015869379044\n",
      "epoch:  144, loss: 0.0311559047549963\n",
      "epoch:  145, loss: 0.031076371669769287\n",
      "epoch:  146, loss: 0.030742084607481956\n",
      "epoch:  147, loss: 0.030742084607481956\n",
      "epoch:  148, loss: 0.030590802431106567\n",
      "epoch:  149, loss: 0.030493153259158134\n",
      "epoch:  150, loss: 0.03014606423676014\n",
      "epoch:  151, loss: 0.03014606423676014\n",
      "epoch:  152, loss: 0.02992713823914528\n",
      "epoch:  153, loss: 0.029806040227413177\n",
      "epoch:  154, loss: 0.02949431538581848\n",
      "epoch:  155, loss: 0.02949431538581848\n",
      "epoch:  156, loss: 0.029218653216958046\n",
      "epoch:  157, loss: 0.02906385436654091\n",
      "epoch:  158, loss: 0.0287855826318264\n",
      "epoch:  159, loss: 0.0287855826318264\n",
      "epoch:  160, loss: 0.028371693566441536\n",
      "epoch:  161, loss: 0.028171448037028313\n",
      "epoch:  162, loss: 0.027977928519248962\n",
      "epoch:  163, loss: 0.027977928519248962\n",
      "epoch:  164, loss: 0.027463991194963455\n",
      "epoch:  165, loss: 0.027213862165808678\n",
      "epoch:  166, loss: 0.027156099677085876\n",
      "epoch:  167, loss: 0.027156099677085876\n",
      "epoch:  168, loss: 0.02636408992111683\n",
      "epoch:  169, loss: 0.026038862764835358\n",
      "epoch:  170, loss: 0.025867609307169914\n",
      "epoch:  171, loss: 0.025867609307169914\n",
      "epoch:  172, loss: 0.025020970031619072\n",
      "epoch:  173, loss: 0.024681901559233665\n",
      "epoch:  174, loss: 0.02435971237719059\n",
      "epoch:  175, loss: 0.02435971237719059\n",
      "epoch:  176, loss: 0.023523755371570587\n",
      "epoch:  177, loss: 0.023223472759127617\n",
      "epoch:  178, loss: 0.022873863577842712\n",
      "epoch:  179, loss: 0.022873863577842712\n",
      "epoch:  180, loss: 0.022050516679883003\n",
      "epoch:  181, loss: 0.021780366078019142\n",
      "epoch:  182, loss: 0.02097454108297825\n",
      "epoch:  183, loss: 0.02097454108297825\n",
      "epoch:  184, loss: 0.020454665645956993\n",
      "epoch:  185, loss: 0.020253710448741913\n",
      "epoch:  186, loss: 0.019346002489328384\n",
      "epoch:  187, loss: 0.019346002489328384\n",
      "epoch:  188, loss: 0.018935561180114746\n",
      "epoch:  189, loss: 0.01876932755112648\n",
      "epoch:  190, loss: 0.018764788284897804\n",
      "epoch:  191, loss: 0.009345698170363903\n",
      "epoch:  192, loss: 0.009316126815974712\n",
      "epoch:  193, loss: 0.009298867546021938\n",
      "epoch:  194, loss: 0.009144253097474575\n",
      "epoch:  195, loss: 0.009144253097474575\n",
      "epoch:  196, loss: 0.009082889184355736\n",
      "epoch:  197, loss: 0.009082889184355736\n",
      "epoch:  198, loss: 0.00899173691868782\n",
      "epoch:  199, loss: 0.00899173691868782\n",
      "epoch:  200, loss: 0.008866949006915092\n",
      "epoch:  201, loss: 0.008858876302838326\n",
      "epoch:  202, loss: 0.008840351365506649\n",
      "epoch:  203, loss: 0.008840345777571201\n",
      "epoch:  204, loss: 0.008732224814593792\n",
      "epoch:  205, loss: 0.008732224814593792\n",
      "epoch:  206, loss: 0.008691076189279556\n",
      "epoch:  207, loss: 0.00861819926649332\n",
      "epoch:  208, loss: 0.00861819926649332\n",
      "epoch:  209, loss: 0.008603472262620926\n",
      "epoch:  210, loss: 0.00860243383795023\n",
      "epoch:  211, loss: 0.008446747437119484\n",
      "epoch:  212, loss: 0.008446747437119484\n",
      "epoch:  213, loss: 0.00837605632841587\n",
      "epoch:  214, loss: 0.008368990384042263\n",
      "epoch:  215, loss: 0.00836174562573433\n",
      "epoch:  216, loss: 0.00836174562573433\n",
      "epoch:  217, loss: 0.00835596863180399\n",
      "epoch:  218, loss: 0.00835596863180399\n",
      "epoch:  219, loss: 0.008315464481711388\n",
      "epoch:  220, loss: 0.008315464481711388\n",
      "epoch:  221, loss: 0.00830419734120369\n",
      "epoch:  222, loss: 0.00830419734120369\n",
      "epoch:  223, loss: 0.00829881988465786\n",
      "epoch:  224, loss: 0.00829881988465786\n",
      "epoch:  225, loss: 0.008281628601253033\n",
      "epoch:  226, loss: 0.008281628601253033\n",
      "epoch:  227, loss: 0.008260592818260193\n",
      "epoch:  228, loss: 0.00826005544513464\n",
      "epoch:  229, loss: 0.008255223743617535\n",
      "epoch:  230, loss: 0.008255223743617535\n",
      "epoch:  231, loss: 0.008251472376286983\n",
      "epoch:  232, loss: 0.008251472376286983\n",
      "epoch:  233, loss: 0.00823413860052824\n",
      "epoch:  234, loss: 0.00823413860052824\n",
      "epoch:  235, loss: 0.00822010450065136\n",
      "epoch:  236, loss: 0.008220034651458263\n",
      "epoch:  237, loss: 0.008216086775064468\n",
      "epoch:  238, loss: 0.008216086775064468\n",
      "epoch:  239, loss: 0.008209753781557083\n",
      "epoch:  240, loss: 0.008209753781557083\n",
      "epoch:  241, loss: 0.008189148269593716\n",
      "epoch:  242, loss: 0.008186934515833855\n",
      "epoch:  243, loss: 0.008183777332305908\n",
      "epoch:  244, loss: 0.008183777332305908\n",
      "epoch:  245, loss: 0.008174548856914043\n",
      "epoch:  246, loss: 0.008174548856914043\n",
      "epoch:  247, loss: 0.008157326839864254\n",
      "epoch:  248, loss: 0.008156918920576572\n",
      "epoch:  249, loss: 0.00815337523818016\n",
      "epoch:  250, loss: 0.00815337523818016\n",
      "epoch:  251, loss: 0.008150624111294746\n",
      "epoch:  252, loss: 0.008150438778102398\n",
      "epoch:  253, loss: 0.008148490451276302\n",
      "epoch:  254, loss: 0.008146545849740505\n",
      "epoch:  255, loss: 0.008146536536514759\n",
      "epoch:  256, loss: 0.00814394373446703\n",
      "epoch:  257, loss: 0.008143037557601929\n",
      "epoch:  258, loss: 0.008141766302287579\n",
      "epoch:  259, loss: 0.008141766302287579\n",
      "epoch:  260, loss: 0.008124816231429577\n",
      "epoch:  261, loss: 0.008124416694045067\n",
      "epoch:  262, loss: 0.00812118873000145\n",
      "epoch:  263, loss: 0.00812118873000145\n",
      "epoch:  264, loss: 0.008118835277855396\n",
      "epoch:  265, loss: 0.008118015713989735\n",
      "epoch:  266, loss: 0.008116373792290688\n",
      "epoch:  267, loss: 0.008116373792290688\n",
      "epoch:  268, loss: 0.0081024793908\n",
      "epoch:  269, loss: 0.008102291263639927\n",
      "epoch:  270, loss: 0.008099782280623913\n",
      "epoch:  271, loss: 0.008099782280623913\n",
      "epoch:  272, loss: 0.008097346872091293\n",
      "epoch:  273, loss: 0.008097346872091293\n",
      "epoch:  274, loss: 0.008091629482805729\n",
      "epoch:  275, loss: 0.008091629482805729\n",
      "epoch:  276, loss: 0.008079009130597115\n",
      "epoch:  277, loss: 0.008078774437308311\n",
      "epoch:  278, loss: 0.008075779303908348\n",
      "epoch:  279, loss: 0.008075779303908348\n",
      "epoch:  280, loss: 0.008072189055383205\n",
      "epoch:  281, loss: 0.008072189055383205\n",
      "epoch:  282, loss: 0.008059792220592499\n",
      "epoch:  283, loss: 0.008058460429310799\n",
      "epoch:  284, loss: 0.00805605947971344\n",
      "epoch:  285, loss: 0.00805605947971344\n",
      "epoch:  286, loss: 0.008053909055888653\n",
      "epoch:  287, loss: 0.008053909055888653\n",
      "epoch:  288, loss: 0.008051970042288303\n",
      "epoch:  289, loss: 0.008051970042288303\n",
      "epoch:  290, loss: 0.0080388393253088\n",
      "epoch:  291, loss: 0.008037074469029903\n",
      "epoch:  292, loss: 0.008034894242882729\n",
      "epoch:  293, loss: 0.008034894242882729\n",
      "epoch:  294, loss: 0.008033158257603645\n",
      "epoch:  295, loss: 0.008033158257603645\n",
      "epoch:  296, loss: 0.00802608858793974\n",
      "epoch:  297, loss: 0.00802608858793974\n",
      "epoch:  298, loss: 0.00801873579621315\n",
      "epoch:  299, loss: 0.008018016815185547\n",
      "epoch:  300, loss: 0.00801607221364975\n",
      "epoch:  301, loss: 0.00801607221364975\n",
      "epoch:  302, loss: 0.008014646358788013\n",
      "epoch:  303, loss: 0.008014646358788013\n",
      "epoch:  304, loss: 0.008013265207409859\n",
      "epoch:  305, loss: 0.008013186976313591\n",
      "epoch:  306, loss: 0.008012077771127224\n",
      "epoch:  307, loss: 0.008012057282030582\n",
      "epoch:  308, loss: 0.008004394359886646\n",
      "epoch:  309, loss: 0.008004394359886646\n",
      "epoch:  310, loss: 0.007999981753528118\n",
      "epoch:  311, loss: 0.00799816008657217\n",
      "epoch:  312, loss: 0.00799816008657217\n",
      "epoch:  313, loss: 0.00799673143774271\n",
      "epoch:  314, loss: 0.00799673143774271\n",
      "epoch:  315, loss: 0.007995624095201492\n",
      "epoch:  316, loss: 0.007994238287210464\n",
      "epoch:  317, loss: 0.007994238287210464\n",
      "epoch:  318, loss: 0.007990173064172268\n",
      "epoch:  319, loss: 0.007988100871443748\n",
      "epoch:  320, loss: 0.007988100871443748\n",
      "epoch:  321, loss: 0.007986868731677532\n",
      "epoch:  322, loss: 0.007986867800354958\n",
      "epoch:  323, loss: 0.007983867079019547\n",
      "epoch:  324, loss: 0.007983867079019547\n",
      "epoch:  325, loss: 0.007976393215358257\n",
      "epoch:  326, loss: 0.007975404150784016\n",
      "epoch:  327, loss: 0.00797413568943739\n",
      "epoch:  328, loss: 0.00797413568943739\n",
      "epoch:  329, loss: 0.007973025552928448\n",
      "epoch:  330, loss: 0.007973010651767254\n",
      "epoch:  331, loss: 0.007968563586473465\n",
      "epoch:  332, loss: 0.007968563586473465\n",
      "epoch:  333, loss: 0.007963274605572224\n",
      "epoch:  334, loss: 0.007962722331285477\n",
      "epoch:  335, loss: 0.00796112883836031\n",
      "epoch:  336, loss: 0.00796112883836031\n",
      "epoch:  337, loss: 0.007959868758916855\n",
      "epoch:  338, loss: 0.007959868758916855\n",
      "epoch:  339, loss: 0.00795908086001873\n",
      "epoch:  340, loss: 0.00795908086001873\n",
      "epoch:  341, loss: 0.007950893603265285\n",
      "epoch:  342, loss: 0.007949700579047203\n",
      "epoch:  343, loss: 0.007948100566864014\n",
      "epoch:  344, loss: 0.007948100566864014\n",
      "epoch:  345, loss: 0.007946619763970375\n",
      "epoch:  346, loss: 0.007946619763970375\n",
      "epoch:  347, loss: 0.00794555526226759\n",
      "epoch:  348, loss: 0.007945554330945015\n",
      "epoch:  349, loss: 0.007939758710563183\n",
      "epoch:  350, loss: 0.007939758710563183\n",
      "epoch:  351, loss: 0.007936016656458378\n",
      "epoch:  352, loss: 0.007936016656458378\n",
      "epoch:  353, loss: 0.00793452002108097\n",
      "epoch:  354, loss: 0.00793452002108097\n",
      "epoch:  355, loss: 0.00793339591473341\n",
      "epoch:  356, loss: 0.007933394983410835\n",
      "epoch:  357, loss: 0.007933004759252071\n",
      "epoch:  358, loss: 0.007933004759252071\n",
      "epoch:  359, loss: 0.007925941608846188\n",
      "epoch:  360, loss: 0.00792501587420702\n",
      "epoch:  361, loss: 0.007923602126538754\n",
      "epoch:  362, loss: 0.007923602126538754\n",
      "epoch:  363, loss: 0.007922478020191193\n",
      "epoch:  364, loss: 0.007922478020191193\n",
      "epoch:  365, loss: 0.007921530865132809\n",
      "epoch:  366, loss: 0.007921523414552212\n",
      "epoch:  367, loss: 0.007921424694359303\n",
      "epoch:  368, loss: 0.007921424694359303\n",
      "epoch:  369, loss: 0.00791777390986681\n",
      "epoch:  370, loss: 0.00791749358177185\n",
      "epoch:  371, loss: 0.00791749358177185\n",
      "epoch:  372, loss: 0.007916437461972237\n",
      "epoch:  373, loss: 0.007916437461972237\n",
      "epoch:  374, loss: 0.007915626280009747\n",
      "epoch:  375, loss: 0.007915626280009747\n",
      "epoch:  376, loss: 0.007914802990853786\n",
      "epoch:  377, loss: 0.007914668880403042\n",
      "epoch:  378, loss: 0.007914101704955101\n",
      "epoch:  379, loss: 0.007914101704955101\n",
      "epoch:  380, loss: 0.007912764325737953\n",
      "epoch:  381, loss: 0.007912764325737953\n",
      "epoch:  382, loss: 0.00791182555258274\n",
      "epoch:  383, loss: 0.00791182555258274\n",
      "epoch:  384, loss: 0.007911057211458683\n",
      "epoch:  385, loss: 0.007911057211458683\n",
      "epoch:  386, loss: 0.007910257205367088\n",
      "epoch:  387, loss: 0.00790994893759489\n",
      "epoch:  388, loss: 0.007907395251095295\n",
      "epoch:  389, loss: 0.007907395251095295\n",
      "epoch:  390, loss: 0.007906042970716953\n",
      "epoch:  391, loss: 0.007906042970716953\n",
      "epoch:  392, loss: 0.00790524110198021\n",
      "epoch:  393, loss: 0.00790524110198021\n",
      "epoch:  394, loss: 0.007904412224888802\n",
      "epoch:  395, loss: 0.00790440570563078\n",
      "epoch:  396, loss: 0.00790204294025898\n",
      "epoch:  397, loss: 0.00790204294025898\n",
      "epoch:  398, loss: 0.007897638715803623\n",
      "epoch:  399, loss: 0.007897194474935532\n",
      "epoch:  400, loss: 0.007896189577877522\n",
      "epoch:  401, loss: 0.007896189577877522\n",
      "epoch:  402, loss: 0.007895124144852161\n",
      "epoch:  403, loss: 0.007895124144852161\n",
      "epoch:  404, loss: 0.007894354872405529\n",
      "epoch:  405, loss: 0.007894354872405529\n",
      "epoch:  406, loss: 0.007890562526881695\n",
      "epoch:  407, loss: 0.007890562526881695\n",
      "epoch:  408, loss: 0.007887348532676697\n",
      "epoch:  409, loss: 0.007887040264904499\n",
      "epoch:  410, loss: 0.007885878905653954\n",
      "epoch:  411, loss: 0.007885878905653954\n",
      "epoch:  412, loss: 0.007884961552917957\n",
      "epoch:  413, loss: 0.007884961552917957\n",
      "epoch:  414, loss: 0.007884880527853966\n",
      "epoch:  415, loss: 0.007884880527853966\n",
      "epoch:  416, loss: 0.007878328673541546\n",
      "epoch:  417, loss: 0.0078772297129035\n",
      "epoch:  418, loss: 0.007875997573137283\n",
      "epoch:  419, loss: 0.007875997573137283\n",
      "epoch:  420, loss: 0.007874933071434498\n",
      "epoch:  421, loss: 0.007874933071434498\n",
      "epoch:  422, loss: 0.007874075323343277\n",
      "epoch:  423, loss: 0.007866617292165756\n",
      "epoch:  424, loss: 0.007866617292165756\n",
      "epoch:  425, loss: 0.007832404226064682\n",
      "epoch:  426, loss: 0.007825853303074837\n",
      "epoch:  427, loss: 0.007823438383638859\n",
      "epoch:  428, loss: 0.007823438383638859\n",
      "epoch:  429, loss: 0.007820477709174156\n",
      "epoch:  430, loss: 0.00782021228224039\n",
      "epoch:  431, loss: 0.007819085381925106\n",
      "epoch:  432, loss: 0.007819085381925106\n",
      "epoch:  433, loss: 0.00781815405935049\n",
      "epoch:  434, loss: 0.007815026678144932\n",
      "epoch:  435, loss: 0.007815025746822357\n",
      "epoch:  436, loss: 0.007813951000571251\n",
      "epoch:  437, loss: 0.007813947275280952\n",
      "epoch:  438, loss: 0.007813114672899246\n",
      "epoch:  439, loss: 0.007813099771738052\n",
      "epoch:  440, loss: 0.007813099771738052\n",
      "epoch:  441, loss: 0.007812198251485825\n",
      "epoch:  442, loss: 0.007812198251485825\n",
      "epoch:  443, loss: 0.007811434101313353\n",
      "epoch:  444, loss: 0.007811434101313353\n",
      "epoch:  445, loss: 0.007810746319591999\n",
      "epoch:  446, loss: 0.007810668554157019\n",
      "epoch:  447, loss: 0.007809937931597233\n",
      "epoch:  448, loss: 0.007809292990714312\n",
      "epoch:  449, loss: 0.007809292990714312\n",
      "epoch:  450, loss: 0.00780857540667057\n",
      "epoch:  451, loss: 0.00780857540667057\n",
      "epoch:  452, loss: 0.0078078368678689\n",
      "epoch:  453, loss: 0.0078078368678689\n",
      "epoch:  454, loss: 0.007804883643984795\n",
      "epoch:  455, loss: 0.007804883643984795\n",
      "epoch:  456, loss: 0.0078016649931669235\n",
      "epoch:  457, loss: 0.007801378145813942\n",
      "epoch:  458, loss: 0.007800514344125986\n",
      "epoch:  459, loss: 0.007800514344125986\n",
      "epoch:  460, loss: 0.007799600251019001\n",
      "epoch:  461, loss: 0.007799600251019001\n",
      "epoch:  462, loss: 0.0077989851124584675\n",
      "epoch:  463, loss: 0.0077989851124584675\n",
      "epoch:  464, loss: 0.00779832573607564\n",
      "epoch:  465, loss: 0.00779832573607564\n",
      "epoch:  466, loss: 0.007797068916261196\n",
      "epoch:  467, loss: 0.007797068916261196\n",
      "epoch:  468, loss: 0.0077931880950927734\n",
      "epoch:  469, loss: 0.0077926283702254295\n",
      "epoch:  470, loss: 0.007791779935359955\n",
      "epoch:  471, loss: 0.007791779935359955\n",
      "epoch:  472, loss: 0.007790858391672373\n",
      "epoch:  473, loss: 0.007790858391672373\n",
      "epoch:  474, loss: 0.007790223695337772\n",
      "epoch:  475, loss: 0.007790223695337772\n",
      "epoch:  476, loss: 0.007789565250277519\n",
      "epoch:  477, loss: 0.007789565250277519\n",
      "epoch:  478, loss: 0.0077890255488455296\n",
      "epoch:  479, loss: 0.007789024617522955\n",
      "epoch:  480, loss: 0.007788414601236582\n",
      "epoch:  481, loss: 0.007788414601236582\n",
      "epoch:  482, loss: 0.007785215973854065\n",
      "epoch:  483, loss: 0.007785215973854065\n",
      "epoch:  484, loss: 0.007782828062772751\n",
      "epoch:  485, loss: 0.007782716769725084\n",
      "epoch:  486, loss: 0.007782716769725084\n",
      "epoch:  487, loss: 0.007781798020005226\n",
      "epoch:  488, loss: 0.007781797554343939\n",
      "epoch:  489, loss: 0.007781099993735552\n",
      "epoch:  490, loss: 0.007780671585351229\n",
      "epoch:  491, loss: 0.007780671585351229\n",
      "epoch:  492, loss: 0.007777791935950518\n",
      "epoch:  493, loss: 0.007777550257742405\n",
      "epoch:  494, loss: 0.007776502054184675\n",
      "epoch:  495, loss: 0.007776502054184675\n",
      "epoch:  496, loss: 0.007775539066642523\n",
      "epoch:  497, loss: 0.00777552230283618\n",
      "epoch:  498, loss: 0.0077734109945595264\n",
      "epoch:  499, loss: 0.0077734109945595264\n",
      "epoch:  500, loss: 0.007771019823849201\n",
      "epoch:  501, loss: 0.007770760916173458\n",
      "epoch:  502, loss: 0.007769812364131212\n",
      "epoch:  503, loss: 0.007769812364131212\n",
      "epoch:  504, loss: 0.007769024930894375\n",
      "epoch:  505, loss: 0.007769024930894375\n",
      "epoch:  506, loss: 0.0077689895406365395\n",
      "epoch:  507, loss: 0.0077689895406365395\n",
      "epoch:  508, loss: 0.007763589266687632\n",
      "epoch:  509, loss: 0.0077626630663871765\n",
      "epoch:  510, loss: 0.0077617112547159195\n",
      "epoch:  511, loss: 0.007761710789054632\n",
      "epoch:  512, loss: 0.007760799955576658\n",
      "epoch:  513, loss: 0.007760799955576658\n",
      "epoch:  514, loss: 0.007760100066661835\n",
      "epoch:  515, loss: 0.007758292835205793\n",
      "epoch:  516, loss: 0.007758292835205793\n",
      "epoch:  517, loss: 0.0077569689601659775\n",
      "epoch:  518, loss: 0.0077569689601659775\n",
      "epoch:  519, loss: 0.007756146602332592\n",
      "epoch:  520, loss: 0.007756136357784271\n",
      "epoch:  521, loss: 0.007756136357784271\n",
      "epoch:  522, loss: 0.007754500489681959\n",
      "epoch:  523, loss: 0.007754496298730373\n",
      "epoch:  524, loss: 0.00770824309438467\n",
      "epoch:  525, loss: 0.00770824309438467\n",
      "epoch:  526, loss: 0.007707224227488041\n",
      "epoch:  527, loss: 0.007706909440457821\n",
      "epoch:  528, loss: 0.007706315256655216\n",
      "epoch:  529, loss: 0.007706315256655216\n",
      "epoch:  530, loss: 0.007701886352151632\n",
      "epoch:  531, loss: 0.0077009848318994045\n",
      "epoch:  532, loss: 0.007700229529291391\n",
      "epoch:  533, loss: 0.007700229529291391\n",
      "epoch:  534, loss: 0.007699443958699703\n",
      "epoch:  535, loss: 0.007699160370975733\n",
      "epoch:  536, loss: 0.007698760833591223\n",
      "epoch:  537, loss: 0.007698760833591223\n",
      "epoch:  538, loss: 0.00769477104768157\n",
      "epoch:  539, loss: 0.007693932857364416\n",
      "epoch:  540, loss: 0.007693229243159294\n",
      "epoch:  541, loss: 0.007693229243159294\n",
      "epoch:  542, loss: 0.007692516315728426\n",
      "epoch:  543, loss: 0.0076918825507164\n",
      "epoch:  544, loss: 0.0076918825507164\n",
      "epoch:  545, loss: 0.007690138649195433\n",
      "epoch:  546, loss: 0.0076897237449884415\n",
      "epoch:  547, loss: 0.007689132355153561\n",
      "epoch:  548, loss: 0.007688210811465979\n",
      "epoch:  549, loss: 0.007688210811465979\n",
      "epoch:  550, loss: 0.007686791475862265\n",
      "epoch:  551, loss: 0.007686447352170944\n",
      "epoch:  552, loss: 0.007685854099690914\n",
      "epoch:  553, loss: 0.007684528827667236\n",
      "epoch:  554, loss: 0.007684528827667236\n",
      "epoch:  555, loss: 0.007682747673243284\n",
      "epoch:  556, loss: 0.007682386320084333\n",
      "epoch:  557, loss: 0.007681726478040218\n",
      "epoch:  558, loss: 0.007681726478040218\n",
      "epoch:  559, loss: 0.007681135553866625\n",
      "epoch:  560, loss: 0.007678935304284096\n",
      "epoch:  561, loss: 0.007678935304284096\n",
      "epoch:  562, loss: 0.007677372545003891\n",
      "epoch:  563, loss: 0.007676991168409586\n",
      "epoch:  564, loss: 0.007676383014768362\n",
      "epoch:  565, loss: 0.007676257286220789\n",
      "epoch:  566, loss: 0.00767569150775671\n",
      "epoch:  567, loss: 0.007673570420593023\n",
      "epoch:  568, loss: 0.007673570420593023\n",
      "epoch:  569, loss: 0.007670904044061899\n",
      "epoch:  570, loss: 0.007670292630791664\n",
      "epoch:  571, loss: 0.007669636979699135\n",
      "epoch:  572, loss: 0.007669636979699135\n",
      "epoch:  573, loss: 0.0076690255664289\n",
      "epoch:  574, loss: 0.0076661305502057076\n",
      "epoch:  575, loss: 0.0076661305502057076\n",
      "epoch:  576, loss: 0.007665480021387339\n",
      "epoch:  577, loss: 0.007662553805857897\n",
      "epoch:  578, loss: 0.007662553805857897\n",
      "epoch:  579, loss: 0.007661846932023764\n",
      "epoch:  580, loss: 0.007659680675715208\n",
      "epoch:  581, loss: 0.007659680675715208\n",
      "epoch:  582, loss: 0.00765881035476923\n",
      "epoch:  583, loss: 0.007657233159989119\n",
      "epoch:  584, loss: 0.007657233159989119\n",
      "epoch:  585, loss: 0.007656097412109375\n",
      "epoch:  586, loss: 0.007654886227101088\n",
      "epoch:  587, loss: 0.007654886227101088\n",
      "epoch:  588, loss: 0.007653886917978525\n",
      "epoch:  589, loss: 0.007652114611119032\n",
      "epoch:  590, loss: 0.00765048386529088\n",
      "epoch:  591, loss: 0.00765048386529088\n",
      "epoch:  592, loss: 0.007649148348718882\n",
      "epoch:  593, loss: 0.007648827973753214\n",
      "epoch:  594, loss: 0.0076460265554487705\n",
      "epoch:  595, loss: 0.0076460265554487705\n",
      "epoch:  596, loss: 0.0076438854448497295\n",
      "epoch:  597, loss: 0.007643240038305521\n",
      "epoch:  598, loss: 0.007642837706953287\n",
      "epoch:  599, loss: 0.007642312441021204\n",
      "epoch:  600, loss: 0.007642312441021204\n",
      "epoch:  601, loss: 0.007639102637767792\n",
      "epoch:  602, loss: 0.007637546863406897\n",
      "epoch:  603, loss: 0.007633748464286327\n",
      "epoch:  604, loss: 0.007633748464286327\n",
      "epoch:  605, loss: 0.00763294193893671\n",
      "epoch:  606, loss: 0.007632905151695013\n",
      "epoch:  607, loss: 0.007632304448634386\n",
      "epoch:  608, loss: 0.00761211384087801\n",
      "epoch:  609, loss: 0.00761211384087801\n",
      "epoch:  610, loss: 0.0076099419966340065\n",
      "epoch:  611, loss: 0.0076099419966340065\n",
      "epoch:  612, loss: 0.007608740124851465\n",
      "epoch:  613, loss: 0.007608725689351559\n",
      "epoch:  614, loss: 0.007608725689351559\n",
      "epoch:  615, loss: 0.007607845589518547\n",
      "epoch:  616, loss: 0.007606646977365017\n",
      "epoch:  617, loss: 0.007606646977365017\n",
      "epoch:  618, loss: 0.007605815306305885\n",
      "epoch:  619, loss: 0.007603805512189865\n",
      "epoch:  620, loss: 0.007603805046528578\n",
      "epoch:  621, loss: 0.007603079546242952\n",
      "epoch:  622, loss: 0.007602977100759745\n",
      "epoch:  623, loss: 0.00760235358029604\n",
      "epoch:  624, loss: 0.007569008972495794\n",
      "epoch:  625, loss: 0.007569008972495794\n",
      "epoch:  626, loss: 0.007559977006167173\n",
      "epoch:  627, loss: 0.007556738797575235\n",
      "epoch:  628, loss: 0.007556459866464138\n",
      "epoch:  629, loss: 0.007556459866464138\n",
      "epoch:  630, loss: 0.007554416079074144\n",
      "epoch:  631, loss: 0.007553244475275278\n",
      "epoch:  632, loss: 0.007552444003522396\n",
      "epoch:  633, loss: 0.007552444003522396\n",
      "epoch:  634, loss: 0.007551690097898245\n",
      "epoch:  635, loss: 0.0075509557500481606\n",
      "epoch:  636, loss: 0.0075509557500481606\n",
      "epoch:  637, loss: 0.007548679132014513\n",
      "epoch:  638, loss: 0.007548679132014513\n",
      "epoch:  639, loss: 0.007547297980636358\n",
      "epoch:  640, loss: 0.00754654873162508\n",
      "epoch:  641, loss: 0.007545748725533485\n",
      "epoch:  642, loss: 0.007544809486716986\n",
      "epoch:  643, loss: 0.007544809486716986\n",
      "epoch:  644, loss: 0.007543930783867836\n",
      "epoch:  645, loss: 0.007543926127254963\n",
      "epoch:  646, loss: 0.007543159648776054\n",
      "epoch:  647, loss: 0.007539463695138693\n",
      "epoch:  648, loss: 0.007539462763816118\n",
      "epoch:  649, loss: 0.007538123056292534\n",
      "epoch:  650, loss: 0.007537932600826025\n",
      "epoch:  651, loss: 0.007537231780588627\n",
      "epoch:  652, loss: 0.007537228986620903\n",
      "epoch:  653, loss: 0.00753658264875412\n",
      "epoch:  654, loss: 0.0075341626070439816\n",
      "epoch:  655, loss: 0.0075341626070439816\n",
      "epoch:  656, loss: 0.007532611954957247\n",
      "epoch:  657, loss: 0.007532611954957247\n",
      "epoch:  658, loss: 0.007531612180173397\n",
      "epoch:  659, loss: 0.007531612180173397\n",
      "epoch:  660, loss: 0.0075309788808226585\n",
      "epoch:  661, loss: 0.007529501803219318\n",
      "epoch:  662, loss: 0.007529501803219318\n",
      "epoch:  663, loss: 0.007525801658630371\n",
      "epoch:  664, loss: 0.007525801658630371\n",
      "epoch:  665, loss: 0.0075241923332214355\n",
      "epoch:  666, loss: 0.0075241923332214355\n",
      "epoch:  667, loss: 0.007523298263549805\n",
      "epoch:  668, loss: 0.007523298263549805\n",
      "epoch:  669, loss: 0.007522569503635168\n",
      "epoch:  670, loss: 0.007504453416913748\n",
      "epoch:  671, loss: 0.007504370529204607\n",
      "epoch:  672, loss: 0.007503621280193329\n",
      "epoch:  673, loss: 0.007502941880375147\n",
      "epoch:  674, loss: 0.007502941880375147\n",
      "epoch:  675, loss: 0.007502319756895304\n",
      "epoch:  676, loss: 0.007502094376832247\n",
      "epoch:  677, loss: 0.007502094376832247\n",
      "epoch:  678, loss: 0.007501403335481882\n",
      "epoch:  679, loss: 0.007499869912862778\n",
      "epoch:  680, loss: 0.007499869912862778\n",
      "epoch:  681, loss: 0.007498707622289658\n",
      "epoch:  682, loss: 0.007498705293983221\n",
      "epoch:  683, loss: 0.007497923914343119\n",
      "epoch:  684, loss: 0.007496727630496025\n",
      "epoch:  685, loss: 0.007496727630496025\n",
      "epoch:  686, loss: 0.0074959127232432365\n",
      "epoch:  687, loss: 0.0074959127232432365\n",
      "epoch:  688, loss: 0.007495251949876547\n",
      "epoch:  689, loss: 0.007495251949876547\n",
      "epoch:  690, loss: 0.007494647521525621\n",
      "epoch:  691, loss: 0.007491153199225664\n",
      "epoch:  692, loss: 0.007491153199225664\n",
      "epoch:  693, loss: 0.00749023724347353\n",
      "epoch:  694, loss: 0.00749023724347353\n",
      "epoch:  695, loss: 0.007489545736461878\n",
      "epoch:  696, loss: 0.007489481940865517\n",
      "epoch:  697, loss: 0.007489481940865517\n",
      "epoch:  698, loss: 0.007488845847547054\n",
      "epoch:  699, loss: 0.007488845847547054\n",
      "epoch:  700, loss: 0.007488253992050886\n",
      "epoch:  701, loss: 0.007486834190785885\n",
      "epoch:  702, loss: 0.007486834190785885\n",
      "epoch:  703, loss: 0.007485132664442062\n",
      "epoch:  704, loss: 0.007485132664442062\n",
      "epoch:  705, loss: 0.007484120782464743\n",
      "epoch:  706, loss: 0.007483827881515026\n",
      "epoch:  707, loss: 0.007482954766601324\n",
      "epoch:  708, loss: 0.0074822925962507725\n",
      "epoch:  709, loss: 0.0074822925962507725\n",
      "epoch:  710, loss: 0.007481701206415892\n",
      "epoch:  711, loss: 0.007481195963919163\n",
      "epoch:  712, loss: 0.007481195963919163\n",
      "epoch:  713, loss: 0.007478642743080854\n",
      "epoch:  714, loss: 0.007478433661162853\n",
      "epoch:  715, loss: 0.007477758917957544\n",
      "epoch:  716, loss: 0.007477758917957544\n",
      "epoch:  717, loss: 0.007477133069187403\n",
      "epoch:  718, loss: 0.007477133069187403\n",
      "epoch:  719, loss: 0.007476577069610357\n",
      "epoch:  720, loss: 0.007476394064724445\n",
      "epoch:  721, loss: 0.007467142306268215\n",
      "epoch:  722, loss: 0.007467142306268215\n",
      "epoch:  723, loss: 0.007455458398908377\n",
      "epoch:  724, loss: 0.007453152909874916\n",
      "epoch:  725, loss: 0.007451747544109821\n",
      "epoch:  726, loss: 0.007451747544109821\n",
      "epoch:  727, loss: 0.007450275123119354\n",
      "epoch:  728, loss: 0.007450275123119354\n",
      "epoch:  729, loss: 0.007449112832546234\n",
      "epoch:  730, loss: 0.0074394806288182735\n",
      "epoch:  731, loss: 0.007439480163156986\n",
      "epoch:  732, loss: 0.007438526023179293\n",
      "epoch:  733, loss: 0.0074384319595992565\n",
      "epoch:  734, loss: 0.007438286207616329\n",
      "epoch:  735, loss: 0.007437010761350393\n",
      "epoch:  736, loss: 0.007433669175952673\n",
      "epoch:  737, loss: 0.007433669175952673\n",
      "epoch:  738, loss: 0.007432118058204651\n",
      "epoch:  739, loss: 0.007432118058204651\n",
      "epoch:  740, loss: 0.007431101519614458\n",
      "epoch:  741, loss: 0.007431101519614458\n",
      "epoch:  742, loss: 0.007430304307490587\n",
      "epoch:  743, loss: 0.00742900837212801\n",
      "epoch:  744, loss: 0.00742900837212801\n",
      "epoch:  745, loss: 0.007428064942359924\n",
      "epoch:  746, loss: 0.0074278065003454685\n",
      "epoch:  747, loss: 0.007385922130197287\n",
      "epoch:  748, loss: 0.007385922130197287\n",
      "epoch:  749, loss: 0.007382112555205822\n",
      "epoch:  750, loss: 0.007382112555205822\n",
      "epoch:  751, loss: 0.007380189839750528\n",
      "epoch:  752, loss: 0.007380189839750528\n",
      "epoch:  753, loss: 0.0073790294118225574\n",
      "epoch:  754, loss: 0.007378044072538614\n",
      "epoch:  755, loss: 0.007359043695032597\n",
      "epoch:  756, loss: 0.007359043695032597\n",
      "epoch:  757, loss: 0.007355780340731144\n",
      "epoch:  758, loss: 0.007355780340731144\n",
      "epoch:  759, loss: 0.007353989873081446\n",
      "epoch:  760, loss: 0.007353989873081446\n",
      "epoch:  761, loss: 0.007352850399911404\n",
      "epoch:  762, loss: 0.007352849468588829\n",
      "epoch:  763, loss: 0.007352003827691078\n",
      "epoch:  764, loss: 0.007352003827691078\n",
      "epoch:  765, loss: 0.00735130300745368\n",
      "epoch:  766, loss: 0.0073502385057508945\n",
      "epoch:  767, loss: 0.0073502385057508945\n",
      "epoch:  768, loss: 0.00734889367595315\n",
      "epoch:  769, loss: 0.00734889367595315\n",
      "epoch:  770, loss: 0.007347948383539915\n",
      "epoch:  771, loss: 0.007347948383539915\n",
      "epoch:  772, loss: 0.007347255013883114\n",
      "epoch:  773, loss: 0.007347255013883114\n",
      "epoch:  774, loss: 0.007346638478338718\n",
      "epoch:  775, loss: 0.007346638478338718\n",
      "epoch:  776, loss: 0.007346060127019882\n",
      "epoch:  777, loss: 0.007346056401729584\n",
      "epoch:  778, loss: 0.007345504593104124\n",
      "epoch:  779, loss: 0.007345504593104124\n",
      "epoch:  780, loss: 0.00734497606754303\n",
      "epoch:  781, loss: 0.007311265449970961\n",
      "epoch:  782, loss: 0.007311265449970961\n",
      "epoch:  783, loss: 0.007310086395591497\n",
      "epoch:  784, loss: 0.007310086395591497\n",
      "epoch:  785, loss: 0.007309162523597479\n",
      "epoch:  786, loss: 0.007309162523597479\n",
      "epoch:  787, loss: 0.007308596279472113\n",
      "epoch:  788, loss: 0.007308596279472113\n",
      "epoch:  789, loss: 0.00730799650773406\n",
      "epoch:  790, loss: 0.007307115010917187\n",
      "epoch:  791, loss: 0.0072829159907996655\n",
      "epoch:  792, loss: 0.0072829159907996655\n",
      "epoch:  793, loss: 0.007280439138412476\n",
      "epoch:  794, loss: 0.007280439138412476\n",
      "epoch:  795, loss: 0.007278780918568373\n",
      "epoch:  796, loss: 0.007278780918568373\n",
      "epoch:  797, loss: 0.007277755532413721\n",
      "epoch:  798, loss: 0.007277755532413721\n",
      "epoch:  799, loss: 0.007276805583387613\n",
      "epoch:  800, loss: 0.007276804652065039\n",
      "epoch:  801, loss: 0.0072760628536343575\n",
      "epoch:  802, loss: 0.0072760628536343575\n",
      "epoch:  803, loss: 0.007275402545928955\n",
      "epoch:  804, loss: 0.007275402545928955\n",
      "epoch:  805, loss: 0.007274783682078123\n",
      "epoch:  806, loss: 0.007273100316524506\n",
      "epoch:  807, loss: 0.007273100316524506\n",
      "epoch:  808, loss: 0.007272133603692055\n",
      "epoch:  809, loss: 0.007272133603692055\n",
      "epoch:  810, loss: 0.007271202746778727\n",
      "epoch:  811, loss: 0.007271202746778727\n",
      "epoch:  812, loss: 0.007270470727235079\n",
      "epoch:  813, loss: 0.007270470727235079\n",
      "epoch:  814, loss: 0.007269855123013258\n",
      "epoch:  815, loss: 0.007269855123013258\n",
      "epoch:  816, loss: 0.007269300054758787\n",
      "epoch:  817, loss: 0.007268386427313089\n",
      "epoch:  818, loss: 0.007268386427313089\n",
      "epoch:  819, loss: 0.0072678192518651485\n",
      "epoch:  820, loss: 0.007267568726092577\n",
      "epoch:  821, loss: 0.007267056964337826\n",
      "epoch:  822, loss: 0.00726705277338624\n",
      "epoch:  823, loss: 0.007266503293067217\n",
      "epoch:  824, loss: 0.007263604085892439\n",
      "epoch:  825, loss: 0.007263604085892439\n",
      "epoch:  826, loss: 0.007262703031301498\n",
      "epoch:  827, loss: 0.007262703031301498\n",
      "epoch:  828, loss: 0.0072620525024831295\n",
      "epoch:  829, loss: 0.0072620525024831295\n",
      "epoch:  830, loss: 0.007261515595018864\n",
      "epoch:  831, loss: 0.007261493243277073\n",
      "epoch:  832, loss: 0.007261493243277073\n",
      "epoch:  833, loss: 0.007260695565491915\n",
      "epoch:  834, loss: 0.007260695565491915\n",
      "epoch:  835, loss: 0.007260042242705822\n",
      "epoch:  836, loss: 0.007260042242705822\n",
      "epoch:  837, loss: 0.007259514182806015\n",
      "epoch:  838, loss: 0.0072586918249726295\n",
      "epoch:  839, loss: 0.0072581772692501545\n",
      "epoch:  840, loss: 0.007256154902279377\n",
      "epoch:  841, loss: 0.007256154902279377\n",
      "epoch:  842, loss: 0.007255518808960915\n",
      "epoch:  843, loss: 0.007255518808960915\n",
      "epoch:  844, loss: 0.007254985626786947\n",
      "epoch:  845, loss: 0.007254985626786947\n",
      "epoch:  846, loss: 0.00725450087338686\n",
      "epoch:  847, loss: 0.00725450087338686\n",
      "epoch:  848, loss: 0.007254069671034813\n",
      "epoch:  849, loss: 0.007254069671034813\n",
      "epoch:  850, loss: 0.007253651507198811\n",
      "epoch:  851, loss: 0.007252460345625877\n",
      "epoch:  852, loss: 0.007252460345625877\n",
      "epoch:  853, loss: 0.007251814939081669\n",
      "epoch:  854, loss: 0.007251814939081669\n",
      "epoch:  855, loss: 0.007251324597746134\n",
      "epoch:  856, loss: 0.007251324597746134\n",
      "epoch:  857, loss: 0.007250851020216942\n",
      "epoch:  858, loss: 0.007250851020216942\n",
      "epoch:  859, loss: 0.007250414229929447\n",
      "epoch:  860, loss: 0.007243075408041477\n",
      "epoch:  861, loss: 0.007243075408041477\n",
      "epoch:  862, loss: 0.007241370622068644\n",
      "epoch:  863, loss: 0.007241370622068644\n",
      "epoch:  864, loss: 0.007239838596433401\n",
      "epoch:  865, loss: 0.007239838596433401\n",
      "epoch:  866, loss: 0.0072385589592158794\n",
      "epoch:  867, loss: 0.007238558493554592\n",
      "epoch:  868, loss: 0.007237488869577646\n",
      "epoch:  869, loss: 0.007237488869577646\n",
      "epoch:  870, loss: 0.007236434146761894\n",
      "epoch:  871, loss: 0.00723628094419837\n",
      "epoch:  872, loss: 0.007235294207930565\n",
      "epoch:  873, loss: 0.007228968199342489\n",
      "epoch:  874, loss: 0.007228666450828314\n",
      "epoch:  875, loss: 0.007228666450828314\n",
      "epoch:  876, loss: 0.007219447288662195\n",
      "epoch:  877, loss: 0.007219447288662195\n",
      "epoch:  878, loss: 0.0072142756544053555\n",
      "epoch:  879, loss: 0.0072142756544053555\n",
      "epoch:  880, loss: 0.007209300063550472\n",
      "epoch:  881, loss: 0.007209300063550472\n",
      "epoch:  882, loss: 0.007205730304121971\n",
      "epoch:  883, loss: 0.007205730304121971\n",
      "epoch:  884, loss: 0.007202987093478441\n",
      "epoch:  885, loss: 0.007202987093478441\n",
      "epoch:  886, loss: 0.007200723513960838\n",
      "epoch:  887, loss: 0.007200723513960838\n",
      "epoch:  888, loss: 0.0071989139541983604\n",
      "epoch:  889, loss: 0.0071989139541983604\n",
      "epoch:  890, loss: 0.007197614759206772\n",
      "epoch:  891, loss: 0.007197614759206772\n",
      "epoch:  892, loss: 0.007196274120360613\n",
      "epoch:  893, loss: 0.007196274120360613\n",
      "epoch:  894, loss: 0.007195384707301855\n",
      "epoch:  895, loss: 0.007195384707301855\n",
      "epoch:  896, loss: 0.00719432532787323\n",
      "epoch:  897, loss: 0.00719432532787323\n",
      "epoch:  898, loss: 0.007193456869572401\n",
      "epoch:  899, loss: 0.007193456869572401\n",
      "epoch:  900, loss: 0.007192461285740137\n",
      "epoch:  901, loss: 0.007192461285740137\n",
      "epoch:  902, loss: 0.007191535085439682\n",
      "epoch:  903, loss: 0.007191535085439682\n",
      "epoch:  904, loss: 0.007190709467977285\n",
      "epoch:  905, loss: 0.007190709467977285\n",
      "epoch:  906, loss: 0.007189780939370394\n",
      "epoch:  907, loss: 0.007189780939370394\n",
      "epoch:  908, loss: 0.007189053110778332\n",
      "epoch:  909, loss: 0.007189053110778332\n",
      "epoch:  910, loss: 0.007188174407929182\n",
      "epoch:  911, loss: 0.007188174407929182\n",
      "epoch:  912, loss: 0.0071874032728374004\n",
      "epoch:  913, loss: 0.0071874032728374004\n",
      "epoch:  914, loss: 0.007186708506196737\n",
      "epoch:  915, loss: 0.007185786962509155\n",
      "epoch:  916, loss: 0.007185786962509155\n",
      "epoch:  917, loss: 0.0071846493519842625\n",
      "epoch:  918, loss: 0.0071846493519842625\n",
      "epoch:  919, loss: 0.007183719892054796\n",
      "epoch:  920, loss: 0.007183719892054796\n",
      "epoch:  921, loss: 0.0071829878725111485\n",
      "epoch:  922, loss: 0.0071829878725111485\n",
      "epoch:  923, loss: 0.007182345725595951\n",
      "epoch:  924, loss: 0.007182345725595951\n",
      "epoch:  925, loss: 0.007181782741099596\n",
      "epoch:  926, loss: 0.007180681452155113\n",
      "epoch:  927, loss: 0.007180681452155113\n",
      "epoch:  928, loss: 0.007179868873208761\n",
      "epoch:  929, loss: 0.007179868873208761\n",
      "epoch:  930, loss: 0.007179158739745617\n",
      "epoch:  931, loss: 0.007179158739745617\n",
      "epoch:  932, loss: 0.007178575731813908\n",
      "epoch:  933, loss: 0.007178575731813908\n",
      "epoch:  934, loss: 0.007178073283284903\n",
      "epoch:  935, loss: 0.007178072817623615\n",
      "epoch:  936, loss: 0.007177634164690971\n",
      "epoch:  937, loss: 0.007177634164690971\n",
      "epoch:  938, loss: 0.007177302613854408\n",
      "epoch:  939, loss: 0.007177302613854408\n",
      "epoch:  940, loss: 0.007176732644438744\n",
      "epoch:  941, loss: 0.007176732644438744\n",
      "epoch:  942, loss: 0.00717630609869957\n",
      "epoch:  943, loss: 0.00717630609869957\n",
      "epoch:  944, loss: 0.007175727281719446\n",
      "epoch:  945, loss: 0.007175727281719446\n",
      "epoch:  946, loss: 0.007175202015787363\n",
      "epoch:  947, loss: 0.007175202015787363\n",
      "epoch:  948, loss: 0.007174723781645298\n",
      "epoch:  949, loss: 0.007174396421760321\n",
      "epoch:  950, loss: 0.007174396421760321\n",
      "epoch:  951, loss: 0.0071738469414412975\n",
      "epoch:  952, loss: 0.0071738469414412975\n",
      "epoch:  953, loss: 0.007173434365540743\n",
      "epoch:  954, loss: 0.007173433899879456\n",
      "epoch:  955, loss: 0.007172951474785805\n",
      "epoch:  956, loss: 0.007172951474785805\n",
      "epoch:  957, loss: 0.00717255799099803\n",
      "epoch:  958, loss: 0.00717255799099803\n",
      "epoch:  959, loss: 0.007172131445258856\n",
      "epoch:  960, loss: 0.007172131445258856\n",
      "epoch:  961, loss: 0.007171736564487219\n",
      "epoch:  962, loss: 0.007171736564487219\n",
      "epoch:  963, loss: 0.00717132119461894\n",
      "epoch:  964, loss: 0.00717132119461894\n",
      "epoch:  965, loss: 0.007170927245169878\n",
      "epoch:  966, loss: 0.007170927245169878\n",
      "epoch:  967, loss: 0.007170523051172495\n",
      "epoch:  968, loss: 0.007170523051172495\n",
      "epoch:  969, loss: 0.007170130964368582\n",
      "epoch:  970, loss: 0.0071701304987072945\n",
      "epoch:  971, loss: 0.00716973515227437\n",
      "epoch:  972, loss: 0.00716973515227437\n",
      "epoch:  973, loss: 0.007169343531131744\n",
      "epoch:  974, loss: 0.007169317454099655\n",
      "epoch:  975, loss: 0.007168847136199474\n",
      "epoch:  976, loss: 0.007168245036154985\n",
      "epoch:  977, loss: 0.007168245036154985\n",
      "epoch:  978, loss: 0.007167443633079529\n",
      "epoch:  979, loss: 0.007167443633079529\n",
      "epoch:  980, loss: 0.0071669756434857845\n",
      "epoch:  981, loss: 0.0071669756434857845\n",
      "epoch:  982, loss: 0.007166167255491018\n",
      "epoch:  983, loss: 0.007166167255491018\n",
      "epoch:  984, loss: 0.00716591440141201\n",
      "epoch:  985, loss: 0.00716591440141201\n",
      "epoch:  986, loss: 0.007165125571191311\n",
      "epoch:  987, loss: 0.007165125571191311\n",
      "epoch:  988, loss: 0.007164740934967995\n",
      "epoch:  989, loss: 0.007164740934967995\n",
      "epoch:  990, loss: 0.0071640764363110065\n",
      "epoch:  991, loss: 0.0071640764363110065\n",
      "epoch:  992, loss: 0.007163508329540491\n",
      "epoch:  993, loss: 0.007163507863879204\n",
      "epoch:  994, loss: 0.0071630231104791164\n",
      "epoch:  995, loss: 0.0071630231104791164\n",
      "epoch:  996, loss: 0.007162579335272312\n",
      "epoch:  997, loss: 0.007162579335272312\n",
      "epoch:  998, loss: 0.007162149995565414\n",
      "epoch:  999, loss: 0.007162143941968679\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size=X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# opt = torch_numopt.ConjugateGradient(model=model, lr=1e-4, line_search_method=\"const\", cg_method=\"PR\")\n",
    "opt = torch_numopt.ConjugateGradient(model=model, lr=10, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"armijo\", cg_method=\"PR\")\n",
    "# opt = torch_numopt.ConjugateGradient(model=model, lr=10, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"wolfe\", cg_method=\"PR\")\n",
    "# opt = torch_numopt.ConjugateGradient(model=model, lr=10, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"strong-wolfe\", cg_method=\"PR\")\n",
    "# opt = torch_numopt.ConjugateGradient(model=model, lr=10, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"goldstein\", cg_method=\"PR\")\n",
    "\n",
    "\n",
    "all_loss = {}\n",
    "for epoch in range(1000):\n",
    "    print(\"epoch: \", epoch, end=\"\")\n",
    "    all_loss[epoch + 1] = 0\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y, loss_fn)\n",
    "\n",
    "        all_loss[epoch + 1] += loss\n",
    "    all_loss[epoch + 1] /= len(data_loader)\n",
    "    print(\", loss: {}\".format(all_loss[epoch + 1].cpu().detach().numpy().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics: R2 = 0.7564386457689405\n",
      "Test metrics:  R2 = 0.7523754777767294\n"
     ]
    }
   ],
   "source": [
    "pred_train = model.forward(X_train).cpu().detach()\n",
    "pred_test = model.forward(X_test).cpu().detach()\n",
    "print(f\"Train metrics: R2 = {r2_score(pred_train, y_train.cpu())}\")\n",
    "print(f\"Test metrics:  R2 = {r2_score(pred_test, y_test.cpu())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.7086195945739746\n",
      "epoch:  1, loss: 0.4349656105041504\n",
      "epoch:  2, loss: 0.4349656105041504\n",
      "epoch:  3, loss: 0.4351779520511627\n",
      "epoch:  4, loss: 0.05551861599087715\n",
      "epoch:  5, loss: 0.047637324780225754\n",
      "epoch:  6, loss: 0.047637324780225754\n",
      "epoch:  7, loss: 0.04303358867764473\n",
      "epoch:  8, loss: 0.04303358867764473\n",
      "epoch:  9, loss: 0.0430341437458992\n",
      "epoch:  10, loss: 0.040385108441114426\n",
      "epoch:  11, loss: 0.040385108441114426\n",
      "epoch:  12, loss: 0.04107861965894699\n",
      "epoch:  13, loss: 0.03822510316967964\n",
      "epoch:  14, loss: 0.03767276555299759\n",
      "epoch:  15, loss: 0.03767276555299759\n",
      "epoch:  16, loss: 0.037673261016607285\n",
      "epoch:  17, loss: 0.036949459463357925\n",
      "epoch:  18, loss: 0.03691224753856659\n",
      "epoch:  19, loss: 0.03691224753856659\n",
      "epoch:  20, loss: 0.03691224753856659\n",
      "epoch:  21, loss: 0.03691224753856659\n",
      "epoch:  22, loss: 0.03691227734088898\n",
      "epoch:  23, loss: 0.036888718605041504\n",
      "epoch:  24, loss: 0.036888718605041504\n",
      "epoch:  25, loss: 0.036852262914180756\n",
      "epoch:  26, loss: 0.03683929145336151\n",
      "epoch:  27, loss: 0.03683929145336151\n",
      "epoch:  28, loss: 0.03683929145336151\n",
      "epoch:  29, loss: 0.036628708243370056\n",
      "epoch:  30, loss: 0.03655773028731346\n",
      "epoch:  31, loss: 0.03655773028731346\n",
      "epoch:  32, loss: 0.03655773028731346\n",
      "epoch:  33, loss: 0.03655773028731346\n",
      "epoch:  34, loss: 0.03655773028731346\n",
      "epoch:  35, loss: 0.03655773028731346\n",
      "epoch:  36, loss: 0.036498937755823135\n",
      "epoch:  37, loss: 0.03642943128943443\n",
      "epoch:  38, loss: 0.036429427564144135\n",
      "epoch:  39, loss: 0.036429427564144135\n",
      "epoch:  40, loss: 0.03641051426529884\n",
      "epoch:  41, loss: 0.03641051426529884\n",
      "epoch:  42, loss: 0.03639322146773338\n",
      "epoch:  43, loss: 0.03638988733291626\n",
      "epoch:  44, loss: 0.03638988733291626\n",
      "epoch:  45, loss: 0.036389898508787155\n",
      "epoch:  46, loss: 0.03634437546133995\n",
      "epoch:  47, loss: 0.03625987097620964\n",
      "epoch:  48, loss: 0.03586079180240631\n",
      "epoch:  49, loss: 0.03586079180240631\n",
      "epoch:  50, loss: 0.03586079180240631\n",
      "epoch:  51, loss: 0.03572392463684082\n",
      "epoch:  52, loss: 0.0357201062142849\n",
      "epoch:  53, loss: 0.0357201062142849\n",
      "epoch:  54, loss: 0.03560899198055267\n",
      "epoch:  55, loss: 0.035541340708732605\n",
      "epoch:  56, loss: 0.03520229086279869\n",
      "epoch:  57, loss: 0.03520229086279869\n",
      "epoch:  58, loss: 0.03513899818062782\n",
      "epoch:  59, loss: 0.034795429557561874\n",
      "epoch:  60, loss: 0.034795429557561874\n",
      "epoch:  61, loss: 0.03479544073343277\n",
      "epoch:  62, loss: 0.03462088853120804\n",
      "epoch:  63, loss: 0.03462088853120804\n",
      "epoch:  64, loss: 0.034455008804798126\n",
      "epoch:  65, loss: 0.03431297838687897\n",
      "epoch:  66, loss: 0.034039199352264404\n",
      "epoch:  67, loss: 0.034039199352264404\n",
      "epoch:  68, loss: 0.033192966133356094\n",
      "epoch:  69, loss: 0.032960694283246994\n",
      "epoch:  70, loss: 0.032960694283246994\n",
      "epoch:  71, loss: 0.032960694283246994\n",
      "epoch:  72, loss: 0.032960694283246994\n",
      "epoch:  73, loss: 0.03066510520875454\n",
      "epoch:  74, loss: 0.026254896074533463\n",
      "epoch:  75, loss: 0.02510693483054638\n",
      "epoch:  76, loss: 0.02418948896229267\n",
      "epoch:  77, loss: 0.02418948896229267\n",
      "epoch:  78, loss: 0.02337469719350338\n",
      "epoch:  79, loss: 0.022806581109762192\n",
      "epoch:  80, loss: 0.022806581109762192\n",
      "epoch:  81, loss: 0.02254493348300457\n",
      "epoch:  82, loss: 0.02254493348300457\n",
      "epoch:  83, loss: 0.02255231887102127\n",
      "epoch:  84, loss: 0.02255231887102127\n",
      "epoch:  85, loss: 0.02255234122276306\n",
      "epoch:  86, loss: 0.022146286442875862\n",
      "epoch:  87, loss: 0.022146286442875862\n",
      "epoch:  88, loss: 0.01832881011068821\n",
      "epoch:  89, loss: 0.017443358898162842\n",
      "epoch:  90, loss: 0.017443358898162842\n",
      "epoch:  91, loss: 0.017062503844499588\n",
      "epoch:  92, loss: 0.017062503844499588\n",
      "epoch:  93, loss: 0.03437358886003494\n",
      "epoch:  94, loss: 0.03437358886003494\n",
      "epoch:  95, loss: 0.018735449761152267\n",
      "epoch:  96, loss: 0.01648930460214615\n",
      "epoch:  97, loss: 0.01648930460214615\n",
      "epoch:  98, loss: 0.01494218036532402\n",
      "epoch:  99, loss: 0.014743005856871605\n",
      "epoch:  100, loss: 0.014743005856871605\n",
      "epoch:  101, loss: 0.014451473951339722\n",
      "epoch:  102, loss: 0.014451473951339722\n",
      "epoch:  103, loss: 0.014452275820076466\n",
      "epoch:  104, loss: 0.014316558837890625\n",
      "epoch:  105, loss: 0.014306632801890373\n",
      "epoch:  106, loss: 0.014306632801890373\n",
      "epoch:  107, loss: 0.014306679368019104\n",
      "epoch:  108, loss: 0.014170029200613499\n",
      "epoch:  109, loss: 0.013895592652261257\n",
      "epoch:  110, loss: 0.013496612198650837\n",
      "epoch:  111, loss: 0.013496612198650837\n",
      "epoch:  112, loss: 0.013496669009327888\n",
      "epoch:  113, loss: 0.012921304441988468\n",
      "epoch:  114, loss: 0.012860183604061604\n",
      "epoch:  115, loss: 0.012860183604061604\n",
      "epoch:  116, loss: 0.012737173587083817\n",
      "epoch:  117, loss: 0.012337816879153252\n",
      "epoch:  118, loss: 0.012314817868173122\n",
      "epoch:  119, loss: 0.012314817868173122\n",
      "epoch:  120, loss: 0.012314817868173122\n",
      "epoch:  121, loss: 0.012158508412539959\n",
      "epoch:  122, loss: 0.012158508412539959\n",
      "epoch:  123, loss: 0.01209139171987772\n",
      "epoch:  124, loss: 0.01198715902864933\n",
      "epoch:  125, loss: 0.01198715902864933\n",
      "epoch:  126, loss: 0.011987175792455673\n",
      "epoch:  127, loss: 0.01191251166164875\n",
      "epoch:  128, loss: 0.01177265215665102\n",
      "epoch:  129, loss: 0.011300389654934406\n",
      "epoch:  130, loss: 0.011112121865153313\n",
      "epoch:  131, loss: 0.011112121865153313\n",
      "epoch:  132, loss: 0.011112124659121037\n",
      "epoch:  133, loss: 0.011112124659121037\n",
      "epoch:  134, loss: 0.011112138628959656\n",
      "epoch:  135, loss: 0.01096511073410511\n",
      "epoch:  136, loss: 0.010857679881155491\n",
      "epoch:  137, loss: 0.010857679881155491\n",
      "epoch:  138, loss: 0.01085768174380064\n",
      "epoch:  139, loss: 0.010811874642968178\n",
      "epoch:  140, loss: 0.010811874642968178\n",
      "epoch:  141, loss: 0.010811879299581051\n",
      "epoch:  142, loss: 0.010777154937386513\n",
      "epoch:  143, loss: 0.010777154937386513\n",
      "epoch:  144, loss: 0.010759408585727215\n",
      "epoch:  145, loss: 0.010569240897893906\n",
      "epoch:  146, loss: 0.010508875362575054\n",
      "epoch:  147, loss: 0.010508875362575054\n",
      "epoch:  148, loss: 0.010384454391896725\n",
      "epoch:  149, loss: 0.010364608839154243\n",
      "epoch:  150, loss: 0.01032760739326477\n",
      "epoch:  151, loss: 0.01032760739326477\n",
      "epoch:  152, loss: 0.010327605530619621\n",
      "epoch:  153, loss: 0.010295920073986053\n",
      "epoch:  154, loss: 0.010295920073986053\n",
      "epoch:  155, loss: 0.010295920073986053\n",
      "epoch:  156, loss: 0.009696891531348228\n",
      "epoch:  157, loss: 0.008949849754571915\n",
      "epoch:  158, loss: 0.008802886120975018\n",
      "epoch:  159, loss: 0.00879194587469101\n",
      "epoch:  160, loss: 0.00879194587469101\n",
      "epoch:  161, loss: 0.008714419789612293\n",
      "epoch:  162, loss: 0.008677464909851551\n",
      "epoch:  163, loss: 0.008672748692333698\n",
      "epoch:  164, loss: 0.008672748692333698\n",
      "epoch:  165, loss: 0.008672748692333698\n",
      "epoch:  166, loss: 0.008557863533496857\n",
      "epoch:  167, loss: 0.008553298190236092\n",
      "epoch:  168, loss: 0.008547999896109104\n",
      "epoch:  169, loss: 0.008547999896109104\n",
      "epoch:  170, loss: 0.008547999896109104\n",
      "epoch:  171, loss: 0.008537615649402142\n",
      "epoch:  172, loss: 0.00853012129664421\n",
      "epoch:  173, loss: 0.008514096960425377\n",
      "epoch:  174, loss: 0.008514096960425377\n",
      "epoch:  175, loss: 0.008511667139828205\n",
      "epoch:  176, loss: 0.008507180958986282\n",
      "epoch:  177, loss: 0.008502419106662273\n",
      "epoch:  178, loss: 0.008502419106662273\n",
      "epoch:  179, loss: 0.00849990639835596\n",
      "epoch:  180, loss: 0.008494519628584385\n",
      "epoch:  181, loss: 0.008494519628584385\n",
      "epoch:  182, loss: 0.008494519628584385\n",
      "epoch:  183, loss: 0.008491739630699158\n",
      "epoch:  184, loss: 0.008481089025735855\n",
      "epoch:  185, loss: 0.00847382377833128\n",
      "epoch:  186, loss: 0.0084691708907485\n",
      "epoch:  187, loss: 0.008454665541648865\n",
      "epoch:  188, loss: 0.00843667984008789\n",
      "epoch:  189, loss: 0.008436249569058418\n",
      "epoch:  190, loss: 0.008436249569058418\n",
      "epoch:  191, loss: 0.008430671878159046\n",
      "epoch:  192, loss: 0.008413921110332012\n",
      "epoch:  193, loss: 0.008408122695982456\n",
      "epoch:  194, loss: 0.008408122695982456\n",
      "epoch:  195, loss: 0.008403200656175613\n",
      "epoch:  196, loss: 0.008400357328355312\n",
      "epoch:  197, loss: 0.008393206633627415\n",
      "epoch:  198, loss: 0.008377053774893284\n",
      "epoch:  199, loss: 0.008371925912797451\n",
      "epoch:  200, loss: 0.008371426723897457\n",
      "epoch:  201, loss: 0.008371426723897457\n",
      "epoch:  202, loss: 0.008371446281671524\n",
      "epoch:  203, loss: 0.008346810936927795\n",
      "epoch:  204, loss: 0.008346810936927795\n",
      "epoch:  205, loss: 0.008343283087015152\n",
      "epoch:  206, loss: 0.008335357531905174\n",
      "epoch:  207, loss: 0.0083303889259696\n",
      "epoch:  208, loss: 0.008317380212247372\n",
      "epoch:  209, loss: 0.008299988694489002\n",
      "epoch:  210, loss: 0.008292976766824722\n",
      "epoch:  211, loss: 0.008292976766824722\n",
      "epoch:  212, loss: 0.008279874920845032\n",
      "epoch:  213, loss: 0.008268960751593113\n",
      "epoch:  214, loss: 0.008268960751593113\n",
      "epoch:  215, loss: 0.008268960751593113\n",
      "epoch:  216, loss: 0.008268960751593113\n",
      "epoch:  217, loss: 0.008268960751593113\n",
      "epoch:  218, loss: 0.008268686011433601\n",
      "epoch:  219, loss: 0.008263321593403816\n",
      "epoch:  220, loss: 0.008256334811449051\n",
      "epoch:  221, loss: 0.00817793793976307\n",
      "epoch:  222, loss: 0.00817793793976307\n",
      "epoch:  223, loss: 0.00817793793976307\n",
      "epoch:  224, loss: 0.00817793793976307\n",
      "epoch:  225, loss: 0.00817793793976307\n",
      "epoch:  226, loss: 0.00817793793976307\n",
      "epoch:  227, loss: 0.00817793793976307\n",
      "epoch:  228, loss: 0.00817793793976307\n",
      "epoch:  229, loss: 0.008177356794476509\n",
      "epoch:  230, loss: 0.00817421730607748\n",
      "epoch:  231, loss: 0.008174112997949123\n",
      "epoch:  232, loss: 0.008174112997949123\n",
      "epoch:  233, loss: 0.008174112997949123\n",
      "epoch:  234, loss: 0.008174112997949123\n",
      "epoch:  235, loss: 0.00814008992165327\n",
      "epoch:  236, loss: 0.008140088990330696\n",
      "epoch:  237, loss: 0.008095079101622105\n",
      "epoch:  238, loss: 0.008092870935797691\n",
      "epoch:  239, loss: 0.008091994561254978\n",
      "epoch:  240, loss: 0.008091236464679241\n",
      "epoch:  241, loss: 0.00806415919214487\n",
      "epoch:  242, loss: 0.00806377362459898\n",
      "epoch:  243, loss: 0.008059784770011902\n",
      "epoch:  244, loss: 0.008058801293373108\n",
      "epoch:  245, loss: 0.008058801293373108\n",
      "epoch:  246, loss: 0.008058801293373108\n",
      "epoch:  247, loss: 0.008057025261223316\n",
      "epoch:  248, loss: 0.008057025261223316\n",
      "epoch:  249, loss: 0.008057025261223316\n",
      "epoch:  250, loss: 0.00805667880922556\n",
      "epoch:  251, loss: 0.00805667880922556\n",
      "epoch:  252, loss: 0.00805667880922556\n",
      "epoch:  253, loss: 0.008055862970650196\n",
      "epoch:  254, loss: 0.008055773563683033\n",
      "epoch:  255, loss: 0.008055773563683033\n",
      "epoch:  256, loss: 0.008055773563683033\n",
      "epoch:  257, loss: 0.008055124431848526\n",
      "epoch:  258, loss: 0.008054823614656925\n",
      "epoch:  259, loss: 0.008053778670728207\n",
      "epoch:  260, loss: 0.008053455501794815\n",
      "epoch:  261, loss: 0.008053455501794815\n",
      "epoch:  262, loss: 0.008053455501794815\n",
      "epoch:  263, loss: 0.008035257458686829\n",
      "epoch:  264, loss: 0.008027755655348301\n",
      "epoch:  265, loss: 0.008019710890948772\n",
      "epoch:  266, loss: 0.008015253581106663\n",
      "epoch:  267, loss: 0.008014040067791939\n",
      "epoch:  268, loss: 0.00801370944827795\n",
      "epoch:  269, loss: 0.00801370944827795\n",
      "epoch:  270, loss: 0.00801370944827795\n",
      "epoch:  271, loss: 0.00801370944827795\n",
      "epoch:  272, loss: 0.008013051934540272\n",
      "epoch:  273, loss: 0.0080125005915761\n",
      "epoch:  274, loss: 0.0080125005915761\n",
      "epoch:  275, loss: 0.0080125005915761\n",
      "epoch:  276, loss: 0.008009644225239754\n",
      "epoch:  277, loss: 0.008007406257092953\n",
      "epoch:  278, loss: 0.008006978780031204\n",
      "epoch:  279, loss: 0.00800696574151516\n",
      "epoch:  280, loss: 0.00800696574151516\n",
      "epoch:  281, loss: 0.008006950840353966\n",
      "epoch:  282, loss: 0.008006942458450794\n",
      "epoch:  283, loss: 0.008006250485777855\n",
      "epoch:  284, loss: 0.008006000891327858\n",
      "epoch:  285, loss: 0.008006000891327858\n",
      "epoch:  286, loss: 0.008006000891327858\n",
      "epoch:  287, loss: 0.008006000891327858\n",
      "epoch:  288, loss: 0.008006000891327858\n",
      "epoch:  289, loss: 0.008006000891327858\n",
      "epoch:  290, loss: 0.008005584590137005\n",
      "epoch:  291, loss: 0.008005292154848576\n",
      "epoch:  292, loss: 0.008005292154848576\n",
      "epoch:  293, loss: 0.00800518412142992\n",
      "epoch:  294, loss: 0.00800455641001463\n",
      "epoch:  295, loss: 0.00800455641001463\n",
      "epoch:  296, loss: 0.00800455641001463\n",
      "epoch:  297, loss: 0.00800455641001463\n",
      "epoch:  298, loss: 0.007999297231435776\n",
      "epoch:  299, loss: 0.007992473430931568\n",
      "epoch:  300, loss: 0.007991609163582325\n",
      "epoch:  301, loss: 0.007991477847099304\n",
      "epoch:  302, loss: 0.00799147691577673\n",
      "epoch:  303, loss: 0.00799147691577673\n",
      "epoch:  304, loss: 0.007991173304617405\n",
      "epoch:  305, loss: 0.007991130463778973\n",
      "epoch:  306, loss: 0.00799097865819931\n",
      "epoch:  307, loss: 0.00799097865819931\n",
      "epoch:  308, loss: 0.007990331389009953\n",
      "epoch:  309, loss: 0.007989974692463875\n",
      "epoch:  310, loss: 0.007989700883626938\n",
      "epoch:  311, loss: 0.007989518344402313\n",
      "epoch:  312, loss: 0.007988933473825455\n",
      "epoch:  313, loss: 0.007988933473825455\n",
      "epoch:  314, loss: 0.007988933473825455\n",
      "epoch:  315, loss: 0.007988933473825455\n",
      "epoch:  316, loss: 0.007988421246409416\n",
      "epoch:  317, loss: 0.007987750694155693\n",
      "epoch:  318, loss: 0.007987123914062977\n",
      "epoch:  319, loss: 0.007987123914062977\n",
      "epoch:  320, loss: 0.007987123914062977\n",
      "epoch:  321, loss: 0.007987123914062977\n",
      "epoch:  322, loss: 0.007986845448613167\n",
      "epoch:  323, loss: 0.007986572571098804\n",
      "epoch:  324, loss: 0.007986572571098804\n",
      "epoch:  325, loss: 0.007986520417034626\n",
      "epoch:  326, loss: 0.007985938340425491\n",
      "epoch:  327, loss: 0.007985110394656658\n",
      "epoch:  328, loss: 0.007984772324562073\n",
      "epoch:  329, loss: 0.007982856594026089\n",
      "epoch:  330, loss: 0.007980858907103539\n",
      "epoch:  331, loss: 0.007980255410075188\n",
      "epoch:  332, loss: 0.00797982420772314\n",
      "epoch:  333, loss: 0.007979190908372402\n",
      "epoch:  334, loss: 0.007978551089763641\n",
      "epoch:  335, loss: 0.007977936416864395\n",
      "epoch:  336, loss: 0.007977936416864395\n",
      "epoch:  337, loss: 0.007977936416864395\n",
      "epoch:  338, loss: 0.007977936416864395\n",
      "epoch:  339, loss: 0.007977936416864395\n",
      "epoch:  340, loss: 0.007977936416864395\n",
      "epoch:  341, loss: 0.007977738045156002\n",
      "epoch:  342, loss: 0.007976687513291836\n",
      "epoch:  343, loss: 0.007976687513291836\n",
      "epoch:  344, loss: 0.007976687513291836\n",
      "epoch:  345, loss: 0.007976687513291836\n",
      "epoch:  346, loss: 0.00797668844461441\n",
      "epoch:  347, loss: 0.007974945940077305\n",
      "epoch:  348, loss: 0.007974804379045963\n",
      "epoch:  349, loss: 0.007974804379045963\n",
      "epoch:  350, loss: 0.007972690276801586\n",
      "epoch:  351, loss: 0.007961775176227093\n",
      "epoch:  352, loss: 0.007961775176227093\n",
      "epoch:  353, loss: 0.007961775176227093\n",
      "epoch:  354, loss: 0.007961775176227093\n",
      "epoch:  355, loss: 0.007961775176227093\n",
      "epoch:  356, loss: 0.007961775176227093\n",
      "epoch:  357, loss: 0.007961775176227093\n",
      "epoch:  358, loss: 0.007961775176227093\n",
      "epoch:  359, loss: 0.007960774935781956\n",
      "epoch:  360, loss: 0.007959895767271519\n",
      "epoch:  361, loss: 0.007958907634019852\n",
      "epoch:  362, loss: 0.007958907634019852\n",
      "epoch:  363, loss: 0.007958769798278809\n",
      "epoch:  364, loss: 0.007957647554576397\n",
      "epoch:  365, loss: 0.007956677116453648\n",
      "epoch:  366, loss: 0.007956492714583874\n",
      "epoch:  367, loss: 0.007956492714583874\n",
      "epoch:  368, loss: 0.007954797707498074\n",
      "epoch:  369, loss: 0.007952947169542313\n",
      "epoch:  370, loss: 0.007952389307320118\n",
      "epoch:  371, loss: 0.007951664738357067\n",
      "epoch:  372, loss: 0.007951664738357067\n",
      "epoch:  373, loss: 0.007951663807034492\n",
      "epoch:  374, loss: 0.007951290346682072\n",
      "epoch:  375, loss: 0.007951153442263603\n",
      "epoch:  376, loss: 0.007951153442263603\n",
      "epoch:  377, loss: 0.007951153442263603\n",
      "epoch:  378, loss: 0.007951153442263603\n",
      "epoch:  379, loss: 0.007951153442263603\n",
      "epoch:  380, loss: 0.007951153442263603\n",
      "epoch:  381, loss: 0.007950528524816036\n",
      "epoch:  382, loss: 0.007948583923280239\n",
      "epoch:  383, loss: 0.007948440499603748\n",
      "epoch:  384, loss: 0.007948440499603748\n",
      "epoch:  385, loss: 0.007948345504701138\n",
      "epoch:  386, loss: 0.007945002056658268\n",
      "epoch:  387, loss: 0.00793940294533968\n",
      "epoch:  388, loss: 0.007939072325825691\n",
      "epoch:  389, loss: 0.007938789203763008\n",
      "epoch:  390, loss: 0.00793810747563839\n",
      "epoch:  391, loss: 0.007932513952255249\n",
      "epoch:  392, loss: 0.007932513952255249\n",
      "epoch:  393, loss: 0.007932513952255249\n",
      "epoch:  394, loss: 0.00793249811977148\n",
      "epoch:  395, loss: 0.007932410575449467\n",
      "epoch:  396, loss: 0.007932302542030811\n",
      "epoch:  397, loss: 0.007932302542030811\n",
      "epoch:  398, loss: 0.007932302542030811\n",
      "epoch:  399, loss: 0.007931748405098915\n",
      "epoch:  400, loss: 0.007930831052362919\n",
      "epoch:  401, loss: 0.00793050043284893\n",
      "epoch:  402, loss: 0.007930370047688484\n",
      "epoch:  403, loss: 0.007930370047688484\n",
      "epoch:  404, loss: 0.007930370047688484\n",
      "epoch:  405, loss: 0.007930370047688484\n",
      "epoch:  406, loss: 0.007930214516818523\n",
      "epoch:  407, loss: 0.007930065505206585\n",
      "epoch:  408, loss: 0.007930065505206585\n",
      "epoch:  409, loss: 0.007930065505206585\n",
      "epoch:  410, loss: 0.007930065505206585\n",
      "epoch:  411, loss: 0.007930065505206585\n",
      "epoch:  412, loss: 0.00792867410928011\n",
      "epoch:  413, loss: 0.00792867410928011\n",
      "epoch:  414, loss: 0.00792867410928011\n",
      "epoch:  415, loss: 0.007928279228508472\n",
      "epoch:  416, loss: 0.00792801845818758\n",
      "epoch:  417, loss: 0.00792801845818758\n",
      "epoch:  418, loss: 0.00792801845818758\n",
      "epoch:  419, loss: 0.00792801845818758\n",
      "epoch:  420, loss: 0.00792801845818758\n",
      "epoch:  421, loss: 0.00792766734957695\n",
      "epoch:  422, loss: 0.007927325554192066\n",
      "epoch:  423, loss: 0.007927176542580128\n",
      "epoch:  424, loss: 0.00792708620429039\n",
      "epoch:  425, loss: 0.00792708620429039\n",
      "epoch:  426, loss: 0.00792708434164524\n",
      "epoch:  427, loss: 0.007926493883132935\n",
      "epoch:  428, loss: 0.007926493883132935\n",
      "epoch:  429, loss: 0.007926493883132935\n",
      "epoch:  430, loss: 0.007926438003778458\n",
      "epoch:  431, loss: 0.007926332764327526\n",
      "epoch:  432, loss: 0.007926332764327526\n",
      "epoch:  433, loss: 0.007926332764327526\n",
      "epoch:  434, loss: 0.007924696430563927\n",
      "epoch:  435, loss: 0.007924425415694714\n",
      "epoch:  436, loss: 0.007924223318696022\n",
      "epoch:  437, loss: 0.0079241544008255\n",
      "epoch:  438, loss: 0.0079241544008255\n",
      "epoch:  439, loss: 0.0079241544008255\n",
      "epoch:  440, loss: 0.0079241544008255\n",
      "epoch:  441, loss: 0.0079241544008255\n",
      "epoch:  442, loss: 0.007923782803118229\n",
      "epoch:  443, loss: 0.007923782803118229\n",
      "epoch:  444, loss: 0.007923782803118229\n",
      "epoch:  445, loss: 0.007923782803118229\n",
      "epoch:  446, loss: 0.007923782803118229\n",
      "epoch:  447, loss: 0.007923363707959652\n",
      "epoch:  448, loss: 0.00792319979518652\n",
      "epoch:  449, loss: 0.00792319979518652\n",
      "epoch:  450, loss: 0.00792319979518652\n",
      "epoch:  451, loss: 0.00792319979518652\n",
      "epoch:  452, loss: 0.00792319979518652\n",
      "epoch:  453, loss: 0.00792319979518652\n",
      "epoch:  454, loss: 0.00792319979518652\n",
      "epoch:  455, loss: 0.00792311504483223\n",
      "epoch:  456, loss: 0.00792311504483223\n",
      "epoch:  457, loss: 0.00792311504483223\n",
      "epoch:  458, loss: 0.00792311504483223\n",
      "epoch:  459, loss: 0.00792311504483223\n",
      "epoch:  460, loss: 0.007923093624413013\n",
      "epoch:  461, loss: 0.007922344841063023\n",
      "epoch:  462, loss: 0.00792168639600277\n",
      "epoch:  463, loss: 0.007920750416815281\n",
      "epoch:  464, loss: 0.00792020559310913\n",
      "epoch:  465, loss: 0.00792020559310913\n",
      "epoch:  466, loss: 0.00792020559310913\n",
      "epoch:  467, loss: 0.00792020559310913\n",
      "epoch:  468, loss: 0.00792020559310913\n",
      "epoch:  469, loss: 0.00792020559310913\n",
      "epoch:  470, loss: 0.00792020559310913\n",
      "epoch:  471, loss: 0.007920071482658386\n",
      "epoch:  472, loss: 0.007915440946817398\n",
      "epoch:  473, loss: 0.007915440946817398\n",
      "epoch:  474, loss: 0.007915440946817398\n",
      "epoch:  475, loss: 0.007914095185697079\n",
      "epoch:  476, loss: 0.007913978770375252\n",
      "epoch:  477, loss: 0.007913978770375252\n",
      "epoch:  478, loss: 0.007913978770375252\n",
      "epoch:  479, loss: 0.007913978770375252\n",
      "epoch:  480, loss: 0.007913362234830856\n",
      "epoch:  481, loss: 0.007913249544799328\n",
      "epoch:  482, loss: 0.007913249544799328\n",
      "epoch:  483, loss: 0.00791321974247694\n",
      "epoch:  484, loss: 0.00791312288492918\n",
      "epoch:  485, loss: 0.00791312288492918\n",
      "epoch:  486, loss: 0.00791312288492918\n",
      "epoch:  487, loss: 0.007910488173365593\n",
      "epoch:  488, loss: 0.007910247892141342\n",
      "epoch:  489, loss: 0.007910025306046009\n",
      "epoch:  490, loss: 0.007910025306046009\n",
      "epoch:  491, loss: 0.007910025306046009\n",
      "epoch:  492, loss: 0.007909925654530525\n",
      "epoch:  493, loss: 0.007909742183983326\n",
      "epoch:  494, loss: 0.007909425534307957\n",
      "epoch:  495, loss: 0.007909076288342476\n",
      "epoch:  496, loss: 0.007908822037279606\n",
      "epoch:  497, loss: 0.007905268110334873\n",
      "epoch:  498, loss: 0.007904067635536194\n",
      "epoch:  499, loss: 0.007903543300926685\n",
      "epoch:  500, loss: 0.007903543300926685\n",
      "epoch:  501, loss: 0.007903543300926685\n",
      "epoch:  502, loss: 0.007901808246970177\n",
      "epoch:  503, loss: 0.007900124415755272\n",
      "epoch:  504, loss: 0.007899733260273933\n",
      "epoch:  505, loss: 0.007899733260273933\n",
      "epoch:  506, loss: 0.007896780967712402\n",
      "epoch:  507, loss: 0.007896436378359795\n",
      "epoch:  508, loss: 0.007896436378359795\n",
      "epoch:  509, loss: 0.007896436378359795\n",
      "epoch:  510, loss: 0.007896436378359795\n",
      "epoch:  511, loss: 0.007896436378359795\n",
      "epoch:  512, loss: 0.00789638888090849\n",
      "epoch:  513, loss: 0.007896147668361664\n",
      "epoch:  514, loss: 0.007896095514297485\n",
      "epoch:  515, loss: 0.007896095514297485\n",
      "epoch:  516, loss: 0.007896095514297485\n",
      "epoch:  517, loss: 0.007896095514297485\n",
      "epoch:  518, loss: 0.007896095514297485\n",
      "epoch:  519, loss: 0.007896095514297485\n",
      "epoch:  520, loss: 0.007896095514297485\n",
      "epoch:  521, loss: 0.007896095514297485\n",
      "epoch:  522, loss: 0.007895510643720627\n",
      "epoch:  523, loss: 0.007893431931734085\n",
      "epoch:  524, loss: 0.007893210276961327\n",
      "epoch:  525, loss: 0.007893090136349201\n",
      "epoch:  526, loss: 0.007893090136349201\n",
      "epoch:  527, loss: 0.007890795357525349\n",
      "epoch:  528, loss: 0.007889390923082829\n",
      "epoch:  529, loss: 0.007889390923082829\n",
      "epoch:  530, loss: 0.007889390923082829\n",
      "epoch:  531, loss: 0.007889390923082829\n",
      "epoch:  532, loss: 0.007889390923082829\n",
      "epoch:  533, loss: 0.007888548076152802\n",
      "epoch:  534, loss: 0.007888337597250938\n",
      "epoch:  535, loss: 0.007887270301580429\n",
      "epoch:  536, loss: 0.007887067273259163\n",
      "epoch:  537, loss: 0.007887067273259163\n",
      "epoch:  538, loss: 0.00788536574691534\n",
      "epoch:  539, loss: 0.00788505282253027\n",
      "epoch:  540, loss: 0.00788505282253027\n",
      "epoch:  541, loss: 0.00788505282253027\n",
      "epoch:  542, loss: 0.00788505282253027\n",
      "epoch:  543, loss: 0.007885053753852844\n",
      "epoch:  544, loss: 0.007885053753852844\n",
      "epoch:  545, loss: 0.007885053753852844\n",
      "epoch:  546, loss: 0.007885053753852844\n",
      "epoch:  547, loss: 0.007884558290243149\n",
      "epoch:  548, loss: 0.007884055376052856\n",
      "epoch:  549, loss: 0.007883888669312\n",
      "epoch:  550, loss: 0.007883888669312\n",
      "epoch:  551, loss: 0.007883888669312\n",
      "epoch:  552, loss: 0.007883888669312\n",
      "epoch:  553, loss: 0.007883888669312\n",
      "epoch:  554, loss: 0.00788369681686163\n",
      "epoch:  555, loss: 0.00788369681686163\n",
      "epoch:  556, loss: 0.00788369681686163\n",
      "epoch:  557, loss: 0.007883310317993164\n",
      "epoch:  558, loss: 0.00788328517228365\n",
      "epoch:  559, loss: 0.00788016989827156\n",
      "epoch:  560, loss: 0.00788016989827156\n",
      "epoch:  561, loss: 0.00788016989827156\n",
      "epoch:  562, loss: 0.00788016989827156\n",
      "epoch:  563, loss: 0.007879232987761497\n",
      "epoch:  564, loss: 0.007879110053181648\n",
      "epoch:  565, loss: 0.007879110053181648\n",
      "epoch:  566, loss: 0.007879110053181648\n",
      "epoch:  567, loss: 0.007879110053181648\n",
      "epoch:  568, loss: 0.007879110053181648\n",
      "epoch:  569, loss: 0.007879110053181648\n",
      "epoch:  570, loss: 0.007877537049353123\n",
      "epoch:  571, loss: 0.007877012714743614\n",
      "epoch:  572, loss: 0.00787672121077776\n",
      "epoch:  573, loss: 0.00787645298987627\n",
      "epoch:  574, loss: 0.00787645298987627\n",
      "epoch:  575, loss: 0.00787645298987627\n",
      "epoch:  576, loss: 0.007876140996813774\n",
      "epoch:  577, loss: 0.007876107469201088\n",
      "epoch:  578, loss: 0.007874920964241028\n",
      "epoch:  579, loss: 0.007874550297856331\n",
      "epoch:  580, loss: 0.007874540984630585\n",
      "epoch:  581, loss: 0.007874540984630585\n",
      "epoch:  582, loss: 0.00787454191595316\n",
      "epoch:  583, loss: 0.007874051108956337\n",
      "epoch:  584, loss: 0.00787399709224701\n",
      "epoch:  585, loss: 0.00787399709224701\n",
      "epoch:  586, loss: 0.007873931899666786\n",
      "epoch:  587, loss: 0.007873931899666786\n",
      "epoch:  588, loss: 0.007873754017055035\n",
      "epoch:  589, loss: 0.007873624563217163\n",
      "epoch:  590, loss: 0.007873624563217163\n",
      "epoch:  591, loss: 0.007873624563217163\n",
      "epoch:  592, loss: 0.007873624563217163\n",
      "epoch:  593, loss: 0.007873510010540485\n",
      "epoch:  594, loss: 0.007873192429542542\n",
      "epoch:  595, loss: 0.007873192429542542\n",
      "epoch:  596, loss: 0.007873192429542542\n",
      "epoch:  597, loss: 0.007873192429542542\n",
      "epoch:  598, loss: 0.007873192429542542\n",
      "epoch:  599, loss: 0.007873192429542542\n",
      "epoch:  600, loss: 0.007873022928833961\n",
      "epoch:  601, loss: 0.00787276215851307\n",
      "epoch:  602, loss: 0.007872498594224453\n",
      "epoch:  603, loss: 0.007872498594224453\n",
      "epoch:  604, loss: 0.007872394286096096\n",
      "epoch:  605, loss: 0.007871989160776138\n",
      "epoch:  606, loss: 0.007871989160776138\n",
      "epoch:  607, loss: 0.007871989160776138\n",
      "epoch:  608, loss: 0.007871989160776138\n",
      "epoch:  609, loss: 0.007871989160776138\n",
      "epoch:  610, loss: 0.007871989160776138\n",
      "epoch:  611, loss: 0.00787174329161644\n",
      "epoch:  612, loss: 0.007871672511100769\n",
      "epoch:  613, loss: 0.007871672511100769\n",
      "epoch:  614, loss: 0.007871672511100769\n",
      "epoch:  615, loss: 0.007866024039685726\n",
      "epoch:  616, loss: 0.007866024039685726\n",
      "epoch:  617, loss: 0.007866024039685726\n",
      "epoch:  618, loss: 0.007866024039685726\n",
      "epoch:  619, loss: 0.007866024039685726\n",
      "epoch:  620, loss: 0.007866024039685726\n",
      "epoch:  621, loss: 0.007866024039685726\n",
      "epoch:  622, loss: 0.00786538328975439\n",
      "epoch:  623, loss: 0.007864793762564659\n",
      "epoch:  624, loss: 0.007864793762564659\n",
      "epoch:  625, loss: 0.007864793762564659\n",
      "epoch:  626, loss: 0.007864793762564659\n",
      "epoch:  627, loss: 0.007864496670663357\n",
      "epoch:  628, loss: 0.00786433182656765\n",
      "epoch:  629, loss: 0.00786433182656765\n",
      "epoch:  630, loss: 0.00786433182656765\n",
      "epoch:  631, loss: 0.00786417257040739\n",
      "epoch:  632, loss: 0.00786417257040739\n",
      "epoch:  633, loss: 0.00786413811147213\n",
      "epoch:  634, loss: 0.00786394253373146\n",
      "epoch:  635, loss: 0.00786394253373146\n",
      "epoch:  636, loss: 0.007863614708185196\n",
      "epoch:  637, loss: 0.007863531820476055\n",
      "epoch:  638, loss: 0.007863531820476055\n",
      "epoch:  639, loss: 0.007863531820476055\n",
      "epoch:  640, loss: 0.007863531820476055\n",
      "epoch:  641, loss: 0.007863531820476055\n",
      "epoch:  642, loss: 0.007863531820476055\n",
      "epoch:  643, loss: 0.007863531820476055\n",
      "epoch:  644, loss: 0.007863531820476055\n",
      "epoch:  645, loss: 0.007863244041800499\n",
      "epoch:  646, loss: 0.00786313135176897\n",
      "epoch:  647, loss: 0.00786313135176897\n",
      "epoch:  648, loss: 0.00786313135176897\n",
      "epoch:  649, loss: 0.007862894795835018\n",
      "epoch:  650, loss: 0.007862778380513191\n",
      "epoch:  651, loss: 0.007862778380513191\n",
      "epoch:  652, loss: 0.007862778380513191\n",
      "epoch:  653, loss: 0.007862778380513191\n",
      "epoch:  654, loss: 0.007862778380513191\n",
      "epoch:  655, loss: 0.007862715981900692\n",
      "epoch:  656, loss: 0.007862705737352371\n",
      "epoch:  657, loss: 0.007862705737352371\n",
      "epoch:  658, loss: 0.007862529717385769\n",
      "epoch:  659, loss: 0.007862431928515434\n",
      "epoch:  660, loss: 0.007862431928515434\n",
      "epoch:  661, loss: 0.007862431928515434\n",
      "epoch:  662, loss: 0.007861952297389507\n",
      "epoch:  663, loss: 0.007861952297389507\n",
      "epoch:  664, loss: 0.007861861027777195\n",
      "epoch:  665, loss: 0.007861861027777195\n",
      "epoch:  666, loss: 0.007849050685763359\n",
      "epoch:  667, loss: 0.00784279778599739\n",
      "epoch:  668, loss: 0.007842659018933773\n",
      "epoch:  669, loss: 0.007842659018933773\n",
      "epoch:  670, loss: 0.007842659018933773\n",
      "epoch:  671, loss: 0.00784088671207428\n",
      "epoch:  672, loss: 0.00784088671207428\n",
      "epoch:  673, loss: 0.00784088671207428\n",
      "epoch:  674, loss: 0.00784088671207428\n",
      "epoch:  675, loss: 0.00784088671207428\n",
      "epoch:  676, loss: 0.00784088671207428\n",
      "epoch:  677, loss: 0.00784088671207428\n",
      "epoch:  678, loss: 0.00784088671207428\n",
      "epoch:  679, loss: 0.00784088671207428\n",
      "epoch:  680, loss: 0.00784088671207428\n",
      "epoch:  681, loss: 0.00784088671207428\n",
      "epoch:  682, loss: 0.007840407080948353\n",
      "epoch:  683, loss: 0.007840407080948353\n",
      "epoch:  684, loss: 0.00784010998904705\n",
      "epoch:  685, loss: 0.007839258760213852\n",
      "epoch:  686, loss: 0.007839076220989227\n",
      "epoch:  687, loss: 0.007838740944862366\n",
      "epoch:  688, loss: 0.007838397286832333\n",
      "epoch:  689, loss: 0.007838397286832333\n",
      "epoch:  690, loss: 0.007838397286832333\n",
      "epoch:  691, loss: 0.007838397286832333\n",
      "epoch:  692, loss: 0.007838397286832333\n",
      "epoch:  693, loss: 0.00783835630863905\n",
      "epoch:  694, loss: 0.00783835630863905\n",
      "epoch:  695, loss: 0.007835330441594124\n",
      "epoch:  696, loss: 0.00783127173781395\n",
      "epoch:  697, loss: 0.007829598151147366\n",
      "epoch:  698, loss: 0.00782653596252203\n",
      "epoch:  699, loss: 0.007822065614163876\n",
      "epoch:  700, loss: 0.007821955718100071\n",
      "epoch:  701, loss: 0.007821955718100071\n",
      "epoch:  702, loss: 0.007821802981197834\n",
      "epoch:  703, loss: 0.007821287028491497\n",
      "epoch:  704, loss: 0.007821286097168922\n",
      "epoch:  705, loss: 0.007821286097168922\n",
      "epoch:  706, loss: 0.007821288891136646\n",
      "epoch:  707, loss: 0.007821005769073963\n",
      "epoch:  708, loss: 0.007820981554687023\n",
      "epoch:  709, loss: 0.007820981554687023\n",
      "epoch:  710, loss: 0.007820974104106426\n",
      "epoch:  711, loss: 0.007816815748810768\n",
      "epoch:  712, loss: 0.007816815748810768\n",
      "epoch:  713, loss: 0.007816815748810768\n",
      "epoch:  714, loss: 0.007816480472683907\n",
      "epoch:  715, loss: 0.007816375233232975\n",
      "epoch:  716, loss: 0.007816375233232975\n",
      "epoch:  717, loss: 0.007816375233232975\n",
      "epoch:  718, loss: 0.007816375233232975\n",
      "epoch:  719, loss: 0.007816375233232975\n",
      "epoch:  720, loss: 0.00781618058681488\n",
      "epoch:  721, loss: 0.007815688848495483\n",
      "epoch:  722, loss: 0.007815688848495483\n",
      "epoch:  723, loss: 0.007814840413630009\n",
      "epoch:  724, loss: 0.007814344950020313\n",
      "epoch:  725, loss: 0.00781424529850483\n",
      "epoch:  726, loss: 0.00781424529850483\n",
      "epoch:  727, loss: 0.00781424529850483\n",
      "epoch:  728, loss: 0.00781424529850483\n",
      "epoch:  729, loss: 0.00781424529850483\n",
      "epoch:  730, loss: 0.00781424529850483\n",
      "epoch:  731, loss: 0.00781424529850483\n",
      "epoch:  732, loss: 0.007813975214958191\n",
      "epoch:  733, loss: 0.00781374704092741\n",
      "epoch:  734, loss: 0.007813142612576485\n",
      "epoch:  735, loss: 0.007813142612576485\n",
      "epoch:  736, loss: 0.007812186144292355\n",
      "epoch:  737, loss: 0.007811375893652439\n",
      "epoch:  738, loss: 0.007811093702912331\n",
      "epoch:  739, loss: 0.007811093702912331\n",
      "epoch:  740, loss: 0.007811093702912331\n",
      "epoch:  741, loss: 0.007810679730027914\n",
      "epoch:  742, loss: 0.007810567039996386\n",
      "epoch:  743, loss: 0.007810566108673811\n",
      "epoch:  744, loss: 0.007810566108673811\n",
      "epoch:  745, loss: 0.007810566108673811\n",
      "epoch:  746, loss: 0.007810606621205807\n",
      "epoch:  747, loss: 0.007810407783836126\n",
      "epoch:  748, loss: 0.007810407783836126\n",
      "epoch:  749, loss: 0.007810305338352919\n",
      "epoch:  750, loss: 0.007810262963175774\n",
      "epoch:  751, loss: 0.007810262963175774\n",
      "epoch:  752, loss: 0.007810262963175774\n",
      "epoch:  753, loss: 0.007810262963175774\n",
      "epoch:  754, loss: 0.007809535134583712\n",
      "epoch:  755, loss: 0.0078094713389873505\n",
      "epoch:  756, loss: 0.0078094713389873505\n",
      "epoch:  757, loss: 0.0078094713389873505\n",
      "epoch:  758, loss: 0.0078094713389873505\n",
      "epoch:  759, loss: 0.0078094713389873505\n",
      "epoch:  760, loss: 0.0077951764687895775\n",
      "epoch:  761, loss: 0.0077951764687895775\n",
      "epoch:  762, loss: 0.007793118711560965\n",
      "epoch:  763, loss: 0.007792168762534857\n",
      "epoch:  764, loss: 0.007791880052536726\n",
      "epoch:  765, loss: 0.007791672833263874\n",
      "epoch:  766, loss: 0.0077907792292535305\n",
      "epoch:  767, loss: 0.007790140341967344\n",
      "epoch:  768, loss: 0.007790131960064173\n",
      "epoch:  769, loss: 0.007790131960064173\n",
      "epoch:  770, loss: 0.00778989726677537\n",
      "epoch:  771, loss: 0.007789720315486193\n",
      "epoch:  772, loss: 0.007789720315486193\n",
      "epoch:  773, loss: 0.007789720315486193\n",
      "epoch:  774, loss: 0.00778572540730238\n",
      "epoch:  775, loss: 0.00778572540730238\n",
      "epoch:  776, loss: 0.00778572540730238\n",
      "epoch:  777, loss: 0.00778572540730238\n",
      "epoch:  778, loss: 0.00778572540730238\n",
      "epoch:  779, loss: 0.00778572540730238\n",
      "epoch:  780, loss: 0.00778572540730238\n",
      "epoch:  781, loss: 0.007784949149936438\n",
      "epoch:  782, loss: 0.00778468931093812\n",
      "epoch:  783, loss: 0.00778468931093812\n",
      "epoch:  784, loss: 0.00778468931093812\n",
      "epoch:  785, loss: 0.00778468931093812\n",
      "epoch:  786, loss: 0.00778468931093812\n",
      "epoch:  787, loss: 0.007784366607666016\n",
      "epoch:  788, loss: 0.007784231565892696\n",
      "epoch:  789, loss: 0.007784231565892696\n",
      "epoch:  790, loss: 0.007784231565892696\n",
      "epoch:  791, loss: 0.007745097856968641\n",
      "epoch:  792, loss: 0.007744705304503441\n",
      "epoch:  793, loss: 0.007743933238089085\n",
      "epoch:  794, loss: 0.007743933238089085\n",
      "epoch:  795, loss: 0.007743933238089085\n",
      "epoch:  796, loss: 0.007743933238089085\n",
      "epoch:  797, loss: 0.007743933238089085\n",
      "epoch:  798, loss: 0.007743902038782835\n",
      "epoch:  799, loss: 0.007743902038782835\n",
      "epoch:  800, loss: 0.007740638218820095\n",
      "epoch:  801, loss: 0.007740638218820095\n",
      "epoch:  802, loss: 0.007740638218820095\n",
      "epoch:  803, loss: 0.007740638684481382\n",
      "epoch:  804, loss: 0.007740589790046215\n",
      "epoch:  805, loss: 0.007740589790046215\n",
      "epoch:  806, loss: 0.007740589790046215\n",
      "epoch:  807, loss: 0.0077405027113854885\n",
      "epoch:  808, loss: 0.0077405027113854885\n",
      "epoch:  809, loss: 0.007740453351289034\n",
      "epoch:  810, loss: 0.007740173023194075\n",
      "epoch:  811, loss: 0.007740173023194075\n",
      "epoch:  812, loss: 0.007739771623164415\n",
      "epoch:  813, loss: 0.007739506661891937\n",
      "epoch:  814, loss: 0.00773950619623065\n",
      "epoch:  815, loss: 0.007739289663732052\n",
      "epoch:  816, loss: 0.007738592568784952\n",
      "epoch:  817, loss: 0.007738592568784952\n",
      "epoch:  818, loss: 0.007738249842077494\n",
      "epoch:  819, loss: 0.007738249842077494\n",
      "epoch:  820, loss: 0.007738230284303427\n",
      "epoch:  821, loss: 0.007738050073385239\n",
      "epoch:  822, loss: 0.007738050073385239\n",
      "epoch:  823, loss: 0.007738062180578709\n",
      "epoch:  824, loss: 0.007735913153737783\n",
      "epoch:  825, loss: 0.007735732011497021\n",
      "epoch:  826, loss: 0.007735731545835733\n",
      "epoch:  827, loss: 0.007735731545835733\n",
      "epoch:  828, loss: 0.007727560121566057\n",
      "epoch:  829, loss: 0.007727374788373709\n",
      "epoch:  830, loss: 0.007727050222456455\n",
      "epoch:  831, loss: 0.007726805750280619\n",
      "epoch:  832, loss: 0.007726805750280619\n",
      "epoch:  833, loss: 0.007721870206296444\n",
      "epoch:  834, loss: 0.007713032886385918\n",
      "epoch:  835, loss: 0.007712610065937042\n",
      "epoch:  836, loss: 0.007712594233453274\n",
      "epoch:  837, loss: 0.007701716851443052\n",
      "epoch:  838, loss: 0.007701371796429157\n",
      "epoch:  839, loss: 0.00770119484513998\n",
      "epoch:  840, loss: 0.00770119484513998\n",
      "epoch:  841, loss: 0.007700739894062281\n",
      "epoch:  842, loss: 0.007700739894062281\n",
      "epoch:  843, loss: 0.007700739894062281\n",
      "epoch:  844, loss: 0.007700739894062281\n",
      "epoch:  845, loss: 0.007700739894062281\n",
      "epoch:  846, loss: 0.007700103335082531\n",
      "epoch:  847, loss: 0.007699728012084961\n",
      "epoch:  848, loss: 0.007699407171458006\n",
      "epoch:  849, loss: 0.007699407171458006\n",
      "epoch:  850, loss: 0.007699407171458006\n",
      "epoch:  851, loss: 0.007698938250541687\n",
      "epoch:  852, loss: 0.007698638364672661\n",
      "epoch:  853, loss: 0.007698548957705498\n",
      "epoch:  854, loss: 0.007698548957705498\n",
      "epoch:  855, loss: 0.007698548957705498\n",
      "epoch:  856, loss: 0.007697412744164467\n",
      "epoch:  857, loss: 0.007697412744164467\n",
      "epoch:  858, loss: 0.007697412744164467\n",
      "epoch:  859, loss: 0.007697412744164467\n",
      "epoch:  860, loss: 0.007697412744164467\n",
      "epoch:  861, loss: 0.007697412744164467\n",
      "epoch:  862, loss: 0.007697412744164467\n",
      "epoch:  863, loss: 0.007697412744164467\n",
      "epoch:  864, loss: 0.007697412744164467\n",
      "epoch:  865, loss: 0.007697412744164467\n",
      "epoch:  866, loss: 0.007697412744164467\n",
      "epoch:  867, loss: 0.007696348708122969\n",
      "epoch:  868, loss: 0.007696256972849369\n",
      "epoch:  869, loss: 0.007696256972849369\n",
      "epoch:  870, loss: 0.007695373147726059\n",
      "epoch:  871, loss: 0.007695260923355818\n",
      "epoch:  872, loss: 0.007695260923355818\n",
      "epoch:  873, loss: 0.007695260923355818\n",
      "epoch:  874, loss: 0.007694466970860958\n",
      "epoch:  875, loss: 0.007694378960877657\n",
      "epoch:  876, loss: 0.007694378960877657\n",
      "epoch:  877, loss: 0.00769437849521637\n",
      "epoch:  878, loss: 0.00769437849521637\n",
      "epoch:  879, loss: 0.007688258308917284\n",
      "epoch:  880, loss: 0.007685746997594833\n",
      "epoch:  881, loss: 0.0076854657381772995\n",
      "epoch:  882, loss: 0.0076854657381772995\n",
      "epoch:  883, loss: 0.0076854657381772995\n",
      "epoch:  884, loss: 0.0076854657381772995\n",
      "epoch:  885, loss: 0.0076854657381772995\n",
      "epoch:  886, loss: 0.007684665732085705\n",
      "epoch:  887, loss: 0.0076845670118927956\n",
      "epoch:  888, loss: 0.0076845670118927956\n",
      "epoch:  889, loss: 0.0076845670118927956\n",
      "epoch:  890, loss: 0.0076845670118927956\n",
      "epoch:  891, loss: 0.00768455071374774\n",
      "epoch:  892, loss: 0.007683908566832542\n",
      "epoch:  893, loss: 0.007683908566832542\n",
      "epoch:  894, loss: 0.007683908566832542\n",
      "epoch:  895, loss: 0.007683908566832542\n",
      "epoch:  896, loss: 0.007683908566832542\n",
      "epoch:  897, loss: 0.007683908566832542\n",
      "epoch:  898, loss: 0.0076832580380141735\n",
      "epoch:  899, loss: 0.0076829916797578335\n",
      "epoch:  900, loss: 0.0076829916797578335\n",
      "epoch:  901, loss: 0.0076829916797578335\n",
      "epoch:  902, loss: 0.0076829916797578335\n",
      "epoch:  903, loss: 0.0076829916797578335\n",
      "epoch:  904, loss: 0.007682663854211569\n",
      "epoch:  905, loss: 0.007682663854211569\n",
      "epoch:  906, loss: 0.007682607043534517\n",
      "epoch:  907, loss: 0.007682160008698702\n",
      "epoch:  908, loss: 0.007681748829782009\n",
      "epoch:  909, loss: 0.007681217044591904\n",
      "epoch:  910, loss: 0.007681120652705431\n",
      "epoch:  911, loss: 0.007681120652705431\n",
      "epoch:  912, loss: 0.007681120652705431\n",
      "epoch:  913, loss: 0.007681120652705431\n",
      "epoch:  914, loss: 0.007681120652705431\n",
      "epoch:  915, loss: 0.007681120652705431\n",
      "epoch:  916, loss: 0.007681120652705431\n",
      "epoch:  917, loss: 0.007680614944547415\n",
      "epoch:  918, loss: 0.0076803481206297874\n",
      "epoch:  919, loss: 0.0076803481206297874\n",
      "epoch:  920, loss: 0.007652068976312876\n",
      "epoch:  921, loss: 0.007649370934814215\n",
      "epoch:  922, loss: 0.007649370469152927\n",
      "epoch:  923, loss: 0.007649370934814215\n",
      "epoch:  924, loss: 0.007649370934814215\n",
      "epoch:  925, loss: 0.007660118397325277\n",
      "epoch:  926, loss: 0.007646374404430389\n",
      "epoch:  927, loss: 0.007645861711353064\n",
      "epoch:  928, loss: 0.007645861711353064\n",
      "epoch:  929, loss: 0.007645861711353064\n",
      "epoch:  930, loss: 0.007645861711353064\n",
      "epoch:  931, loss: 0.007644783239811659\n",
      "epoch:  932, loss: 0.007644440978765488\n",
      "epoch:  933, loss: 0.007644369266927242\n",
      "epoch:  934, loss: 0.007644369266927242\n",
      "epoch:  935, loss: 0.007643049582839012\n",
      "epoch:  936, loss: 0.007642832119017839\n",
      "epoch:  937, loss: 0.007642832119017839\n",
      "epoch:  938, loss: 0.007632470689713955\n",
      "epoch:  939, loss: 0.007632161024957895\n",
      "epoch:  940, loss: 0.007632160559296608\n",
      "epoch:  941, loss: 0.007632155437022448\n",
      "epoch:  942, loss: 0.007630395703017712\n",
      "epoch:  943, loss: 0.007630194071680307\n",
      "epoch:  944, loss: 0.007630021311342716\n",
      "epoch:  945, loss: 0.007630020845681429\n",
      "epoch:  946, loss: 0.007630020845681429\n",
      "epoch:  947, loss: 0.007630020845681429\n",
      "epoch:  948, loss: 0.007630020845681429\n",
      "epoch:  949, loss: 0.007630020845681429\n",
      "epoch:  950, loss: 0.007630020845681429\n",
      "epoch:  951, loss: 0.0076287891715765\n",
      "epoch:  952, loss: 0.00762870954349637\n",
      "epoch:  953, loss: 0.00762870954349637\n",
      "epoch:  954, loss: 0.00762870954349637\n",
      "epoch:  955, loss: 0.00762870954349637\n",
      "epoch:  956, loss: 0.00762870954349637\n",
      "epoch:  957, loss: 0.007628709077835083\n",
      "epoch:  958, loss: 0.007628709077835083\n",
      "epoch:  959, loss: 0.007628042716532946\n",
      "epoch:  960, loss: 0.007627374492585659\n",
      "epoch:  961, loss: 0.007627374492585659\n",
      "epoch:  962, loss: 0.007627374492585659\n",
      "epoch:  963, loss: 0.007627374492585659\n",
      "epoch:  964, loss: 0.0076273656450212\n",
      "epoch:  965, loss: 0.007627076469361782\n",
      "epoch:  966, loss: 0.007627076469361782\n",
      "epoch:  967, loss: 0.007627076935023069\n",
      "epoch:  968, loss: 0.007621172349900007\n",
      "epoch:  969, loss: 0.0076143620535731316\n",
      "epoch:  970, loss: 0.007611550390720367\n",
      "epoch:  971, loss: 0.0076113371178507805\n",
      "epoch:  972, loss: 0.007611336652189493\n",
      "epoch:  973, loss: 0.007611336652189493\n",
      "epoch:  974, loss: 0.007608925458043814\n",
      "epoch:  975, loss: 0.007608442101627588\n",
      "epoch:  976, loss: 0.0076073757372796535\n",
      "epoch:  977, loss: 0.007606583647429943\n",
      "epoch:  978, loss: 0.007605437655001879\n",
      "epoch:  979, loss: 0.007605437655001879\n",
      "epoch:  980, loss: 0.007605394348502159\n",
      "epoch:  981, loss: 0.007605043705552816\n",
      "epoch:  982, loss: 0.007605042774230242\n",
      "epoch:  983, loss: 0.007605042774230242\n",
      "epoch:  984, loss: 0.0075846207328140736\n",
      "epoch:  985, loss: 0.007583090104162693\n",
      "epoch:  986, loss: 0.007583090104162693\n",
      "epoch:  987, loss: 0.007583090104162693\n",
      "epoch:  988, loss: 0.0075818574987351894\n",
      "epoch:  989, loss: 0.007581522222608328\n",
      "epoch:  990, loss: 0.007581522222608328\n",
      "epoch:  991, loss: 0.007581522222608328\n",
      "epoch:  992, loss: 0.007581522222608328\n",
      "epoch:  993, loss: 0.0075795515440404415\n",
      "epoch:  994, loss: 0.007578573189675808\n",
      "epoch:  995, loss: 0.00757845863699913\n",
      "epoch:  996, loss: 0.00757845863699913\n",
      "epoch:  997, loss: 0.007578475400805473\n",
      "epoch:  998, loss: 0.007577301003038883\n",
      "epoch:  999, loss: 0.0075765871442854404\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size=X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# opt = torch_numopt.ConjugateGradient(model=model, lr=1e-4, line_search_method=\"const\", cg_method=\"FR\")\n",
    "opt = torch_numopt.ConjugateGradient(model=model, lr=10, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"armijo\", cg_method=\"DY\")\n",
    "# opt = torch_numopt.ConjugateGradient(model=model, lr=10, c1=0.1, c2=0.9, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"wolfe\", cg_method=\"FR\")\n",
    "# opt = torch_numopt.ConjugateGradient(model=model, lr=10, c1=0.1, c2=0.9, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"strong-wolfe\", cg_method=\"FR\")\n",
    "# opt = torch_numopt.ConjugateGradient(model=model, lr=10, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"goldstein\", cg_method=\"FR\")\n",
    "\n",
    "all_loss = {}\n",
    "for epoch in range(1000):\n",
    "    print(\"epoch: \", epoch, end=\"\")\n",
    "    all_loss[epoch + 1] = 0\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y, loss_fn)\n",
    "\n",
    "        all_loss[epoch + 1] += loss\n",
    "    all_loss[epoch + 1] /= len(data_loader)\n",
    "    print(\", loss: {}\".format(all_loss[epoch + 1].cpu().detach().numpy().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics: R2 = 0.7406362185206627\n",
      "Test metrics:  R2 = 0.7195633889184088\n"
     ]
    }
   ],
   "source": [
    "pred_train = model.forward(X_train).cpu().detach()\n",
    "pred_test = model.forward(X_test).cpu().detach()\n",
    "print(f\"Train metrics: R2 = {r2_score(pred_train, y_train.cpu())}\")\n",
    "print(f\"Test metrics:  R2 = {r2_score(pred_test, y_test.cpu())}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_numopt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
