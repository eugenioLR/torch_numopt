{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch_numopt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import *\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.f1 = nn.Linear(input_size, 10, device=device)\n",
    "        self.f2 = nn.Linear(10, 20, device=device)\n",
    "        self.f3 = nn.Linear(20, 20, device=device)\n",
    "        self.f4 = nn.Linear(20, 10, device=device)\n",
    "        self.f5 = nn.Linear(10, 1, device=device)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        # self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.f1(x))\n",
    "        x = self.activation(self.f2(x))\n",
    "        x = self.activation(self.f3(x))\n",
    "        x = self.activation(self.f4(x))\n",
    "        x = self.f5(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([800, 10]) torch.Size([800, 1])\n",
      "torch.Size([200, 10]) torch.Size([200, 1])\n"
     ]
    }
   ],
   "source": [
    "# X, y = load_diabetes(return_X_y=True, scaled=False)\n",
    "# X, y = make_regression(n_samples=1000, n_features=100)\n",
    "X, y = make_friedman1(n_samples=1000, noise=1e-2)\n",
    "\n",
    "X_scaler = MinMaxScaler()\n",
    "X = X_scaler.fit_transform(X)\n",
    "X = torch.Tensor(X).to(device)\n",
    "\n",
    "y_scaler = MinMaxScaler()\n",
    "y = y_scaler.fit_transform(y.reshape((-1, 1)))\n",
    "y = torch.Tensor(y).to(device)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "torch_data = TensorDataset(X_train, y_train)\n",
    "data_loader = DataLoader(torch_data, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.1955823302268982\n",
      "epoch:  1, loss: 0.12823089957237244\n",
      "epoch:  2, loss: 0.08811270445585251\n",
      "epoch:  3, loss: 0.06461028009653091\n",
      "epoch:  4, loss: 0.05101602524518967\n",
      "epoch:  5, loss: 0.04324067011475563\n",
      "epoch:  6, loss: 0.03883751481771469\n",
      "epoch:  7, loss: 0.03636486828327179\n",
      "epoch:  8, loss: 0.034985460340976715\n",
      "epoch:  9, loss: 0.03421953693032265\n",
      "epoch:  10, loss: 0.03379528969526291\n",
      "epoch:  11, loss: 0.03356044739484787\n",
      "epoch:  12, loss: 0.03343008831143379\n",
      "epoch:  13, loss: 0.03335714712738991\n",
      "epoch:  14, loss: 0.03331570327281952\n",
      "epoch:  15, loss: 0.03329147771000862\n",
      "epoch:  16, loss: 0.033276621252298355\n",
      "epoch:  17, loss: 0.033261869102716446\n",
      "epoch:  18, loss: 0.03324167802929878\n",
      "epoch:  19, loss: 0.03323246166110039\n",
      "epoch:  20, loss: 0.03320414572954178\n",
      "epoch:  21, loss: 0.03319245204329491\n",
      "epoch:  22, loss: 0.03315110132098198\n",
      "epoch:  23, loss: 0.0331377238035202\n",
      "epoch:  24, loss: 0.03307965397834778\n",
      "epoch:  25, loss: 0.033044055104255676\n",
      "epoch:  26, loss: 0.033015791326761246\n",
      "epoch:  27, loss: 0.03297269344329834\n",
      "epoch:  28, loss: 0.03294719010591507\n",
      "epoch:  29, loss: 0.03293834254145622\n",
      "epoch:  30, loss: 0.03290322795510292\n",
      "epoch:  31, loss: 0.032882049679756165\n",
      "epoch:  32, loss: 0.032868385314941406\n",
      "epoch:  33, loss: 0.032839443534612656\n",
      "epoch:  34, loss: 0.0328218899667263\n",
      "epoch:  35, loss: 0.03280584514141083\n",
      "epoch:  36, loss: 0.032781682908535004\n",
      "epoch:  37, loss: 0.032780639827251434\n",
      "epoch:  38, loss: 0.03274601697921753\n",
      "epoch:  39, loss: 0.0327255018055439\n",
      "epoch:  40, loss: 0.032717086374759674\n",
      "epoch:  41, loss: 0.032687727361917496\n",
      "epoch:  42, loss: 0.03266998752951622\n",
      "epoch:  43, loss: 0.03265495225787163\n",
      "epoch:  44, loss: 0.03262953460216522\n",
      "epoch:  45, loss: 0.032629530876874924\n",
      "epoch:  46, loss: 0.032592128962278366\n",
      "epoch:  47, loss: 0.032570015639066696\n",
      "epoch:  48, loss: 0.0325608067214489\n",
      "epoch:  49, loss: 0.032528482377529144\n",
      "epoch:  50, loss: 0.032509077340364456\n",
      "epoch:  51, loss: 0.03249169513583183\n",
      "epoch:  52, loss: 0.03246355429291725\n",
      "epoch:  53, loss: 0.03244620934128761\n",
      "epoch:  54, loss: 0.03242151811718941\n",
      "epoch:  55, loss: 0.032396309077739716\n",
      "epoch:  56, loss: 0.03238643333315849\n",
      "epoch:  57, loss: 0.03234855830669403\n",
      "epoch:  58, loss: 0.032325953245162964\n",
      "epoch:  59, loss: 0.03230636939406395\n",
      "epoch:  60, loss: 0.03227248787879944\n",
      "epoch:  61, loss: 0.03225187212228775\n",
      "epoch:  62, loss: 0.032223042100667953\n",
      "epoch:  63, loss: 0.032192789018154144\n",
      "epoch:  64, loss: 0.03218381106853485\n",
      "epoch:  65, loss: 0.03213682770729065\n",
      "epoch:  66, loss: 0.032109081745147705\n",
      "epoch:  67, loss: 0.032089103013277054\n",
      "epoch:  68, loss: 0.03204638510942459\n",
      "epoch:  69, loss: 0.03202061355113983\n",
      "epoch:  70, loss: 0.03199053183197975\n",
      "epoch:  71, loss: 0.031950779259204865\n",
      "epoch:  72, loss: 0.03194987028837204\n",
      "epoch:  73, loss: 0.03188604488968849\n",
      "epoch:  74, loss: 0.031849004328250885\n",
      "epoch:  75, loss: 0.03183400630950928\n",
      "epoch:  76, loss: 0.03177488222718239\n",
      "epoch:  77, loss: 0.03174017369747162\n",
      "epoch:  78, loss: 0.03171302750706673\n",
      "epoch:  79, loss: 0.031657204031944275\n",
      "epoch:  80, loss: 0.03162391483783722\n",
      "epoch:  81, loss: 0.03158523142337799\n",
      "epoch:  82, loss: 0.03153129294514656\n",
      "epoch:  83, loss: 0.03149884566664696\n",
      "epoch:  84, loss: 0.031446877866983414\n",
      "epoch:  85, loss: 0.0313953161239624\n",
      "epoch:  86, loss: 0.03138754889369011\n",
      "epoch:  87, loss: 0.031299274414777756\n",
      "epoch:  88, loss: 0.031248964369297028\n",
      "epoch:  89, loss: 0.031227029860019684\n",
      "epoch:  90, loss: 0.03114081174135208\n",
      "epoch:  91, loss: 0.031091228127479553\n",
      "epoch:  92, loss: 0.031055007129907608\n",
      "epoch:  93, loss: 0.030969448387622833\n",
      "epoch:  94, loss: 0.03091997653245926\n",
      "epoch:  95, loss: 0.030868498608469963\n",
      "epoch:  96, loss: 0.03078363835811615\n",
      "epoch:  97, loss: 0.03073400817811489\n",
      "epoch:  98, loss: 0.030669772997498512\n",
      "epoch:  99, loss: 0.03058263286948204\n",
      "epoch:  100, loss: 0.03053138218820095\n",
      "epoch:  101, loss: 0.0304538756608963\n",
      "epoch:  102, loss: 0.0303634125739336\n",
      "epoch:  103, loss: 0.030310049653053284\n",
      "epoch:  104, loss: 0.03021697700023651\n",
      "epoch:  105, loss: 0.030123163014650345\n",
      "epoch:  106, loss: 0.030067572370171547\n",
      "epoch:  107, loss: 0.02996039390563965\n",
      "epoch:  108, loss: 0.029860462993383408\n",
      "epoch:  109, loss: 0.029801450669765472\n",
      "epoch:  110, loss: 0.029680415987968445\n",
      "epoch:  111, loss: 0.029571523889899254\n",
      "epoch:  112, loss: 0.029508011415600777\n",
      "epoch:  113, loss: 0.029371250420808792\n",
      "epoch:  114, loss: 0.029253603890538216\n",
      "epoch:  115, loss: 0.029184818267822266\n",
      "epoch:  116, loss: 0.02903568185865879\n",
      "epoch:  117, loss: 0.02890307828783989\n",
      "epoch:  118, loss: 0.028827162459492683\n",
      "epoch:  119, loss: 0.028667209669947624\n",
      "epoch:  120, loss: 0.02851814776659012\n",
      "epoch:  121, loss: 0.028433198109269142\n",
      "epoch:  122, loss: 0.028266949579119682\n",
      "epoch:  123, loss: 0.028092054650187492\n",
      "epoch:  124, loss: 0.027996258810162544\n",
      "epoch:  125, loss: 0.027824420481920242\n",
      "epoch:  126, loss: 0.027621161192655563\n",
      "epoch:  127, loss: 0.027512289583683014\n",
      "epoch:  128, loss: 0.027342520654201508\n",
      "epoch:  129, loss: 0.02710084430873394\n",
      "epoch:  130, loss: 0.02697620540857315\n",
      "epoch:  131, loss: 0.026814401149749756\n",
      "epoch:  132, loss: 0.02652708813548088\n",
      "epoch:  133, loss: 0.02638433687388897\n",
      "epoch:  134, loss: 0.026248488575220108\n",
      "epoch:  135, loss: 0.02589545212686062\n",
      "epoch:  136, loss: 0.025729261338710785\n",
      "epoch:  137, loss: 0.02563718520104885\n",
      "epoch:  138, loss: 0.0252013448625803\n",
      "epoch:  139, loss: 0.025006499141454697\n",
      "epoch:  140, loss: 0.024988997727632523\n",
      "epoch:  141, loss: 0.02443954534828663\n",
      "epoch:  142, loss: 0.024212265387177467\n",
      "epoch:  143, loss: 0.024089151993393898\n",
      "epoch:  144, loss: 0.023624813184142113\n",
      "epoch:  145, loss: 0.02334524132311344\n",
      "epoch:  146, loss: 0.023204805329442024\n",
      "epoch:  147, loss: 0.022723663598299026\n",
      "epoch:  148, loss: 0.022398656234145164\n",
      "epoch:  149, loss: 0.02224605530500412\n",
      "epoch:  150, loss: 0.021758759394288063\n",
      "epoch:  151, loss: 0.02137986198067665\n",
      "epoch:  152, loss: 0.021215856075286865\n",
      "epoch:  153, loss: 0.020709328353405\n",
      "epoch:  154, loss: 0.02029358595609665\n",
      "epoch:  155, loss: 0.020124562084674835\n",
      "epoch:  156, loss: 0.019581077620387077\n",
      "epoch:  157, loss: 0.019150078296661377\n",
      "epoch:  158, loss: 0.018986012786626816\n",
      "epoch:  159, loss: 0.018383942544460297\n",
      "epoch:  160, loss: 0.017979424446821213\n",
      "epoch:  161, loss: 0.017831021919846535\n",
      "epoch:  162, loss: 0.017158394679427147\n",
      "epoch:  163, loss: 0.01681990548968315\n",
      "epoch:  164, loss: 0.016691109165549278\n",
      "epoch:  165, loss: 0.015944117680191994\n",
      "epoch:  166, loss: 0.015704700723290443\n",
      "epoch:  167, loss: 0.015483001247048378\n",
      "epoch:  168, loss: 0.014810135588049889\n",
      "epoch:  169, loss: 0.014673433266580105\n",
      "epoch:  170, loss: 0.014114460907876492\n",
      "epoch:  171, loss: 0.013826899230480194\n",
      "epoch:  172, loss: 0.01376651506870985\n",
      "epoch:  173, loss: 0.013078698888421059\n",
      "epoch:  174, loss: 0.012980879284441471\n",
      "epoch:  175, loss: 0.012454033829271793\n",
      "epoch:  176, loss: 0.01231538038700819\n",
      "epoch:  177, loss: 0.011948480270802975\n",
      "epoch:  178, loss: 0.01174505427479744\n",
      "epoch:  179, loss: 0.011537224054336548\n",
      "epoch:  180, loss: 0.01126004382967949\n",
      "epoch:  181, loss: 0.011138899251818657\n",
      "epoch:  182, loss: 0.010848909616470337\n",
      "epoch:  183, loss: 0.010710109025239944\n",
      "epoch:  184, loss: 0.010501504875719547\n",
      "epoch:  185, loss: 0.010298597626388073\n",
      "epoch:  186, loss: 0.010207153856754303\n",
      "epoch:  187, loss: 0.009987672790884972\n",
      "epoch:  188, loss: 0.009957972913980484\n",
      "epoch:  189, loss: 0.008757233619689941\n",
      "epoch:  190, loss: 0.008715194649994373\n",
      "epoch:  191, loss: 0.008691485971212387\n",
      "epoch:  192, loss: 0.008595602586865425\n",
      "epoch:  193, loss: 0.008584942668676376\n",
      "epoch:  194, loss: 0.00851449090987444\n",
      "epoch:  195, loss: 0.008503420278429985\n",
      "epoch:  196, loss: 0.008458870463073254\n",
      "epoch:  197, loss: 0.00844207126647234\n",
      "epoch:  198, loss: 0.008429463021457195\n",
      "epoch:  199, loss: 0.008393581956624985\n",
      "epoch:  200, loss: 0.008389024995267391\n",
      "epoch:  201, loss: 0.0083131929859519\n",
      "epoch:  202, loss: 0.008131290785968304\n",
      "epoch:  203, loss: 0.008125368505716324\n",
      "epoch:  204, loss: 0.008115273900330067\n",
      "epoch:  205, loss: 0.008104020729660988\n",
      "epoch:  206, loss: 0.008102103136479855\n",
      "epoch:  207, loss: 0.008087755180895329\n",
      "epoch:  208, loss: 0.008085447363555431\n",
      "epoch:  209, loss: 0.008075646124780178\n",
      "epoch:  210, loss: 0.008071632124483585\n",
      "epoch:  211, loss: 0.008070391602814198\n",
      "epoch:  212, loss: 0.008029342629015446\n",
      "epoch:  213, loss: 0.007987165823578835\n",
      "epoch:  214, loss: 0.007984764873981476\n",
      "epoch:  215, loss: 0.007983842864632607\n",
      "epoch:  216, loss: 0.007981570437550545\n",
      "epoch:  217, loss: 0.00793849490582943\n",
      "epoch:  218, loss: 0.00793621875345707\n",
      "epoch:  219, loss: 0.007932459935545921\n",
      "epoch:  220, loss: 0.007926678285002708\n",
      "epoch:  221, loss: 0.007925760000944138\n",
      "epoch:  222, loss: 0.007920574396848679\n",
      "epoch:  223, loss: 0.007918719202280045\n",
      "epoch:  224, loss: 0.00791813712567091\n",
      "epoch:  225, loss: 0.007913580164313316\n",
      "epoch:  226, loss: 0.007887028157711029\n",
      "epoch:  227, loss: 0.0078849196434021\n",
      "epoch:  228, loss: 0.007884284481406212\n",
      "epoch:  229, loss: 0.00787978246808052\n",
      "epoch:  230, loss: 0.007878954522311687\n",
      "epoch:  231, loss: 0.007878524251282215\n",
      "epoch:  232, loss: 0.007875395007431507\n",
      "epoch:  233, loss: 0.00787484273314476\n",
      "epoch:  234, loss: 0.007874545641243458\n",
      "epoch:  235, loss: 0.00787335354834795\n",
      "epoch:  236, loss: 0.007871831767261028\n",
      "epoch:  237, loss: 0.007871524430811405\n",
      "epoch:  238, loss: 0.007870858535170555\n",
      "epoch:  239, loss: 0.007869148626923561\n",
      "epoch:  240, loss: 0.007868850603699684\n",
      "epoch:  241, loss: 0.00786720123142004\n",
      "epoch:  242, loss: 0.007866546511650085\n",
      "epoch:  243, loss: 0.007866302505135536\n",
      "epoch:  244, loss: 0.00786519143730402\n",
      "epoch:  245, loss: 0.007864180952310562\n",
      "epoch:  246, loss: 0.007863957434892654\n",
      "epoch:  247, loss: 0.0078626349568367\n",
      "epoch:  248, loss: 0.007862057536840439\n",
      "epoch:  249, loss: 0.007861851714551449\n",
      "epoch:  250, loss: 0.007860424928367138\n",
      "epoch:  251, loss: 0.007860011421144009\n",
      "epoch:  252, loss: 0.00785984005779028\n",
      "epoch:  253, loss: 0.007854783907532692\n",
      "epoch:  254, loss: 0.007845362648367882\n",
      "epoch:  255, loss: 0.007844599895179272\n",
      "epoch:  256, loss: 0.007844393141567707\n",
      "epoch:  257, loss: 0.007843408733606339\n",
      "epoch:  258, loss: 0.007842792198061943\n",
      "epoch:  259, loss: 0.007842601276934147\n",
      "epoch:  260, loss: 0.007841586135327816\n",
      "epoch:  261, loss: 0.007841121405363083\n",
      "epoch:  262, loss: 0.007840948179364204\n",
      "epoch:  263, loss: 0.007840804755687714\n",
      "epoch:  264, loss: 0.007840102538466454\n",
      "epoch:  265, loss: 0.007839367724955082\n",
      "epoch:  266, loss: 0.007839184254407883\n",
      "epoch:  267, loss: 0.007838288322091103\n",
      "epoch:  268, loss: 0.007837703451514244\n",
      "epoch:  269, loss: 0.007837516255676746\n",
      "epoch:  270, loss: 0.007837360724806786\n",
      "epoch:  271, loss: 0.007836810313165188\n",
      "epoch:  272, loss: 0.007836132310330868\n",
      "epoch:  273, loss: 0.00783591065555811\n",
      "epoch:  274, loss: 0.007835786789655685\n",
      "epoch:  275, loss: 0.007834872230887413\n",
      "epoch:  276, loss: 0.00783468410372734\n",
      "epoch:  277, loss: 0.00783455464988947\n",
      "epoch:  278, loss: 0.00783455092459917\n",
      "epoch:  279, loss: 0.00783352367579937\n",
      "epoch:  280, loss: 0.007833374664187431\n",
      "epoch:  281, loss: 0.007833268493413925\n",
      "epoch:  282, loss: 0.007832367904484272\n",
      "epoch:  283, loss: 0.007832234725356102\n",
      "epoch:  284, loss: 0.00783212948590517\n",
      "epoch:  285, loss: 0.007832031697034836\n",
      "epoch:  286, loss: 0.007831930182874203\n",
      "epoch:  287, loss: 0.007831829600036144\n",
      "epoch:  288, loss: 0.007831737399101257\n",
      "epoch:  289, loss: 0.007831632159650326\n",
      "epoch:  290, loss: 0.00783153809607029\n",
      "epoch:  291, loss: 0.007831438444554806\n",
      "epoch:  292, loss: 0.00783135648816824\n",
      "epoch:  293, loss: 0.00783124566078186\n",
      "epoch:  294, loss: 0.007830753922462463\n",
      "epoch:  295, loss: 0.0078301802277565\n",
      "epoch:  296, loss: 0.007830009795725346\n",
      "epoch:  297, loss: 0.007829871959984303\n",
      "epoch:  298, loss: 0.007829737849533558\n",
      "epoch:  299, loss: 0.007826928049325943\n",
      "epoch:  300, loss: 0.00781946163624525\n",
      "epoch:  301, loss: 0.00781888235360384\n",
      "epoch:  302, loss: 0.007818701677024364\n",
      "epoch:  303, loss: 0.007818572223186493\n",
      "epoch:  304, loss: 0.007817592471837997\n",
      "epoch:  305, loss: 0.007817361503839493\n",
      "epoch:  306, loss: 0.007817079313099384\n",
      "epoch:  307, loss: 0.007816171273589134\n",
      "epoch:  308, loss: 0.007816022261977196\n",
      "epoch:  309, loss: 0.00781562365591526\n",
      "epoch:  310, loss: 0.007814940065145493\n",
      "epoch:  311, loss: 0.007814798504114151\n",
      "epoch:  312, loss: 0.007814131677150726\n",
      "epoch:  313, loss: 0.00781374890357256\n",
      "epoch:  314, loss: 0.007813631556928158\n",
      "epoch:  315, loss: 0.007812737487256527\n",
      "epoch:  316, loss: 0.007812504656612873\n",
      "epoch:  317, loss: 0.0078123873099684715\n",
      "epoch:  318, loss: 0.007811601739376783\n",
      "epoch:  319, loss: 0.007811225485056639\n",
      "epoch:  320, loss: 0.007811101619154215\n",
      "epoch:  321, loss: 0.007810153532773256\n",
      "epoch:  322, loss: 0.007809899281710386\n",
      "epoch:  323, loss: 0.007809777744114399\n",
      "epoch:  324, loss: 0.007808140944689512\n",
      "epoch:  325, loss: 0.007798667997121811\n",
      "epoch:  326, loss: 0.007797952275723219\n",
      "epoch:  327, loss: 0.007797756232321262\n",
      "epoch:  328, loss: 0.007796536199748516\n",
      "epoch:  329, loss: 0.0077960859052836895\n",
      "epoch:  330, loss: 0.0077959271147847176\n",
      "epoch:  331, loss: 0.007795544806867838\n",
      "epoch:  332, loss: 0.007794437929987907\n",
      "epoch:  333, loss: 0.007794235832989216\n",
      "epoch:  334, loss: 0.007794082164764404\n",
      "epoch:  335, loss: 0.007788968738168478\n",
      "epoch:  336, loss: 0.0077806799672544\n",
      "epoch:  337, loss: 0.00778003316372633\n",
      "epoch:  338, loss: 0.007779324427247047\n",
      "epoch:  339, loss: 0.007778183091431856\n",
      "epoch:  340, loss: 0.007777981925755739\n",
      "epoch:  341, loss: 0.007776841055601835\n",
      "epoch:  342, loss: 0.0077763451263308525\n",
      "epoch:  343, loss: 0.007776170969009399\n",
      "epoch:  344, loss: 0.007774824276566505\n",
      "epoch:  345, loss: 0.007774496451020241\n",
      "epoch:  346, loss: 0.0077734715305268764\n",
      "epoch:  347, loss: 0.00777259049937129\n",
      "epoch:  348, loss: 0.007772359997034073\n",
      "epoch:  349, loss: 0.0077708810567855835\n",
      "epoch:  350, loss: 0.007770348805934191\n",
      "epoch:  351, loss: 0.007770143914967775\n",
      "epoch:  352, loss: 0.007768944371491671\n",
      "epoch:  353, loss: 0.007768349722027779\n",
      "epoch:  354, loss: 0.00776815228164196\n",
      "epoch:  355, loss: 0.007766920141875744\n",
      "epoch:  356, loss: 0.00776651268824935\n",
      "epoch:  357, loss: 0.007766346912831068\n",
      "epoch:  358, loss: 0.0077650584280490875\n",
      "epoch:  359, loss: 0.007764780893921852\n",
      "epoch:  360, loss: 0.007764250040054321\n",
      "epoch:  361, loss: 0.007763190194964409\n",
      "epoch:  362, loss: 0.0077629899606108665\n",
      "epoch:  363, loss: 0.007762134075164795\n",
      "epoch:  364, loss: 0.007761445827782154\n",
      "epoch:  365, loss: 0.007761270273476839\n",
      "epoch:  366, loss: 0.007760176435112953\n",
      "epoch:  367, loss: 0.007759693078696728\n",
      "epoch:  368, loss: 0.007759519852697849\n",
      "epoch:  369, loss: 0.0077582369558513165\n",
      "epoch:  370, loss: 0.007757882587611675\n",
      "epoch:  371, loss: 0.0077577317133545876\n",
      "epoch:  372, loss: 0.007757601793855429\n",
      "epoch:  373, loss: 0.007745387498289347\n",
      "epoch:  374, loss: 0.007744442205876112\n",
      "epoch:  375, loss: 0.00774427130818367\n",
      "epoch:  376, loss: 0.007744145579636097\n",
      "epoch:  377, loss: 0.007743211928755045\n",
      "epoch:  378, loss: 0.007734110578894615\n",
      "epoch:  379, loss: 0.007733368780463934\n",
      "epoch:  380, loss: 0.0077332123182713985\n",
      "epoch:  381, loss: 0.007732789497822523\n",
      "epoch:  382, loss: 0.0077320788986980915\n",
      "epoch:  383, loss: 0.007731928955763578\n",
      "epoch:  384, loss: 0.007731815800070763\n",
      "epoch:  385, loss: 0.007730575744062662\n",
      "epoch:  386, loss: 0.007722771260887384\n",
      "epoch:  387, loss: 0.007722201757133007\n",
      "epoch:  388, loss: 0.007722059264779091\n",
      "epoch:  389, loss: 0.007721257396042347\n",
      "epoch:  390, loss: 0.007720894645899534\n",
      "epoch:  391, loss: 0.00772077264264226\n",
      "epoch:  392, loss: 0.007719917222857475\n",
      "epoch:  393, loss: 0.007719649467617273\n",
      "epoch:  394, loss: 0.007719531189650297\n",
      "epoch:  395, loss: 0.007718621287494898\n",
      "epoch:  396, loss: 0.007718417793512344\n",
      "epoch:  397, loss: 0.007718313485383987\n",
      "epoch:  398, loss: 0.007716940715909004\n",
      "epoch:  399, loss: 0.007709359750151634\n",
      "epoch:  400, loss: 0.00770871527493\n",
      "epoch:  401, loss: 0.007708563935011625\n",
      "epoch:  402, loss: 0.007707979530096054\n",
      "epoch:  403, loss: 0.007707389537245035\n",
      "epoch:  404, loss: 0.00770723819732666\n",
      "epoch:  405, loss: 0.0077067031525075436\n",
      "epoch:  406, loss: 0.0077061764895915985\n",
      "epoch:  407, loss: 0.007706029806286097\n",
      "epoch:  408, loss: 0.007705846335738897\n",
      "epoch:  409, loss: 0.007705004420131445\n",
      "epoch:  410, loss: 0.007704826071858406\n",
      "epoch:  411, loss: 0.0077047161757946014\n",
      "epoch:  412, loss: 0.007704604882746935\n",
      "epoch:  413, loss: 0.007704226300120354\n",
      "epoch:  414, loss: 0.007703580893576145\n",
      "epoch:  415, loss: 0.007703433744609356\n",
      "epoch:  416, loss: 0.007703067734837532\n",
      "epoch:  417, loss: 0.0077023874036967754\n",
      "epoch:  418, loss: 0.007702203467488289\n",
      "epoch:  419, loss: 0.007702096831053495\n",
      "epoch:  420, loss: 0.007701980881392956\n",
      "epoch:  421, loss: 0.007701024878770113\n",
      "epoch:  422, loss: 0.007700862362980843\n",
      "epoch:  423, loss: 0.007700737565755844\n",
      "epoch:  424, loss: 0.007699962239712477\n",
      "epoch:  425, loss: 0.0076996455900371075\n",
      "epoch:  426, loss: 0.007699483539909124\n",
      "epoch:  427, loss: 0.0076988451182842255\n",
      "epoch:  428, loss: 0.007698338478803635\n",
      "epoch:  429, loss: 0.007698158733546734\n",
      "epoch:  430, loss: 0.007698037661612034\n",
      "epoch:  431, loss: 0.007697808090597391\n",
      "epoch:  432, loss: 0.007696835789829493\n",
      "epoch:  433, loss: 0.007696645800024271\n",
      "epoch:  434, loss: 0.007696517743170261\n",
      "epoch:  435, loss: 0.007695557549595833\n",
      "epoch:  436, loss: 0.007695293985307217\n",
      "epoch:  437, loss: 0.007695142645388842\n",
      "epoch:  438, loss: 0.007694451604038477\n",
      "epoch:  439, loss: 0.007693999446928501\n",
      "epoch:  440, loss: 0.007693816442042589\n",
      "epoch:  441, loss: 0.007693672087043524\n",
      "epoch:  442, loss: 0.007692701183259487\n",
      "epoch:  443, loss: 0.007692465092986822\n",
      "epoch:  444, loss: 0.007692341692745686\n",
      "epoch:  445, loss: 0.007691474165767431\n",
      "epoch:  446, loss: 0.00769115099683404\n",
      "epoch:  447, loss: 0.007691008038818836\n",
      "epoch:  448, loss: 0.0076899747364223\n",
      "epoch:  449, loss: 0.007689676713198423\n",
      "epoch:  450, loss: 0.0076895058155059814\n",
      "epoch:  451, loss: 0.007688305340707302\n",
      "epoch:  452, loss: 0.0076878610998392105\n",
      "epoch:  453, loss: 0.007687671110033989\n",
      "epoch:  454, loss: 0.007687103934586048\n",
      "epoch:  455, loss: 0.00768604502081871\n",
      "epoch:  456, loss: 0.007685807999223471\n",
      "epoch:  457, loss: 0.007685569580644369\n",
      "epoch:  458, loss: 0.0076841916888952255\n",
      "epoch:  459, loss: 0.007683932315558195\n",
      "epoch:  460, loss: 0.007683758158236742\n",
      "epoch:  461, loss: 0.007682866416871548\n",
      "epoch:  462, loss: 0.007682234048843384\n",
      "epoch:  463, loss: 0.007682012859731913\n",
      "epoch:  464, loss: 0.007681549992412329\n",
      "epoch:  465, loss: 0.007680550217628479\n",
      "epoch:  466, loss: 0.007680302485823631\n",
      "epoch:  467, loss: 0.007679860573261976\n",
      "epoch:  468, loss: 0.007678759749978781\n",
      "epoch:  469, loss: 0.007678500842303038\n",
      "epoch:  470, loss: 0.00767833786085248\n",
      "epoch:  471, loss: 0.007677271496504545\n",
      "epoch:  472, loss: 0.007676863577216864\n",
      "epoch:  473, loss: 0.007676688954234123\n",
      "epoch:  474, loss: 0.007675844244658947\n",
      "epoch:  475, loss: 0.007675266359001398\n",
      "epoch:  476, loss: 0.0076750945299863815\n",
      "epoch:  477, loss: 0.007674344815313816\n",
      "epoch:  478, loss: 0.007673625834286213\n",
      "epoch:  479, loss: 0.007673435378819704\n",
      "epoch:  480, loss: 0.007673280313611031\n",
      "epoch:  481, loss: 0.007672092877328396\n",
      "epoch:  482, loss: 0.007661170791834593\n",
      "epoch:  483, loss: 0.007660045754164457\n",
      "epoch:  484, loss: 0.007659824565052986\n",
      "epoch:  485, loss: 0.007659264374524355\n",
      "epoch:  486, loss: 0.0076584285125136375\n",
      "epoch:  487, loss: 0.0076582045294344425\n",
      "epoch:  488, loss: 0.007658056449145079\n",
      "epoch:  489, loss: 0.007656927220523357\n",
      "epoch:  490, loss: 0.007656665984541178\n",
      "epoch:  491, loss: 0.007656533736735582\n",
      "epoch:  492, loss: 0.0076558273285627365\n",
      "epoch:  493, loss: 0.007655266672372818\n",
      "epoch:  494, loss: 0.0076550813391804695\n",
      "epoch:  495, loss: 0.007654950488358736\n",
      "epoch:  496, loss: 0.007654198445379734\n",
      "epoch:  497, loss: 0.007653656415641308\n",
      "epoch:  498, loss: 0.007653474807739258\n",
      "epoch:  499, loss: 0.007653337903320789\n",
      "epoch:  500, loss: 0.007652880158275366\n",
      "epoch:  501, loss: 0.007652069441974163\n",
      "epoch:  502, loss: 0.007651843596249819\n",
      "epoch:  503, loss: 0.007651706226170063\n",
      "epoch:  504, loss: 0.007651118095964193\n",
      "epoch:  505, loss: 0.007650481071323156\n",
      "epoch:  506, loss: 0.007650256156921387\n",
      "epoch:  507, loss: 0.007650130894035101\n",
      "epoch:  508, loss: 0.007649552077054977\n",
      "epoch:  509, loss: 0.007648942992091179\n",
      "epoch:  510, loss: 0.007648749742656946\n",
      "epoch:  511, loss: 0.0076486109755933285\n",
      "epoch:  512, loss: 0.007647648453712463\n",
      "epoch:  513, loss: 0.007647398859262466\n",
      "epoch:  514, loss: 0.007647254969924688\n",
      "epoch:  515, loss: 0.007646240759640932\n",
      "epoch:  516, loss: 0.007645986042916775\n",
      "epoch:  517, loss: 0.007645831909030676\n",
      "epoch:  518, loss: 0.007645734585821629\n",
      "epoch:  519, loss: 0.007644747383892536\n",
      "epoch:  520, loss: 0.007644451688975096\n",
      "epoch:  521, loss: 0.007644317112863064\n",
      "epoch:  522, loss: 0.007643408607691526\n",
      "epoch:  523, loss: 0.007643089164048433\n",
      "epoch:  524, loss: 0.007642930839210749\n",
      "epoch:  525, loss: 0.007642796263098717\n",
      "epoch:  526, loss: 0.007642675656825304\n",
      "epoch:  527, loss: 0.007642557844519615\n",
      "epoch:  528, loss: 0.00764243071898818\n",
      "epoch:  529, loss: 0.007641999516636133\n",
      "epoch:  530, loss: 0.007641192525625229\n",
      "epoch:  531, loss: 0.007641013711690903\n",
      "epoch:  532, loss: 0.007640645373612642\n",
      "epoch:  533, loss: 0.007639825344085693\n",
      "epoch:  534, loss: 0.007639605551958084\n",
      "epoch:  535, loss: 0.007639450021088123\n",
      "epoch:  536, loss: 0.007639314979314804\n",
      "epoch:  537, loss: 0.007638507056981325\n",
      "epoch:  538, loss: 0.007638013456016779\n",
      "epoch:  539, loss: 0.007637851405888796\n",
      "epoch:  540, loss: 0.00763716408982873\n",
      "epoch:  541, loss: 0.007636523339897394\n",
      "epoch:  542, loss: 0.007636288180947304\n",
      "epoch:  543, loss: 0.007636154070496559\n",
      "epoch:  544, loss: 0.0076360153034329414\n",
      "epoch:  545, loss: 0.007634888868778944\n",
      "epoch:  546, loss: 0.007634646259248257\n",
      "epoch:  547, loss: 0.007634489331394434\n",
      "epoch:  548, loss: 0.0076333750039339066\n",
      "epoch:  549, loss: 0.007633069064468145\n",
      "epoch:  550, loss: 0.007632906548678875\n",
      "epoch:  551, loss: 0.007632010616362095\n",
      "epoch:  552, loss: 0.007631317246705294\n",
      "epoch:  553, loss: 0.007631127722561359\n",
      "epoch:  554, loss: 0.007630635052919388\n",
      "epoch:  555, loss: 0.00762971444055438\n",
      "epoch:  556, loss: 0.00762943085283041\n",
      "epoch:  557, loss: 0.007629263214766979\n",
      "epoch:  558, loss: 0.007628117688000202\n",
      "epoch:  559, loss: 0.007627802900969982\n",
      "epoch:  560, loss: 0.00762764411047101\n",
      "epoch:  561, loss: 0.007626725826412439\n",
      "epoch:  562, loss: 0.007626134902238846\n",
      "epoch:  563, loss: 0.007625967729836702\n",
      "epoch:  564, loss: 0.007625814527273178\n",
      "epoch:  565, loss: 0.007624546065926552\n",
      "epoch:  566, loss: 0.007624308578670025\n",
      "epoch:  567, loss: 0.0076241339556872845\n",
      "epoch:  568, loss: 0.007622838020324707\n",
      "epoch:  569, loss: 0.0076225860975682735\n",
      "epoch:  570, loss: 0.007622388657182455\n",
      "epoch:  571, loss: 0.007621875032782555\n",
      "epoch:  572, loss: 0.007620657794177532\n",
      "epoch:  573, loss: 0.007620364893227816\n",
      "epoch:  574, loss: 0.007620163727551699\n",
      "epoch:  575, loss: 0.007619860582053661\n",
      "epoch:  576, loss: 0.00761841656640172\n",
      "epoch:  577, loss: 0.007618088740855455\n",
      "epoch:  578, loss: 0.007617876399308443\n",
      "epoch:  579, loss: 0.007616913877427578\n",
      "epoch:  580, loss: 0.007616106420755386\n",
      "epoch:  581, loss: 0.007615851238369942\n",
      "epoch:  582, loss: 0.00761567335575819\n",
      "epoch:  583, loss: 0.007614448666572571\n",
      "epoch:  584, loss: 0.007614062633365393\n",
      "epoch:  585, loss: 0.007613828405737877\n",
      "epoch:  586, loss: 0.007613673340529203\n",
      "epoch:  587, loss: 0.007613172754645348\n",
      "epoch:  588, loss: 0.007612291723489761\n",
      "epoch:  589, loss: 0.007612060289829969\n",
      "epoch:  590, loss: 0.007611905224621296\n",
      "epoch:  591, loss: 0.007610889617353678\n",
      "epoch:  592, loss: 0.0076104821637272835\n",
      "epoch:  593, loss: 0.007610303349792957\n",
      "epoch:  594, loss: 0.007610146887600422\n",
      "epoch:  595, loss: 0.0076090069487690926\n",
      "epoch:  596, loss: 0.007608713582158089\n",
      "epoch:  597, loss: 0.007608505431562662\n",
      "epoch:  598, loss: 0.007608348038047552\n",
      "epoch:  599, loss: 0.007608205080032349\n",
      "epoch:  600, loss: 0.007608069106936455\n",
      "epoch:  601, loss: 0.007607711479067802\n",
      "epoch:  602, loss: 0.00760674336925149\n",
      "epoch:  603, loss: 0.007606504950672388\n",
      "epoch:  604, loss: 0.007606338243931532\n",
      "epoch:  605, loss: 0.00760617246851325\n",
      "epoch:  606, loss: 0.007605064660310745\n",
      "epoch:  607, loss: 0.007604771293699741\n",
      "epoch:  608, loss: 0.007604620419442654\n",
      "epoch:  609, loss: 0.0076037985272705555\n",
      "epoch:  610, loss: 0.007603186182677746\n",
      "epoch:  611, loss: 0.007602991070598364\n",
      "epoch:  612, loss: 0.00760283786803484\n",
      "epoch:  613, loss: 0.007602709345519543\n",
      "epoch:  614, loss: 0.007601553108543158\n",
      "epoch:  615, loss: 0.007601269520819187\n",
      "epoch:  616, loss: 0.0076011065393686295\n",
      "epoch:  617, loss: 0.007600949611514807\n",
      "epoch:  618, loss: 0.0076008141040802\n",
      "epoch:  619, loss: 0.007600771728903055\n",
      "epoch:  620, loss: 0.0075995223596692085\n",
      "epoch:  621, loss: 0.007599258795380592\n",
      "epoch:  622, loss: 0.0075990925543010235\n",
      "epoch:  623, loss: 0.007598386611789465\n",
      "epoch:  624, loss: 0.007597621064633131\n",
      "epoch:  625, loss: 0.007597379852086306\n",
      "epoch:  626, loss: 0.007597208023071289\n",
      "epoch:  627, loss: 0.007582840975373983\n",
      "epoch:  628, loss: 0.007582516875118017\n",
      "epoch:  629, loss: 0.007582337129861116\n",
      "epoch:  630, loss: 0.007581065408885479\n",
      "epoch:  631, loss: 0.007580663077533245\n",
      "epoch:  632, loss: 0.0075804912485182285\n",
      "epoch:  633, loss: 0.007579292170703411\n",
      "epoch:  634, loss: 0.007578902877867222\n",
      "epoch:  635, loss: 0.007578724063932896\n",
      "epoch:  636, loss: 0.007577595300972462\n",
      "epoch:  637, loss: 0.00757711473852396\n",
      "epoch:  638, loss: 0.007576937787234783\n",
      "epoch:  639, loss: 0.007575836963951588\n",
      "epoch:  640, loss: 0.007575347553938627\n",
      "epoch:  641, loss: 0.007575171999633312\n",
      "epoch:  642, loss: 0.007574049290269613\n",
      "epoch:  643, loss: 0.007573549635708332\n",
      "epoch:  644, loss: 0.0075733596459031105\n",
      "epoch:  645, loss: 0.0075719826854765415\n",
      "epoch:  646, loss: 0.00757155055180192\n",
      "epoch:  647, loss: 0.0075713652186095715\n",
      "epoch:  648, loss: 0.007570149842649698\n",
      "epoch:  649, loss: 0.007569674868136644\n",
      "epoch:  650, loss: 0.007569491863250732\n",
      "epoch:  651, loss: 0.00756832305341959\n",
      "epoch:  652, loss: 0.007567879743874073\n",
      "epoch:  653, loss: 0.007567704189568758\n",
      "epoch:  654, loss: 0.0075666881166398525\n",
      "epoch:  655, loss: 0.007566109299659729\n",
      "epoch:  656, loss: 0.007565915118902922\n",
      "epoch:  657, loss: 0.007564930710941553\n",
      "epoch:  658, loss: 0.00756431557238102\n",
      "epoch:  659, loss: 0.007564112078398466\n",
      "epoch:  660, loss: 0.007563288323581219\n",
      "epoch:  661, loss: 0.007562527433037758\n",
      "epoch:  662, loss: 0.007562314160168171\n",
      "epoch:  663, loss: 0.007561804726719856\n",
      "epoch:  664, loss: 0.007560793776065111\n",
      "epoch:  665, loss: 0.007560541853308678\n",
      "epoch:  666, loss: 0.007560383062809706\n",
      "epoch:  667, loss: 0.007559517398476601\n",
      "epoch:  668, loss: 0.007558837532997131\n",
      "epoch:  669, loss: 0.007558636832982302\n",
      "epoch:  670, loss: 0.007558083161711693\n",
      "epoch:  671, loss: 0.0075570945627987385\n",
      "epoch:  672, loss: 0.007556864991784096\n",
      "epoch:  673, loss: 0.007556708063930273\n",
      "epoch:  674, loss: 0.007555733900517225\n",
      "epoch:  675, loss: 0.00755512248724699\n",
      "epoch:  676, loss: 0.007554932031780481\n",
      "epoch:  677, loss: 0.007554046344012022\n",
      "epoch:  678, loss: 0.007553349249064922\n",
      "epoch:  679, loss: 0.007553136441856623\n",
      "epoch:  680, loss: 0.007552976720035076\n",
      "epoch:  681, loss: 0.0075522251427173615\n",
      "epoch:  682, loss: 0.007539600133895874\n",
      "epoch:  683, loss: 0.007538010366261005\n",
      "epoch:  684, loss: 0.007537721190601587\n",
      "epoch:  685, loss: 0.00753754423931241\n",
      "epoch:  686, loss: 0.007536118850111961\n",
      "epoch:  687, loss: 0.0075358129106462\n",
      "epoch:  688, loss: 0.007535636890679598\n",
      "epoch:  689, loss: 0.007534307893365622\n",
      "epoch:  690, loss: 0.007533935364335775\n",
      "epoch:  691, loss: 0.007533744443207979\n",
      "epoch:  692, loss: 0.007532754447311163\n",
      "epoch:  693, loss: 0.007532138843089342\n",
      "epoch:  694, loss: 0.007531898096203804\n",
      "epoch:  695, loss: 0.007531727664172649\n",
      "epoch:  696, loss: 0.007530958857387304\n",
      "epoch:  697, loss: 0.0075300224125385284\n",
      "epoch:  698, loss: 0.007529799826443195\n",
      "epoch:  699, loss: 0.007529616821557283\n",
      "epoch:  700, loss: 0.007528177462518215\n",
      "epoch:  701, loss: 0.007527882233262062\n",
      "epoch:  702, loss: 0.007527704816311598\n",
      "epoch:  703, loss: 0.007526191882789135\n",
      "epoch:  704, loss: 0.0075259399600327015\n",
      "epoch:  705, loss: 0.007525745779275894\n",
      "epoch:  706, loss: 0.007525570224970579\n",
      "epoch:  707, loss: 0.0075241802260279655\n",
      "epoch:  708, loss: 0.007523863110691309\n",
      "epoch:  709, loss: 0.007523671258240938\n",
      "epoch:  710, loss: 0.007522463798522949\n",
      "epoch:  711, loss: 0.007521972060203552\n",
      "epoch:  712, loss: 0.007521757856011391\n",
      "epoch:  713, loss: 0.007521581836044788\n",
      "epoch:  714, loss: 0.0075201536528766155\n",
      "epoch:  715, loss: 0.007519851438701153\n",
      "epoch:  716, loss: 0.007519658654928207\n",
      "epoch:  717, loss: 0.007518346421420574\n",
      "epoch:  718, loss: 0.007517923600971699\n",
      "epoch:  719, loss: 0.007517706602811813\n",
      "epoch:  720, loss: 0.007516843266785145\n",
      "epoch:  721, loss: 0.007516021840274334\n",
      "epoch:  722, loss: 0.007515738718211651\n",
      "epoch:  723, loss: 0.007515547797083855\n",
      "epoch:  724, loss: 0.007514272816479206\n",
      "epoch:  725, loss: 0.007513770833611488\n",
      "epoch:  726, loss: 0.00751355430111289\n",
      "epoch:  727, loss: 0.007513372227549553\n",
      "epoch:  728, loss: 0.007513250689953566\n",
      "epoch:  729, loss: 0.007511758711189032\n",
      "epoch:  730, loss: 0.007511442992836237\n",
      "epoch:  731, loss: 0.007511226460337639\n",
      "epoch:  732, loss: 0.007510874420404434\n",
      "epoch:  733, loss: 0.007509625516831875\n",
      "epoch:  734, loss: 0.0075092618353664875\n",
      "epoch:  735, loss: 0.007509059738367796\n",
      "epoch:  736, loss: 0.007508010137826204\n",
      "epoch:  737, loss: 0.007507357746362686\n",
      "epoch:  738, loss: 0.007507087662816048\n",
      "epoch:  739, loss: 0.007506898138672113\n",
      "epoch:  740, loss: 0.007506722118705511\n",
      "epoch:  741, loss: 0.007506337016820908\n",
      "epoch:  742, loss: 0.007505069486796856\n",
      "epoch:  743, loss: 0.007504751905798912\n",
      "epoch:  744, loss: 0.007504546083509922\n",
      "epoch:  745, loss: 0.0075037600472569466\n",
      "epoch:  746, loss: 0.007502787280827761\n",
      "epoch:  747, loss: 0.007502494379878044\n",
      "epoch:  748, loss: 0.007502292282879353\n",
      "epoch:  749, loss: 0.007502120919525623\n",
      "epoch:  750, loss: 0.007501938380300999\n",
      "epoch:  751, loss: 0.007501109968870878\n",
      "epoch:  752, loss: 0.007500176317989826\n",
      "epoch:  753, loss: 0.0074998498894274235\n",
      "epoch:  754, loss: 0.007499635219573975\n",
      "epoch:  755, loss: 0.007498875260353088\n",
      "epoch:  756, loss: 0.007497849408537149\n",
      "epoch:  757, loss: 0.00749753974378109\n",
      "epoch:  758, loss: 0.007497322745621204\n",
      "epoch:  759, loss: 0.00749712809920311\n",
      "epoch:  760, loss: 0.0074958945624530315\n",
      "epoch:  761, loss: 0.007495224010199308\n",
      "epoch:  762, loss: 0.007494975347071886\n",
      "epoch:  763, loss: 0.00749477744102478\n",
      "epoch:  764, loss: 0.007494483143091202\n",
      "epoch:  765, loss: 0.007493000011891127\n",
      "epoch:  766, loss: 0.007492673583328724\n",
      "epoch:  767, loss: 0.007492446806281805\n",
      "epoch:  768, loss: 0.007492248434573412\n",
      "epoch:  769, loss: 0.0074908058159053326\n",
      "epoch:  770, loss: 0.007490338757634163\n",
      "epoch:  771, loss: 0.007490077055990696\n",
      "epoch:  772, loss: 0.0074895392172038555\n",
      "epoch:  773, loss: 0.00748828100040555\n",
      "epoch:  774, loss: 0.007487902417778969\n",
      "epoch:  775, loss: 0.007487688213586807\n",
      "epoch:  776, loss: 0.007487480528652668\n",
      "epoch:  777, loss: 0.007486003451049328\n",
      "epoch:  778, loss: 0.0074855368584394455\n",
      "epoch:  779, loss: 0.007485263515263796\n",
      "epoch:  780, loss: 0.007485008332878351\n",
      "epoch:  781, loss: 0.007483459543436766\n",
      "epoch:  782, loss: 0.0074830311350524426\n",
      "epoch:  783, loss: 0.007482815999537706\n",
      "epoch:  784, loss: 0.007482617627829313\n",
      "epoch:  785, loss: 0.007481100503355265\n",
      "epoch:  786, loss: 0.0074806720949709415\n",
      "epoch:  787, loss: 0.007480435539036989\n",
      "epoch:  788, loss: 0.007480234373360872\n",
      "epoch:  789, loss: 0.007478737737983465\n",
      "epoch:  790, loss: 0.007478327956050634\n",
      "epoch:  791, loss: 0.007478079758584499\n",
      "epoch:  792, loss: 0.007477887440472841\n",
      "epoch:  793, loss: 0.007477698381990194\n",
      "epoch:  794, loss: 0.007477519568055868\n",
      "epoch:  795, loss: 0.0074773384258151054\n",
      "epoch:  796, loss: 0.007477168925106525\n",
      "epoch:  797, loss: 0.007476983591914177\n",
      "epoch:  798, loss: 0.007476796396076679\n",
      "epoch:  799, loss: 0.007476229686290026\n",
      "epoch:  800, loss: 0.00747502688318491\n",
      "epoch:  801, loss: 0.007474708370864391\n",
      "epoch:  802, loss: 0.007474484853446484\n",
      "epoch:  803, loss: 0.007474288810044527\n",
      "epoch:  804, loss: 0.0074741048738360405\n",
      "epoch:  805, loss: 0.007473919540643692\n",
      "epoch:  806, loss: 0.007473738398402929\n",
      "epoch:  807, loss: 0.007473553530871868\n",
      "epoch:  808, loss: 0.007473372854292393\n",
      "epoch:  809, loss: 0.0074731819331645966\n",
      "epoch:  810, loss: 0.007472996134310961\n",
      "epoch:  811, loss: 0.0074728126637637615\n",
      "epoch:  812, loss: 0.007472623605281115\n",
      "epoch:  813, loss: 0.0074724419973790646\n",
      "epoch:  814, loss: 0.007472250610589981\n",
      "epoch:  815, loss: 0.007471440825611353\n",
      "epoch:  816, loss: 0.007470381446182728\n",
      "epoch:  817, loss: 0.007470077835023403\n",
      "epoch:  818, loss: 0.007469845470041037\n",
      "epoch:  819, loss: 0.007469630800187588\n",
      "epoch:  820, loss: 0.007469438947737217\n",
      "epoch:  821, loss: 0.00746924290433526\n",
      "epoch:  822, loss: 0.00746905105188489\n",
      "epoch:  823, loss: 0.007468859665095806\n",
      "epoch:  824, loss: 0.007468651048839092\n",
      "epoch:  825, loss: 0.0074684517458081245\n",
      "epoch:  826, loss: 0.007468232419341803\n",
      "epoch:  827, loss: 0.007468040566891432\n",
      "epoch:  828, loss: 0.0074678282253444195\n",
      "epoch:  829, loss: 0.007466999813914299\n",
      "epoch:  830, loss: 0.007465777453035116\n",
      "epoch:  831, loss: 0.007465418428182602\n",
      "epoch:  832, loss: 0.0074651907198131084\n",
      "epoch:  833, loss: 0.007464843802154064\n",
      "epoch:  834, loss: 0.007463082671165466\n",
      "epoch:  835, loss: 0.007462678011506796\n",
      "epoch:  836, loss: 0.007462422363460064\n",
      "epoch:  837, loss: 0.007462203502655029\n",
      "epoch:  838, loss: 0.0074613080359995365\n",
      "epoch:  839, loss: 0.007460122928023338\n",
      "epoch:  840, loss: 0.007459791377186775\n",
      "epoch:  841, loss: 0.007459553424268961\n",
      "epoch:  842, loss: 0.007457916624844074\n",
      "epoch:  843, loss: 0.007457322906702757\n",
      "epoch:  844, loss: 0.007457035593688488\n",
      "epoch:  845, loss: 0.007456822786480188\n",
      "epoch:  846, loss: 0.007456587627530098\n",
      "epoch:  847, loss: 0.007455457467585802\n",
      "epoch:  848, loss: 0.007454403210431337\n",
      "epoch:  849, loss: 0.007454046048223972\n",
      "epoch:  850, loss: 0.007453812751919031\n",
      "epoch:  851, loss: 0.0074521638453006744\n",
      "epoch:  852, loss: 0.007451506797224283\n",
      "epoch:  853, loss: 0.007451238576322794\n",
      "epoch:  854, loss: 0.007450994104146957\n",
      "epoch:  855, loss: 0.0074506173841655254\n",
      "epoch:  856, loss: 0.0074317967519164085\n",
      "epoch:  857, loss: 0.007429115008562803\n",
      "epoch:  858, loss: 0.0074285827577114105\n",
      "epoch:  859, loss: 0.007428320590406656\n",
      "epoch:  860, loss: 0.00742742232978344\n",
      "epoch:  861, loss: 0.007425959222018719\n",
      "epoch:  862, loss: 0.007425584830343723\n",
      "epoch:  863, loss: 0.007425342686474323\n",
      "epoch:  864, loss: 0.007423506584018469\n",
      "epoch:  865, loss: 0.007422992028295994\n",
      "epoch:  866, loss: 0.007422700058668852\n",
      "epoch:  867, loss: 0.007422700524330139\n",
      "epoch:  868, loss: 0.007420675829052925\n",
      "epoch:  869, loss: 0.007420153822749853\n",
      "epoch:  870, loss: 0.007419881876558065\n",
      "epoch:  871, loss: 0.0074196504428982735\n",
      "epoch:  872, loss: 0.007418791763484478\n",
      "epoch:  873, loss: 0.007417478132992983\n",
      "epoch:  874, loss: 0.007417075801640749\n",
      "epoch:  875, loss: 0.007416832260787487\n",
      "epoch:  876, loss: 0.007416598964482546\n",
      "epoch:  877, loss: 0.007415439002215862\n",
      "epoch:  878, loss: 0.0074144876562058926\n",
      "epoch:  879, loss: 0.007414151914417744\n",
      "epoch:  880, loss: 0.007413892075419426\n",
      "epoch:  881, loss: 0.0074136704206466675\n",
      "epoch:  882, loss: 0.007412550505250692\n",
      "epoch:  883, loss: 0.007411522790789604\n",
      "epoch:  884, loss: 0.007411190774291754\n",
      "epoch:  885, loss: 0.007410943508148193\n",
      "epoch:  886, loss: 0.007410725578665733\n",
      "epoch:  887, loss: 0.007410502061247826\n",
      "epoch:  888, loss: 0.0074095819145441055\n",
      "epoch:  889, loss: 0.007408380042761564\n",
      "epoch:  890, loss: 0.007407988887280226\n",
      "epoch:  891, loss: 0.007407755125313997\n",
      "epoch:  892, loss: 0.007407519035041332\n",
      "epoch:  893, loss: 0.0074073090218007565\n",
      "epoch:  894, loss: 0.007407084107398987\n",
      "epoch:  895, loss: 0.007406885735690594\n",
      "epoch:  896, loss: 0.007406661286950111\n",
      "epoch:  897, loss: 0.007405649404972792\n",
      "epoch:  898, loss: 0.007404561620205641\n",
      "epoch:  899, loss: 0.007404229138046503\n",
      "epoch:  900, loss: 0.007403967902064323\n",
      "epoch:  901, loss: 0.007403758820146322\n",
      "epoch:  902, loss: 0.007403533905744553\n",
      "epoch:  903, loss: 0.007403334602713585\n",
      "epoch:  904, loss: 0.007403108291327953\n",
      "epoch:  905, loss: 0.00740290991961956\n",
      "epoch:  906, loss: 0.0074026919901371\n",
      "epoch:  907, loss: 0.007401830051094294\n",
      "epoch:  908, loss: 0.007400626316666603\n",
      "epoch:  909, loss: 0.00740026542916894\n",
      "epoch:  910, loss: 0.007400010712444782\n",
      "epoch:  911, loss: 0.007399785332381725\n",
      "epoch:  912, loss: 0.007398596499115229\n",
      "epoch:  913, loss: 0.0073976460844278336\n",
      "epoch:  914, loss: 0.007397300563752651\n",
      "epoch:  915, loss: 0.007397044915705919\n",
      "epoch:  916, loss: 0.007396820932626724\n",
      "epoch:  917, loss: 0.007395765278488398\n",
      "epoch:  918, loss: 0.00739466305822134\n",
      "epoch:  919, loss: 0.007394292391836643\n",
      "epoch:  920, loss: 0.007394049316644669\n",
      "epoch:  921, loss: 0.007392968982458115\n",
      "epoch:  922, loss: 0.007391860708594322\n",
      "epoch:  923, loss: 0.007391517981886864\n",
      "epoch:  924, loss: 0.007391264662146568\n",
      "epoch:  925, loss: 0.007390828337520361\n",
      "epoch:  926, loss: 0.007389140781015158\n",
      "epoch:  927, loss: 0.007388755679130554\n",
      "epoch:  928, loss: 0.0073884702287614346\n",
      "epoch:  929, loss: 0.007388238795101643\n",
      "epoch:  930, loss: 0.007386756129562855\n",
      "epoch:  931, loss: 0.007385958917438984\n",
      "epoch:  932, loss: 0.007385675795376301\n",
      "epoch:  933, loss: 0.007385415490716696\n",
      "epoch:  934, loss: 0.007385188713669777\n",
      "epoch:  935, loss: 0.00738456379622221\n",
      "epoch:  936, loss: 0.007383001036942005\n",
      "epoch:  937, loss: 0.007382619194686413\n",
      "epoch:  938, loss: 0.007382359355688095\n",
      "epoch:  939, loss: 0.007381821982562542\n",
      "epoch:  940, loss: 0.007380170747637749\n",
      "epoch:  941, loss: 0.007379761431366205\n",
      "epoch:  942, loss: 0.0073794894851744175\n",
      "epoch:  943, loss: 0.007379250600934029\n",
      "epoch:  944, loss: 0.007377347908914089\n",
      "epoch:  945, loss: 0.007376926485449076\n",
      "epoch:  946, loss: 0.00737660750746727\n",
      "epoch:  947, loss: 0.00737637048587203\n",
      "epoch:  948, loss: 0.007374833803623915\n",
      "epoch:  949, loss: 0.007374024484306574\n",
      "epoch:  950, loss: 0.007373697590082884\n",
      "epoch:  951, loss: 0.0073734475299716\n",
      "epoch:  952, loss: 0.007371918763965368\n",
      "epoch:  953, loss: 0.007371142040938139\n",
      "epoch:  954, loss: 0.0073707373812794685\n",
      "epoch:  955, loss: 0.007370484992861748\n",
      "epoch:  956, loss: 0.007370244711637497\n",
      "epoch:  957, loss: 0.007369603496044874\n",
      "epoch:  958, loss: 0.007367982063442469\n",
      "epoch:  959, loss: 0.007367602549493313\n",
      "epoch:  960, loss: 0.007367306388914585\n",
      "epoch:  961, loss: 0.007366824895143509\n",
      "epoch:  962, loss: 0.007365077268332243\n",
      "epoch:  963, loss: 0.007364613935351372\n",
      "epoch:  964, loss: 0.007364311255514622\n",
      "epoch:  965, loss: 0.00736421812325716\n",
      "epoch:  966, loss: 0.007362127769738436\n",
      "epoch:  967, loss: 0.007361689582467079\n",
      "epoch:  968, loss: 0.0073613435961306095\n",
      "epoch:  969, loss: 0.007361084222793579\n",
      "epoch:  970, loss: 0.0073608518578112125\n",
      "epoch:  971, loss: 0.007359908428043127\n",
      "epoch:  972, loss: 0.00735856220126152\n",
      "epoch:  973, loss: 0.007358221337199211\n",
      "epoch:  974, loss: 0.007357885129749775\n",
      "epoch:  975, loss: 0.0073576378636062145\n",
      "epoch:  976, loss: 0.0073568145744502544\n",
      "epoch:  977, loss: 0.0073553877882659435\n",
      "epoch:  978, loss: 0.0073549444787204266\n",
      "epoch:  979, loss: 0.00735465157777071\n",
      "epoch:  980, loss: 0.007354396395385265\n",
      "epoch:  981, loss: 0.007353001274168491\n",
      "epoch:  982, loss: 0.0073519982397556305\n",
      "epoch:  983, loss: 0.007351686246693134\n",
      "epoch:  984, loss: 0.007351362146437168\n",
      "epoch:  985, loss: 0.0073512885719537735\n",
      "epoch:  986, loss: 0.00734917214140296\n",
      "epoch:  987, loss: 0.007348624523729086\n",
      "epoch:  988, loss: 0.00734829343855381\n",
      "epoch:  989, loss: 0.007348037324845791\n",
      "epoch:  990, loss: 0.00734777981415391\n",
      "epoch:  991, loss: 0.007346696685999632\n",
      "epoch:  992, loss: 0.007345378398895264\n",
      "epoch:  993, loss: 0.007345020305365324\n",
      "epoch:  994, loss: 0.007344668731093407\n",
      "epoch:  995, loss: 0.007344417739659548\n",
      "epoch:  996, loss: 0.0073437620885670185\n",
      "epoch:  997, loss: 0.007342101540416479\n",
      "epoch:  998, loss: 0.007341585122048855\n",
      "epoch:  999, loss: 0.00734128849580884\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size=X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# opt = torch_numopt.ConjugateGradient(params=model.parameters(), model=model, lr=1e-4, line_search_method=\"const\", cg_method=\"PR\")\n",
    "opt = torch_numopt.ConjugateGradient(params=model.parameters(), model=model, lr=10, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"armijo\", cg_method=\"PR\")\n",
    "# opt = torch_numopt.ConjugateGradient(params=model.parameters(), model=model, lr=10, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"wolfe\", cg_method=\"PR\")\n",
    "# opt = torch_numopt.ConjugateGradient(params=model.parameters(), model=model, lr=10, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"strong-wolfe\", cg_method=\"PR\")\n",
    "# opt = torch_numopt.ConjugateGradient(params=model.parameters(), model=model, lr=10, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"goldstein\", cg_method=\"PR\")\n",
    "\n",
    "\n",
    "all_loss = {}\n",
    "for epoch in range(1000):\n",
    "    print(\"epoch: \", epoch, end=\"\")\n",
    "    all_loss[epoch + 1] = 0\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y, loss_fn)\n",
    "\n",
    "        all_loss[epoch + 1] += loss\n",
    "    all_loss[epoch + 1] /= len(data_loader)\n",
    "    print(\", loss: {}\".format(all_loss[epoch + 1].detach().numpy().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics: R2 = 0.7150366249418955\n",
      "Test metrics:  R2 = 0.6745394780094169\n"
     ]
    }
   ],
   "source": [
    "pred_train = model.forward(X_train).detach()\n",
    "pred_test = model.forward(X_test).detach()\n",
    "print(f\"Train metrics: R2 = {r2_score(pred_train, y_train)}\")\n",
    "print(f\"Test metrics:  R2 = {r2_score(pred_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.1170051097869873\n",
      "epoch:  1, loss: 0.07843513786792755\n",
      "epoch:  2, loss: 0.05806645378470421\n",
      "epoch:  3, loss: 0.047126688063144684\n",
      "epoch:  4, loss: 0.04114944115281105\n",
      "epoch:  5, loss: 0.0378275103867054\n",
      "epoch:  6, loss: 0.035966720432043076\n",
      "epoch:  7, loss: 0.034918833523988724\n",
      "epoch:  8, loss: 0.03432825207710266\n",
      "epoch:  9, loss: 0.033993154764175415\n",
      "epoch:  10, loss: 0.033801399171352386\n",
      "epoch:  11, loss: 0.033690307289361954\n",
      "epoch:  12, loss: 0.0336250364780426\n",
      "epoch:  13, loss: 0.03358563780784607\n",
      "epoch:  14, loss: 0.03357809782028198\n",
      "epoch:  15, loss: 0.03353321924805641\n",
      "epoch:  16, loss: 0.03350593149662018\n",
      "epoch:  17, loss: 0.033502787351608276\n",
      "epoch:  18, loss: 0.03347083553671837\n",
      "epoch:  19, loss: 0.033451322466135025\n",
      "epoch:  20, loss: 0.03344447910785675\n",
      "epoch:  21, loss: 0.03342122212052345\n",
      "epoch:  22, loss: 0.03340663015842438\n",
      "epoch:  23, loss: 0.03339357301592827\n",
      "epoch:  24, loss: 0.033376652747392654\n",
      "epoch:  25, loss: 0.03337284177541733\n",
      "epoch:  26, loss: 0.03335300460457802\n",
      "epoch:  27, loss: 0.03334078937768936\n",
      "epoch:  28, loss: 0.03333425894379616\n",
      "epoch:  29, loss: 0.03331984952092171\n",
      "epoch:  30, loss: 0.0333186611533165\n",
      "epoch:  31, loss: 0.033301305025815964\n",
      "epoch:  32, loss: 0.03329060226678848\n",
      "epoch:  33, loss: 0.03328429535031319\n",
      "epoch:  34, loss: 0.033271584659814835\n",
      "epoch:  35, loss: 0.033269256353378296\n",
      "epoch:  36, loss: 0.03325394168496132\n",
      "epoch:  37, loss: 0.03324444591999054\n",
      "epoch:  38, loss: 0.03323713317513466\n",
      "epoch:  39, loss: 0.03322576358914375\n",
      "epoch:  40, loss: 0.03322156146168709\n",
      "epoch:  41, loss: 0.03320794925093651\n",
      "epoch:  42, loss: 0.033199477940797806\n",
      "epoch:  43, loss: 0.033191535621881485\n",
      "epoch:  44, loss: 0.03318142145872116\n",
      "epoch:  45, loss: 0.033176127821207047\n",
      "epoch:  46, loss: 0.033163998275995255\n",
      "epoch:  47, loss: 0.03316217288374901\n",
      "epoch:  48, loss: 0.033147405833005905\n",
      "epoch:  49, loss: 0.033138275146484375\n",
      "epoch:  50, loss: 0.03313139081001282\n",
      "epoch:  51, loss: 0.033120378851890564\n",
      "epoch:  52, loss: 0.033116210252046585\n",
      "epoch:  53, loss: 0.03310279920697212\n",
      "epoch:  54, loss: 0.03310209512710571\n",
      "epoch:  55, loss: 0.03308570012450218\n",
      "epoch:  56, loss: 0.03307559713721275\n",
      "epoch:  57, loss: 0.03306933492422104\n",
      "epoch:  58, loss: 0.03305703029036522\n",
      "epoch:  59, loss: 0.033054202795028687\n",
      "epoch:  60, loss: 0.03303908184170723\n",
      "epoch:  61, loss: 0.033029697835445404\n",
      "epoch:  62, loss: 0.03302166983485222\n",
      "epoch:  63, loss: 0.03301021084189415\n",
      "epoch:  64, loss: 0.03300492465496063\n",
      "epoch:  65, loss: 0.03299080207943916\n",
      "epoch:  66, loss: 0.0329892598092556\n",
      "epoch:  67, loss: 0.03297166898846626\n",
      "epoch:  68, loss: 0.03296084702014923\n",
      "epoch:  69, loss: 0.03295326605439186\n",
      "epoch:  70, loss: 0.03293989226222038\n",
      "epoch:  71, loss: 0.032935839146375656\n",
      "epoch:  72, loss: 0.03291912004351616\n",
      "epoch:  73, loss: 0.03290872648358345\n",
      "epoch:  74, loss: 0.03289883956313133\n",
      "epoch:  75, loss: 0.03288585692644119\n",
      "epoch:  76, loss: 0.03287917748093605\n",
      "epoch:  77, loss: 0.03286297246813774\n",
      "epoch:  78, loss: 0.03286029025912285\n",
      "epoch:  79, loss: 0.032839760184288025\n",
      "epoch:  80, loss: 0.03282713517546654\n",
      "epoch:  81, loss: 0.032817330211400986\n",
      "epoch:  82, loss: 0.03280142322182655\n",
      "epoch:  83, loss: 0.03279608488082886\n",
      "epoch:  84, loss: 0.032775744795799255\n",
      "epoch:  85, loss: 0.032763123512268066\n",
      "epoch:  86, loss: 0.032749708741903305\n",
      "epoch:  87, loss: 0.03273366391658783\n",
      "epoch:  88, loss: 0.032723795622587204\n",
      "epoch:  89, loss: 0.032703474164009094\n",
      "epoch:  90, loss: 0.03270050138235092\n",
      "epoch:  91, loss: 0.03267407789826393\n",
      "epoch:  92, loss: 0.03265786170959473\n",
      "epoch:  93, loss: 0.03264503926038742\n",
      "epoch:  94, loss: 0.032624002546072006\n",
      "epoch:  95, loss: 0.032616909593343735\n",
      "epoch:  96, loss: 0.03258904442191124\n",
      "epoch:  97, loss: 0.03257182240486145\n",
      "epoch:  98, loss: 0.03255387023091316\n",
      "epoch:  99, loss: 0.032531414180994034\n",
      "epoch:  100, loss: 0.032521750777959824\n",
      "epoch:  101, loss: 0.032491739839315414\n",
      "epoch:  102, loss: 0.03247324377298355\n",
      "epoch:  103, loss: 0.03245440125465393\n",
      "epoch:  104, loss: 0.03242979943752289\n",
      "epoch:  105, loss: 0.03241949528455734\n",
      "epoch:  106, loss: 0.03238595649600029\n",
      "epoch:  107, loss: 0.0323653444647789\n",
      "epoch:  108, loss: 0.03234210982918739\n",
      "epoch:  109, loss: 0.03231453523039818\n",
      "epoch:  110, loss: 0.032303158193826675\n",
      "epoch:  111, loss: 0.03226478770375252\n",
      "epoch:  112, loss: 0.03224130719900131\n",
      "epoch:  113, loss: 0.03221734240651131\n",
      "epoch:  114, loss: 0.03218488395214081\n",
      "epoch:  115, loss: 0.03217416629195213\n",
      "epoch:  116, loss: 0.03212848678231239\n",
      "epoch:  117, loss: 0.032100629061460495\n",
      "epoch:  118, loss: 0.03207352012395859\n",
      "epoch:  119, loss: 0.03203422203660011\n",
      "epoch:  120, loss: 0.03202641382813454\n",
      "epoch:  121, loss: 0.031969308853149414\n",
      "epoch:  122, loss: 0.03193502873182297\n",
      "epoch:  123, loss: 0.03190963342785835\n",
      "epoch:  124, loss: 0.03185947984457016\n",
      "epoch:  125, loss: 0.03182833641767502\n",
      "epoch:  126, loss: 0.03178517147898674\n",
      "epoch:  127, loss: 0.03173971548676491\n",
      "epoch:  128, loss: 0.03171954303979874\n",
      "epoch:  129, loss: 0.0316508486866951\n",
      "epoch:  130, loss: 0.031609516590833664\n",
      "epoch:  131, loss: 0.03157316520810127\n",
      "epoch:  132, loss: 0.031509872525930405\n",
      "epoch:  133, loss: 0.0314716100692749\n",
      "epoch:  134, loss: 0.03142037242650986\n",
      "epoch:  135, loss: 0.031361933797597885\n",
      "epoch:  136, loss: 0.03135190159082413\n",
      "epoch:  137, loss: 0.031255003064870834\n",
      "epoch:  138, loss: 0.031199125573039055\n",
      "epoch:  139, loss: 0.031172122806310654\n",
      "epoch:  140, loss: 0.031079810112714767\n",
      "epoch:  141, loss: 0.0310257188975811\n",
      "epoch:  142, loss: 0.030984535813331604\n",
      "epoch:  143, loss: 0.030892811715602875\n",
      "epoch:  144, loss: 0.030839232727885246\n",
      "epoch:  145, loss: 0.030784204602241516\n",
      "epoch:  146, loss: 0.03069368563592434\n",
      "epoch:  147, loss: 0.03064042143523693\n",
      "epoch:  148, loss: 0.030569100752472878\n",
      "epoch:  149, loss: 0.030478889122605324\n",
      "epoch:  150, loss: 0.030425265431404114\n",
      "epoch:  151, loss: 0.03034226968884468\n",
      "epoch:  152, loss: 0.030250007286667824\n",
      "epoch:  153, loss: 0.030195273458957672\n",
      "epoch:  154, loss: 0.03009692206978798\n",
      "epoch:  155, loss: 0.0300022941082716\n",
      "epoch:  156, loss: 0.029945261776447296\n",
      "epoch:  157, loss: 0.029834091663360596\n",
      "epoch:  158, loss: 0.029731495305895805\n",
      "epoch:  159, loss: 0.029670104384422302\n",
      "epoch:  160, loss: 0.02954946458339691\n",
      "epoch:  161, loss: 0.02943827211856842\n",
      "epoch:  162, loss: 0.029373502358794212\n",
      "epoch:  163, loss: 0.029249828308820724\n",
      "epoch:  164, loss: 0.029128557071089745\n",
      "epoch:  165, loss: 0.029059071093797684\n",
      "epoch:  166, loss: 0.028935953974723816\n",
      "epoch:  167, loss: 0.028799381107091904\n",
      "epoch:  168, loss: 0.028723249211907387\n",
      "epoch:  169, loss: 0.028592048212885857\n",
      "epoch:  170, loss: 0.028442442417144775\n",
      "epoch:  171, loss: 0.0283599141985178\n",
      "epoch:  172, loss: 0.028236540034413338\n",
      "epoch:  173, loss: 0.028063874691724777\n",
      "epoch:  174, loss: 0.027971044182777405\n",
      "epoch:  175, loss: 0.02785414271056652\n",
      "epoch:  176, loss: 0.027655070647597313\n",
      "epoch:  177, loss: 0.027553625404834747\n",
      "epoch:  178, loss: 0.027469629421830177\n",
      "epoch:  179, loss: 0.02723275125026703\n",
      "epoch:  180, loss: 0.027114834636449814\n",
      "epoch:  181, loss: 0.027089865878224373\n",
      "epoch:  182, loss: 0.02679181657731533\n",
      "epoch:  183, loss: 0.026651596650481224\n",
      "epoch:  184, loss: 0.026573626324534416\n",
      "epoch:  185, loss: 0.026318490505218506\n",
      "epoch:  186, loss: 0.026157479733228683\n",
      "epoch:  187, loss: 0.02607041411101818\n",
      "epoch:  188, loss: 0.025830047205090523\n",
      "epoch:  189, loss: 0.025635333731770515\n",
      "epoch:  190, loss: 0.02553674206137657\n",
      "epoch:  191, loss: 0.025335706770420074\n",
      "epoch:  192, loss: 0.02509349025785923\n",
      "epoch:  193, loss: 0.0249810628592968\n",
      "epoch:  194, loss: 0.024820225313305855\n",
      "epoch:  195, loss: 0.024525253102183342\n",
      "epoch:  196, loss: 0.02439902350306511\n",
      "epoch:  197, loss: 0.024289190769195557\n",
      "epoch:  198, loss: 0.02393140271306038\n",
      "epoch:  199, loss: 0.023791451007127762\n",
      "epoch:  200, loss: 0.023719176650047302\n",
      "epoch:  201, loss: 0.02331928163766861\n",
      "epoch:  202, loss: 0.023165056481957436\n",
      "epoch:  203, loss: 0.023161059245467186\n",
      "epoch:  204, loss: 0.022678781300783157\n",
      "epoch:  205, loss: 0.02251303382217884\n",
      "epoch:  206, loss: 0.022430427372455597\n",
      "epoch:  207, loss: 0.022034762427210808\n",
      "epoch:  208, loss: 0.021856555715203285\n",
      "epoch:  209, loss: 0.02177167683839798\n",
      "epoch:  210, loss: 0.021361365914344788\n",
      "epoch:  211, loss: 0.021184928715229034\n",
      "epoch:  212, loss: 0.021101191639900208\n",
      "epoch:  213, loss: 0.020688233897089958\n",
      "epoch:  214, loss: 0.02050994336605072\n",
      "epoch:  215, loss: 0.020428262650966644\n",
      "epoch:  216, loss: 0.01999570056796074\n",
      "epoch:  217, loss: 0.019831586629152298\n",
      "epoch:  218, loss: 0.019779695197939873\n",
      "epoch:  219, loss: 0.01928839273750782\n",
      "epoch:  220, loss: 0.01915280893445015\n",
      "epoch:  221, loss: 0.01900002919137478\n",
      "epoch:  222, loss: 0.01858862116932869\n",
      "epoch:  223, loss: 0.01847945898771286\n",
      "epoch:  224, loss: 0.01816626265645027\n",
      "epoch:  225, loss: 0.01790422946214676\n",
      "epoch:  226, loss: 0.017822396010160446\n",
      "epoch:  227, loss: 0.017416566610336304\n",
      "epoch:  228, loss: 0.01724925823509693\n",
      "epoch:  229, loss: 0.0172403734177351\n",
      "epoch:  230, loss: 0.016721608117222786\n",
      "epoch:  231, loss: 0.01661810837686062\n",
      "epoch:  232, loss: 0.01633763127028942\n",
      "epoch:  233, loss: 0.016074251383543015\n",
      "epoch:  234, loss: 0.016004230827093124\n",
      "epoch:  235, loss: 0.015579869970679283\n",
      "epoch:  236, loss: 0.015464872121810913\n",
      "epoch:  237, loss: 0.015231759287416935\n",
      "epoch:  238, loss: 0.014952662400901318\n",
      "epoch:  239, loss: 0.014891078695654869\n",
      "epoch:  240, loss: 0.014483890496194363\n",
      "epoch:  241, loss: 0.014393267221748829\n",
      "epoch:  242, loss: 0.01409445982426405\n",
      "epoch:  243, loss: 0.013918943703174591\n",
      "epoch:  244, loss: 0.01380081381648779\n",
      "epoch:  245, loss: 0.01346911396831274\n",
      "epoch:  246, loss: 0.01341632567346096\n",
      "epoch:  247, loss: 0.013053646311163902\n",
      "epoch:  248, loss: 0.012983200140297413\n",
      "epoch:  249, loss: 0.012667196802794933\n",
      "epoch:  250, loss: 0.012571810744702816\n",
      "epoch:  251, loss: 0.012299719266593456\n",
      "epoch:  252, loss: 0.01218270044773817\n",
      "epoch:  253, loss: 0.0119444215670228\n",
      "epoch:  254, loss: 0.011813793331384659\n",
      "epoch:  255, loss: 0.011594431474804878\n",
      "epoch:  256, loss: 0.011462802067399025\n",
      "epoch:  257, loss: 0.011219250969588757\n",
      "epoch:  258, loss: 0.011130272410809994\n",
      "epoch:  259, loss: 0.010891553945839405\n",
      "epoch:  260, loss: 0.010818123817443848\n",
      "epoch:  261, loss: 0.010403872467577457\n",
      "epoch:  262, loss: 0.008676944300532341\n",
      "epoch:  263, loss: 0.008661886677145958\n",
      "epoch:  264, loss: 0.008111986331641674\n",
      "epoch:  265, loss: 0.007848517969250679\n",
      "epoch:  266, loss: 0.007838479243218899\n",
      "epoch:  267, loss: 0.007779744919389486\n",
      "epoch:  268, loss: 0.007770296186208725\n",
      "epoch:  269, loss: 0.007727170363068581\n",
      "epoch:  270, loss: 0.007716032676398754\n",
      "epoch:  271, loss: 0.007690167520195246\n",
      "epoch:  272, loss: 0.007671109866350889\n",
      "epoch:  273, loss: 0.007666891906410456\n",
      "epoch:  274, loss: 0.007535204757004976\n",
      "epoch:  275, loss: 0.007388819940388203\n",
      "epoch:  276, loss: 0.007377807050943375\n",
      "epoch:  277, loss: 0.007374216802418232\n",
      "epoch:  278, loss: 0.0073465146124362946\n",
      "epoch:  279, loss: 0.007341951131820679\n",
      "epoch:  280, loss: 0.0073305945843458176\n",
      "epoch:  281, loss: 0.007316851522773504\n",
      "epoch:  282, loss: 0.007314157672226429\n",
      "epoch:  283, loss: 0.007297626696527004\n",
      "epoch:  284, loss: 0.007292077410966158\n",
      "epoch:  285, loss: 0.007290022913366556\n",
      "epoch:  286, loss: 0.007273433264344931\n",
      "epoch:  287, loss: 0.007270092144608498\n",
      "epoch:  288, loss: 0.007263293955475092\n",
      "epoch:  289, loss: 0.007251584902405739\n",
      "epoch:  290, loss: 0.007249291054904461\n",
      "epoch:  291, loss: 0.007233675569295883\n",
      "epoch:  292, loss: 0.007230621762573719\n",
      "epoch:  293, loss: 0.007222430314868689\n",
      "epoch:  294, loss: 0.007212870288640261\n",
      "epoch:  295, loss: 0.007210799027234316\n",
      "epoch:  296, loss: 0.007198011502623558\n",
      "epoch:  297, loss: 0.007193229626864195\n",
      "epoch:  298, loss: 0.007191510871052742\n",
      "epoch:  299, loss: 0.007179711479693651\n",
      "epoch:  300, loss: 0.007174772676080465\n",
      "epoch:  301, loss: 0.007173080928623676\n",
      "epoch:  302, loss: 0.007161919493228197\n",
      "epoch:  303, loss: 0.007157283369451761\n",
      "epoch:  304, loss: 0.007155593018978834\n",
      "epoch:  305, loss: 0.00714294845238328\n",
      "epoch:  306, loss: 0.007140121888369322\n",
      "epoch:  307, loss: 0.007138616871088743\n",
      "epoch:  308, loss: 0.007118838839232922\n",
      "epoch:  309, loss: 0.007012674119323492\n",
      "epoch:  310, loss: 0.007003047503530979\n",
      "epoch:  311, loss: 0.007000665180385113\n",
      "epoch:  312, loss: 0.006989976391196251\n",
      "epoch:  313, loss: 0.0069830031134188175\n",
      "epoch:  314, loss: 0.006981099024415016\n",
      "epoch:  315, loss: 0.00697066355496645\n",
      "epoch:  316, loss: 0.006965430453419685\n",
      "epoch:  317, loss: 0.006963716354221106\n",
      "epoch:  318, loss: 0.006955444347113371\n",
      "epoch:  319, loss: 0.006948490627110004\n",
      "epoch:  320, loss: 0.006946604698896408\n",
      "epoch:  321, loss: 0.006940783467143774\n",
      "epoch:  322, loss: 0.00693197175860405\n",
      "epoch:  323, loss: 0.006929969415068626\n",
      "epoch:  324, loss: 0.006922313943505287\n",
      "epoch:  325, loss: 0.00691527733579278\n",
      "epoch:  326, loss: 0.006913455668836832\n",
      "epoch:  327, loss: 0.0069094751961529255\n",
      "epoch:  328, loss: 0.006899427622556686\n",
      "epoch:  329, loss: 0.006897362414747477\n",
      "epoch:  330, loss: 0.006897162180393934\n",
      "epoch:  331, loss: 0.006883625872433186\n",
      "epoch:  332, loss: 0.006881135515868664\n",
      "epoch:  333, loss: 0.0068796658888459206\n",
      "epoch:  334, loss: 0.0068684774450957775\n",
      "epoch:  335, loss: 0.006864869501441717\n",
      "epoch:  336, loss: 0.006863316521048546\n",
      "epoch:  337, loss: 0.006856164429336786\n",
      "epoch:  338, loss: 0.006848654709756374\n",
      "epoch:  339, loss: 0.006846750155091286\n",
      "epoch:  340, loss: 0.0068395109847188\n",
      "epoch:  341, loss: 0.006830787751823664\n",
      "epoch:  342, loss: 0.006828533485531807\n",
      "epoch:  343, loss: 0.006821064744144678\n",
      "epoch:  344, loss: 0.006810495164245367\n",
      "epoch:  345, loss: 0.006807870697230101\n",
      "epoch:  346, loss: 0.0068040103651583195\n",
      "epoch:  347, loss: 0.006790244020521641\n",
      "epoch:  348, loss: 0.006787427701056004\n",
      "epoch:  349, loss: 0.006781876087188721\n",
      "epoch:  350, loss: 0.006768754217773676\n",
      "epoch:  351, loss: 0.006765862926840782\n",
      "epoch:  352, loss: 0.00676394859328866\n",
      "epoch:  353, loss: 0.0067475177347660065\n",
      "epoch:  354, loss: 0.0067443340085446835\n",
      "epoch:  355, loss: 0.006742430850863457\n",
      "epoch:  356, loss: 0.006726555991917849\n",
      "epoch:  357, loss: 0.006722819991409779\n",
      "epoch:  358, loss: 0.006720826495438814\n",
      "epoch:  359, loss: 0.0067054289393126965\n",
      "epoch:  360, loss: 0.00670107826590538\n",
      "epoch:  361, loss: 0.006698889657855034\n",
      "epoch:  362, loss: 0.006686375010758638\n",
      "epoch:  363, loss: 0.006678839214146137\n",
      "epoch:  364, loss: 0.006676384713500738\n",
      "epoch:  365, loss: 0.00666972529143095\n",
      "epoch:  366, loss: 0.00665661646053195\n",
      "epoch:  367, loss: 0.0066535212099552155\n",
      "epoch:  368, loss: 0.00665146391838789\n",
      "epoch:  369, loss: 0.006636772770434618\n",
      "epoch:  370, loss: 0.006630489602684975\n",
      "epoch:  371, loss: 0.006628037896007299\n",
      "epoch:  372, loss: 0.006620906759053469\n",
      "epoch:  373, loss: 0.006607385817915201\n",
      "epoch:  374, loss: 0.006604051683098078\n",
      "epoch:  375, loss: 0.00660189613699913\n",
      "epoch:  376, loss: 0.006584372371435165\n",
      "epoch:  377, loss: 0.006580137647688389\n",
      "epoch:  378, loss: 0.00657784566283226\n",
      "epoch:  379, loss: 0.0065641868859529495\n",
      "epoch:  380, loss: 0.006556807551532984\n",
      "epoch:  381, loss: 0.006554250605404377\n",
      "epoch:  382, loss: 0.0065453387796878815\n",
      "epoch:  383, loss: 0.006533908657729626\n",
      "epoch:  384, loss: 0.006530757527798414\n",
      "epoch:  385, loss: 0.0065285684540867805\n",
      "epoch:  386, loss: 0.0065114907920360565\n",
      "epoch:  387, loss: 0.006506325211375952\n",
      "epoch:  388, loss: 0.006503770127892494\n",
      "epoch:  389, loss: 0.0064926124177873135\n",
      "epoch:  390, loss: 0.006480728276073933\n",
      "epoch:  391, loss: 0.006477183662354946\n",
      "epoch:  392, loss: 0.006474778521806002\n",
      "epoch:  393, loss: 0.006459178403019905\n",
      "epoch:  394, loss: 0.006450982764363289\n",
      "epoch:  395, loss: 0.006447892170399427\n",
      "epoch:  396, loss: 0.0064454637467861176\n",
      "epoch:  397, loss: 0.0064320736564695835\n",
      "epoch:  398, loss: 0.006422323174774647\n",
      "epoch:  399, loss: 0.006419233046472073\n",
      "epoch:  400, loss: 0.0064168162643909454\n",
      "epoch:  401, loss: 0.006400028243660927\n",
      "epoch:  402, loss: 0.006393289193511009\n",
      "epoch:  403, loss: 0.006390439812093973\n",
      "epoch:  404, loss: 0.006388100329786539\n",
      "epoch:  405, loss: 0.006373799871653318\n",
      "epoch:  406, loss: 0.00636511342599988\n",
      "epoch:  407, loss: 0.006362052168697119\n",
      "epoch:  408, loss: 0.006359538994729519\n",
      "epoch:  409, loss: 0.006345315370708704\n",
      "epoch:  410, loss: 0.006336130667477846\n",
      "epoch:  411, loss: 0.00633276579901576\n",
      "epoch:  412, loss: 0.006330232135951519\n",
      "epoch:  413, loss: 0.006320251617580652\n",
      "epoch:  414, loss: 0.006305514369159937\n",
      "epoch:  415, loss: 0.00630141980946064\n",
      "epoch:  416, loss: 0.00629856251180172\n",
      "epoch:  417, loss: 0.006284340750426054\n",
      "epoch:  418, loss: 0.006272959057241678\n",
      "epoch:  419, loss: 0.006269253324717283\n",
      "epoch:  420, loss: 0.0062665934674441814\n",
      "epoch:  421, loss: 0.006250523496419191\n",
      "epoch:  422, loss: 0.006241646595299244\n",
      "epoch:  423, loss: 0.006238208618015051\n",
      "epoch:  424, loss: 0.006235646549612284\n",
      "epoch:  425, loss: 0.006216926965862513\n",
      "epoch:  426, loss: 0.006210157182067633\n",
      "epoch:  427, loss: 0.006207210943102837\n",
      "epoch:  428, loss: 0.0062055326998233795\n",
      "epoch:  429, loss: 0.006185473408550024\n",
      "epoch:  430, loss: 0.006180144380778074\n",
      "epoch:  431, loss: 0.006177309900522232\n",
      "epoch:  432, loss: 0.006166961044073105\n",
      "epoch:  433, loss: 0.0061538126319646835\n",
      "epoch:  434, loss: 0.0061495667323470116\n",
      "epoch:  435, loss: 0.006146908272057772\n",
      "epoch:  436, loss: 0.006133399438112974\n",
      "epoch:  437, loss: 0.006122850812971592\n",
      "epoch:  438, loss: 0.006119142286479473\n",
      "epoch:  439, loss: 0.006116526201367378\n",
      "epoch:  440, loss: 0.006097231525927782\n",
      "epoch:  441, loss: 0.006091113667935133\n",
      "epoch:  442, loss: 0.006088105030357838\n",
      "epoch:  443, loss: 0.006080972030758858\n",
      "epoch:  444, loss: 0.006063866429030895\n",
      "epoch:  445, loss: 0.006058992352336645\n",
      "epoch:  446, loss: 0.006055970676243305\n",
      "epoch:  447, loss: 0.006045013200491667\n",
      "epoch:  448, loss: 0.00602946151047945\n",
      "epoch:  449, loss: 0.006024752743542194\n",
      "epoch:  450, loss: 0.006021752953529358\n",
      "epoch:  451, loss: 0.006011344026774168\n",
      "epoch:  452, loss: 0.0059960936196148396\n",
      "epoch:  453, loss: 0.005991318263113499\n",
      "epoch:  454, loss: 0.00598838459700346\n",
      "epoch:  455, loss: 0.005981306545436382\n",
      "epoch:  456, loss: 0.005964175332337618\n",
      "epoch:  457, loss: 0.005959063768386841\n",
      "epoch:  458, loss: 0.005956050008535385\n",
      "epoch:  459, loss: 0.005955600645393133\n",
      "epoch:  460, loss: 0.005933701992034912\n",
      "epoch:  461, loss: 0.0059275091625750065\n",
      "epoch:  462, loss: 0.00592442462220788\n",
      "epoch:  463, loss: 0.0059218150563538074\n",
      "epoch:  464, loss: 0.00590704008936882\n",
      "epoch:  465, loss: 0.005897459574043751\n",
      "epoch:  466, loss: 0.005893604829907417\n",
      "epoch:  467, loss: 0.005890870001167059\n",
      "epoch:  468, loss: 0.005888266023248434\n",
      "epoch:  469, loss: 0.005869234912097454\n",
      "epoch:  470, loss: 0.005863462574779987\n",
      "epoch:  471, loss: 0.005860407371073961\n",
      "epoch:  472, loss: 0.005857956595718861\n",
      "epoch:  473, loss: 0.005841745529323816\n",
      "epoch:  474, loss: 0.005834410898387432\n",
      "epoch:  475, loss: 0.005831222515553236\n",
      "epoch:  476, loss: 0.005828746594488621\n",
      "epoch:  477, loss: 0.005815972574055195\n",
      "epoch:  478, loss: 0.005806259345263243\n",
      "epoch:  479, loss: 0.005802702158689499\n",
      "epoch:  480, loss: 0.0058002476580441\n",
      "epoch:  481, loss: 0.005794771481305361\n",
      "epoch:  482, loss: 0.005780267063528299\n",
      "epoch:  483, loss: 0.005775591358542442\n",
      "epoch:  484, loss: 0.005772879347205162\n",
      "epoch:  485, loss: 0.005770630668848753\n",
      "epoch:  486, loss: 0.005754044279456139\n",
      "epoch:  487, loss: 0.005748001858592033\n",
      "epoch:  488, loss: 0.005744926165789366\n",
      "epoch:  489, loss: 0.0057425713166594505\n",
      "epoch:  490, loss: 0.005735263228416443\n",
      "epoch:  491, loss: 0.005721707828342915\n",
      "epoch:  492, loss: 0.005716907326132059\n",
      "epoch:  493, loss: 0.005714127793908119\n",
      "epoch:  494, loss: 0.005711812525987625\n",
      "epoch:  495, loss: 0.005704186391085386\n",
      "epoch:  496, loss: 0.00569101283326745\n",
      "epoch:  497, loss: 0.005686681251972914\n",
      "epoch:  498, loss: 0.005683974828571081\n",
      "epoch:  499, loss: 0.005681721493601799\n",
      "epoch:  500, loss: 0.005667871329933405\n",
      "epoch:  501, loss: 0.005659961607307196\n",
      "epoch:  502, loss: 0.00565634248778224\n",
      "epoch:  503, loss: 0.005653962958604097\n",
      "epoch:  504, loss: 0.005651728715747595\n",
      "epoch:  505, loss: 0.0056358641013503075\n",
      "epoch:  506, loss: 0.005629345308989286\n",
      "epoch:  507, loss: 0.005626276135444641\n",
      "epoch:  508, loss: 0.005623859819024801\n",
      "epoch:  509, loss: 0.005616186186671257\n",
      "epoch:  510, loss: 0.005603517405688763\n",
      "epoch:  511, loss: 0.005598926916718483\n",
      "epoch:  512, loss: 0.005596131086349487\n",
      "epoch:  513, loss: 0.00559384236112237\n",
      "epoch:  514, loss: 0.005580865778028965\n",
      "epoch:  515, loss: 0.005572265945374966\n",
      "epoch:  516, loss: 0.005568432621657848\n",
      "epoch:  517, loss: 0.005565923638641834\n",
      "epoch:  518, loss: 0.005563646554946899\n",
      "epoch:  519, loss: 0.0055498904548585415\n",
      "epoch:  520, loss: 0.00554173719137907\n",
      "epoch:  521, loss: 0.005538253579288721\n",
      "epoch:  522, loss: 0.005535723641514778\n",
      "epoch:  523, loss: 0.005535711068660021\n",
      "epoch:  524, loss: 0.005517501849681139\n",
      "epoch:  525, loss: 0.005511281080543995\n",
      "epoch:  526, loss: 0.005508056841790676\n",
      "epoch:  527, loss: 0.005505649838596582\n",
      "epoch:  528, loss: 0.005496471654623747\n",
      "epoch:  529, loss: 0.005485175643116236\n",
      "epoch:  530, loss: 0.005480523221194744\n",
      "epoch:  531, loss: 0.005477794446051121\n",
      "epoch:  532, loss: 0.005475437734276056\n",
      "epoch:  533, loss: 0.005470974370837212\n",
      "epoch:  534, loss: 0.005455912556499243\n",
      "epoch:  535, loss: 0.005450861062854528\n",
      "epoch:  536, loss: 0.005447801202535629\n",
      "epoch:  537, loss: 0.005445454735308886\n",
      "epoch:  538, loss: 0.005443185567855835\n",
      "epoch:  539, loss: 0.005428260657936335\n",
      "epoch:  540, loss: 0.005421014036983252\n",
      "epoch:  541, loss: 0.005417483858764172\n",
      "epoch:  542, loss: 0.005414870567619801\n",
      "epoch:  543, loss: 0.005412527360022068\n",
      "epoch:  544, loss: 0.005406423006206751\n",
      "epoch:  545, loss: 0.0053923968225717545\n",
      "epoch:  546, loss: 0.005387008655816317\n",
      "epoch:  547, loss: 0.0053838100284338\n",
      "epoch:  548, loss: 0.005381321534514427\n",
      "epoch:  549, loss: 0.00537898251786828\n",
      "epoch:  550, loss: 0.005362352356314659\n",
      "epoch:  551, loss: 0.005356055684387684\n",
      "epoch:  552, loss: 0.005352616310119629\n",
      "epoch:  553, loss: 0.005349970422685146\n",
      "epoch:  554, loss: 0.0053476132452487946\n",
      "epoch:  555, loss: 0.00533727603033185\n",
      "epoch:  556, loss: 0.005326442886143923\n",
      "epoch:  557, loss: 0.005321743432432413\n",
      "epoch:  558, loss: 0.005318807438015938\n",
      "epoch:  559, loss: 0.00531630078330636\n",
      "epoch:  560, loss: 0.005313944071531296\n",
      "epoch:  561, loss: 0.005297797732055187\n",
      "epoch:  562, loss: 0.005291021894663572\n",
      "epoch:  563, loss: 0.005287344567477703\n",
      "epoch:  564, loss: 0.005284575745463371\n",
      "epoch:  565, loss: 0.005282172001898289\n",
      "epoch:  566, loss: 0.005267488304525614\n",
      "epoch:  567, loss: 0.005258822813630104\n",
      "epoch:  568, loss: 0.005254372488707304\n",
      "epoch:  569, loss: 0.005251337308436632\n",
      "epoch:  570, loss: 0.0052486602216959\n",
      "epoch:  571, loss: 0.005238394718617201\n",
      "epoch:  572, loss: 0.005226072855293751\n",
      "epoch:  573, loss: 0.0052207219414412975\n",
      "epoch:  574, loss: 0.005217421799898148\n",
      "epoch:  575, loss: 0.0052147842943668365\n",
      "epoch:  576, loss: 0.005212294403463602\n",
      "epoch:  577, loss: 0.005195694509893656\n",
      "epoch:  578, loss: 0.005188146606087685\n",
      "epoch:  579, loss: 0.005184197798371315\n",
      "epoch:  580, loss: 0.005181280896067619\n",
      "epoch:  581, loss: 0.005178758874535561\n",
      "epoch:  582, loss: 0.005170581862330437\n",
      "epoch:  583, loss: 0.005156497471034527\n",
      "epoch:  584, loss: 0.005150573793798685\n",
      "epoch:  585, loss: 0.005147043615579605\n",
      "epoch:  586, loss: 0.0051442040130496025\n",
      "epoch:  587, loss: 0.0051372237503528595\n",
      "epoch:  588, loss: 0.005121591500937939\n",
      "epoch:  589, loss: 0.0051152706146240234\n",
      "epoch:  590, loss: 0.005111392587423325\n",
      "epoch:  591, loss: 0.005108432844281197\n",
      "epoch:  592, loss: 0.005105713382363319\n",
      "epoch:  593, loss: 0.005087821278721094\n",
      "epoch:  594, loss: 0.005079766735434532\n",
      "epoch:  595, loss: 0.0050755650736391544\n",
      "epoch:  596, loss: 0.005072334781289101\n",
      "epoch:  597, loss: 0.005069537088274956\n",
      "epoch:  598, loss: 0.005068075843155384\n",
      "epoch:  599, loss: 0.0050492966547608376\n",
      "epoch:  600, loss: 0.005041281692683697\n",
      "epoch:  601, loss: 0.005036894232034683\n",
      "epoch:  602, loss: 0.00503362575545907\n",
      "epoch:  603, loss: 0.005030796397477388\n",
      "epoch:  604, loss: 0.005028141662478447\n",
      "epoch:  605, loss: 0.0050188531167805195\n",
      "epoch:  606, loss: 0.0050054071471095085\n",
      "epoch:  607, loss: 0.004999242722988129\n",
      "epoch:  608, loss: 0.004995371215045452\n",
      "epoch:  609, loss: 0.004992337431758642\n",
      "epoch:  610, loss: 0.004989613778889179\n",
      "epoch:  611, loss: 0.004988992121070623\n",
      "epoch:  612, loss: 0.004969598725438118\n",
      "epoch:  613, loss: 0.004961633589118719\n",
      "epoch:  614, loss: 0.004957123659551144\n",
      "epoch:  615, loss: 0.004953825380653143\n",
      "epoch:  616, loss: 0.004951013717800379\n",
      "epoch:  617, loss: 0.004948324989527464\n",
      "epoch:  618, loss: 0.004941229708492756\n",
      "epoch:  619, loss: 0.00492652365937829\n",
      "epoch:  620, loss: 0.004919759463518858\n",
      "epoch:  621, loss: 0.004915595520287752\n",
      "epoch:  622, loss: 0.004912576638162136\n",
      "epoch:  623, loss: 0.004909766837954521\n",
      "epoch:  624, loss: 0.00490714143961668\n",
      "epoch:  625, loss: 0.004891806747764349\n",
      "epoch:  626, loss: 0.004882626235485077\n",
      "epoch:  627, loss: 0.004877578001469374\n",
      "epoch:  628, loss: 0.004874291829764843\n",
      "epoch:  629, loss: 0.004871405195444822\n",
      "epoch:  630, loss: 0.004868726711720228\n",
      "epoch:  631, loss: 0.004848817829042673\n",
      "epoch:  632, loss: 0.004842590540647507\n",
      "epoch:  633, loss: 0.004838377702981234\n",
      "epoch:  634, loss: 0.004835331346839666\n",
      "epoch:  635, loss: 0.004832437727600336\n",
      "epoch:  636, loss: 0.004830230493098497\n",
      "epoch:  637, loss: 0.004811703693121672\n",
      "epoch:  638, loss: 0.004803289659321308\n",
      "epoch:  639, loss: 0.004798858892172575\n",
      "epoch:  640, loss: 0.004795501008629799\n",
      "epoch:  641, loss: 0.004792443010956049\n",
      "epoch:  642, loss: 0.0047896429896354675\n",
      "epoch:  643, loss: 0.00477547338232398\n",
      "epoch:  644, loss: 0.004764558281749487\n",
      "epoch:  645, loss: 0.004759060684591532\n",
      "epoch:  646, loss: 0.004755403846502304\n",
      "epoch:  647, loss: 0.004752302076667547\n",
      "epoch:  648, loss: 0.00474949786439538\n",
      "epoch:  649, loss: 0.004748194478452206\n",
      "epoch:  650, loss: 0.0047294567339122295\n",
      "epoch:  651, loss: 0.0047205593436956406\n",
      "epoch:  652, loss: 0.004715221468359232\n",
      "epoch:  653, loss: 0.004711504559963942\n",
      "epoch:  654, loss: 0.0047082118690013885\n",
      "epoch:  655, loss: 0.00470516923815012\n",
      "epoch:  656, loss: 0.004702254664152861\n",
      "epoch:  657, loss: 0.00468450179323554\n",
      "epoch:  658, loss: 0.004675406496971846\n",
      "epoch:  659, loss: 0.004670141264796257\n",
      "epoch:  660, loss: 0.0046661775559186935\n",
      "epoch:  661, loss: 0.004662808962166309\n",
      "epoch:  662, loss: 0.004659831058233976\n",
      "epoch:  663, loss: 0.00465693324804306\n",
      "epoch:  664, loss: 0.004647304303944111\n",
      "epoch:  665, loss: 0.004634285345673561\n",
      "epoch:  666, loss: 0.004626785404980183\n",
      "epoch:  667, loss: 0.004621998872607946\n",
      "epoch:  668, loss: 0.004618478007614613\n",
      "epoch:  669, loss: 0.004615326412022114\n",
      "epoch:  670, loss: 0.004612363409250975\n",
      "epoch:  671, loss: 0.004608917981386185\n",
      "epoch:  672, loss: 0.004591851960867643\n",
      "epoch:  673, loss: 0.004583073314279318\n",
      "epoch:  674, loss: 0.004577640909701586\n",
      "epoch:  675, loss: 0.004573670681566\n",
      "epoch:  676, loss: 0.004570348653942347\n",
      "epoch:  677, loss: 0.004567319061607122\n",
      "epoch:  678, loss: 0.0045644319616258144\n",
      "epoch:  679, loss: 0.004555308725684881\n",
      "epoch:  680, loss: 0.004542018286883831\n",
      "epoch:  681, loss: 0.004534706007689238\n",
      "epoch:  682, loss: 0.004529847763478756\n",
      "epoch:  683, loss: 0.004526112228631973\n",
      "epoch:  684, loss: 0.004522926639765501\n",
      "epoch:  685, loss: 0.004519963636994362\n",
      "epoch:  686, loss: 0.0045171030797064304\n",
      "epoch:  687, loss: 0.004503258969634771\n",
      "epoch:  688, loss: 0.004492717795073986\n",
      "epoch:  689, loss: 0.004486417863518\n",
      "epoch:  690, loss: 0.0044820355251431465\n",
      "epoch:  691, loss: 0.00447856355458498\n",
      "epoch:  692, loss: 0.004475405439734459\n",
      "epoch:  693, loss: 0.004472491331398487\n",
      "epoch:  694, loss: 0.004469602834433317\n",
      "epoch:  695, loss: 0.004456595983356237\n",
      "epoch:  696, loss: 0.004445632919669151\n",
      "epoch:  697, loss: 0.004439071286469698\n",
      "epoch:  698, loss: 0.004434509668499231\n",
      "epoch:  699, loss: 0.004430861212313175\n",
      "epoch:  700, loss: 0.004427672829478979\n",
      "epoch:  701, loss: 0.0044246516190469265\n",
      "epoch:  702, loss: 0.0044217477552592754\n",
      "epoch:  703, loss: 0.004419983830302954\n",
      "epoch:  704, loss: 0.004403639584779739\n",
      "epoch:  705, loss: 0.004394154530018568\n",
      "epoch:  706, loss: 0.004388143308460712\n",
      "epoch:  707, loss: 0.004383742343634367\n",
      "epoch:  708, loss: 0.004380205180495977\n",
      "epoch:  709, loss: 0.004377022385597229\n",
      "epoch:  710, loss: 0.004374024458229542\n",
      "epoch:  711, loss: 0.0043711005710065365\n",
      "epoch:  712, loss: 0.004368238616734743\n",
      "epoch:  713, loss: 0.00435902364552021\n",
      "epoch:  714, loss: 0.004346773028373718\n",
      "epoch:  715, loss: 0.004339433740824461\n",
      "epoch:  716, loss: 0.004334382247179747\n",
      "epoch:  717, loss: 0.004330211319029331\n",
      "epoch:  718, loss: 0.004326791036874056\n",
      "epoch:  719, loss: 0.00432368041947484\n",
      "epoch:  720, loss: 0.0043207756243646145\n",
      "epoch:  721, loss: 0.00431791553273797\n",
      "epoch:  722, loss: 0.004315502010285854\n",
      "epoch:  723, loss: 0.004300589673221111\n",
      "epoch:  724, loss: 0.004291593562811613\n",
      "epoch:  725, loss: 0.004285633098334074\n",
      "epoch:  726, loss: 0.004281076602637768\n",
      "epoch:  727, loss: 0.004277316853404045\n",
      "epoch:  728, loss: 0.0042741019278764725\n",
      "epoch:  729, loss: 0.0042709847912192345\n",
      "epoch:  730, loss: 0.004268063697963953\n",
      "epoch:  731, loss: 0.004265174735337496\n",
      "epoch:  732, loss: 0.00426073931157589\n",
      "epoch:  733, loss: 0.00424700602889061\n",
      "epoch:  734, loss: 0.004238452762365341\n",
      "epoch:  735, loss: 0.004232720006257296\n",
      "epoch:  736, loss: 0.004228244535624981\n",
      "epoch:  737, loss: 0.004224620293825865\n",
      "epoch:  738, loss: 0.004221322946250439\n",
      "epoch:  739, loss: 0.004218248650431633\n",
      "epoch:  740, loss: 0.00421527773141861\n",
      "epoch:  741, loss: 0.004212408792227507\n",
      "epoch:  742, loss: 0.004209591541439295\n",
      "epoch:  743, loss: 0.0041971635073423386\n",
      "epoch:  744, loss: 0.004187801852822304\n",
      "epoch:  745, loss: 0.00418140459805727\n",
      "epoch:  746, loss: 0.0041766599752008915\n",
      "epoch:  747, loss: 0.004172803368419409\n",
      "epoch:  748, loss: 0.004169480875134468\n",
      "epoch:  749, loss: 0.004166305996477604\n",
      "epoch:  750, loss: 0.004163392819464207\n",
      "epoch:  751, loss: 0.004160500597208738\n",
      "epoch:  752, loss: 0.004157694987952709\n",
      "epoch:  753, loss: 0.00415493780747056\n",
      "epoch:  754, loss: 0.004143826197832823\n",
      "epoch:  755, loss: 0.004134293179959059\n",
      "epoch:  756, loss: 0.004127861466258764\n",
      "epoch:  757, loss: 0.004122935235500336\n",
      "epoch:  758, loss: 0.0041191126219928265\n",
      "epoch:  759, loss: 0.004115499090403318\n",
      "epoch:  760, loss: 0.004112317226827145\n",
      "epoch:  761, loss: 0.0041093467734754086\n",
      "epoch:  762, loss: 0.004106524400413036\n",
      "epoch:  763, loss: 0.004103758838027716\n",
      "epoch:  764, loss: 0.0041010212153196335\n",
      "epoch:  765, loss: 0.0040914155542850494\n",
      "epoch:  766, loss: 0.004081538412719965\n",
      "epoch:  767, loss: 0.004074566066265106\n",
      "epoch:  768, loss: 0.004069286398589611\n",
      "epoch:  769, loss: 0.004064999055117369\n",
      "epoch:  770, loss: 0.004061560146510601\n",
      "epoch:  771, loss: 0.004058332648128271\n",
      "epoch:  772, loss: 0.004055353347212076\n",
      "epoch:  773, loss: 0.004052476491779089\n",
      "epoch:  774, loss: 0.004049680661410093\n",
      "epoch:  775, loss: 0.004046933259814978\n",
      "epoch:  776, loss: 0.004037732724100351\n",
      "epoch:  777, loss: 0.004027192946523428\n",
      "epoch:  778, loss: 0.0040203663520514965\n",
      "epoch:  779, loss: 0.004014636855572462\n",
      "epoch:  780, loss: 0.004010932520031929\n",
      "epoch:  781, loss: 0.0040075513534247875\n",
      "epoch:  782, loss: 0.004004480317234993\n",
      "epoch:  783, loss: 0.004001562483608723\n",
      "epoch:  784, loss: 0.00399875408038497\n",
      "epoch:  785, loss: 0.003996009938418865\n",
      "epoch:  786, loss: 0.003993303515017033\n",
      "epoch:  787, loss: 0.003984022885560989\n",
      "epoch:  788, loss: 0.003974040038883686\n",
      "epoch:  789, loss: 0.003966907039284706\n",
      "epoch:  790, loss: 0.003961961250752211\n",
      "epoch:  791, loss: 0.003957642707973719\n",
      "epoch:  792, loss: 0.003954315558075905\n",
      "epoch:  793, loss: 0.003951183054596186\n",
      "epoch:  794, loss: 0.003948368597775698\n",
      "epoch:  795, loss: 0.003946408629417419\n",
      "epoch:  796, loss: 0.003933278378099203\n",
      "epoch:  797, loss: 0.003925368655472994\n",
      "epoch:  798, loss: 0.003919078037142754\n",
      "epoch:  799, loss: 0.003914594650268555\n",
      "epoch:  800, loss: 0.003910639323294163\n",
      "epoch:  801, loss: 0.003907178528606892\n",
      "epoch:  802, loss: 0.0039040150586515665\n",
      "epoch:  803, loss: 0.003901073709130287\n",
      "epoch:  804, loss: 0.0038982387632131577\n",
      "epoch:  805, loss: 0.003895494854077697\n",
      "epoch:  806, loss: 0.0038927753921598196\n",
      "epoch:  807, loss: 0.0038900929503142834\n",
      "epoch:  808, loss: 0.0038884582463651896\n",
      "epoch:  809, loss: 0.0038772830739617348\n",
      "epoch:  810, loss: 0.0038690129294991493\n",
      "epoch:  811, loss: 0.003862790996208787\n",
      "epoch:  812, loss: 0.0038580528926104307\n",
      "epoch:  813, loss: 0.003853958798572421\n",
      "epoch:  814, loss: 0.003850353416055441\n",
      "epoch:  815, loss: 0.0038471550215035677\n",
      "epoch:  816, loss: 0.0038441172800958157\n",
      "epoch:  817, loss: 0.0038413212168961763\n",
      "epoch:  818, loss: 0.0038385423831641674\n",
      "epoch:  819, loss: 0.003835903014987707\n",
      "epoch:  820, loss: 0.003833240829408169\n",
      "epoch:  821, loss: 0.0038306203205138445\n",
      "epoch:  822, loss: 0.0038280359003692865\n",
      "epoch:  823, loss: 0.0038238419219851494\n",
      "epoch:  824, loss: 0.0038142697885632515\n",
      "epoch:  825, loss: 0.0038074806798249483\n",
      "epoch:  826, loss: 0.0038017951883375645\n",
      "epoch:  827, loss: 0.0037972747813910246\n",
      "epoch:  828, loss: 0.003793293610215187\n",
      "epoch:  829, loss: 0.0037897005677223206\n",
      "epoch:  830, loss: 0.0037864400073885918\n",
      "epoch:  831, loss: 0.0037834299728274345\n",
      "epoch:  832, loss: 0.0037805819883942604\n",
      "epoch:  833, loss: 0.0037777996622025967\n",
      "epoch:  834, loss: 0.0037751318886876106\n",
      "epoch:  835, loss: 0.00377248739823699\n",
      "epoch:  836, loss: 0.003769875271245837\n",
      "epoch:  837, loss: 0.0037673020269721746\n",
      "epoch:  838, loss: 0.0037647366989403963\n",
      "epoch:  839, loss: 0.0037622093223035336\n",
      "epoch:  840, loss: 0.0037607813719660044\n",
      "epoch:  841, loss: 0.0037511044647544622\n",
      "epoch:  842, loss: 0.0037439996376633644\n",
      "epoch:  843, loss: 0.003737939056009054\n",
      "epoch:  844, loss: 0.0037333571817725897\n",
      "epoch:  845, loss: 0.0037290812470018864\n",
      "epoch:  846, loss: 0.0037256234791129827\n",
      "epoch:  847, loss: 0.0037221431266516447\n",
      "epoch:  848, loss: 0.00371926580555737\n",
      "epoch:  849, loss: 0.0037162017542868853\n",
      "epoch:  850, loss: 0.0037134194280952215\n",
      "epoch:  851, loss: 0.003710638266056776\n",
      "epoch:  852, loss: 0.003708029631525278\n",
      "epoch:  853, loss: 0.0037053916603326797\n",
      "epoch:  854, loss: 0.0037028160877525806\n",
      "epoch:  855, loss: 0.0037002710159868\n",
      "epoch:  856, loss: 0.0036977657582610846\n",
      "epoch:  857, loss: 0.003695263061672449\n",
      "epoch:  858, loss: 0.0036927766632288694\n",
      "epoch:  859, loss: 0.0036865512374788523\n",
      "epoch:  860, loss: 0.003679319517686963\n",
      "epoch:  861, loss: 0.0036732254084199667\n",
      "epoch:  862, loss: 0.003668410237878561\n",
      "epoch:  863, loss: 0.0036640255711972713\n",
      "epoch:  864, loss: 0.0036602800246328115\n",
      "epoch:  865, loss: 0.003656758926808834\n",
      "epoch:  866, loss: 0.003653577296063304\n",
      "epoch:  867, loss: 0.003650508588179946\n",
      "epoch:  868, loss: 0.0036477469839155674\n",
      "epoch:  869, loss: 0.003644949523732066\n",
      "epoch:  870, loss: 0.0036422917619347572\n",
      "epoch:  871, loss: 0.0036396754439920187\n",
      "epoch:  872, loss: 0.0036371522583067417\n",
      "epoch:  873, loss: 0.003634604625403881\n",
      "epoch:  874, loss: 0.003632096340879798\n",
      "epoch:  875, loss: 0.0036296132020652294\n",
      "epoch:  876, loss: 0.0036271780263632536\n",
      "epoch:  877, loss: 0.003624713746830821\n",
      "epoch:  878, loss: 0.0036222522612661123\n",
      "epoch:  879, loss: 0.0036198103334754705\n",
      "epoch:  880, loss: 0.003617075039073825\n",
      "epoch:  881, loss: 0.003609025152400136\n",
      "epoch:  882, loss: 0.003603041870519519\n",
      "epoch:  883, loss: 0.003597529139369726\n",
      "epoch:  884, loss: 0.003592987544834614\n",
      "epoch:  885, loss: 0.0035888878628611565\n",
      "epoch:  886, loss: 0.003585376776754856\n",
      "epoch:  887, loss: 0.003581975121051073\n",
      "epoch:  888, loss: 0.003579026088118553\n",
      "epoch:  889, loss: 0.0035760337486863136\n",
      "epoch:  890, loss: 0.0035733424592763186\n",
      "epoch:  891, loss: 0.0035705931950360537\n",
      "epoch:  892, loss: 0.0035679421853274107\n",
      "epoch:  893, loss: 0.003565344261005521\n",
      "epoch:  894, loss: 0.003562824334949255\n",
      "epoch:  895, loss: 0.0035603416617959738\n",
      "epoch:  896, loss: 0.003557895077392459\n",
      "epoch:  897, loss: 0.003555449191480875\n",
      "epoch:  898, loss: 0.003553023561835289\n",
      "epoch:  899, loss: 0.0035506021231412888\n",
      "epoch:  900, loss: 0.0035482090897858143\n",
      "epoch:  901, loss: 0.003545813262462616\n",
      "epoch:  902, loss: 0.003545625600963831\n",
      "epoch:  903, loss: 0.0035377764143049717\n",
      "epoch:  904, loss: 0.003531644120812416\n",
      "epoch:  905, loss: 0.003526114160194993\n",
      "epoch:  906, loss: 0.003521334147080779\n",
      "epoch:  907, loss: 0.0035171026829630136\n",
      "epoch:  908, loss: 0.0035133222118020058\n",
      "epoch:  909, loss: 0.003509865142405033\n",
      "epoch:  910, loss: 0.00350663042627275\n",
      "epoch:  911, loss: 0.0035036432091146708\n",
      "epoch:  912, loss: 0.0035007542464882135\n",
      "epoch:  913, loss: 0.003497942117974162\n",
      "epoch:  914, loss: 0.003495309269055724\n",
      "epoch:  915, loss: 0.0034926962107419968\n",
      "epoch:  916, loss: 0.0034901925828307867\n",
      "epoch:  917, loss: 0.0034876607824116945\n",
      "epoch:  918, loss: 0.003485240740701556\n",
      "epoch:  919, loss: 0.00348276412114501\n",
      "epoch:  920, loss: 0.0034803247544914484\n",
      "epoch:  921, loss: 0.003477903315797448\n",
      "epoch:  922, loss: 0.003475488629192114\n",
      "epoch:  923, loss: 0.003473107237368822\n",
      "epoch:  924, loss: 0.0034707263112068176\n",
      "epoch:  925, loss: 0.0034683705307543278\n",
      "epoch:  926, loss: 0.003466014051809907\n",
      "epoch:  927, loss: 0.0034636729396879673\n",
      "epoch:  928, loss: 0.0034613332245498896\n",
      "epoch:  929, loss: 0.0034589972347021103\n",
      "epoch:  930, loss: 0.0034566605463624\n",
      "epoch:  931, loss: 0.0034543268848210573\n",
      "epoch:  932, loss: 0.0034520027693361044\n",
      "epoch:  933, loss: 0.003449679585173726\n",
      "epoch:  934, loss: 0.0034477505832910538\n",
      "epoch:  935, loss: 0.0034411242231726646\n",
      "epoch:  936, loss: 0.0034355041570961475\n",
      "epoch:  937, loss: 0.0034305292647331953\n",
      "epoch:  938, loss: 0.0034262281842529774\n",
      "epoch:  939, loss: 0.0034222195390611887\n",
      "epoch:  940, loss: 0.0034186847042292356\n",
      "epoch:  941, loss: 0.003415292827412486\n",
      "epoch:  942, loss: 0.003412226215004921\n",
      "epoch:  943, loss: 0.0034092217683792114\n",
      "epoch:  944, loss: 0.003406452015042305\n",
      "epoch:  945, loss: 0.0034036850556731224\n",
      "epoch:  946, loss: 0.003401096211746335\n",
      "epoch:  947, loss: 0.0033984703477472067\n",
      "epoch:  948, loss: 0.0033960083965212107\n",
      "epoch:  949, loss: 0.0033934800885617733\n",
      "epoch:  950, loss: 0.0033910751808434725\n",
      "epoch:  951, loss: 0.0033886253368109465\n",
      "epoch:  952, loss: 0.003386277938261628\n",
      "epoch:  953, loss: 0.0033838809467852116\n",
      "epoch:  954, loss: 0.003381580114364624\n",
      "epoch:  955, loss: 0.0033792180474847555\n",
      "epoch:  956, loss: 0.0033769269939512014\n",
      "epoch:  957, loss: 0.003374558174982667\n",
      "epoch:  958, loss: 0.0033722585067152977\n",
      "epoch:  959, loss: 0.003369911340996623\n",
      "epoch:  960, loss: 0.0033675802405923605\n",
      "epoch:  961, loss: 0.0033652624115347862\n",
      "epoch:  962, loss: 0.0033629548270255327\n",
      "epoch:  963, loss: 0.0033606574870646\n",
      "epoch:  964, loss: 0.0033583692274987698\n",
      "epoch:  965, loss: 0.0033560844603925943\n",
      "epoch:  966, loss: 0.0033537966664880514\n",
      "epoch:  967, loss: 0.0033502825535833836\n",
      "epoch:  968, loss: 0.003344164229929447\n",
      "epoch:  969, loss: 0.003339658956974745\n",
      "epoch:  970, loss: 0.003334681037813425\n",
      "epoch:  971, loss: 0.0033309555146843195\n",
      "epoch:  972, loss: 0.003327449318021536\n",
      "epoch:  973, loss: 0.0033241950441151857\n",
      "epoch:  974, loss: 0.003321070922538638\n",
      "epoch:  975, loss: 0.0033180660102516413\n",
      "epoch:  976, loss: 0.0033152394462376833\n",
      "epoch:  977, loss: 0.0033124887850135565\n",
      "epoch:  978, loss: 0.003309779567644\n",
      "epoch:  979, loss: 0.003307163016870618\n",
      "epoch:  980, loss: 0.003304610028862953\n",
      "epoch:  981, loss: 0.0033021315466612577\n",
      "epoch:  982, loss: 0.003299663309007883\n",
      "epoch:  983, loss: 0.003297208808362484\n",
      "epoch:  984, loss: 0.003294798079878092\n",
      "epoch:  985, loss: 0.0032924094703048468\n",
      "epoch:  986, loss: 0.0032900720834732056\n",
      "epoch:  987, loss: 0.0032877398189157248\n",
      "epoch:  988, loss: 0.003285449231043458\n",
      "epoch:  989, loss: 0.0032831262797117233\n",
      "epoch:  990, loss: 0.003280868288129568\n",
      "epoch:  991, loss: 0.003278569784015417\n",
      "epoch:  992, loss: 0.0032763087656348944\n",
      "epoch:  993, loss: 0.0032740288879722357\n",
      "epoch:  994, loss: 0.003271793946623802\n",
      "epoch:  995, loss: 0.0032695308327674866\n",
      "epoch:  996, loss: 0.0032673177774995565\n",
      "epoch:  997, loss: 0.0032650786451995373\n",
      "epoch:  998, loss: 0.0032628863118588924\n",
      "epoch:  999, loss: 0.0032606488093733788\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size=X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# opt = torch_numopt.ConjugateGradient(params=model.parameters(), model=model, lr=1e-4, line_search_method=\"const\", cg_method=\"FR\")\n",
    "opt = torch_numopt.ConjugateGradient(params=model.parameters(), model=model, lr=10, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"armijo\", cg_method=\"FR\")\n",
    "# opt = torch_numopt.ConjugateGradient(params=model.parameters(), model=model, lr=10, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"wolfe\", cg_method=\"FR\")\n",
    "# opt = torch_numopt.ConjugateGradient(params=model.parameters(), model=model, lr=10, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"strong-wolfe\", cg_method=\"FR\")\n",
    "# opt = torch_numopt.ConjugateGradient(params=model.parameters(), model=model, lr=10, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"goldstein\", cg_method=\"FR\")\n",
    "\n",
    "all_loss = {}\n",
    "for epoch in range(1000):\n",
    "    print(\"epoch: \", epoch, end=\"\")\n",
    "    all_loss[epoch + 1] = 0\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y, loss_fn)\n",
    "\n",
    "        all_loss[epoch + 1] += loss\n",
    "    all_loss[epoch + 1] /= len(data_loader)\n",
    "    print(\", loss: {}\".format(all_loss[epoch + 1].detach().numpy().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics: R2 = 0.8883654709902937\n",
      "Test metrics:  R2 = 0.8763749073839097\n"
     ]
    }
   ],
   "source": [
    "pred_train = model.forward(X_train).detach()\n",
    "pred_test = model.forward(X_test).detach()\n",
    "print(f\"Train metrics: R2 = {r2_score(pred_train, y_train)}\")\n",
    "print(f\"Test metrics:  R2 = {r2_score(pred_test, y_test)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
