{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch_numopt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import *\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.f1 = nn.Linear(input_size, 10, device=device)\n",
    "        self.f2 = nn.Linear(10, 20, device=device)\n",
    "        self.f3 = nn.Linear(20, 20, device=device)\n",
    "        self.f4 = nn.Linear(20, 10, device=device)\n",
    "        self.f5 = nn.Linear(10, 1, device=device)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        # self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.f1(x))\n",
    "        x = self.activation(self.f2(x))\n",
    "        x = self.activation(self.f3(x))\n",
    "        x = self.activation(self.f4(x))\n",
    "        x = self.f5(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([800, 10]) torch.Size([800, 1])\n",
      "torch.Size([200, 10]) torch.Size([200, 1])\n"
     ]
    }
   ],
   "source": [
    "# X, y = load_diabetes(return_X_y=True, scaled=False)\n",
    "# X, y = make_regression(n_samples=1000, n_features=100)\n",
    "X, y = make_friedman1(n_samples=1000, noise=1e-2)\n",
    "\n",
    "X_scaler = MinMaxScaler()\n",
    "X = X_scaler.fit_transform(X)\n",
    "X = torch.Tensor(X).to(device)\n",
    "\n",
    "y_scaler = MinMaxScaler()\n",
    "y = y_scaler.fit_transform(y.reshape((-1, 1)))\n",
    "y = torch.Tensor(y).to(device)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "torch_data = TensorDataset(X_train, y_train)\n",
    "data_loader = DataLoader(torch_data, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.4928751289844513\n",
      "epoch:  1, loss: 0.2277892678976059\n",
      "epoch:  2, loss: 0.20452290773391724\n",
      "epoch:  3, loss: 0.11910425126552582\n",
      "epoch:  4, loss: 0.014592969790101051\n",
      "epoch:  5, loss: 0.010401159524917603\n",
      "epoch:  6, loss: 0.010214787907898426\n",
      "epoch:  7, loss: 0.00910700298845768\n",
      "epoch:  8, loss: 0.008887479081749916\n",
      "epoch:  9, loss: 0.00857711210846901\n",
      "epoch:  10, loss: 0.008355841971933842\n",
      "epoch:  11, loss: 0.008006341755390167\n",
      "epoch:  12, loss: 0.0076352995820343494\n",
      "epoch:  13, loss: 0.006370640359818935\n",
      "epoch:  14, loss: 0.006293326150625944\n",
      "epoch:  15, loss: 0.005120198242366314\n",
      "epoch:  16, loss: 0.004983052145689726\n",
      "epoch:  17, loss: 0.004961550701409578\n",
      "epoch:  18, loss: 0.004648225381970406\n",
      "epoch:  19, loss: 0.00454372214153409\n",
      "epoch:  20, loss: 0.0044871787540614605\n",
      "epoch:  21, loss: 0.004363866522908211\n",
      "epoch:  22, loss: 0.004291203338652849\n",
      "epoch:  23, loss: 0.0041812751442193985\n",
      "epoch:  24, loss: 0.004168940242379904\n",
      "epoch:  25, loss: 0.004072027280926704\n",
      "epoch:  26, loss: 0.004043850116431713\n",
      "epoch:  27, loss: 0.004027456510812044\n",
      "epoch:  28, loss: 0.003988453187048435\n",
      "epoch:  29, loss: 0.003957081586122513\n",
      "epoch:  30, loss: 0.0039213234558701515\n",
      "epoch:  31, loss: 0.003902171039953828\n",
      "epoch:  32, loss: 0.003869276028126478\n",
      "epoch:  33, loss: 0.003851383225992322\n",
      "epoch:  34, loss: 0.003823780920356512\n",
      "epoch:  35, loss: 0.0038120627868920565\n",
      "epoch:  36, loss: 0.0037884721532464027\n",
      "epoch:  37, loss: 0.003775260644033551\n",
      "epoch:  38, loss: 0.003770376555621624\n",
      "epoch:  39, loss: 0.003733202815055847\n",
      "epoch:  40, loss: 0.003726910101249814\n",
      "epoch:  41, loss: 0.0036843917332589626\n",
      "epoch:  42, loss: 0.0036669555120170116\n",
      "epoch:  43, loss: 0.0036621701437979937\n",
      "epoch:  44, loss: 0.003584720427170396\n",
      "epoch:  45, loss: 0.003562341909855604\n",
      "epoch:  46, loss: 0.0034965763334184885\n",
      "epoch:  47, loss: 0.003473557299003005\n",
      "epoch:  48, loss: 0.003412364050745964\n",
      "epoch:  49, loss: 0.0034082585480064154\n",
      "epoch:  50, loss: 0.0032373247668147087\n",
      "epoch:  51, loss: 0.0031847073696553707\n",
      "epoch:  52, loss: 0.003007262945175171\n",
      "epoch:  53, loss: 0.002760496689006686\n",
      "epoch:  54, loss: 0.002138624433428049\n",
      "epoch:  55, loss: 0.001972226658836007\n",
      "epoch:  56, loss: 0.0015130969695746899\n",
      "epoch:  57, loss: 0.001373325940221548\n",
      "epoch:  58, loss: 0.0013170725433155894\n",
      "epoch:  59, loss: 0.001152891549281776\n",
      "epoch:  60, loss: 0.0011361287906765938\n",
      "epoch:  61, loss: 0.0009785593720152974\n",
      "epoch:  62, loss: 0.0009398047695867717\n",
      "epoch:  63, loss: 0.0009198833722621202\n",
      "epoch:  64, loss: 0.0008668943773955107\n",
      "epoch:  65, loss: 0.0008413961622864008\n",
      "epoch:  66, loss: 0.0007955174660310149\n",
      "epoch:  67, loss: 0.0007880011689849198\n",
      "epoch:  68, loss: 0.0007473116856999695\n",
      "epoch:  69, loss: 0.0007471268763765693\n",
      "epoch:  70, loss: 0.000709359475877136\n",
      "epoch:  71, loss: 0.000698679534252733\n",
      "epoch:  72, loss: 0.0006898994906805456\n",
      "epoch:  73, loss: 0.0006887356285005808\n",
      "epoch:  74, loss: 0.0006617263425141573\n",
      "epoch:  75, loss: 0.0006587210227735341\n",
      "epoch:  76, loss: 0.0006307039293460548\n",
      "epoch:  77, loss: 0.0006228696438483894\n",
      "epoch:  78, loss: 0.0006178232724778354\n",
      "epoch:  79, loss: 0.0006157809402793646\n",
      "epoch:  80, loss: 0.0005961253773421049\n",
      "epoch:  81, loss: 0.0005865939310751855\n",
      "epoch:  82, loss: 0.000566482893191278\n",
      "epoch:  83, loss: 0.0005623895558528602\n",
      "epoch:  84, loss: 0.0005434916820377111\n",
      "epoch:  85, loss: 0.000538190477527678\n",
      "epoch:  86, loss: 0.0005205601919442415\n",
      "epoch:  87, loss: 0.0005175412516109645\n",
      "epoch:  88, loss: 0.0005014334456063807\n",
      "epoch:  89, loss: 0.0004989050794392824\n",
      "epoch:  90, loss: 0.0004840062465518713\n",
      "epoch:  91, loss: 0.00048004317795857787\n",
      "epoch:  92, loss: 0.00047762729809619486\n",
      "epoch:  93, loss: 0.0004717758565675467\n",
      "epoch:  94, loss: 0.0004669722984544933\n",
      "epoch:  95, loss: 0.00046171064605005085\n",
      "epoch:  96, loss: 0.00045717519242316484\n",
      "epoch:  97, loss: 0.0004524394462350756\n",
      "epoch:  98, loss: 0.00044846456148661673\n",
      "epoch:  99, loss: 0.000444112898549065\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size=X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "opt = torch_numopt.AGD(\n",
    "    model=model,\n",
    "    lr=1,\n",
    "    mu=0.001,\n",
    "    mu_dec=0.1,\n",
    "    use_diagonal=False,\n",
    "    c1=1e-4,\n",
    "    tau=0.5,\n",
    "    line_search_method=\"backtrack\",\n",
    "    line_search_cond=\"armijo\",\n",
    ")\n",
    "\n",
    "all_loss = []\n",
    "patience = 0\n",
    "max_patience = 10\n",
    "for epoch in range(100):\n",
    "    print(\"epoch: \", epoch, end=\"\")\n",
    "    all_loss.append(0)\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y, loss_fn)\n",
    "\n",
    "        all_loss[epoch] += loss\n",
    "\n",
    "    all_loss[epoch] /= len(data_loader)\n",
    "\n",
    "    if epoch > 0 and all_loss[epoch - 1] <= all_loss[epoch]:\n",
    "        patience -= 1\n",
    "    else:\n",
    "        patience = max_patience\n",
    "\n",
    "    print(\", loss: {}\".format(all_loss[epoch].detach().cpu().numpy().item()))\n",
    "\n",
    "    if patience <= 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics: R2 = 0.9869390729385881\n",
      "Test metrics:  R2 = 0.9858332090313062\n"
     ]
    }
   ],
   "source": [
    "pred_train = model.forward(X_train).detach()\n",
    "pred_test = model.forward(X_test).detach()\n",
    "print(f\"Train metrics: R2 = {r2_score(pred_train, y_train)}\")\n",
    "print(f\"Test metrics:  R2 = {r2_score(pred_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.473205029964447\n",
      "epoch:  1, loss: 0.3820722699165344\n",
      "epoch:  2, loss: 0.3008015751838684\n",
      "epoch:  3, loss: 0.24184522032737732\n",
      "epoch:  4, loss: 0.11892692744731903\n",
      "epoch:  5, loss: 0.013040613383054733\n",
      "epoch:  6, loss: 0.010139682330191135\n",
      "epoch:  7, loss: 0.008021051995456219\n",
      "epoch:  8, loss: 0.006697891745716333\n",
      "epoch:  9, loss: 0.005750979296863079\n",
      "epoch:  10, loss: 0.005151944234967232\n",
      "epoch:  11, loss: 0.004576586652547121\n",
      "epoch:  12, loss: 0.004169571213424206\n",
      "epoch:  13, loss: 0.0038120809476822615\n",
      "epoch:  14, loss: 0.003420913126319647\n",
      "epoch:  15, loss: 0.0030864491127431393\n",
      "epoch:  16, loss: 0.002792555605992675\n",
      "epoch:  17, loss: 0.0025406316854059696\n",
      "epoch:  18, loss: 0.0023104448337107897\n",
      "epoch:  19, loss: 0.002110644243657589\n",
      "epoch:  20, loss: 0.0019379639998078346\n",
      "epoch:  21, loss: 0.0017896337667480111\n",
      "epoch:  22, loss: 0.0016643169801682234\n",
      "epoch:  23, loss: 0.0015239000786095858\n",
      "epoch:  24, loss: 0.0012564589269459248\n",
      "epoch:  25, loss: 0.0011447183787822723\n",
      "epoch:  26, loss: 0.00108527357224375\n",
      "epoch:  27, loss: 0.0010419209720566869\n",
      "epoch:  28, loss: 0.0009443405433557928\n",
      "epoch:  29, loss: 0.0009065214544534683\n",
      "epoch:  30, loss: 0.0008735319715924561\n",
      "epoch:  31, loss: 0.0008470754255540669\n",
      "epoch:  32, loss: 0.0008245876524597406\n",
      "epoch:  33, loss: 0.000805958523415029\n",
      "epoch:  34, loss: 0.0007921954966150224\n",
      "epoch:  35, loss: 0.000780248548835516\n",
      "epoch:  36, loss: 0.0007687972974963486\n",
      "epoch:  37, loss: 0.0007594241760671139\n",
      "epoch:  38, loss: 0.0007500383071601391\n",
      "epoch:  39, loss: 0.0007411856204271317\n",
      "epoch:  40, loss: 0.0007333873072639108\n",
      "epoch:  41, loss: 0.0007260450511239469\n",
      "epoch:  42, loss: 0.0007185733411461115\n",
      "epoch:  43, loss: 0.000711946515366435\n",
      "epoch:  44, loss: 0.0007057293551042676\n",
      "epoch:  45, loss: 0.0006995446747168899\n",
      "epoch:  46, loss: 0.0006796452216804028\n",
      "epoch:  47, loss: 0.0006569261895492673\n",
      "epoch:  48, loss: 0.0006517009460367262\n",
      "epoch:  49, loss: 0.0006392606883309782\n",
      "epoch:  50, loss: 0.0006174307782202959\n",
      "epoch:  51, loss: 0.0006020263535901904\n",
      "epoch:  52, loss: 0.0005933578941039741\n",
      "epoch:  53, loss: 0.0005863204714842141\n",
      "epoch:  54, loss: 0.0005808929563499987\n",
      "epoch:  55, loss: 0.000576227146666497\n",
      "epoch:  56, loss: 0.00057265255600214\n",
      "epoch:  57, loss: 0.0005691947299055755\n",
      "epoch:  58, loss: 0.0005654800916090608\n",
      "epoch:  59, loss: 0.0005334402085281909\n",
      "epoch:  60, loss: 0.0005261824699118733\n",
      "epoch:  61, loss: 0.0005205619381740689\n",
      "epoch:  62, loss: 0.0005170276272110641\n",
      "epoch:  63, loss: 0.000506984069943428\n",
      "epoch:  64, loss: 0.0004962765378877521\n",
      "epoch:  65, loss: 0.0004948817659169436\n",
      "epoch:  66, loss: 0.00048527997569181025\n",
      "epoch:  67, loss: 0.0004796943685505539\n",
      "epoch:  68, loss: 0.00047320977319031954\n",
      "epoch:  69, loss: 0.0004613711789716035\n",
      "epoch:  70, loss: 0.00045711876009590924\n",
      "epoch:  71, loss: 0.0004537716449704021\n",
      "epoch:  72, loss: 0.0004438548057805747\n",
      "epoch:  73, loss: 0.000439957482740283\n",
      "epoch:  74, loss: 0.0004369608941487968\n",
      "epoch:  75, loss: 0.00042758401832543314\n",
      "epoch:  76, loss: 0.00042368299909867346\n",
      "epoch:  77, loss: 0.00042291689896956086\n",
      "epoch:  78, loss: 0.00041007398976944387\n",
      "epoch:  79, loss: 0.00040409606299363077\n",
      "epoch:  80, loss: 0.0004000364278908819\n",
      "epoch:  81, loss: 0.0003971649566665292\n",
      "epoch:  82, loss: 0.0003942530311178416\n",
      "epoch:  83, loss: 0.0003936147259082645\n",
      "epoch:  84, loss: 0.000369061715900898\n",
      "epoch:  85, loss: 0.00036639798781834543\n",
      "epoch:  86, loss: 0.000355538068106398\n",
      "epoch:  87, loss: 0.00034779441193677485\n",
      "epoch:  88, loss: 0.00034411504748277366\n",
      "epoch:  89, loss: 0.0003383543808013201\n",
      "epoch:  90, loss: 0.0003271226887591183\n",
      "epoch:  91, loss: 0.00032465666299685836\n",
      "epoch:  92, loss: 0.00032464051037095487\n",
      "epoch:  93, loss: 0.00031247202423401177\n",
      "epoch:  94, loss: 0.00030957756098359823\n",
      "epoch:  95, loss: 0.0003074357518926263\n",
      "epoch:  96, loss: 0.0003058481961488724\n",
      "epoch:  97, loss: 0.0003048227517865598\n",
      "epoch:  98, loss: 0.00030378217343240976\n",
      "epoch:  99, loss: 0.0003029464278370142\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size=X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "opt = torch_numopt.AGD(\n",
    "    model=model,\n",
    "    lr=1,\n",
    "    mu=0.001,\n",
    "    mu_dec=0.1,\n",
    "    use_diagonal=True,\n",
    "    c1=1e-4,\n",
    "    tau=0.1,\n",
    "    line_search_method=\"backtrack\",\n",
    "    line_search_cond=\"armijo\",\n",
    ")\n",
    "\n",
    "all_loss = []\n",
    "patience = 0\n",
    "max_patience = 10\n",
    "for epoch in range(100):\n",
    "    print(\"epoch: \", epoch, end=\"\")\n",
    "    all_loss.append(0)\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y, loss_fn)\n",
    "\n",
    "        all_loss[epoch] += loss\n",
    "\n",
    "    all_loss[epoch] /= len(data_loader)\n",
    "\n",
    "    if epoch > 0 and all_loss[epoch - 1] <= all_loss[epoch]:\n",
    "        patience -= 1\n",
    "    else:\n",
    "        patience = max_patience\n",
    "\n",
    "    print(\", loss: {}\".format(all_loss[epoch].detach().cpu().numpy().item()))\n",
    "\n",
    "    if patience <= 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics: R2 = 0.9910935767870599\n",
      "Test metrics:  R2 = 0.9884574572398859\n"
     ]
    }
   ],
   "source": [
    "pred_train = model.forward(X_train).detach()\n",
    "pred_test = model.forward(X_test).detach()\n",
    "print(f\"Train metrics: R2 = {r2_score(pred_train, y_train)}\")\n",
    "print(f\"Test metrics:  R2 = {r2_score(pred_test, y_test)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
