{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch_numopt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import *\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.f1 = nn.Linear(input_size, 10, device=device)\n",
    "        self.f2 = nn.Linear(10, 20, device=device)\n",
    "        self.f3 = nn.Linear(20, 20, device=device)\n",
    "        self.f4 = nn.Linear(20, 10, device=device)\n",
    "        self.f5 = nn.Linear(10, 1, device=device)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        # self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.f1(x))\n",
    "        x = self.activation(self.f2(x))\n",
    "        x = self.activation(self.f3(x))\n",
    "        x = self.activation(self.f4(x))\n",
    "        x = self.f5(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([800, 10]) torch.Size([800, 1])\n",
      "torch.Size([200, 10]) torch.Size([200, 1])\n"
     ]
    }
   ],
   "source": [
    "# X, y = load_diabetes(return_X_y=True, scaled=False)\n",
    "# X, y = make_regression(n_samples=1000, n_features=100)\n",
    "X, y = make_friedman1(n_samples=1000, noise=1e-2)\n",
    "\n",
    "X_scaler = MinMaxScaler()\n",
    "X = X_scaler.fit_transform(X)\n",
    "X = torch.Tensor(X).to(device)\n",
    "\n",
    "y_scaler = MinMaxScaler()\n",
    "y = y_scaler.fit_transform(y.reshape((-1, 1)))\n",
    "y = torch.Tensor(y).to(device)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "torch_data = TensorDataset(X_train, y_train)\n",
    "data_loader = DataLoader(torch_data, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.12559948861598969\n",
      "epoch:  1, loss: 0.035581815987825394\n",
      "epoch:  2, loss: 0.033347368240356445\n",
      "epoch:  3, loss: 0.025747235864400864\n",
      "epoch:  4, loss: 0.015934277325868607\n",
      "epoch:  5, loss: 0.011155315674841404\n",
      "epoch:  6, loss: 0.010485287755727768\n",
      "epoch:  7, loss: 0.00880791898816824\n",
      "epoch:  8, loss: 0.00858562346547842\n",
      "epoch:  9, loss: 0.008376938290894032\n",
      "epoch:  10, loss: 0.008293519727885723\n",
      "epoch:  11, loss: 0.008150013163685799\n",
      "epoch:  12, loss: 0.008119949139654636\n",
      "epoch:  13, loss: 0.008109564892947674\n",
      "epoch:  14, loss: 0.008050483651459217\n",
      "epoch:  15, loss: 0.00801115296781063\n",
      "epoch:  16, loss: 0.007957524619996548\n",
      "epoch:  17, loss: 0.007782727479934692\n",
      "epoch:  18, loss: 0.00763406977057457\n",
      "epoch:  19, loss: 0.007101581897586584\n",
      "epoch:  20, loss: 0.006993509363383055\n",
      "epoch:  21, loss: 0.006554123014211655\n",
      "epoch:  22, loss: 0.006508281454443932\n",
      "epoch:  23, loss: 0.006059879902750254\n",
      "epoch:  24, loss: 0.005971224047243595\n",
      "epoch:  25, loss: 0.005427566822618246\n",
      "epoch:  26, loss: 0.0052408212795853615\n",
      "epoch:  27, loss: 0.0045884232968091965\n",
      "epoch:  28, loss: 0.004376356024295092\n",
      "epoch:  29, loss: 0.004288000520318747\n",
      "epoch:  30, loss: 0.004054634366184473\n",
      "epoch:  31, loss: 0.003974457737058401\n",
      "epoch:  32, loss: 0.0037817007396370173\n",
      "epoch:  33, loss: 0.003725087968632579\n",
      "epoch:  34, loss: 0.003693615086376667\n",
      "epoch:  35, loss: 0.003621147247031331\n",
      "epoch:  36, loss: 0.0035735038109123707\n",
      "epoch:  37, loss: 0.003505946369841695\n",
      "epoch:  38, loss: 0.0034719377290457487\n",
      "epoch:  39, loss: 0.0034067409578710794\n",
      "epoch:  40, loss: 0.003371019382029772\n",
      "epoch:  41, loss: 0.003338146023452282\n",
      "epoch:  42, loss: 0.003218329744413495\n",
      "epoch:  43, loss: 0.003166471840813756\n",
      "epoch:  44, loss: 0.003118522698059678\n",
      "epoch:  45, loss: 0.0030935530085116625\n",
      "epoch:  46, loss: 0.003021182958036661\n",
      "epoch:  47, loss: 0.0029560555703938007\n",
      "epoch:  48, loss: 0.0028825418557971716\n",
      "epoch:  49, loss: 0.002829614793881774\n",
      "epoch:  50, loss: 0.002750231884419918\n",
      "epoch:  51, loss: 0.0026872893795371056\n",
      "epoch:  52, loss: 0.0026026510167866945\n",
      "epoch:  53, loss: 0.0025435546413064003\n",
      "epoch:  54, loss: 0.002459175419062376\n",
      "epoch:  55, loss: 0.0024032332003116608\n",
      "epoch:  56, loss: 0.002321098232641816\n",
      "epoch:  57, loss: 0.00227713561616838\n",
      "epoch:  58, loss: 0.0021948448847979307\n",
      "epoch:  59, loss: 0.002158168237656355\n",
      "epoch:  60, loss: 0.002078237244859338\n",
      "epoch:  61, loss: 0.0020482465624809265\n",
      "epoch:  62, loss: 0.00197021197527647\n",
      "epoch:  63, loss: 0.0019435598514974117\n",
      "epoch:  64, loss: 0.0018657894106581807\n",
      "epoch:  65, loss: 0.0018379546236246824\n",
      "epoch:  66, loss: 0.0017596731195226312\n",
      "epoch:  67, loss: 0.0017300997860729694\n",
      "epoch:  68, loss: 0.0016528250416740775\n",
      "epoch:  69, loss: 0.0016229101456701756\n",
      "epoch:  70, loss: 0.001548168365843594\n",
      "epoch:  71, loss: 0.001517493394203484\n",
      "epoch:  72, loss: 0.0014430343871936202\n",
      "epoch:  73, loss: 0.0014062306145206094\n",
      "epoch:  74, loss: 0.0013324759202077985\n",
      "epoch:  75, loss: 0.0012940306914970279\n",
      "epoch:  76, loss: 0.0012224920792505145\n",
      "epoch:  77, loss: 0.001186963403597474\n",
      "epoch:  78, loss: 0.0011191247031092644\n",
      "epoch:  79, loss: 0.0010900864144787192\n",
      "epoch:  80, loss: 0.0010226685553789139\n",
      "epoch:  81, loss: 0.000996495014987886\n",
      "epoch:  82, loss: 0.0009312971960753202\n",
      "epoch:  83, loss: 0.0009110504179261625\n",
      "epoch:  84, loss: 0.0008479414973407984\n",
      "epoch:  85, loss: 0.0008349281270056963\n",
      "epoch:  86, loss: 0.0007748962962068617\n",
      "epoch:  87, loss: 0.0007690303027629852\n",
      "epoch:  88, loss: 0.0007111752056516707\n",
      "epoch:  89, loss: 0.0006924308836460114\n",
      "epoch:  90, loss: 0.0006801806739531457\n",
      "epoch:  91, loss: 0.000680048018693924\n",
      "epoch:  92, loss: 0.0006394275114871562\n",
      "epoch:  93, loss: 0.0006375541561283171\n",
      "epoch:  94, loss: 0.000599960214458406\n",
      "epoch:  95, loss: 0.0005888162995688617\n",
      "epoch:  96, loss: 0.0005814300966449082\n",
      "epoch:  97, loss: 0.0005658960435539484\n",
      "epoch:  98, loss: 0.0005532858776859939\n",
      "epoch:  99, loss: 0.0005391319282352924\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size=X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "opt = torch_numopt.AGD(\n",
    "    model=model,\n",
    "    lr=1,\n",
    "    mu=0.001,\n",
    "    mu_dec=0.1,\n",
    "    use_diagonal=False,\n",
    "    c1=1e-4,\n",
    "    tau=0.5,\n",
    "    line_search_method=\"backtrack\",\n",
    "    line_search_cond=\"armijo\",\n",
    ")\n",
    "\n",
    "all_loss = []\n",
    "patience = 0\n",
    "max_patience = 10\n",
    "for epoch in range(100):\n",
    "    print(\"epoch: \", epoch, end=\"\")\n",
    "    all_loss.append(0)\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y, loss_fn)\n",
    "\n",
    "        all_loss[epoch] += loss\n",
    "\n",
    "    all_loss[epoch] /= len(data_loader)\n",
    "\n",
    "    if epoch > 0 and all_loss[epoch - 1] <= all_loss[epoch]:\n",
    "        patience -= 1\n",
    "    else:\n",
    "        patience = max_patience\n",
    "\n",
    "    print(\", loss: {}\".format(all_loss[epoch].detach().cpu().numpy().item()))\n",
    "\n",
    "    if patience <= 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics: R2 = 0.9844142734213406\n",
      "Test metrics:  R2 = 0.9803881135091433\n"
     ]
    }
   ],
   "source": [
    "pred_train = model.forward(X_train).cpu().detach()\n",
    "pred_test = model.forward(X_test).cpu().detach()\n",
    "print(f\"Train metrics: R2 = {r2_score(pred_train, y_train.cpu())}\")\n",
    "print(f\"Test metrics:  R2 = {r2_score(pred_test, y_test.cpu())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eugeniolr/Documents/invAI/torch_numopt/src/torch_numopt/utils.py:33: UserWarning: torch.linalg.svd: During SVD computation with the selected cusolver driver, batches 0 failed to converge. A more accurate method will be used to compute the SVD as a fallback. Check doc at https://pytorch.org/docs/stable/generated/torch.linalg.svd.html (Triggered internally at /pytorch/aten/src/ATen/native/cuda/linalg/BatchLinearAlgebraLib.cpp:690.)\n",
      "  U, S, Vt = torch.linalg.svd(mat)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", loss: 0.4110935926437378\n",
      "epoch:  1, loss: 0.31154173612594604\n",
      "epoch:  2, loss: 0.02129548043012619\n",
      "epoch:  3, loss: 0.012182684615254402\n",
      "epoch:  4, loss: 0.01039897184818983\n",
      "epoch:  5, loss: 0.00929376482963562\n",
      "epoch:  6, loss: 0.00858816597610712\n",
      "epoch:  7, loss: 0.007958177477121353\n",
      "epoch:  8, loss: 0.007371501065790653\n",
      "epoch:  9, loss: 0.00693253381177783\n",
      "epoch:  10, loss: 0.006589781027287245\n",
      "epoch:  11, loss: 0.006199134513735771\n",
      "epoch:  12, loss: 0.005843295715749264\n",
      "epoch:  13, loss: 0.005442881025373936\n",
      "epoch:  14, loss: 0.005006241612136364\n",
      "epoch:  15, loss: 0.004628636874258518\n",
      "epoch:  16, loss: 0.0043015433475375175\n",
      "epoch:  17, loss: 0.004032894968986511\n",
      "epoch:  18, loss: 0.0038242631126195192\n",
      "epoch:  19, loss: 0.003657487453892827\n",
      "epoch:  20, loss: 0.00351967616006732\n",
      "epoch:  21, loss: 0.0034084634389728308\n",
      "epoch:  22, loss: 0.003318477887660265\n",
      "epoch:  23, loss: 0.0032388572581112385\n",
      "epoch:  24, loss: 0.0031822349410504103\n",
      "epoch:  25, loss: 0.0029739998281002045\n",
      "epoch:  26, loss: 0.00289717479608953\n",
      "epoch:  27, loss: 0.0028561525978147984\n",
      "epoch:  28, loss: 0.0028267668094486\n",
      "epoch:  29, loss: 0.0028041463810950518\n",
      "epoch:  30, loss: 0.0027802016120404005\n",
      "epoch:  31, loss: 0.0027602785266935825\n",
      "epoch:  32, loss: 0.002743999008089304\n",
      "epoch:  33, loss: 0.0027265911921858788\n",
      "epoch:  34, loss: 0.0027103768661618233\n",
      "epoch:  35, loss: 0.0026925483252853155\n",
      "epoch:  36, loss: 0.002659659832715988\n",
      "epoch:  37, loss: 0.0026594579685479403\n",
      "epoch:  38, loss: 0.00227964180521667\n",
      "epoch:  39, loss: 0.0020267190411686897\n",
      "epoch:  40, loss: 0.0012245377292856574\n",
      "epoch:  41, loss: 0.0009650482097640634\n",
      "epoch:  42, loss: 0.0008511601481586695\n",
      "epoch:  43, loss: 0.0007875741575844586\n",
      "epoch:  44, loss: 0.0007407547091133893\n",
      "epoch:  45, loss: 0.000706388324033469\n",
      "epoch:  46, loss: 0.0006223595701158047\n",
      "epoch:  47, loss: 0.0005605727783404291\n",
      "epoch:  48, loss: 0.000543078756891191\n",
      "epoch:  49, loss: 0.0005189342773519456\n",
      "epoch:  50, loss: 0.00048279642942361534\n",
      "epoch:  51, loss: 0.00046783225843682885\n",
      "epoch:  52, loss: 0.0004601822583936155\n",
      "epoch:  53, loss: 0.00044392410200089216\n",
      "epoch:  54, loss: 0.00042984282481484115\n",
      "epoch:  55, loss: 0.0004252674989402294\n",
      "epoch:  56, loss: 0.00040839932626113296\n",
      "epoch:  57, loss: 0.0004003701906185597\n",
      "epoch:  58, loss: 0.00039569794898852706\n",
      "epoch:  59, loss: 0.00039378172368742526\n",
      "epoch:  60, loss: 0.000381921068765223\n",
      "epoch:  61, loss: 0.00037656485801562667\n",
      "epoch:  62, loss: 0.0003727896837517619\n",
      "epoch:  63, loss: 0.00036202644696459174\n",
      "epoch:  64, loss: 0.00035539260716177523\n",
      "epoch:  65, loss: 0.0003499995218589902\n",
      "epoch:  66, loss: 0.0003443336463533342\n",
      "epoch:  67, loss: 0.00032838640618138015\n",
      "epoch:  68, loss: 0.00032748468220233917\n",
      "epoch:  69, loss: 0.00030412012711167336\n",
      "epoch:  70, loss: 0.0002942729042842984\n",
      "epoch:  71, loss: 0.0002910243929363787\n",
      "epoch:  72, loss: 0.0002697054296731949\n",
      "epoch:  73, loss: 0.00026270924718119204\n",
      "epoch:  74, loss: 0.0002592970267869532\n",
      "epoch:  75, loss: 0.00025725620798766613\n",
      "epoch:  76, loss: 0.00025516696041449904\n",
      "epoch:  77, loss: 0.00025194662157446146\n",
      "epoch:  78, loss: 0.00025043674395419657\n",
      "epoch:  79, loss: 0.00024953074171207845\n",
      "epoch:  80, loss: 0.0002478314854670316\n",
      "epoch:  81, loss: 0.00024658534675836563\n",
      "epoch:  82, loss: 0.00024574826238676906\n",
      "epoch:  83, loss: 0.00024473073426634073\n",
      "epoch:  84, loss: 0.00024349591694772243\n",
      "epoch:  85, loss: 0.00024289793509524316\n",
      "epoch:  86, loss: 0.00024254829622805119\n",
      "epoch:  87, loss: 0.00024235298042185605\n",
      "epoch:  88, loss: 0.00024165773356799036\n",
      "epoch:  89, loss: 0.00024121043679770082\n",
      "epoch:  90, loss: 0.00024083854805212468\n",
      "epoch:  91, loss: 0.00024026249593589455\n",
      "epoch:  92, loss: 0.00023998091637622565\n",
      "epoch:  93, loss: 0.0002398089854978025\n",
      "epoch:  94, loss: 0.0002396751951891929\n",
      "epoch:  95, loss: 0.00023962925479281694\n",
      "epoch:  96, loss: 0.00023912939650472254\n",
      "epoch:  97, loss: 0.0002388478460488841\n",
      "epoch:  98, loss: 0.00023843154485803097\n",
      "epoch:  99, loss: 0.00023796797904651612\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size=X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "opt = torch_numopt.AGD(\n",
    "    model=model,\n",
    "    lr=1,\n",
    "    mu=0.001,\n",
    "    mu_dec=0.1,\n",
    "    use_diagonal=True,\n",
    "    c1=1e-4,\n",
    "    tau=0.1,\n",
    "    line_search_method=\"backtrack\",\n",
    "    line_search_cond=\"armijo\",\n",
    ")\n",
    "\n",
    "all_loss = []\n",
    "patience = 0\n",
    "max_patience = 10\n",
    "for epoch in range(100):\n",
    "    print(\"epoch: \", epoch, end=\"\")\n",
    "    all_loss.append(0)\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y, loss_fn)\n",
    "\n",
    "        all_loss[epoch] += loss\n",
    "\n",
    "    all_loss[epoch] /= len(data_loader)\n",
    "\n",
    "    if epoch > 0 and all_loss[epoch - 1] <= all_loss[epoch]:\n",
    "        patience -= 1\n",
    "    else:\n",
    "        patience = max_patience\n",
    "\n",
    "    print(\", loss: {}\".format(all_loss[epoch].cpu().detach().numpy().item()))\n",
    "\n",
    "    if patience <= 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics: R2 = 0.9931194738836279\n",
      "Test metrics:  R2 = 0.9917655275459792\n"
     ]
    }
   ],
   "source": [
    "pred_train = model.forward(X_train).cpu().detach()\n",
    "pred_test = model.forward(X_test).cpu().detach()\n",
    "print(f\"Train metrics: R2 = {r2_score(pred_train, y_train.cpu())}\")\n",
    "print(f\"Test metrics:  R2 = {r2_score(pred_test, y_test.cpu())}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_numopt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
