{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pytorch_soom\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.f1 = nn.Linear(input_size, 10, device=device)\n",
    "        self.f2 = nn.Linear(10, 20, device=device)\n",
    "        self.f3 = nn.Linear(20, 20, device=device)\n",
    "        self.f4 = nn.Linear(20, 10, device=device)\n",
    "        self.f5 = nn.Linear(10, 1, device=device)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        # self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.f1(x))\n",
    "        x = self.activation(self.f2(x))\n",
    "        x = self.activation(self.f3(x))\n",
    "        x = self.activation(self.f4(x))\n",
    "        x = self.f5(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_diabetes(return_X_y = True, scaled=False)\n",
    "\n",
    "X_scaler = MinMaxScaler()\n",
    "X = X_scaler.fit_transform(X)\n",
    "\n",
    "y_scaler = MinMaxScaler()\n",
    "y = y_scaler.fit_transform(y.reshape((-1, 1)))\n",
    "\n",
    "torch_data = TensorDataset(torch.Tensor(X).to(device), torch.Tensor(y).to(device))\n",
    "data_loader = DataLoader(torch_data, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.09158577024936676\n",
      "epoch:  1, loss: 0.06394219398498535\n",
      "epoch:  2, loss: 0.05308891460299492\n",
      "epoch:  3, loss: 0.045529838651418686\n",
      "epoch:  4, loss: 0.03201550245285034\n",
      "epoch:  5, loss: 0.028594907373189926\n",
      "epoch:  6, loss: 0.027501296252012253\n",
      "epoch:  7, loss: 0.026138441637158394\n",
      "epoch:  8, loss: 0.02559693716466427\n",
      "epoch:  9, loss: 0.0254640132188797\n",
      "epoch:  10, loss: 0.025071555748581886\n",
      "epoch:  11, loss: 0.0240323506295681\n",
      "epoch:  12, loss: 0.023709913715720177\n",
      "epoch:  13, loss: 0.023540427908301353\n",
      "epoch:  14, loss: 0.023364083841443062\n",
      "epoch:  15, loss: 0.02319788560271263\n",
      "epoch:  16, loss: 0.022997938096523285\n",
      "epoch:  17, loss: 0.022748541086912155\n",
      "epoch:  18, loss: 0.022612858563661575\n",
      "epoch:  19, loss: 0.022460652515292168\n",
      "epoch:  20, loss: 0.022332023829221725\n",
      "epoch:  21, loss: 0.022201498970389366\n",
      "epoch:  22, loss: 0.022159777581691742\n",
      "epoch:  23, loss: 0.022060180082917213\n",
      "epoch:  24, loss: 0.022010117769241333\n",
      "epoch:  25, loss: 0.021956801414489746\n",
      "epoch:  26, loss: 0.02194960229098797\n",
      "epoch:  27, loss: 0.021878531202673912\n",
      "epoch:  28, loss: 0.021864356473088264\n",
      "epoch:  29, loss: 0.021783150732517242\n",
      "epoch:  30, loss: 0.02166687697172165\n",
      "epoch:  31, loss: 0.021634839475154877\n",
      "epoch:  32, loss: 0.021605299785733223\n",
      "epoch:  33, loss: 0.021599017083644867\n",
      "epoch:  34, loss: 0.021562695503234863\n",
      "epoch:  35, loss: 0.021562401205301285\n",
      "epoch:  36, loss: 0.021518919616937637\n",
      "epoch:  37, loss: 0.021503262221813202\n",
      "epoch:  38, loss: 0.021486477926373482\n",
      "epoch:  39, loss: 0.021443497389554977\n",
      "epoch:  40, loss: 0.02142162062227726\n",
      "epoch:  41, loss: 0.021404121071100235\n",
      "epoch:  42, loss: 0.02138626016676426\n",
      "epoch:  43, loss: 0.021365312859416008\n",
      "epoch:  44, loss: 0.02131154201924801\n",
      "epoch:  45, loss: 0.021281301975250244\n",
      "epoch:  46, loss: 0.02127678319811821\n",
      "epoch:  47, loss: 0.021268513053655624\n",
      "epoch:  48, loss: 0.02125098742544651\n",
      "epoch:  49, loss: 0.021231168881058693\n",
      "epoch:  50, loss: 0.021199367940425873\n",
      "epoch:  51, loss: 0.021152975037693977\n",
      "epoch:  52, loss: 0.02114582620561123\n",
      "epoch:  53, loss: 0.021104201674461365\n",
      "epoch:  54, loss: 0.021103108301758766\n",
      "epoch:  55, loss: 0.021074475720524788\n",
      "epoch:  56, loss: 0.021074116230010986\n",
      "epoch:  57, loss: 0.02107325941324234\n",
      "epoch:  58, loss: 0.02104397676885128\n",
      "epoch:  59, loss: 0.02101786993443966\n",
      "epoch:  60, loss: 0.021007919684052467\n",
      "epoch:  61, loss: 0.020997002720832825\n",
      "epoch:  62, loss: 0.02099522575736046\n",
      "epoch:  63, loss: 0.020973125472664833\n",
      "epoch:  64, loss: 0.020960766822099686\n",
      "epoch:  65, loss: 0.020955592393875122\n",
      "epoch:  66, loss: 0.02094864472746849\n",
      "epoch:  67, loss: 0.02094162255525589\n",
      "epoch:  68, loss: 0.020937277004122734\n",
      "epoch:  69, loss: 0.020924337208271027\n",
      "epoch:  70, loss: 0.020921677350997925\n",
      "epoch:  71, loss: 0.020913124084472656\n",
      "epoch:  72, loss: 0.020904185250401497\n",
      "epoch:  73, loss: 0.020898563787341118\n",
      "epoch:  74, loss: 0.020895589143037796\n",
      "epoch:  75, loss: 0.02089252881705761\n",
      "epoch:  76, loss: 0.020881053060293198\n",
      "epoch:  77, loss: 0.02086808905005455\n",
      "epoch:  78, loss: 0.02085881680250168\n",
      "epoch:  79, loss: 0.020852038636803627\n",
      "epoch:  80, loss: 0.020844994112849236\n",
      "epoch:  81, loss: 0.020841529592871666\n",
      "epoch:  82, loss: 0.02083834633231163\n",
      "epoch:  83, loss: 0.020824333652853966\n",
      "epoch:  84, loss: 0.020811494439840317\n",
      "epoch:  85, loss: 0.020805414766073227\n",
      "epoch:  86, loss: 0.020800737664103508\n",
      "epoch:  87, loss: 0.02079501934349537\n",
      "epoch:  88, loss: 0.0207844078540802\n",
      "epoch:  89, loss: 0.020782265812158585\n",
      "epoch:  90, loss: 0.02077464945614338\n",
      "epoch:  91, loss: 0.020765889436006546\n",
      "epoch:  92, loss: 0.020764142274856567\n",
      "epoch:  93, loss: 0.020759286358952522\n",
      "epoch:  94, loss: 0.02075563557446003\n",
      "epoch:  95, loss: 0.02074766345322132\n",
      "epoch:  96, loss: 0.02073998562991619\n",
      "epoch:  97, loss: 0.020716356113553047\n",
      "epoch:  98, loss: 0.02069709077477455\n",
      "epoch:  99, loss: 0.02068416401743889\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size = X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "opt = pytorch_soom.AGD(model.parameters(), lr=1, mu=0.001, mu_dec=0.1, model=model, use_diagonal=False, c1=1e-4, tau=0.5, line_search_method=\"backtrack\", line_search_cond=\"armijo\")\n",
    "\n",
    "all_loss = []\n",
    "patience = 0\n",
    "max_patience = 10\n",
    "for epoch in range(100):\n",
    "    print('epoch: ', epoch, end='')\n",
    "    all_loss.append(0)\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y, loss_fn)\n",
    "        # opt.update(loss)\n",
    "\n",
    "        all_loss[epoch] += loss\n",
    "    \n",
    "    all_loss[epoch] /= len(data_loader)\n",
    "\n",
    "    if epoch > 0 and all_loss[epoch-1] <= all_loss[epoch]:\n",
    "        patience -= 1\n",
    "    else:\n",
    "        patience = max_patience\n",
    "        \n",
    "\n",
    "    print(', loss: {}'.format(all_loss[epoch].detach().cpu().numpy().item()))\n",
    "\n",
    "    if patience <= 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.18761606514453888\n",
      "epoch:  1, loss: 0.10066165030002594\n",
      "epoch:  2, loss: 0.06383821368217468\n",
      "epoch:  3, loss: 0.03253023326396942\n",
      "epoch:  4, loss: 0.028588728979229927\n",
      "epoch:  5, loss: 0.027696512639522552\n",
      "epoch:  6, loss: 0.027269072830677032\n",
      "epoch:  7, loss: 0.02692682109773159\n",
      "epoch:  8, loss: 0.026460733264684677\n",
      "epoch:  9, loss: 0.025815337896347046\n",
      "epoch:  10, loss: 0.025243690237402916\n",
      "epoch:  11, loss: 0.024593718349933624\n",
      "epoch:  12, loss: 0.024219801649451256\n",
      "epoch:  13, loss: 0.02368350140750408\n",
      "epoch:  14, loss: 0.02355155162513256\n",
      "epoch:  15, loss: 0.02268300950527191\n",
      "epoch:  16, loss: 0.022643180564045906\n",
      "epoch:  17, loss: 0.022183038294315338\n",
      "epoch:  18, loss: 0.02216476760804653\n",
      "epoch:  19, loss: 0.021751604974269867\n",
      "epoch:  20, loss: 0.021504821255803108\n",
      "epoch:  21, loss: 0.021483253687620163\n",
      "epoch:  22, loss: 0.02130015566945076\n",
      "epoch:  23, loss: 0.021171094849705696\n",
      "epoch:  24, loss: 0.02108692191541195\n",
      "epoch:  25, loss: 0.0210048146545887\n",
      "epoch:  26, loss: 0.020899096503853798\n",
      "epoch:  27, loss: 0.02080574259161949\n",
      "epoch:  28, loss: 0.020787803456187248\n",
      "epoch:  29, loss: 0.020566754043102264\n",
      "epoch:  30, loss: 0.02040860429406166\n",
      "epoch:  31, loss: 0.020288871601223946\n",
      "epoch:  32, loss: 0.02017281949520111\n",
      "epoch:  33, loss: 0.020117972046136856\n",
      "epoch:  34, loss: 0.019990134984254837\n",
      "epoch:  35, loss: 0.019924340769648552\n",
      "epoch:  36, loss: 0.01991480030119419\n",
      "epoch:  37, loss: 0.019847040995955467\n",
      "epoch:  38, loss: 0.019833847880363464\n",
      "epoch:  39, loss: 0.019819514825940132\n",
      "epoch:  40, loss: 0.019780945032835007\n",
      "epoch:  41, loss: 0.019768567755818367\n",
      "epoch:  42, loss: 0.01972043141722679\n",
      "epoch:  43, loss: 0.01971563510596752\n",
      "epoch:  44, loss: 0.019674228504300117\n",
      "epoch:  45, loss: 0.01964651420712471\n",
      "epoch:  46, loss: 0.01956409402191639\n",
      "epoch:  47, loss: 0.01954019069671631\n",
      "epoch:  48, loss: 0.019528700038790703\n",
      "epoch:  49, loss: 0.019495081156492233\n",
      "epoch:  50, loss: 0.01947629638016224\n",
      "epoch:  51, loss: 0.01947217434644699\n",
      "epoch:  52, loss: 0.019458388909697533\n",
      "epoch:  53, loss: 0.01945364661514759\n",
      "epoch:  54, loss: 0.019419699907302856\n",
      "epoch:  55, loss: 0.01937301829457283\n",
      "epoch:  56, loss: 0.019353371113538742\n",
      "epoch:  57, loss: 0.019347336143255234\n",
      "epoch:  58, loss: 0.019316254183650017\n",
      "epoch:  59, loss: 0.019307581707835197\n",
      "epoch:  60, loss: 0.01929941214621067\n",
      "epoch:  61, loss: 0.019278472289443016\n",
      "epoch:  62, loss: 0.019277125597000122\n",
      "epoch:  63, loss: 0.01926480233669281\n",
      "epoch:  64, loss: 0.0192571934312582\n",
      "epoch:  65, loss: 0.01923474110662937\n",
      "epoch:  66, loss: 0.01922501251101494\n",
      "epoch:  67, loss: 0.01922067441046238\n",
      "epoch:  68, loss: 0.019216010347008705\n",
      "epoch:  69, loss: 0.019206611439585686\n",
      "epoch:  70, loss: 0.01919708587229252\n",
      "epoch:  71, loss: 0.0191922839730978\n",
      "epoch:  72, loss: 0.01917887106537819\n",
      "epoch:  73, loss: 0.01917046122252941\n",
      "epoch:  74, loss: 0.019169006496667862\n",
      "epoch:  75, loss: 0.019144902005791664\n",
      "epoch:  76, loss: 0.01913665607571602\n",
      "epoch:  77, loss: 0.01913389004766941\n",
      "epoch:  78, loss: 0.019123341888189316\n",
      "epoch:  79, loss: 0.019119778648018837\n",
      "epoch:  80, loss: 0.019116705283522606\n",
      "epoch:  81, loss: 0.01911218836903572\n",
      "epoch:  82, loss: 0.019111379981040955\n",
      "epoch:  83, loss: 0.01910959556698799\n",
      "epoch:  84, loss: 0.019108202308416367\n",
      "epoch:  85, loss: 0.01910688728094101\n",
      "epoch:  86, loss: 0.019106604158878326\n",
      "epoch:  87, loss: 0.019105039536952972\n",
      "epoch:  88, loss: 0.01910480298101902\n",
      "epoch:  89, loss: 0.019104039296507835\n",
      "epoch:  90, loss: 0.01910374127328396\n",
      "epoch:  91, loss: 0.019102128222584724\n",
      "epoch:  92, loss: 0.019101733341813087\n",
      "epoch:  93, loss: 0.019101276993751526\n",
      "epoch:  94, loss: 0.019099976867437363\n",
      "epoch:  95, loss: 0.019099632278084755\n",
      "epoch:  96, loss: 0.019099444150924683\n",
      "epoch:  97, loss: 0.019099224358797073\n",
      "epoch:  98, loss: 0.019099125638604164\n",
      "epoch:  99, loss: 0.019098816439509392\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size = X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "opt = pytorch_soom.AGD(model.parameters(), lr=1, mu=0.001, mu_dec=0.1, model=model, use_diagonal=True, c1=1e-4, tau=0.5, line_search_method=\"backtrack\", line_search_cond=\"armijo\")\n",
    "\n",
    "all_loss = []\n",
    "patience = 0\n",
    "max_patience = 10\n",
    "for epoch in range(100):\n",
    "    print('epoch: ', epoch, end='')\n",
    "    all_loss.append(0)\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y, loss_fn)\n",
    "        # opt.update(loss)\n",
    "\n",
    "        all_loss[epoch] += loss\n",
    "    \n",
    "    all_loss[epoch] /= len(data_loader)\n",
    "\n",
    "    if epoch > 0 and all_loss[epoch-1] <= all_loss[epoch]:\n",
    "        patience -= 1\n",
    "    else:\n",
    "        patience = max_patience\n",
    "        \n",
    "\n",
    "    print(', loss: {}'.format(all_loss[epoch].detach().cpu().numpy().item()))\n",
    "\n",
    "    if patience <= 0:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
