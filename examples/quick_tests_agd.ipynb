{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch_numopt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import *\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.f1 = nn.Linear(input_size, 10, device=device)\n",
    "        self.f2 = nn.Linear(10, 20, device=device)\n",
    "        self.f3 = nn.Linear(20, 20, device=device)\n",
    "        self.f4 = nn.Linear(20, 10, device=device)\n",
    "        self.f5 = nn.Linear(10, 1, device=device)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        # self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.f1(x))\n",
    "        x = self.activation(self.f2(x))\n",
    "        x = self.activation(self.f3(x))\n",
    "        x = self.activation(self.f4(x))\n",
    "        x = self.f5(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([800, 10]) torch.Size([800, 1])\n",
      "torch.Size([200, 10]) torch.Size([200, 1])\n"
     ]
    }
   ],
   "source": [
    "# X, y = load_diabetes(return_X_y=True, scaled=False)\n",
    "# X, y = make_regression(n_samples=1000, n_features=100)\n",
    "X, y = make_friedman1(n_samples=1000, noise=1e-2)\n",
    "\n",
    "X_scaler = MinMaxScaler()\n",
    "X = X_scaler.fit_transform(X)\n",
    "X = torch.Tensor(X).to(device)\n",
    "\n",
    "y_scaler = MinMaxScaler()\n",
    "y = y_scaler.fit_transform(y.reshape((-1, 1)))\n",
    "y = torch.Tensor(y).to(device)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "torch_data = TensorDataset(X_train, y_train)\n",
    "data_loader = DataLoader(torch_data, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.30757418274879456\n",
      "epoch:  1, loss: 0.03823048993945122\n",
      "epoch:  2, loss: 0.02052762545645237\n",
      "epoch:  3, loss: 0.013146094977855682\n",
      "epoch:  4, loss: 0.00920481700450182\n",
      "epoch:  5, loss: 0.008544151671230793\n",
      "epoch:  6, loss: 0.008072946220636368\n",
      "epoch:  7, loss: 0.007920420728623867\n",
      "epoch:  8, loss: 0.007875592447817326\n",
      "epoch:  9, loss: 0.007734591141343117\n",
      "epoch:  10, loss: 0.007684823125600815\n",
      "epoch:  11, loss: 0.007662080228328705\n",
      "epoch:  12, loss: 0.0076121985912323\n",
      "epoch:  13, loss: 0.007584067527204752\n",
      "epoch:  14, loss: 0.007540901657193899\n",
      "epoch:  15, loss: 0.007516184821724892\n",
      "epoch:  16, loss: 0.007510055787861347\n",
      "epoch:  17, loss: 0.0074190436862409115\n",
      "epoch:  18, loss: 0.007374202366918325\n",
      "epoch:  19, loss: 0.00725740659981966\n",
      "epoch:  20, loss: 0.0071539198979735374\n",
      "epoch:  21, loss: 0.007144456263631582\n",
      "epoch:  22, loss: 0.0063970647752285\n",
      "epoch:  23, loss: 0.00634212838485837\n",
      "epoch:  24, loss: 0.005457266233861446\n",
      "epoch:  25, loss: 0.005418885964900255\n",
      "epoch:  26, loss: 0.0045792460441589355\n",
      "epoch:  27, loss: 0.004427659325301647\n",
      "epoch:  28, loss: 0.004407966509461403\n",
      "epoch:  29, loss: 0.004148251377046108\n",
      "epoch:  30, loss: 0.004068188369274139\n",
      "epoch:  31, loss: 0.00402587978169322\n",
      "epoch:  32, loss: 0.003931222949177027\n",
      "epoch:  33, loss: 0.003885250072926283\n",
      "epoch:  34, loss: 0.003802739316597581\n",
      "epoch:  35, loss: 0.0037978976033627987\n",
      "epoch:  36, loss: 0.0037241748068481684\n",
      "epoch:  37, loss: 0.0037068044766783714\n",
      "epoch:  38, loss: 0.0036969196517020464\n",
      "epoch:  39, loss: 0.0036702340003103018\n",
      "epoch:  40, loss: 0.003656635293737054\n",
      "epoch:  41, loss: 0.0036337976343929768\n",
      "epoch:  42, loss: 0.0036271121352910995\n",
      "epoch:  43, loss: 0.0036058323457837105\n",
      "epoch:  44, loss: 0.0036029554903507233\n",
      "epoch:  45, loss: 0.0035840554628521204\n",
      "epoch:  46, loss: 0.003583386540412903\n",
      "epoch:  47, loss: 0.0035636406391859055\n",
      "epoch:  48, loss: 0.003563078586012125\n",
      "epoch:  49, loss: 0.003544864244759083\n",
      "epoch:  50, loss: 0.003538606921210885\n",
      "epoch:  51, loss: 0.0035346762742847204\n",
      "epoch:  52, loss: 0.0035264482721686363\n",
      "epoch:  53, loss: 0.003521933685988188\n",
      "epoch:  54, loss: 0.003518056822940707\n",
      "epoch:  55, loss: 0.0035043368116021156\n",
      "epoch:  56, loss: 0.003502937499433756\n",
      "epoch:  57, loss: 0.0034867804497480392\n",
      "epoch:  58, loss: 0.003481789492070675\n",
      "epoch:  59, loss: 0.00348043255507946\n",
      "epoch:  60, loss: 0.0034764823503792286\n",
      "epoch:  61, loss: 0.0034648405853658915\n",
      "epoch:  62, loss: 0.0034623018000274897\n",
      "epoch:  63, loss: 0.003461889224126935\n",
      "epoch:  64, loss: 0.003445287235081196\n",
      "epoch:  65, loss: 0.0034431302919983864\n",
      "epoch:  66, loss: 0.0034412837121635675\n",
      "epoch:  67, loss: 0.0034309166949242353\n",
      "epoch:  68, loss: 0.0034289811737835407\n",
      "epoch:  69, loss: 0.0034256423823535442\n",
      "epoch:  70, loss: 0.0034127389080822468\n",
      "epoch:  71, loss: 0.0034088497050106525\n",
      "epoch:  72, loss: 0.0034057353623211384\n",
      "epoch:  73, loss: 0.0034043644554913044\n",
      "epoch:  74, loss: 0.0033932605292648077\n",
      "epoch:  75, loss: 0.003390647703781724\n",
      "epoch:  76, loss: 0.0033893329091370106\n",
      "epoch:  77, loss: 0.0033707760740071535\n",
      "epoch:  78, loss: 0.003366050310432911\n",
      "epoch:  79, loss: 0.0033646300435066223\n",
      "epoch:  80, loss: 0.003363908501341939\n",
      "epoch:  81, loss: 0.0033490429632365704\n",
      "epoch:  82, loss: 0.003345348872244358\n",
      "epoch:  83, loss: 0.003341957461088896\n",
      "epoch:  84, loss: 0.003341144882142544\n",
      "epoch:  85, loss: 0.0033300071954727173\n",
      "epoch:  86, loss: 0.003327137790620327\n",
      "epoch:  87, loss: 0.0033227496314793825\n",
      "epoch:  88, loss: 0.0033201943151652813\n",
      "epoch:  89, loss: 0.0033122082240879536\n",
      "epoch:  90, loss: 0.003310062224045396\n",
      "epoch:  91, loss: 0.0033054989762604237\n",
      "epoch:  92, loss: 0.0033032100182026625\n",
      "epoch:  93, loss: 0.003295675152912736\n",
      "epoch:  94, loss: 0.003290737746283412\n",
      "epoch:  95, loss: 0.0032831078860908747\n",
      "epoch:  96, loss: 0.00327973417006433\n",
      "epoch:  97, loss: 0.0032726344652473927\n",
      "epoch:  98, loss: 0.00327024282887578\n",
      "epoch:  99, loss: 0.003261329373344779\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size=X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "opt = torch_numopt.AGD(\n",
    "    model=model,\n",
    "    lr=1,\n",
    "    mu=0.001,\n",
    "    mu_dec=0.1,\n",
    "    use_diagonal=False,\n",
    "    c1=1e-4,\n",
    "    tau=0.5,\n",
    "    line_search_method=\"backtrack\",\n",
    "    line_search_cond=\"armijo\",\n",
    ")\n",
    "\n",
    "all_loss = []\n",
    "patience = 0\n",
    "max_patience = 10\n",
    "for epoch in range(100):\n",
    "    print(\"epoch: \", epoch, end=\"\")\n",
    "    all_loss.append(0)\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y, loss_fn)\n",
    "\n",
    "        all_loss[epoch] += loss\n",
    "\n",
    "    all_loss[epoch] /= len(data_loader)\n",
    "\n",
    "    if epoch > 0 and all_loss[epoch - 1] <= all_loss[epoch]:\n",
    "        patience -= 1\n",
    "    else:\n",
    "        patience = max_patience\n",
    "\n",
    "    print(\", loss: {}\".format(all_loss[epoch].detach().cpu().numpy().item()))\n",
    "\n",
    "    if patience <= 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics: R2 = 0.8833358338063181\n",
      "Test metrics:  R2 = 0.8498516952569561\n"
     ]
    }
   ],
   "source": [
    "pred_train = model.forward(X_train).cpu().detach()\n",
    "pred_test = model.forward(X_test).cpu().detach()\n",
    "print(f\"Train metrics: R2 = {r2_score(pred_train, y_train.cpu())}\")\n",
    "print(f\"Test metrics:  R2 = {r2_score(pred_test, y_test.cpu())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.035894494503736496\n",
      "epoch:  1, loss: 0.033560965210199356\n",
      "epoch:  2, loss: 0.031222915276885033\n",
      "epoch:  3, loss: 0.028881406411528587\n",
      "epoch:  4, loss: 0.026570569723844528\n",
      "epoch:  5, loss: 0.011423084884881973\n",
      "epoch:  6, loss: 0.009013116359710693\n",
      "epoch:  7, loss: 0.008268359117209911\n",
      "epoch:  8, loss: 0.0071680280379951\n",
      "epoch:  9, loss: 0.0062040844932198524\n",
      "epoch:  10, loss: 0.005235495511442423\n",
      "epoch:  11, loss: 0.00437856325879693\n",
      "epoch:  12, loss: 0.0035416418686509132\n",
      "epoch:  13, loss: 0.0029061888344585896\n",
      "epoch:  14, loss: 0.0023310857359319925\n",
      "epoch:  15, loss: 0.001947912503965199\n",
      "epoch:  16, loss: 0.0016874006250873208\n",
      "epoch:  17, loss: 0.0014796913601458073\n",
      "epoch:  18, loss: 0.0013282144209370017\n",
      "epoch:  19, loss: 0.0011659227311611176\n",
      "epoch:  20, loss: 0.0010186745785176754\n",
      "epoch:  21, loss: 0.0008995245443657041\n",
      "epoch:  22, loss: 0.0008015843923203647\n",
      "epoch:  23, loss: 0.0007184420246630907\n",
      "epoch:  24, loss: 0.0007011370034888387\n",
      "epoch:  25, loss: 0.0004386439104564488\n",
      "epoch:  26, loss: 0.00035972220939584076\n",
      "epoch:  27, loss: 0.0003237772034481168\n",
      "epoch:  28, loss: 0.00030764119583182037\n",
      "epoch:  29, loss: 0.0002234542916994542\n",
      "epoch:  30, loss: 0.00020335677254479378\n",
      "epoch:  31, loss: 0.00019177922513335943\n",
      "epoch:  32, loss: 0.0001744078181218356\n",
      "epoch:  33, loss: 0.0001535574992885813\n",
      "epoch:  34, loss: 0.00014474544150289148\n",
      "epoch:  35, loss: 0.00013944956299383193\n",
      "epoch:  36, loss: 0.0001327402569586411\n",
      "epoch:  37, loss: 0.00012227491242811084\n",
      "epoch:  38, loss: 0.00011799218191299587\n",
      "epoch:  39, loss: 0.00011567480396479368\n",
      "epoch:  40, loss: 0.00011210356751689687\n",
      "epoch:  41, loss: 0.00010459178884048015\n",
      "epoch:  42, loss: 0.0001023611766868271\n",
      "epoch:  43, loss: 0.00010083384404424578\n",
      "epoch:  44, loss: 9.608791879145429e-05\n",
      "epoch:  45, loss: 9.293384937336668e-05\n",
      "epoch:  46, loss: 9.174035949399695e-05\n",
      "epoch:  47, loss: 9.084250632440671e-05\n",
      "epoch:  48, loss: 9.043358295457438e-05\n",
      "epoch:  49, loss: 8.723876817384735e-05\n",
      "epoch:  50, loss: 8.565388270653784e-05\n",
      "epoch:  51, loss: 8.530745253665373e-05\n",
      "epoch:  52, loss: 8.2258484326303e-05\n",
      "epoch:  53, loss: 8.120419079205021e-05\n",
      "epoch:  54, loss: 8.047663141041994e-05\n",
      "epoch:  55, loss: 7.987353455973789e-05\n",
      "epoch:  56, loss: 7.936765905469656e-05\n",
      "epoch:  57, loss: 7.90027916082181e-05\n",
      "epoch:  58, loss: 7.857735181460157e-05\n",
      "epoch:  59, loss: 7.740822184132412e-05\n",
      "epoch:  60, loss: 7.68129393691197e-05\n",
      "epoch:  61, loss: 7.651174382772297e-05\n",
      "epoch:  62, loss: 7.617293886141852e-05\n",
      "epoch:  63, loss: 7.522755186073482e-05\n",
      "epoch:  64, loss: 7.473536970792338e-05\n",
      "epoch:  65, loss: 7.440042099915445e-05\n",
      "epoch:  66, loss: 7.361204916378483e-05\n",
      "epoch:  67, loss: 7.278214616235346e-05\n",
      "epoch:  68, loss: 7.238252146635205e-05\n",
      "epoch:  69, loss: 7.215865480247885e-05\n",
      "epoch:  70, loss: 7.05610218574293e-05\n",
      "epoch:  71, loss: 6.99116790201515e-05\n",
      "epoch:  72, loss: 6.935972487553954e-05\n",
      "epoch:  73, loss: 6.814543303335086e-05\n",
      "epoch:  74, loss: 6.752158515155315e-05\n",
      "epoch:  75, loss: 6.708485307171941e-05\n",
      "epoch:  76, loss: 6.657841731794178e-05\n",
      "epoch:  77, loss: 6.551408296218142e-05\n",
      "epoch:  78, loss: 6.513207335956395e-05\n",
      "epoch:  79, loss: 6.485284393420443e-05\n",
      "epoch:  80, loss: 6.47807028144598e-05\n",
      "epoch:  81, loss: 6.388891051756218e-05\n",
      "epoch:  82, loss: 6.339242827380076e-05\n",
      "epoch:  83, loss: 6.306308932835236e-05\n",
      "epoch:  84, loss: 6.280113302636892e-05\n",
      "epoch:  85, loss: 6.257015775190666e-05\n",
      "epoch:  86, loss: 6.237106572370976e-05\n",
      "epoch:  87, loss: 6.23105515842326e-05\n",
      "epoch:  88, loss: 6.142583151813596e-05\n",
      "epoch:  89, loss: 6.107849185355008e-05\n",
      "epoch:  90, loss: 6.084781853132881e-05\n",
      "epoch:  91, loss: 6.0613157984334975e-05\n",
      "epoch:  92, loss: 6.0425947594922036e-05\n",
      "epoch:  93, loss: 6.027604467817582e-05\n",
      "epoch:  94, loss: 6.0150323406560346e-05\n",
      "epoch:  95, loss: 6.00056373514235e-05\n",
      "epoch:  96, loss: 5.990411955281161e-05\n",
      "epoch:  97, loss: 5.9822676121257246e-05\n",
      "epoch:  98, loss: 5.9707079344661906e-05\n",
      "epoch:  99, loss: 5.960173439234495e-05\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size=X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "opt = torch_numopt.AGD(\n",
    "    model=model,\n",
    "    lr=1,\n",
    "    mu=0.001,\n",
    "    mu_dec=0.1,\n",
    "    use_diagonal=True,\n",
    "    c1=1e-4,\n",
    "    tau=0.1,\n",
    "    line_search_method=\"backtrack\",\n",
    "    line_search_cond=\"armijo\",\n",
    ")\n",
    "\n",
    "all_loss = []\n",
    "patience = 0\n",
    "max_patience = 10\n",
    "for epoch in range(100):\n",
    "    print(\"epoch: \", epoch, end=\"\")\n",
    "    all_loss.append(0)\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y, loss_fn)\n",
    "\n",
    "        all_loss[epoch] += loss\n",
    "\n",
    "    all_loss[epoch] /= len(data_loader)\n",
    "\n",
    "    if epoch > 0 and all_loss[epoch - 1] <= all_loss[epoch]:\n",
    "        patience -= 1\n",
    "    else:\n",
    "        patience = max_patience\n",
    "\n",
    "    print(\", loss: {}\".format(all_loss[epoch].cpu().detach().numpy().item()))\n",
    "\n",
    "    if patience <= 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics: R2 = 0.9980894675879775\n",
      "Test metrics:  R2 = 0.9957860388863368\n"
     ]
    }
   ],
   "source": [
    "pred_train = model.forward(X_train).cpu().detach()\n",
    "pred_test = model.forward(X_test).cpu().detach()\n",
    "print(f\"Train metrics: R2 = {r2_score(pred_train, y_train.cpu())}\")\n",
    "print(f\"Test metrics:  R2 = {r2_score(pred_test, y_test.cpu())}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_numopt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
