{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch_numopt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import *\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.f1 = nn.Linear(input_size, 10, device=device)\n",
    "        self.f2 = nn.Linear(10, 20, device=device)\n",
    "        self.f3 = nn.Linear(20, 20, device=device)\n",
    "        self.f4 = nn.Linear(20, 10, device=device)\n",
    "        self.f5 = nn.Linear(10, 1, device=device)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        # self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.f1(x))\n",
    "        x = self.activation(self.f2(x))\n",
    "        x = self.activation(self.f3(x))\n",
    "        x = self.activation(self.f4(x))\n",
    "        x = self.f5(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([800, 10]) torch.Size([800, 1])\n",
      "torch.Size([200, 10]) torch.Size([200, 1])\n"
     ]
    }
   ],
   "source": [
    "# X, y = load_diabetes(return_X_y=True, scaled=False)\n",
    "# X, y = make_regression(n_samples=1000, n_features=100)\n",
    "X, y = make_friedman1(n_samples=1000, noise=1e-2)\n",
    "\n",
    "X_scaler = MinMaxScaler()\n",
    "X = X_scaler.fit_transform(X)\n",
    "X = torch.Tensor(X).to(device)\n",
    "\n",
    "y_scaler = MinMaxScaler()\n",
    "y = y_scaler.fit_transform(y.reshape((-1, 1)))\n",
    "y = torch.Tensor(y).to(device)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "torch_data = TensorDataset(X_train, y_train)\n",
    "data_loader = DataLoader(torch_data, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.5545493364334106\n",
      "epoch:  1, loss: 0.08372308313846588\n",
      "epoch:  2, loss: 0.03995230048894882\n",
      "epoch:  3, loss: 0.026942646130919456\n",
      "epoch:  4, loss: 0.01772207021713257\n",
      "epoch:  5, loss: 0.013669883832335472\n",
      "epoch:  6, loss: 0.012841365300118923\n",
      "epoch:  7, loss: 0.010877660475671291\n",
      "epoch:  8, loss: 0.010600652545690536\n",
      "epoch:  9, loss: 0.009986817836761475\n",
      "epoch:  10, loss: 0.009718048386275768\n",
      "epoch:  11, loss: 0.009625028818845749\n",
      "epoch:  12, loss: 0.009317740797996521\n",
      "epoch:  13, loss: 0.009213874116539955\n",
      "epoch:  14, loss: 0.00904080644249916\n",
      "epoch:  15, loss: 0.00897318497300148\n",
      "epoch:  16, loss: 0.008603876456618309\n",
      "epoch:  17, loss: 0.008455225266516209\n",
      "epoch:  18, loss: 0.008254572749137878\n",
      "epoch:  19, loss: 0.007989335805177689\n",
      "epoch:  20, loss: 0.00769325764849782\n",
      "epoch:  21, loss: 0.007504921872168779\n",
      "epoch:  22, loss: 0.007323227822780609\n",
      "epoch:  23, loss: 0.007184075657278299\n",
      "epoch:  24, loss: 0.006882842164486647\n",
      "epoch:  25, loss: 0.006721580866724253\n",
      "epoch:  26, loss: 0.006140450481325388\n",
      "epoch:  27, loss: 0.006063530687242746\n",
      "epoch:  28, loss: 0.005857373122125864\n",
      "epoch:  29, loss: 0.004787428770214319\n",
      "epoch:  30, loss: 0.004748647101223469\n",
      "epoch:  31, loss: 0.00461584934964776\n",
      "epoch:  32, loss: 0.004483468364924192\n",
      "epoch:  33, loss: 0.0043593100272119045\n",
      "epoch:  34, loss: 0.004271756391972303\n",
      "epoch:  35, loss: 0.0042051333002746105\n",
      "epoch:  36, loss: 0.004141731187701225\n",
      "epoch:  37, loss: 0.00408754451200366\n",
      "epoch:  38, loss: 0.003984841518104076\n",
      "epoch:  39, loss: 0.0038868384435772896\n",
      "epoch:  40, loss: 0.0038452947046607733\n",
      "epoch:  41, loss: 0.003525438252836466\n",
      "epoch:  42, loss: 0.003393127815797925\n",
      "epoch:  43, loss: 0.003055720590054989\n",
      "epoch:  44, loss: 0.0030070440843701363\n",
      "epoch:  45, loss: 0.002669923473149538\n",
      "epoch:  46, loss: 0.0026087029837071896\n",
      "epoch:  47, loss: 0.002580431057140231\n",
      "epoch:  48, loss: 0.0024670916609466076\n",
      "epoch:  49, loss: 0.002448281506076455\n",
      "epoch:  50, loss: 0.0023308033123612404\n",
      "epoch:  51, loss: 0.002321251668035984\n",
      "epoch:  52, loss: 0.0022202676627784967\n",
      "epoch:  53, loss: 0.0021880476269870996\n",
      "epoch:  54, loss: 0.0021628832910209894\n",
      "epoch:  55, loss: 0.0021534219849854708\n",
      "epoch:  56, loss: 0.0020799897611141205\n",
      "epoch:  57, loss: 0.002058421727269888\n",
      "epoch:  58, loss: 0.0019899115432053804\n",
      "epoch:  59, loss: 0.001969908596947789\n",
      "epoch:  60, loss: 0.0019025474321097136\n",
      "epoch:  61, loss: 0.0018784591229632497\n",
      "epoch:  62, loss: 0.0018182595958933234\n",
      "epoch:  63, loss: 0.0018049990758299828\n",
      "epoch:  64, loss: 0.0017448192229494452\n",
      "epoch:  65, loss: 0.001728803152218461\n",
      "epoch:  66, loss: 0.001674256636761129\n",
      "epoch:  67, loss: 0.0016688802279531956\n",
      "epoch:  68, loss: 0.0016115823527798057\n",
      "epoch:  69, loss: 0.0016040974296629429\n",
      "epoch:  70, loss: 0.0015502753667533398\n",
      "epoch:  71, loss: 0.0015445947647094727\n",
      "epoch:  72, loss: 0.0014903976116329432\n",
      "epoch:  73, loss: 0.001477765734307468\n",
      "epoch:  74, loss: 0.0014266218058764935\n",
      "epoch:  75, loss: 0.0014191069640219212\n",
      "epoch:  76, loss: 0.001364159514196217\n",
      "epoch:  77, loss: 0.0013472699793055654\n",
      "epoch:  78, loss: 0.0013352425303310156\n",
      "epoch:  79, loss: 0.001334686647169292\n",
      "epoch:  80, loss: 0.001296193222515285\n",
      "epoch:  81, loss: 0.0012821092968806624\n",
      "epoch:  82, loss: 0.0012723590480163693\n",
      "epoch:  83, loss: 0.001269871019758284\n",
      "epoch:  84, loss: 0.0012385911541059613\n",
      "epoch:  85, loss: 0.0012260251678526402\n",
      "epoch:  86, loss: 0.0012150785187259316\n",
      "epoch:  87, loss: 0.0012127143563702703\n",
      "epoch:  88, loss: 0.0011878253426402807\n",
      "epoch:  89, loss: 0.0011825485853478312\n",
      "epoch:  90, loss: 0.0011558105470612645\n",
      "epoch:  91, loss: 0.001153182121925056\n",
      "epoch:  92, loss: 0.0011272276751697063\n",
      "epoch:  93, loss: 0.0011189858196303248\n",
      "epoch:  94, loss: 0.0011117709800601006\n",
      "epoch:  95, loss: 0.0010996165219694376\n",
      "epoch:  96, loss: 0.0010931596625596285\n",
      "epoch:  97, loss: 0.0010873832507058978\n",
      "epoch:  98, loss: 0.00106781255453825\n",
      "epoch:  99, loss: 0.001065018936060369\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size=X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "opt = torch_numopt.AGD(\n",
    "    model=model,\n",
    "    lr=1,\n",
    "    mu=0.001,\n",
    "    mu_dec=0.1,\n",
    "    use_diagonal=False,\n",
    "    c1=1e-4,\n",
    "    tau=0.5,\n",
    "    line_search_method=\"backtrack\",\n",
    "    line_search_cond=\"armijo\",\n",
    ")\n",
    "\n",
    "all_loss = []\n",
    "patience = 0\n",
    "max_patience = 10\n",
    "for epoch in range(100):\n",
    "    print(\"epoch: \", epoch, end=\"\")\n",
    "    all_loss.append(0)\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y, loss_fn)\n",
    "\n",
    "        all_loss[epoch] += loss\n",
    "\n",
    "    all_loss[epoch] /= len(data_loader)\n",
    "\n",
    "    if epoch > 0 and all_loss[epoch - 1] <= all_loss[epoch]:\n",
    "        patience -= 1\n",
    "    else:\n",
    "        patience = max_patience\n",
    "\n",
    "    print(\", loss: {}\".format(all_loss[epoch].detach().cpu().numpy().item()))\n",
    "\n",
    "    if patience <= 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics: R2 = 0.9734574527057158\n",
      "Test metrics:  R2 = 0.9645880973329118\n"
     ]
    }
   ],
   "source": [
    "pred_train = model.forward(X_train).cpu().detach()\n",
    "pred_test = model.forward(X_test).cpu().detach()\n",
    "print(f\"Train metrics: R2 = {r2_score(pred_train, y_train.cpu())}\")\n",
    "print(f\"Test metrics:  R2 = {r2_score(pred_test, y_test.cpu())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.06564666330814362\n",
      "epoch:  1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eugeniolr/Documents/invAI/torch_numopt/src/torch_numopt/utils.py:33: UserWarning: torch.linalg.svd: During SVD computation with the selected cusolver driver, batches 0 failed to converge. A more accurate method will be used to compute the SVD as a fallback. Check doc at https://pytorch.org/docs/stable/generated/torch.linalg.svd.html (Triggered internally at /pytorch/aten/src/ATen/native/cuda/linalg/BatchLinearAlgebraLib.cpp:690.)\n",
      "  U, S, Vt = torch.linalg.svd(mat)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", loss: 0.03889923542737961\n",
      "epoch:  2, loss: 0.03779246285557747\n",
      "epoch:  3, loss: 0.020640326663851738\n",
      "epoch:  4, loss: 0.014044285751879215\n",
      "epoch:  5, loss: 0.009291606023907661\n",
      "epoch:  6, loss: 0.008633079007267952\n",
      "epoch:  7, loss: 0.008133187890052795\n",
      "epoch:  8, loss: 0.007452551741153002\n",
      "epoch:  9, loss: 0.00661282055079937\n",
      "epoch:  10, loss: 0.005887879990041256\n",
      "epoch:  11, loss: 0.005394181702286005\n",
      "epoch:  12, loss: 0.00501747103407979\n",
      "epoch:  13, loss: 0.004645932000130415\n",
      "epoch:  14, loss: 0.004267852287739515\n",
      "epoch:  15, loss: 0.003881315002217889\n",
      "epoch:  16, loss: 0.0034876184072345495\n",
      "epoch:  17, loss: 0.0030859021935611963\n",
      "epoch:  18, loss: 0.002737503033131361\n",
      "epoch:  19, loss: 0.0024413124192506075\n",
      "epoch:  20, loss: 0.002185117918998003\n",
      "epoch:  21, loss: 0.0019957709591835737\n",
      "epoch:  22, loss: 0.0018404260044917464\n",
      "epoch:  23, loss: 0.0013938195770606399\n",
      "epoch:  24, loss: 0.0010860806796699762\n",
      "epoch:  25, loss: 0.0009699227521196008\n",
      "epoch:  26, loss: 0.0009068384533748031\n",
      "epoch:  27, loss: 0.0008877619984559715\n",
      "epoch:  28, loss: 0.0007460148190148175\n",
      "epoch:  29, loss: 0.0006977040902711451\n",
      "epoch:  30, loss: 0.0006698608049191535\n",
      "epoch:  31, loss: 0.0006527699297294021\n",
      "epoch:  32, loss: 0.0006223937962204218\n",
      "epoch:  33, loss: 0.0005973694496788085\n",
      "epoch:  34, loss: 0.0005858734366483986\n",
      "epoch:  35, loss: 0.0005788353155367076\n",
      "epoch:  36, loss: 0.0005617487477138638\n",
      "epoch:  37, loss: 0.0005516026867553592\n",
      "epoch:  38, loss: 0.0005467262817546725\n",
      "epoch:  39, loss: 0.0005356441251933575\n",
      "epoch:  40, loss: 0.0005243403720669448\n",
      "epoch:  41, loss: 0.0005177961429581046\n",
      "epoch:  42, loss: 0.000514051120262593\n",
      "epoch:  43, loss: 0.0004985466366633773\n",
      "epoch:  44, loss: 0.0004912681761197746\n",
      "epoch:  45, loss: 0.00048628528020344675\n",
      "epoch:  46, loss: 0.00048343100934289396\n",
      "epoch:  47, loss: 0.0004693955124821514\n",
      "epoch:  48, loss: 0.00046363292494788766\n",
      "epoch:  49, loss: 0.00046069175004959106\n",
      "epoch:  50, loss: 0.0004508376878220588\n",
      "epoch:  51, loss: 0.00044533307664096355\n",
      "epoch:  52, loss: 0.0004416819429025054\n",
      "epoch:  53, loss: 0.0004341225139796734\n",
      "epoch:  54, loss: 0.0004241877468302846\n",
      "epoch:  55, loss: 0.00041640689596533775\n",
      "epoch:  56, loss: 0.00039671725244261324\n",
      "epoch:  57, loss: 0.000387870502891019\n",
      "epoch:  58, loss: 0.00038213530206121504\n",
      "epoch:  59, loss: 0.00038202761788852513\n",
      "epoch:  60, loss: 0.00036219722824171185\n",
      "epoch:  61, loss: 0.00035577506059780717\n",
      "epoch:  62, loss: 0.000352348608430475\n",
      "epoch:  63, loss: 0.0003340861003380269\n",
      "epoch:  64, loss: 0.00032695994013920426\n",
      "epoch:  65, loss: 0.00032341424957849085\n",
      "epoch:  66, loss: 0.0003175373421981931\n",
      "epoch:  67, loss: 0.00031059046159498394\n",
      "epoch:  68, loss: 0.0003068579244427383\n",
      "epoch:  69, loss: 0.0002958600816782564\n",
      "epoch:  70, loss: 0.00029107541195116937\n",
      "epoch:  71, loss: 0.00028798094717785716\n",
      "epoch:  72, loss: 0.00028569961432367563\n",
      "epoch:  73, loss: 0.0002816109627019614\n",
      "epoch:  74, loss: 0.00027780813979916275\n",
      "epoch:  75, loss: 0.00027632739511318505\n",
      "epoch:  76, loss: 0.0002745073288679123\n",
      "epoch:  77, loss: 0.00027071728254668415\n",
      "epoch:  78, loss: 0.000268864503595978\n",
      "epoch:  79, loss: 0.00026786766829900444\n",
      "epoch:  80, loss: 0.0002671679831109941\n",
      "epoch:  81, loss: 0.0002666883810888976\n",
      "epoch:  82, loss: 0.0002662936458364129\n",
      "epoch:  83, loss: 0.0002657934674061835\n",
      "epoch:  84, loss: 0.000265431241132319\n",
      "epoch:  85, loss: 0.00026492885081097484\n",
      "epoch:  86, loss: 0.0002643989573698491\n",
      "epoch:  87, loss: 0.0002640603925101459\n",
      "epoch:  88, loss: 0.0002636104472912848\n",
      "epoch:  89, loss: 0.0002631263341754675\n",
      "epoch:  90, loss: 0.00026284129125997424\n",
      "epoch:  91, loss: 0.0002624071785248816\n",
      "epoch:  92, loss: 0.0002620045852381736\n",
      "epoch:  93, loss: 0.00026165973395109177\n",
      "epoch:  94, loss: 0.00026119413087144494\n",
      "epoch:  95, loss: 0.00026094328495673835\n",
      "epoch:  96, loss: 0.00026061589596793056\n",
      "epoch:  97, loss: 0.00026023545069620013\n",
      "epoch:  98, loss: 0.0002600136795081198\n",
      "epoch:  99, loss: 0.00025974787422455847\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size=X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "opt = torch_numopt.AGD(\n",
    "    model=model,\n",
    "    lr=1,\n",
    "    mu=0.001,\n",
    "    mu_dec=0.1,\n",
    "    use_diagonal=True,\n",
    "    c1=1e-4,\n",
    "    tau=0.1,\n",
    "    line_search_method=\"backtrack\",\n",
    "    line_search_cond=\"armijo\",\n",
    ")\n",
    "\n",
    "all_loss = []\n",
    "patience = 0\n",
    "max_patience = 10\n",
    "for epoch in range(100):\n",
    "    print(\"epoch: \", epoch, end=\"\")\n",
    "    all_loss.append(0)\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y, loss_fn)\n",
    "\n",
    "        all_loss[epoch] += loss\n",
    "\n",
    "    all_loss[epoch] /= len(data_loader)\n",
    "\n",
    "    if epoch > 0 and all_loss[epoch - 1] <= all_loss[epoch]:\n",
    "        patience -= 1\n",
    "    else:\n",
    "        patience = max_patience\n",
    "\n",
    "    print(\", loss: {}\".format(all_loss[epoch].cpu().detach().numpy().item()))\n",
    "\n",
    "    if patience <= 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics: R2 = 0.9935522849884326\n",
      "Test metrics:  R2 = 0.9885562738980905\n"
     ]
    }
   ],
   "source": [
    "pred_train = model.forward(X_train).cpu().detach()\n",
    "pred_test = model.forward(X_test).cpu().detach()\n",
    "print(f\"Train metrics: R2 = {r2_score(pred_train, y_train.cpu())}\")\n",
    "print(f\"Test metrics:  R2 = {r2_score(pred_test, y_test.cpu())}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_numopt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
