{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch_numopt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.f1 = nn.Linear(input_size, 10, device=device)\n",
    "        self.f2 = nn.Linear(10, 20, device=device)\n",
    "        self.f3 = nn.Linear(20, 20, device=device)\n",
    "        self.f4 = nn.Linear(20, 10, device=device)\n",
    "        self.f5 = nn.Linear(10, 1, device=device)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        # self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.f1(x))\n",
    "        x = self.activation(self.f2(x))\n",
    "        x = self.activation(self.f3(x))\n",
    "        x = self.activation(self.f4(x))\n",
    "        x = self.f5(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_diabetes(return_X_y=True, scaled=False)\n",
    "\n",
    "X_scaler = MinMaxScaler()\n",
    "X = X_scaler.fit_transform(X)\n",
    "\n",
    "y_scaler = MinMaxScaler()\n",
    "y = y_scaler.fit_transform(y.reshape((-1, 1)))\n",
    "\n",
    "torch_data = TensorDataset(torch.Tensor(X).to(device), torch.Tensor(y).to(device))\n",
    "data_loader = DataLoader(torch_data, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.4143599569797516\n",
      "epoch:  1, loss: 0.32063186168670654\n",
      "epoch:  2, loss: 0.28701144456863403\n",
      "epoch:  3, loss: 0.03955715522170067\n",
      "epoch:  4, loss: 0.03308854252099991\n",
      "epoch:  5, loss: 0.030783135443925858\n",
      "epoch:  6, loss: 0.029517097398638725\n",
      "epoch:  7, loss: 0.029021598398685455\n",
      "epoch:  8, loss: 0.028166640549898148\n",
      "epoch:  9, loss: 0.027926409617066383\n",
      "epoch:  10, loss: 0.027836577966809273\n",
      "epoch:  11, loss: 0.027816487476229668\n",
      "epoch:  12, loss: 0.027093276381492615\n",
      "epoch:  13, loss: 0.027086913585662842\n",
      "epoch:  14, loss: 0.026605214923620224\n",
      "epoch:  15, loss: 0.02647796832025051\n",
      "epoch:  16, loss: 0.02643202431499958\n",
      "epoch:  17, loss: 0.02639693394303322\n",
      "epoch:  18, loss: 0.0261886827647686\n",
      "epoch:  19, loss: 0.02615484595298767\n",
      "epoch:  20, loss: 0.026142122223973274\n",
      "epoch:  21, loss: 0.026088343933224678\n",
      "epoch:  22, loss: 0.026087898761034012\n",
      "epoch:  23, loss: 0.02602299675345421\n",
      "epoch:  24, loss: 0.026013804599642754\n",
      "epoch:  25, loss: 0.02600683830678463\n",
      "epoch:  26, loss: 0.025953713804483414\n",
      "epoch:  27, loss: 0.02594980224967003\n",
      "epoch:  28, loss: 0.025943322107195854\n",
      "epoch:  29, loss: 0.025893107056617737\n",
      "epoch:  30, loss: 0.025879818946123123\n",
      "epoch:  31, loss: 0.0258724857121706\n",
      "epoch:  32, loss: 0.025851445272564888\n",
      "epoch:  33, loss: 0.02584548480808735\n",
      "epoch:  34, loss: 0.02583944797515869\n",
      "epoch:  35, loss: 0.025814441964030266\n",
      "epoch:  36, loss: 0.02575642615556717\n",
      "epoch:  37, loss: 0.025723548606038094\n",
      "epoch:  38, loss: 0.02564687468111515\n",
      "epoch:  39, loss: 0.025613538920879364\n",
      "epoch:  40, loss: 0.025554941967129707\n",
      "epoch:  41, loss: 0.02553861029446125\n",
      "epoch:  42, loss: 0.025526324287056923\n",
      "epoch:  43, loss: 0.025498496368527412\n",
      "epoch:  44, loss: 0.02549244835972786\n",
      "epoch:  45, loss: 0.025432800874114037\n",
      "epoch:  46, loss: 0.025422250851988792\n",
      "epoch:  47, loss: 0.025397108867764473\n",
      "epoch:  48, loss: 0.025364095345139503\n",
      "epoch:  49, loss: 0.025357211008667946\n",
      "epoch:  50, loss: 0.025340380147099495\n",
      "epoch:  51, loss: 0.025323687121272087\n",
      "epoch:  52, loss: 0.02529679611325264\n",
      "epoch:  53, loss: 0.025280216708779335\n",
      "epoch:  54, loss: 0.025270406156778336\n",
      "epoch:  55, loss: 0.025240294635295868\n",
      "epoch:  56, loss: 0.025238055735826492\n",
      "epoch:  57, loss: 0.025219833478331566\n",
      "epoch:  58, loss: 0.025214076042175293\n",
      "epoch:  59, loss: 0.025206686928868294\n",
      "epoch:  60, loss: 0.02519906684756279\n",
      "epoch:  61, loss: 0.02519742213189602\n",
      "epoch:  62, loss: 0.025189321488142014\n",
      "epoch:  63, loss: 0.025182247161865234\n",
      "epoch:  64, loss: 0.025171056389808655\n",
      "epoch:  65, loss: 0.025158856064081192\n",
      "epoch:  66, loss: 0.025116829201579094\n",
      "epoch:  67, loss: 0.025067459791898727\n",
      "epoch:  68, loss: 0.025043286383152008\n",
      "epoch:  69, loss: 0.025033803656697273\n",
      "epoch:  70, loss: 0.024994799867272377\n",
      "epoch:  71, loss: 0.024987267330288887\n",
      "epoch:  72, loss: 0.024953700602054596\n",
      "epoch:  73, loss: 0.024947552010416985\n",
      "epoch:  74, loss: 0.02494727447628975\n",
      "epoch:  75, loss: 0.02493409626185894\n",
      "epoch:  76, loss: 0.024914365261793137\n",
      "epoch:  77, loss: 0.02489744871854782\n",
      "epoch:  78, loss: 0.024886447936296463\n",
      "epoch:  79, loss: 0.024886317551136017\n",
      "epoch:  80, loss: 0.02487940527498722\n",
      "epoch:  81, loss: 0.02487891912460327\n",
      "epoch:  82, loss: 0.024866143241524696\n",
      "epoch:  83, loss: 0.024861285462975502\n",
      "epoch:  84, loss: 0.024853145703673363\n",
      "epoch:  85, loss: 0.02484971657395363\n",
      "epoch:  86, loss: 0.024847252294421196\n",
      "epoch:  87, loss: 0.024845832958817482\n",
      "epoch:  88, loss: 0.024843847379088402\n",
      "epoch:  89, loss: 0.024842455983161926\n",
      "epoch:  90, loss: 0.024835165590047836\n",
      "epoch:  91, loss: 0.024832816794514656\n",
      "epoch:  92, loss: 0.024830332025885582\n",
      "epoch:  93, loss: 0.024829305708408356\n",
      "epoch:  94, loss: 0.024827491492033005\n",
      "epoch:  95, loss: 0.024825874716043472\n",
      "epoch:  96, loss: 0.024824365973472595\n",
      "epoch:  97, loss: 0.024822726845741272\n",
      "epoch:  98, loss: 0.024821851402521133\n",
      "epoch:  99, loss: 0.024821218103170395\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size=X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "opt = torch_numopt.AGD(\n",
    "    model.parameters(),\n",
    "    lr=1,\n",
    "    mu=0.001,\n",
    "    mu_dec=0.1,\n",
    "    model=model,\n",
    "    use_diagonal=False,\n",
    "    c1=1e-4,\n",
    "    tau=0.5,\n",
    "    line_search_method=\"backtrack\",\n",
    "    line_search_cond=\"armijo\",\n",
    ")\n",
    "\n",
    "all_loss = []\n",
    "patience = 0\n",
    "max_patience = 10\n",
    "for epoch in range(100):\n",
    "    print(\"epoch: \", epoch, end=\"\")\n",
    "    all_loss.append(0)\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y, loss_fn)\n",
    "\n",
    "        all_loss[epoch] += loss\n",
    "\n",
    "    all_loss[epoch] /= len(data_loader)\n",
    "\n",
    "    if epoch > 0 and all_loss[epoch - 1] <= all_loss[epoch]:\n",
    "        patience -= 1\n",
    "    else:\n",
    "        patience = max_patience\n",
    "\n",
    "    print(\", loss: {}\".format(all_loss[epoch].detach().cpu().numpy().item()))\n",
    "\n",
    "    if patience <= 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.33186641335487366\n",
      "epoch:  1, loss: 0.13097988069057465\n",
      "epoch:  2, loss: 0.05423133447766304\n",
      "epoch:  3, loss: 0.043436627835035324\n",
      "epoch:  4, loss: 0.031093105673789978\n",
      "epoch:  5, loss: 0.028638187795877457\n",
      "epoch:  6, loss: 0.027640851214528084\n",
      "epoch:  7, loss: 0.026913736015558243\n",
      "epoch:  8, loss: 0.026571044698357582\n",
      "epoch:  9, loss: 0.02619623765349388\n",
      "epoch:  10, loss: 0.02598540298640728\n",
      "epoch:  11, loss: 0.025838101282715797\n",
      "epoch:  12, loss: 0.025686677545309067\n",
      "epoch:  13, loss: 0.02554275281727314\n",
      "epoch:  14, loss: 0.025423740968108177\n",
      "epoch:  15, loss: 0.025319743901491165\n",
      "epoch:  16, loss: 0.025178784504532814\n",
      "epoch:  17, loss: 0.02508353814482689\n",
      "epoch:  18, loss: 0.025005122646689415\n",
      "epoch:  19, loss: 0.024940500035881996\n",
      "epoch:  20, loss: 0.02487681806087494\n",
      "epoch:  21, loss: 0.024835428223013878\n",
      "epoch:  22, loss: 0.024798540398478508\n",
      "epoch:  23, loss: 0.024725237861275673\n",
      "epoch:  24, loss: 0.024636441841721535\n",
      "epoch:  25, loss: 0.024546850472688675\n",
      "epoch:  26, loss: 0.024475939571857452\n",
      "epoch:  27, loss: 0.02436351403594017\n",
      "epoch:  28, loss: 0.02428608387708664\n",
      "epoch:  29, loss: 0.02420460432767868\n",
      "epoch:  30, loss: 0.02410995028913021\n",
      "epoch:  31, loss: 0.024016624316573143\n",
      "epoch:  32, loss: 0.023942770436406136\n",
      "epoch:  33, loss: 0.02393062599003315\n",
      "epoch:  34, loss: 0.023919198662042618\n",
      "epoch:  35, loss: 0.023829709738492966\n",
      "epoch:  36, loss: 0.023678608238697052\n",
      "epoch:  37, loss: 0.023513032123446465\n",
      "epoch:  38, loss: 0.02332487888634205\n",
      "epoch:  39, loss: 0.023128274828195572\n",
      "epoch:  40, loss: 0.022985350340604782\n",
      "epoch:  41, loss: 0.02290351502597332\n",
      "epoch:  42, loss: 0.02280372567474842\n",
      "epoch:  43, loss: 0.02274862863123417\n",
      "epoch:  44, loss: 0.02270662598311901\n",
      "epoch:  45, loss: 0.022667862474918365\n",
      "epoch:  46, loss: 0.022642988711595535\n",
      "epoch:  47, loss: 0.022562295198440552\n",
      "epoch:  48, loss: 0.022546114400029182\n",
      "epoch:  49, loss: 0.022536054253578186\n",
      "epoch:  50, loss: 0.022511927410960197\n",
      "epoch:  51, loss: 0.02250107377767563\n",
      "epoch:  52, loss: 0.022495197132229805\n",
      "epoch:  53, loss: 0.022490287199616432\n",
      "epoch:  54, loss: 0.02247561886906624\n",
      "epoch:  55, loss: 0.02247515507042408\n",
      "epoch:  56, loss: 0.02245907485485077\n",
      "epoch:  57, loss: 0.022444387897849083\n",
      "epoch:  58, loss: 0.02243492379784584\n",
      "epoch:  59, loss: 0.022423822432756424\n",
      "epoch:  60, loss: 0.0224215816706419\n",
      "epoch:  61, loss: 0.02242072857916355\n",
      "epoch:  62, loss: 0.02237694151699543\n",
      "epoch:  63, loss: 0.0223655104637146\n",
      "epoch:  64, loss: 0.022356655448675156\n",
      "epoch:  65, loss: 0.02234724909067154\n",
      "epoch:  66, loss: 0.022336920723319054\n",
      "epoch:  67, loss: 0.02233104780316353\n",
      "epoch:  68, loss: 0.0223255455493927\n",
      "epoch:  69, loss: 0.022321181371808052\n",
      "epoch:  70, loss: 0.022317564114928246\n",
      "epoch:  71, loss: 0.022314874455332756\n",
      "epoch:  72, loss: 0.02231212705373764\n",
      "epoch:  73, loss: 0.022310517728328705\n",
      "epoch:  74, loss: 0.022309916093945503\n",
      "epoch:  75, loss: 0.022308485582470894\n",
      "epoch:  76, loss: 0.022306112572550774\n",
      "epoch:  77, loss: 0.022304454818367958\n",
      "epoch:  78, loss: 0.022304026409983635\n",
      "epoch:  79, loss: 0.02230369858443737\n",
      "epoch:  80, loss: 0.022303598001599312\n",
      "epoch:  81, loss: 0.02230207808315754\n",
      "epoch:  82, loss: 0.022300489246845245\n",
      "epoch:  83, loss: 0.022299688309431076\n",
      "epoch:  84, loss: 0.022298350930213928\n",
      "epoch:  85, loss: 0.02229592576622963\n",
      "epoch:  86, loss: 0.022295858711004257\n",
      "epoch:  87, loss: 0.022293176501989365\n",
      "epoch:  88, loss: 0.022292982786893845\n",
      "epoch:  89, loss: 0.022292746230959892\n",
      "epoch:  90, loss: 0.02229171060025692\n",
      "epoch:  91, loss: 0.022290395572781563\n",
      "epoch:  92, loss: 0.022288653999567032\n",
      "epoch:  93, loss: 0.02228684537112713\n",
      "epoch:  94, loss: 0.022286491468548775\n",
      "epoch:  95, loss: 0.022284509614109993\n",
      "epoch:  96, loss: 0.02228110283613205\n",
      "epoch:  97, loss: 0.022279886528849602\n",
      "epoch:  98, loss: 0.022278979420661926\n",
      "epoch:  99, loss: 0.02227703295648098\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size=X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "opt = torch_numopt.AGD(\n",
    "    model.parameters(),\n",
    "    lr=1,\n",
    "    mu=0.001,\n",
    "    mu_dec=0.1,\n",
    "    model=model,\n",
    "    use_diagonal=True,\n",
    "    c1=1e-4,\n",
    "    tau=0.1,\n",
    "    line_search_method=\"backtrack\",\n",
    "    line_search_cond=\"armijo\",\n",
    ")\n",
    "\n",
    "all_loss = []\n",
    "patience = 0\n",
    "max_patience = 10\n",
    "for epoch in range(100):\n",
    "    print(\"epoch: \", epoch, end=\"\")\n",
    "    all_loss.append(0)\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y, loss_fn)\n",
    "\n",
    "        all_loss[epoch] += loss\n",
    "\n",
    "    all_loss[epoch] /= len(data_loader)\n",
    "\n",
    "    if epoch > 0 and all_loss[epoch - 1] <= all_loss[epoch]:\n",
    "        patience -= 1\n",
    "    else:\n",
    "        patience = max_patience\n",
    "\n",
    "    print(\", loss: {}\".format(all_loss[epoch].detach().cpu().numpy().item()))\n",
    "\n",
    "    if patience <= 0:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
