{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pytorch_soom\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.f1 = nn.Linear(input_size, 10, device=device)\n",
    "        self.f2 = nn.Linear(10, 20, device=device)\n",
    "        self.f3 = nn.Linear(20, 20, device=device)\n",
    "        self.f4 = nn.Linear(20, 10, device=device)\n",
    "        self.f5 = nn.Linear(10, 1, device=device)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        # self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.f1(x))\n",
    "        x = self.activation(self.f2(x))\n",
    "        x = self.activation(self.f3(x))\n",
    "        x = self.activation(self.f4(x))\n",
    "        x = self.f5(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_diabetes(return_X_y = True, scaled=False)\n",
    "\n",
    "X_scaler = MinMaxScaler()\n",
    "X = X_scaler.fit_transform(X)\n",
    "\n",
    "y_scaler = MinMaxScaler()\n",
    "y = y_scaler.fit_transform(y.reshape((-1, 1)))\n",
    "\n",
    "torch_data = TensorDataset(torch.Tensor(X).to(device), torch.Tensor(y).to(device))\n",
    "data_loader = DataLoader(torch_data, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.06829997897148132\n",
      "epoch:  1, loss: 0.05813758820295334\n",
      "epoch:  2, loss: 0.05378345027565956\n",
      "epoch:  3, loss: 0.04686214402318001\n",
      "epoch:  4, loss: 0.034382425248622894\n",
      "epoch:  5, loss: 0.030990509316325188\n",
      "epoch:  6, loss: 0.029864884912967682\n",
      "epoch:  7, loss: 0.029835103079676628\n",
      "epoch:  8, loss: 0.028558358550071716\n",
      "epoch:  9, loss: 0.02830231562256813\n",
      "epoch:  10, loss: 0.02792144939303398\n",
      "epoch:  11, loss: 0.02752295322716236\n",
      "epoch:  12, loss: 0.027108754962682724\n",
      "epoch:  13, loss: 0.02680131420493126\n",
      "epoch:  14, loss: 0.02664206176996231\n",
      "epoch:  15, loss: 0.02625430002808571\n",
      "epoch:  16, loss: 0.02615406922996044\n",
      "epoch:  17, loss: 0.02565639652311802\n",
      "epoch:  18, loss: 0.025568075478076935\n",
      "epoch:  19, loss: 0.025379784405231476\n",
      "epoch:  20, loss: 0.02490023337304592\n",
      "epoch:  21, loss: 0.024666495621204376\n",
      "epoch:  22, loss: 0.02458113431930542\n",
      "epoch:  23, loss: 0.02425813116133213\n",
      "epoch:  24, loss: 0.02418137900531292\n",
      "epoch:  25, loss: 0.02415066957473755\n",
      "epoch:  26, loss: 0.02395145408809185\n",
      "epoch:  27, loss: 0.02389882132411003\n",
      "epoch:  28, loss: 0.02387532964348793\n",
      "epoch:  29, loss: 0.023778000846505165\n",
      "epoch:  30, loss: 0.02366861142218113\n",
      "epoch:  31, loss: 0.023660030215978622\n",
      "epoch:  32, loss: 0.023608429357409477\n",
      "epoch:  33, loss: 0.0235956571996212\n",
      "epoch:  34, loss: 0.02357431687414646\n",
      "epoch:  35, loss: 0.023502890020608902\n",
      "epoch:  36, loss: 0.02350238710641861\n",
      "epoch:  37, loss: 0.023490309715270996\n",
      "epoch:  38, loss: 0.023484274744987488\n",
      "epoch:  39, loss: 0.023435622453689575\n",
      "epoch:  40, loss: 0.023420225828886032\n",
      "epoch:  41, loss: 0.02340291067957878\n",
      "epoch:  42, loss: 0.02333894930779934\n",
      "epoch:  43, loss: 0.023318499326705933\n",
      "epoch:  44, loss: 0.023309454321861267\n",
      "epoch:  45, loss: 0.02327786572277546\n",
      "epoch:  46, loss: 0.023262979462742805\n",
      "epoch:  47, loss: 0.02325407601892948\n",
      "epoch:  48, loss: 0.023218709975481033\n",
      "epoch:  49, loss: 0.02321154810488224\n",
      "epoch:  50, loss: 0.023182300850749016\n",
      "epoch:  51, loss: 0.02317722514271736\n",
      "epoch:  52, loss: 0.02312183566391468\n",
      "epoch:  53, loss: 0.023079195991158485\n",
      "epoch:  54, loss: 0.023045921698212624\n",
      "epoch:  55, loss: 0.023024890571832657\n",
      "epoch:  56, loss: 0.022994913160800934\n",
      "epoch:  57, loss: 0.022988753393292427\n",
      "epoch:  58, loss: 0.02294224500656128\n",
      "epoch:  59, loss: 0.022900814190506935\n",
      "epoch:  60, loss: 0.022878985852003098\n",
      "epoch:  61, loss: 0.022857872769236565\n",
      "epoch:  62, loss: 0.022758932784199715\n",
      "epoch:  63, loss: 0.022716617211699486\n",
      "epoch:  64, loss: 0.022624125704169273\n",
      "epoch:  65, loss: 0.022562557831406593\n",
      "epoch:  66, loss: 0.02250737138092518\n",
      "epoch:  67, loss: 0.02236282266676426\n",
      "epoch:  68, loss: 0.022356877103447914\n",
      "epoch:  69, loss: 0.022201914340257645\n",
      "epoch:  70, loss: 0.02200513705611229\n",
      "epoch:  71, loss: 0.021924948319792747\n",
      "epoch:  72, loss: 0.021880697458982468\n",
      "epoch:  73, loss: 0.021819792687892914\n",
      "epoch:  74, loss: 0.02164429798722267\n",
      "epoch:  75, loss: 0.021559759974479675\n",
      "epoch:  76, loss: 0.021455343812704086\n",
      "epoch:  77, loss: 0.021337442100048065\n",
      "epoch:  78, loss: 0.021333593875169754\n",
      "epoch:  79, loss: 0.02110857143998146\n",
      "epoch:  80, loss: 0.02101018652319908\n",
      "epoch:  81, loss: 0.0209715124219656\n",
      "epoch:  82, loss: 0.020857669413089752\n",
      "epoch:  83, loss: 0.020801130682229996\n",
      "epoch:  84, loss: 0.020723221823573112\n",
      "epoch:  85, loss: 0.020695477724075317\n",
      "epoch:  86, loss: 0.02062649466097355\n",
      "epoch:  87, loss: 0.020532788708806038\n",
      "epoch:  88, loss: 0.02048514597117901\n",
      "epoch:  89, loss: 0.020466424524784088\n",
      "epoch:  90, loss: 0.020416613668203354\n",
      "epoch:  91, loss: 0.020359354093670845\n",
      "epoch:  92, loss: 0.0203438438475132\n",
      "epoch:  93, loss: 0.02033277601003647\n",
      "epoch:  94, loss: 0.020290741696953773\n",
      "epoch:  95, loss: 0.020159652456641197\n",
      "epoch:  96, loss: 0.020120060071349144\n",
      "epoch:  97, loss: 0.02006400376558304\n",
      "epoch:  98, loss: 0.02003672532737255\n",
      "epoch:  99, loss: 0.020020106807351112\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size = X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "opt = pytorch_soom.AGD(model.parameters(), lr=1, mu=0.001, mu_dec=0.1, model=model, use_diagonal=False, c1=1e-4, tau=0.5, line_search_method=\"backtrack\", line_search_cond=\"armijo\")\n",
    "\n",
    "all_loss = []\n",
    "patience = 0\n",
    "max_patience = 10\n",
    "for epoch in range(100):\n",
    "    print('epoch: ', epoch, end='')\n",
    "    all_loss.append(0)\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y, loss_fn)\n",
    "\n",
    "        all_loss[epoch] += loss\n",
    "    \n",
    "    all_loss[epoch] /= len(data_loader)\n",
    "\n",
    "    if epoch > 0 and all_loss[epoch-1] <= all_loss[epoch]:\n",
    "        patience -= 1\n",
    "    else:\n",
    "        patience = max_patience\n",
    "        \n",
    "\n",
    "    print(', loss: {}'.format(all_loss[epoch].detach().cpu().numpy().item()))\n",
    "\n",
    "    if patience <= 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.3240857422351837\n",
      "epoch:  1, loss: 0.21067820489406586\n",
      "epoch:  2, loss: 0.17104612290859222\n",
      "epoch:  3, loss: 0.06354469060897827\n",
      "epoch:  4, loss: 0.051769256591796875\n",
      "epoch:  5, loss: 0.046311840415000916\n",
      "epoch:  6, loss: 0.042809393256902695\n",
      "epoch:  7, loss: 0.032168056815862656\n",
      "epoch:  8, loss: 0.029955752193927765\n",
      "epoch:  9, loss: 0.02869473770260811\n",
      "epoch:  10, loss: 0.028109196573495865\n",
      "epoch:  11, loss: 0.027837641537189484\n",
      "epoch:  12, loss: 0.027670446783304214\n",
      "epoch:  13, loss: 0.02755667455494404\n",
      "epoch:  14, loss: 0.027445875108242035\n",
      "epoch:  15, loss: 0.027359776198863983\n",
      "epoch:  16, loss: 0.02729720063507557\n",
      "epoch:  17, loss: 0.027219567447900772\n",
      "epoch:  18, loss: 0.027035871520638466\n",
      "epoch:  19, loss: 0.02681972272694111\n",
      "epoch:  20, loss: 0.026701845228672028\n",
      "epoch:  21, loss: 0.02659487910568714\n",
      "epoch:  22, loss: 0.02649693191051483\n",
      "epoch:  23, loss: 0.026419643312692642\n",
      "epoch:  24, loss: 0.026222681626677513\n",
      "epoch:  25, loss: 0.026145461946725845\n",
      "epoch:  26, loss: 0.026119044050574303\n",
      "epoch:  27, loss: 0.02604631893336773\n",
      "epoch:  28, loss: 0.02602524124085903\n",
      "epoch:  29, loss: 0.02596263960003853\n",
      "epoch:  30, loss: 0.025807341560721397\n",
      "epoch:  31, loss: 0.02578681893646717\n",
      "epoch:  32, loss: 0.025764575228095055\n",
      "epoch:  33, loss: 0.025745538994669914\n",
      "epoch:  34, loss: 0.025725271552801132\n",
      "epoch:  35, loss: 0.02571307308971882\n",
      "epoch:  36, loss: 0.025703679770231247\n",
      "epoch:  37, loss: 0.025694342330098152\n",
      "epoch:  38, loss: 0.025685347616672516\n",
      "epoch:  39, loss: 0.025676971301436424\n",
      "epoch:  40, loss: 0.0256694033741951\n",
      "epoch:  41, loss: 0.02558917924761772\n",
      "epoch:  42, loss: 0.025521129369735718\n",
      "epoch:  43, loss: 0.025471854954957962\n",
      "epoch:  44, loss: 0.02545882761478424\n",
      "epoch:  45, loss: 0.02545270510017872\n",
      "epoch:  46, loss: 0.025440210476517677\n",
      "epoch:  47, loss: 0.02540961280465126\n",
      "epoch:  48, loss: 0.025380533188581467\n",
      "epoch:  49, loss: 0.025367097929120064\n",
      "epoch:  50, loss: 0.02535763569176197\n",
      "epoch:  51, loss: 0.025333192199468613\n",
      "epoch:  52, loss: 0.025321777909994125\n",
      "epoch:  53, loss: 0.025314632803201675\n",
      "epoch:  54, loss: 0.025311602279543877\n",
      "epoch:  55, loss: 0.025308068841695786\n",
      "epoch:  56, loss: 0.025304513052105904\n",
      "epoch:  57, loss: 0.02530374377965927\n",
      "epoch:  58, loss: 0.02528984658420086\n",
      "epoch:  59, loss: 0.025282500311732292\n",
      "epoch:  60, loss: 0.025276020169258118\n",
      "epoch:  61, loss: 0.02527172490954399\n",
      "epoch:  62, loss: 0.025269191712141037\n",
      "epoch:  63, loss: 0.0252671018242836\n",
      "epoch:  64, loss: 0.0252668596804142\n",
      "epoch:  65, loss: 0.02525981143116951\n",
      "epoch:  66, loss: 0.02525370754301548\n",
      "epoch:  67, loss: 0.025250036269426346\n",
      "epoch:  68, loss: 0.025247342884540558\n",
      "epoch:  69, loss: 0.025244388729333878\n",
      "epoch:  70, loss: 0.02524344064295292\n",
      "epoch:  71, loss: 0.025242192670702934\n",
      "epoch:  72, loss: 0.025240732356905937\n",
      "epoch:  73, loss: 0.025238709524273872\n",
      "epoch:  74, loss: 0.02523789182305336\n",
      "epoch:  75, loss: 0.02523748390376568\n",
      "epoch:  76, loss: 0.02523551508784294\n",
      "epoch:  77, loss: 0.025234075263142586\n",
      "epoch:  78, loss: 0.025233415886759758\n",
      "epoch:  79, loss: 0.02523273602128029\n",
      "epoch:  80, loss: 0.02523091994225979\n",
      "epoch:  81, loss: 0.025229834020137787\n",
      "epoch:  82, loss: 0.02522934600710869\n",
      "epoch:  83, loss: 0.025227557867765427\n",
      "epoch:  84, loss: 0.025227274745702744\n",
      "epoch:  85, loss: 0.025227047502994537\n",
      "epoch:  86, loss: 0.025226905941963196\n",
      "epoch:  87, loss: 0.025226706638932228\n",
      "epoch:  88, loss: 0.025226525962352753\n",
      "epoch:  89, loss: 0.02522650919854641\n",
      "epoch:  90, loss: 0.02522648312151432\n",
      "epoch:  91, loss: 0.025226468220353127\n",
      "epoch:  92, loss: 0.025226466357707977\n",
      "epoch:  93, loss: 0.025226440280675888\n",
      "epoch:  94, loss: 0.02522638812661171\n",
      "epoch:  95, loss: 0.025226380676031113\n",
      "epoch:  96, loss: 0.02522636018693447\n",
      "epoch:  97, loss: 0.02522633597254753\n",
      "epoch:  98, loss: 0.025226283818483353\n",
      "epoch:  99, loss: 0.025226246565580368\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size = X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "opt = pytorch_soom.AGD(model.parameters(), lr=1, mu=0.001, mu_dec=0.1, model=model, use_diagonal=True, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"armijo\")\n",
    "\n",
    "all_loss = []\n",
    "patience = 0\n",
    "max_patience = 10\n",
    "for epoch in range(100):\n",
    "    print('epoch: ', epoch, end='')\n",
    "    all_loss.append(0)\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y, loss_fn)\n",
    "\n",
    "        all_loss[epoch] += loss\n",
    "    \n",
    "    all_loss[epoch] /= len(data_loader)\n",
    "\n",
    "    if epoch > 0 and all_loss[epoch-1] <= all_loss[epoch]:\n",
    "        patience -= 1\n",
    "    else:\n",
    "        patience = max_patience\n",
    "        \n",
    "\n",
    "    print(', loss: {}'.format(all_loss[epoch].detach().cpu().numpy().item()))\n",
    "\n",
    "    if patience <= 0:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
