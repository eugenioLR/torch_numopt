{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pytorch_soom\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.f1 = nn.Linear(input_size, 10, device=device)\n",
    "        self.f2 = nn.Linear(10, 20, device=device)\n",
    "        self.f3 = nn.Linear(20, 20, device=device)\n",
    "        self.f4 = nn.Linear(20, 10, device=device)\n",
    "        self.f5 = nn.Linear(10, 1, device=device)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        # self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.f1(x))\n",
    "        x = self.activation(self.f2(x))\n",
    "        x = self.activation(self.f3(x))\n",
    "        x = self.activation(self.f4(x))\n",
    "        x = self.f5(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_diabetes(return_X_y = True, scaled=False)\n",
    "\n",
    "X_scaler = MinMaxScaler()\n",
    "X = X_scaler.fit_transform(X)\n",
    "\n",
    "y_scaler = MinMaxScaler()\n",
    "y = y_scaler.fit_transform(y.reshape((-1, 1)))\n",
    "\n",
    "torch_data = TensorDataset(torch.Tensor(X).to(device), torch.Tensor(y).to(device))\n",
    "data_loader = DataLoader(torch_data, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.43689537048339844\n",
      "epoch:  1, loss: 0.05907290056347847\n",
      "epoch:  2, loss: 0.04560737684369087\n",
      "epoch:  3, loss: 0.041200727224349976\n",
      "epoch:  4, loss: 0.03535321727395058\n",
      "epoch:  5, loss: 0.033263131976127625\n",
      "epoch:  6, loss: 0.03005622699856758\n",
      "epoch:  7, loss: 0.029424870386719704\n",
      "epoch:  8, loss: 0.028712015599012375\n",
      "epoch:  9, loss: 0.028429320082068443\n",
      "epoch:  10, loss: 0.02814001403748989\n",
      "epoch:  11, loss: 0.028067830950021744\n",
      "epoch:  12, loss: 0.027942219749093056\n",
      "epoch:  13, loss: 0.027717486023902893\n",
      "epoch:  14, loss: 0.027663206681609154\n",
      "epoch:  15, loss: 0.027647634968161583\n",
      "epoch:  16, loss: 0.027562296018004417\n",
      "epoch:  17, loss: 0.02752431109547615\n",
      "epoch:  18, loss: 0.027462512254714966\n",
      "epoch:  19, loss: 0.02742578461766243\n",
      "epoch:  20, loss: 0.027356518432497978\n",
      "epoch:  21, loss: 0.027337083593010902\n",
      "epoch:  22, loss: 0.027305660769343376\n",
      "epoch:  23, loss: 0.027239618822932243\n",
      "epoch:  24, loss: 0.027230247855186462\n",
      "epoch:  25, loss: 0.026729874312877655\n",
      "epoch:  26, loss: 0.0266176238656044\n",
      "epoch:  27, loss: 0.026582108810544014\n",
      "epoch:  28, loss: 0.026372672989964485\n",
      "epoch:  29, loss: 0.026232456788420677\n",
      "epoch:  30, loss: 0.02604065090417862\n",
      "epoch:  31, loss: 0.02596958540380001\n",
      "epoch:  32, loss: 0.025937940925359726\n",
      "epoch:  33, loss: 0.02591959945857525\n",
      "epoch:  34, loss: 0.02591164968907833\n",
      "epoch:  35, loss: 0.02587692253291607\n",
      "epoch:  36, loss: 0.025841789320111275\n",
      "epoch:  37, loss: 0.025827055796980858\n",
      "epoch:  38, loss: 0.025815267115831375\n",
      "epoch:  39, loss: 0.025808274745941162\n",
      "epoch:  40, loss: 0.025807462632656097\n",
      "epoch:  41, loss: 0.025800053030252457\n",
      "epoch:  42, loss: 0.025797784328460693\n",
      "epoch:  43, loss: 0.02579282969236374\n",
      "epoch:  44, loss: 0.02578982524573803\n",
      "epoch:  45, loss: 0.025786662474274635\n",
      "epoch:  46, loss: 0.025786103680729866\n",
      "epoch:  47, loss: 0.02578604593873024\n",
      "epoch:  48, loss: 0.025781726464629173\n",
      "epoch:  49, loss: 0.02578028477728367\n",
      "epoch:  50, loss: 0.025775691494345665\n",
      "epoch:  51, loss: 0.0257743950933218\n",
      "epoch:  52, loss: 0.025772418826818466\n",
      "epoch:  53, loss: 0.025770165026187897\n",
      "epoch:  54, loss: 0.025768747553229332\n",
      "epoch:  55, loss: 0.025767844170331955\n",
      "epoch:  56, loss: 0.025766296312212944\n",
      "epoch:  57, loss: 0.025764992460608482\n",
      "epoch:  58, loss: 0.025764860212802887\n",
      "epoch:  59, loss: 0.025762854143977165\n",
      "epoch:  60, loss: 0.025760015472769737\n",
      "epoch:  61, loss: 0.025758951902389526\n",
      "epoch:  62, loss: 0.02575843781232834\n",
      "epoch:  63, loss: 0.02575843222439289\n",
      "epoch:  64, loss: 0.025753043591976166\n",
      "epoch:  65, loss: 0.02575148642063141\n",
      "epoch:  66, loss: 0.025750920176506042\n",
      "epoch:  67, loss: 0.025747481733560562\n",
      "epoch:  68, loss: 0.025746231898665428\n",
      "epoch:  69, loss: 0.02574530430138111\n",
      "epoch:  70, loss: 0.02574380673468113\n",
      "epoch:  71, loss: 0.02574124000966549\n",
      "epoch:  72, loss: 0.025739410892128944\n",
      "epoch:  73, loss: 0.0257380623370409\n",
      "epoch:  74, loss: 0.025738053023815155\n",
      "epoch:  75, loss: 0.025732815265655518\n",
      "epoch:  76, loss: 0.025722257792949677\n",
      "epoch:  77, loss: 0.025720927864313126\n",
      "epoch:  78, loss: 0.025719115510582924\n",
      "epoch:  79, loss: 0.025716658681631088\n",
      "epoch:  80, loss: 0.025714917108416557\n",
      "epoch:  81, loss: 0.02571491338312626\n",
      "epoch:  82, loss: 0.025714805349707603\n",
      "epoch:  83, loss: 0.02571120485663414\n",
      "epoch:  84, loss: 0.02571040205657482\n",
      "epoch:  85, loss: 0.02570968121290207\n",
      "epoch:  86, loss: 0.0257096067070961\n",
      "epoch:  87, loss: 0.025707127526402473\n",
      "epoch:  88, loss: 0.025702159851789474\n",
      "epoch:  89, loss: 0.025701573118567467\n",
      "epoch:  90, loss: 0.025699583813548088\n",
      "epoch:  91, loss: 0.025698166340589523\n",
      "epoch:  92, loss: 0.025696290656924248\n",
      "epoch:  93, loss: 0.025695165619254112\n",
      "epoch:  94, loss: 0.025693917647004128\n",
      "epoch:  95, loss: 0.02569320984184742\n",
      "epoch:  96, loss: 0.02569320797920227\n",
      "epoch:  97, loss: 0.025691520422697067\n",
      "epoch:  98, loss: 0.025690091773867607\n",
      "epoch:  99, loss: 0.025686942040920258\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size = X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "opt = pytorch_soom.AGD(model.parameters(), lr=1, mu=0.001, mu_dec=0.1, model=model, use_diagonal=False, c1=1e-4, tau=0.5, line_search_method=\"backtrack\", line_search_cond=\"armijo\")\n",
    "\n",
    "all_loss = []\n",
    "patience = 0\n",
    "max_patience = 10\n",
    "for epoch in range(100):\n",
    "    print('epoch: ', epoch, end='')\n",
    "    all_loss.append(0)\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y, loss_fn)\n",
    "\n",
    "        all_loss[epoch] += loss\n",
    "    \n",
    "    all_loss[epoch] /= len(data_loader)\n",
    "\n",
    "    if epoch > 0 and all_loss[epoch-1] <= all_loss[epoch]:\n",
    "        patience -= 1\n",
    "    else:\n",
    "        patience = max_patience\n",
    "        \n",
    "\n",
    "    print(', loss: {}'.format(all_loss[epoch].detach().cpu().numpy().item()))\n",
    "\n",
    "    if patience <= 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.05859089270234108\n",
      "epoch:  1, loss: 0.057018253952264786\n",
      "epoch:  2, loss: 0.04958638176321983\n",
      "epoch:  3, loss: 0.0476008877158165\n",
      "epoch:  4, loss: 0.03126613423228264\n",
      "epoch:  5, loss: 0.030467860400676727\n",
      "epoch:  6, loss: 0.027141306549310684\n",
      "epoch:  7, loss: 0.026371723040938377\n",
      "epoch:  8, loss: 0.025270327925682068\n",
      "epoch:  9, loss: 0.02481643669307232\n",
      "epoch:  10, loss: 0.0244156401604414\n",
      "epoch:  11, loss: 0.02395530976355076\n",
      "epoch:  12, loss: 0.023531224578619003\n",
      "epoch:  13, loss: 0.0234550591558218\n",
      "epoch:  14, loss: 0.023382103070616722\n",
      "epoch:  15, loss: 0.02317837066948414\n",
      "epoch:  16, loss: 0.022751931101083755\n",
      "epoch:  17, loss: 0.022498449310660362\n",
      "epoch:  18, loss: 0.022329850122332573\n",
      "epoch:  19, loss: 0.02223997563123703\n",
      "epoch:  20, loss: 0.021974263712763786\n",
      "epoch:  21, loss: 0.021951016038656235\n",
      "epoch:  22, loss: 0.0217050239443779\n",
      "epoch:  23, loss: 0.021404843777418137\n",
      "epoch:  24, loss: 0.021258600056171417\n",
      "epoch:  25, loss: 0.020987771451473236\n",
      "epoch:  26, loss: 0.020738951861858368\n",
      "epoch:  27, loss: 0.020388703793287277\n",
      "epoch:  28, loss: 0.020225483924150467\n",
      "epoch:  29, loss: 0.02011096477508545\n",
      "epoch:  30, loss: 0.019800527021288872\n",
      "epoch:  31, loss: 0.019669316709041595\n",
      "epoch:  32, loss: 0.019442779943346977\n",
      "epoch:  33, loss: 0.01926814392209053\n",
      "epoch:  34, loss: 0.019135216251015663\n",
      "epoch:  35, loss: 0.01893274299800396\n",
      "epoch:  36, loss: 0.018785512074828148\n",
      "epoch:  37, loss: 0.018620071932673454\n",
      "epoch:  38, loss: 0.01854434236884117\n",
      "epoch:  39, loss: 0.018485408276319504\n",
      "epoch:  40, loss: 0.01821322739124298\n",
      "epoch:  41, loss: 0.018054794520139694\n",
      "epoch:  42, loss: 0.017850743606686592\n",
      "epoch:  43, loss: 0.017689146101474762\n",
      "epoch:  44, loss: 0.017607036978006363\n",
      "epoch:  45, loss: 0.017367184162139893\n",
      "epoch:  46, loss: 0.01726491004228592\n",
      "epoch:  47, loss: 0.017076263204216957\n",
      "epoch:  48, loss: 0.01691255532205105\n",
      "epoch:  49, loss: 0.01679530180990696\n",
      "epoch:  50, loss: 0.016648786142468452\n",
      "epoch:  51, loss: 0.016591712832450867\n",
      "epoch:  52, loss: 0.01657092571258545\n",
      "epoch:  53, loss: 0.016459206119179726\n",
      "epoch:  54, loss: 0.01638192869722843\n",
      "epoch:  55, loss: 0.016365159302949905\n",
      "epoch:  56, loss: 0.01628619246184826\n",
      "epoch:  57, loss: 0.01613721065223217\n",
      "epoch:  58, loss: 0.016007522121071815\n",
      "epoch:  59, loss: 0.015881964936852455\n",
      "epoch:  60, loss: 0.015754476189613342\n",
      "epoch:  61, loss: 0.015667060390114784\n",
      "epoch:  62, loss: 0.015595567412674427\n",
      "epoch:  63, loss: 0.015515902079641819\n",
      "epoch:  64, loss: 0.015416957437992096\n",
      "epoch:  65, loss: 0.015393353998661041\n",
      "epoch:  66, loss: 0.015296862460672855\n",
      "epoch:  67, loss: 0.01522219367325306\n",
      "epoch:  68, loss: 0.015186399221420288\n",
      "epoch:  69, loss: 0.015075424686074257\n",
      "epoch:  70, loss: 0.014895944856107235\n",
      "epoch:  71, loss: 0.014832688495516777\n",
      "epoch:  72, loss: 0.01472137775272131\n",
      "epoch:  73, loss: 0.014667415991425514\n",
      "epoch:  74, loss: 0.014560304582118988\n",
      "epoch:  75, loss: 0.014467895030975342\n",
      "epoch:  76, loss: 0.014455674216151237\n",
      "epoch:  77, loss: 0.014410356990993023\n",
      "epoch:  78, loss: 0.014376836828887463\n",
      "epoch:  79, loss: 0.014356953091919422\n",
      "epoch:  80, loss: 0.01433834433555603\n",
      "epoch:  81, loss: 0.014332561753690243\n",
      "epoch:  82, loss: 0.014330307953059673\n",
      "epoch:  83, loss: 0.014204977080225945\n",
      "epoch:  84, loss: 0.014185063540935516\n",
      "epoch:  85, loss: 0.014155998826026917\n",
      "epoch:  86, loss: 0.014105694368481636\n",
      "epoch:  87, loss: 0.014057233929634094\n",
      "epoch:  88, loss: 0.014029065147042274\n",
      "epoch:  89, loss: 0.01401680987328291\n",
      "epoch:  90, loss: 0.013963628560304642\n",
      "epoch:  91, loss: 0.01393094938248396\n",
      "epoch:  92, loss: 0.013914366252720356\n",
      "epoch:  93, loss: 0.013904519379138947\n",
      "epoch:  94, loss: 0.013879249803721905\n",
      "epoch:  95, loss: 0.013865339569747448\n",
      "epoch:  96, loss: 0.013850226067006588\n",
      "epoch:  97, loss: 0.013839714229106903\n",
      "epoch:  98, loss: 0.01383147668093443\n",
      "epoch:  99, loss: 0.013826234266161919\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size = X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "opt = pytorch_soom.AGD(model.parameters(), lr=1, mu=0.001, mu_dec=0.1, model=model, use_diagonal=True, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"armijo\")\n",
    "\n",
    "all_loss = []\n",
    "patience = 0\n",
    "max_patience = 10\n",
    "for epoch in range(100):\n",
    "    print('epoch: ', epoch, end='')\n",
    "    all_loss.append(0)\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y, loss_fn)\n",
    "\n",
    "        all_loss[epoch] += loss\n",
    "    \n",
    "    all_loss[epoch] /= len(data_loader)\n",
    "\n",
    "    if epoch > 0 and all_loss[epoch-1] <= all_loss[epoch]:\n",
    "        patience -= 1\n",
    "    else:\n",
    "        patience = max_patience\n",
    "        \n",
    "\n",
    "    print(', loss: {}'.format(all_loss[epoch].detach().cpu().numpy().item()))\n",
    "\n",
    "    if patience <= 0:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
