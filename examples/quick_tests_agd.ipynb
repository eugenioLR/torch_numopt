{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch_numopt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import *\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.f1 = nn.Linear(input_size, 10, device=device)\n",
    "        self.f2 = nn.Linear(10, 20, device=device)\n",
    "        self.f3 = nn.Linear(20, 20, device=device)\n",
    "        self.f4 = nn.Linear(20, 10, device=device)\n",
    "        self.f5 = nn.Linear(10, 1, device=device)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        # self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.f1(x))\n",
    "        x = self.activation(self.f2(x))\n",
    "        x = self.activation(self.f3(x))\n",
    "        x = self.activation(self.f4(x))\n",
    "        x = self.f5(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([800, 10]) torch.Size([800, 1])\n",
      "torch.Size([200, 10]) torch.Size([200, 1])\n"
     ]
    }
   ],
   "source": [
    "# X, y = load_diabetes(return_X_y=True, scaled=False)\n",
    "# X, y = make_regression(n_samples=1000, n_features=100)\n",
    "X, y = make_friedman1(n_samples=1000, noise=1e-2)\n",
    "\n",
    "X_scaler = MinMaxScaler()\n",
    "X = X_scaler.fit_transform(X)\n",
    "X = torch.Tensor(X).to(device)\n",
    "\n",
    "y_scaler = MinMaxScaler()\n",
    "y = y_scaler.fit_transform(y.reshape((-1, 1)))\n",
    "y = torch.Tensor(y).to(device)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "torch_data = TensorDataset(X_train, y_train)\n",
    "data_loader = DataLoader(torch_data, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.6167407631874084\n",
      "epoch:  1, loss: 0.03265359252691269\n",
      "epoch:  2, loss: 0.021853359416127205\n",
      "epoch:  3, loss: 0.014389431104063988\n",
      "epoch:  4, loss: 0.009962748736143112\n",
      "epoch:  5, loss: 0.00967622920870781\n",
      "epoch:  6, loss: 0.008795213885605335\n",
      "epoch:  7, loss: 0.008696391247212887\n",
      "epoch:  8, loss: 0.008534779772162437\n",
      "epoch:  9, loss: 0.008449076674878597\n",
      "epoch:  10, loss: 0.008374212309718132\n",
      "epoch:  11, loss: 0.008369117975234985\n",
      "epoch:  12, loss: 0.00836736336350441\n",
      "epoch:  13, loss: 0.0080410810187459\n",
      "epoch:  14, loss: 0.007960304617881775\n",
      "epoch:  15, loss: 0.007912663742899895\n",
      "epoch:  16, loss: 0.0077943624928593636\n",
      "epoch:  17, loss: 0.007705420255661011\n",
      "epoch:  18, loss: 0.007455704268068075\n",
      "epoch:  19, loss: 0.007395199034363031\n",
      "epoch:  20, loss: 0.007271638140082359\n",
      "epoch:  21, loss: 0.006751551758497953\n",
      "epoch:  22, loss: 0.005550604313611984\n",
      "epoch:  23, loss: 0.005236185621470213\n",
      "epoch:  24, loss: 0.004805380012840033\n",
      "epoch:  25, loss: 0.004683044273406267\n",
      "epoch:  26, loss: 0.004631774965673685\n",
      "epoch:  27, loss: 0.004477208945900202\n",
      "epoch:  28, loss: 0.004394827876240015\n",
      "epoch:  29, loss: 0.004331117961555719\n",
      "epoch:  30, loss: 0.004325257148593664\n",
      "epoch:  31, loss: 0.004284391645342112\n",
      "epoch:  32, loss: 0.004094726871699095\n",
      "epoch:  33, loss: 0.004082684405148029\n",
      "epoch:  34, loss: 0.003951402381062508\n",
      "epoch:  35, loss: 0.0039153327234089375\n",
      "epoch:  36, loss: 0.003902718424797058\n",
      "epoch:  37, loss: 0.0038537620566785336\n",
      "epoch:  38, loss: 0.003832648042589426\n",
      "epoch:  39, loss: 0.0038162339478731155\n",
      "epoch:  40, loss: 0.0038137396331876516\n",
      "epoch:  41, loss: 0.0037659769877791405\n",
      "epoch:  42, loss: 0.0037584463134407997\n",
      "epoch:  43, loss: 0.003709985874593258\n",
      "epoch:  44, loss: 0.0037016260903328657\n",
      "epoch:  45, loss: 0.0036521691363304853\n",
      "epoch:  46, loss: 0.00363069586455822\n",
      "epoch:  47, loss: 0.003612129483371973\n",
      "epoch:  48, loss: 0.003541200654581189\n",
      "epoch:  49, loss: 0.0034738427493721247\n",
      "epoch:  50, loss: 0.003410108620300889\n",
      "epoch:  51, loss: 0.0033850560430437326\n",
      "epoch:  52, loss: 0.0029817023314535618\n",
      "epoch:  53, loss: 0.002395279472693801\n",
      "epoch:  54, loss: 0.0015347820008173585\n",
      "epoch:  55, loss: 0.001402506255544722\n",
      "epoch:  56, loss: 0.0013955311151221395\n",
      "epoch:  57, loss: 0.0012019933201372623\n",
      "epoch:  58, loss: 0.0011512960772961378\n",
      "epoch:  59, loss: 0.0011236253194510937\n",
      "epoch:  60, loss: 0.0010531630832701921\n",
      "epoch:  61, loss: 0.0010187821462750435\n",
      "epoch:  62, loss: 0.0009600435150787234\n",
      "epoch:  63, loss: 0.0009387952159158885\n",
      "epoch:  64, loss: 0.0009263899992220104\n",
      "epoch:  65, loss: 0.0009050134685821831\n",
      "epoch:  66, loss: 0.0008917461382225156\n",
      "epoch:  67, loss: 0.0008764411322772503\n",
      "epoch:  68, loss: 0.000875326688401401\n",
      "epoch:  69, loss: 0.0008629270014353096\n",
      "epoch:  70, loss: 0.0008604523609392345\n",
      "epoch:  71, loss: 0.0008595602703280747\n",
      "epoch:  72, loss: 0.0008554914384149015\n",
      "epoch:  73, loss: 0.0008544849697500467\n",
      "epoch:  74, loss: 0.0008500545518472791\n",
      "epoch:  75, loss: 0.000846824434120208\n",
      "epoch:  76, loss: 0.0008426576969213784\n",
      "epoch:  77, loss: 0.0008417826611548662\n",
      "epoch:  78, loss: 0.0008384722750633955\n",
      "epoch:  79, loss: 0.000837663363199681\n",
      "epoch:  80, loss: 0.0008376303594559431\n",
      "epoch:  81, loss: 0.0008318395703099668\n",
      "epoch:  82, loss: 0.0008300889167003334\n",
      "epoch:  83, loss: 0.0008288823300972581\n",
      "epoch:  84, loss: 0.0008287701057270169\n",
      "epoch:  85, loss: 0.0008247510995715857\n",
      "epoch:  86, loss: 0.000822477275505662\n",
      "epoch:  87, loss: 0.0008184832986444235\n",
      "epoch:  88, loss: 0.0008158948039636016\n",
      "epoch:  89, loss: 0.0008154145907610655\n",
      "epoch:  90, loss: 0.0008087055175565183\n",
      "epoch:  91, loss: 0.0008054093341343105\n",
      "epoch:  92, loss: 0.0007995006162673235\n",
      "epoch:  93, loss: 0.0007979096262715757\n",
      "epoch:  94, loss: 0.0007924386882223189\n",
      "epoch:  95, loss: 0.0007920123753137887\n",
      "epoch:  96, loss: 0.0007870484259910882\n",
      "epoch:  97, loss: 0.0007860517362132668\n",
      "epoch:  98, loss: 0.0007810943643562496\n",
      "epoch:  99, loss: 0.000779924332164228\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size=X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "opt = torch_numopt.AGD(\n",
    "    model=model,\n",
    "    lr=1,\n",
    "    mu=0.001,\n",
    "    mu_dec=0.1,\n",
    "    fletcher=False,\n",
    "    c1=1e-4,\n",
    "    tau=0.5,\n",
    "    line_search_method=\"backtrack\",\n",
    "    line_search_cond=\"armijo\",\n",
    ")\n",
    "\n",
    "all_loss = []\n",
    "patience = 0\n",
    "max_patience = 10\n",
    "for epoch in range(100):\n",
    "    print(\"epoch: \", epoch, end=\"\")\n",
    "    all_loss.append(0)\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y, loss_fn)\n",
    "\n",
    "        all_loss[epoch] += loss\n",
    "\n",
    "    all_loss[epoch] /= len(data_loader)\n",
    "\n",
    "    if epoch > 0 and all_loss[epoch - 1] <= all_loss[epoch]:\n",
    "        patience -= 1\n",
    "    else:\n",
    "        patience = max_patience\n",
    "\n",
    "    print(\", loss: {}\".format(all_loss[epoch].detach().cpu().numpy().item()))\n",
    "\n",
    "    if patience <= 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics: R2 = 0.9796319335137847\n",
      "Test metrics:  R2 = 0.973907384360492\n"
     ]
    }
   ],
   "source": [
    "pred_train = model.forward(X_train).cpu().detach()\n",
    "pred_test = model.forward(X_test).cpu().detach()\n",
    "print(f\"Train metrics: R2 = {r2_score(pred_train, y_train.cpu())}\")\n",
    "print(f\"Test metrics:  R2 = {r2_score(pred_test, y_test.cpu())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.1307525336742401\n",
      "epoch:  1, loss: 0.073084257543087\n",
      "epoch:  2, loss: 0.06910500675439835\n",
      "epoch:  3, loss: 0.024820100516080856\n",
      "epoch:  4, loss: 0.016336336731910706\n",
      "epoch:  5, loss: 0.012859340757131577\n",
      "epoch:  6, loss: 0.011659717187285423\n",
      "epoch:  7, loss: 0.010847141966223717\n",
      "epoch:  8, loss: 0.010246441699564457\n",
      "epoch:  9, loss: 0.009714744053781033\n",
      "epoch:  10, loss: 0.009232643991708755\n",
      "epoch:  11, loss: 0.008823040872812271\n",
      "epoch:  12, loss: 0.008493347093462944\n",
      "epoch:  13, loss: 0.008206170983612537\n",
      "epoch:  14, loss: 0.007954934611916542\n",
      "epoch:  15, loss: 0.007663401309400797\n",
      "epoch:  16, loss: 0.007268233690410852\n",
      "epoch:  17, loss: 0.006912610959261656\n",
      "epoch:  18, loss: 0.006687356159090996\n",
      "epoch:  19, loss: 0.006480179261416197\n",
      "epoch:  20, loss: 0.006327059119939804\n",
      "epoch:  21, loss: 0.006123474333435297\n",
      "epoch:  22, loss: 0.005564858205616474\n",
      "epoch:  23, loss: 0.005481732543557882\n",
      "epoch:  24, loss: 0.005403767805546522\n",
      "epoch:  25, loss: 0.0053127543069422245\n",
      "epoch:  26, loss: 0.005204362329095602\n",
      "epoch:  27, loss: 0.005120721645653248\n",
      "epoch:  28, loss: 0.005073238164186478\n",
      "epoch:  29, loss: 0.004920888692140579\n",
      "epoch:  30, loss: 0.0047987583093345165\n",
      "epoch:  31, loss: 0.004780793096870184\n",
      "epoch:  32, loss: 0.004744078032672405\n",
      "epoch:  33, loss: 0.004546884912997484\n",
      "epoch:  34, loss: 0.004454893991351128\n",
      "epoch:  35, loss: 0.0042914277873933315\n",
      "epoch:  36, loss: 0.004201933275908232\n",
      "epoch:  37, loss: 0.0041351825930178165\n",
      "epoch:  38, loss: 0.004089768044650555\n",
      "epoch:  39, loss: 0.004040501080453396\n",
      "epoch:  40, loss: 0.004006038885563612\n",
      "epoch:  41, loss: 0.0039719645865261555\n",
      "epoch:  42, loss: 0.00393961975350976\n",
      "epoch:  43, loss: 0.003909059334546328\n",
      "epoch:  44, loss: 0.0038866261020302773\n",
      "epoch:  45, loss: 0.0038631707429885864\n",
      "epoch:  46, loss: 0.003841505851596594\n",
      "epoch:  47, loss: 0.003823201870545745\n",
      "epoch:  48, loss: 0.00380555447191\n",
      "epoch:  49, loss: 0.003788484027609229\n",
      "epoch:  50, loss: 0.00377754308283329\n",
      "epoch:  51, loss: 0.0037614721804857254\n",
      "epoch:  52, loss: 0.003742539556697011\n",
      "epoch:  53, loss: 0.003722582943737507\n",
      "epoch:  54, loss: 0.0036974542308598757\n",
      "epoch:  55, loss: 0.003610401414334774\n",
      "epoch:  56, loss: 0.003588402643799782\n",
      "epoch:  57, loss: 0.00356695125810802\n",
      "epoch:  58, loss: 0.003555856877937913\n",
      "epoch:  59, loss: 0.003552253358066082\n",
      "epoch:  60, loss: 0.003475380130112171\n",
      "epoch:  61, loss: 0.0034529906697571278\n",
      "epoch:  62, loss: 0.0034308985341340303\n",
      "epoch:  63, loss: 0.003324738470837474\n",
      "epoch:  64, loss: 0.003272926202043891\n",
      "epoch:  65, loss: 0.003223828971385956\n",
      "epoch:  66, loss: 0.003176935948431492\n",
      "epoch:  67, loss: 0.003125707618892193\n",
      "epoch:  68, loss: 0.0030710427090525627\n",
      "epoch:  69, loss: 0.0030135756824165583\n",
      "epoch:  70, loss: 0.002937647048383951\n",
      "epoch:  71, loss: 0.0028586292173713446\n",
      "epoch:  72, loss: 0.002792341634631157\n",
      "epoch:  73, loss: 0.002726225182414055\n",
      "epoch:  74, loss: 0.0026660666335374117\n",
      "epoch:  75, loss: 0.0024529313668608665\n",
      "epoch:  76, loss: 0.002134073292836547\n",
      "epoch:  77, loss: 0.0019847285002470016\n",
      "epoch:  78, loss: 0.001892212312668562\n",
      "epoch:  79, loss: 0.0015099220909178257\n",
      "epoch:  80, loss: 0.0013789028162136674\n",
      "epoch:  81, loss: 0.0013126175617799163\n",
      "epoch:  82, loss: 0.0012685772962868214\n",
      "epoch:  83, loss: 0.0011054493952542543\n",
      "epoch:  84, loss: 0.0010653211502358317\n",
      "epoch:  85, loss: 0.0009008339839056134\n",
      "epoch:  86, loss: 0.0008338564657606184\n",
      "epoch:  87, loss: 0.0007980749942362309\n",
      "epoch:  88, loss: 0.0007731238147243857\n",
      "epoch:  89, loss: 0.0007274891249835491\n",
      "epoch:  90, loss: 0.0006541560869663954\n",
      "epoch:  91, loss: 0.0006261952221393585\n",
      "epoch:  92, loss: 0.0006069658556953073\n",
      "epoch:  93, loss: 0.000591672956943512\n",
      "epoch:  94, loss: 0.0005317375180311501\n",
      "epoch:  95, loss: 0.0005119312554597855\n",
      "epoch:  96, loss: 0.0004996186471544206\n",
      "epoch:  97, loss: 0.0004890552954748273\n",
      "epoch:  98, loss: 0.00048049510223791003\n",
      "epoch:  99, loss: 0.0004735983966384083\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size=X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "opt = torch_numopt.AGD(\n",
    "    model=model,\n",
    "    lr=1,\n",
    "    mu=0.001,\n",
    "    mu_dec=0.1,\n",
    "    fletcher=True,\n",
    "    c1=1e-4,\n",
    "    tau=0.1,\n",
    "    line_search_method=\"backtrack\",\n",
    "    line_search_cond=\"armijo\",\n",
    "    solver=\"pinv\"\n",
    ")\n",
    "\n",
    "all_loss = []\n",
    "patience = 0\n",
    "max_patience = 10\n",
    "for epoch in range(100):\n",
    "    print(\"epoch: \", epoch, end=\"\")\n",
    "    all_loss.append(0)\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y, loss_fn)\n",
    "\n",
    "        all_loss[epoch] += loss\n",
    "\n",
    "    all_loss[epoch] /= len(data_loader)\n",
    "\n",
    "    if epoch > 0 and all_loss[epoch - 1] <= all_loss[epoch]:\n",
    "        patience -= 1\n",
    "    else:\n",
    "        patience = max_patience\n",
    "\n",
    "    print(\", loss: {}\".format(all_loss[epoch].cpu().detach().numpy().item()))\n",
    "\n",
    "    if patience <= 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics: R2 = 0.987889966202478\n",
      "Test metrics:  R2 = 0.9794473056414267\n"
     ]
    }
   ],
   "source": [
    "pred_train = model.forward(X_train).cpu().detach()\n",
    "pred_test = model.forward(X_test).cpu().detach()\n",
    "print(f\"Train metrics: R2 = {r2_score(pred_train, y_train.cpu())}\")\n",
    "print(f\"Test metrics:  R2 = {r2_score(pred_test, y_test.cpu())}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_numopt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
