{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch_numopt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import *\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.f1 = nn.Linear(input_size, 10, device=device)\n",
    "        self.f2 = nn.Linear(10, 20, device=device)\n",
    "        self.f3 = nn.Linear(20, 20, device=device)\n",
    "        self.f4 = nn.Linear(20, 10, device=device)\n",
    "        self.f5 = nn.Linear(10, 1, device=device)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        # self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.f1(x))\n",
    "        x = self.activation(self.f2(x))\n",
    "        x = self.activation(self.f3(x))\n",
    "        x = self.activation(self.f4(x))\n",
    "        x = self.f5(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([800, 10]) torch.Size([800, 1])\n",
      "torch.Size([200, 10]) torch.Size([200, 1])\n"
     ]
    }
   ],
   "source": [
    "# X, y = load_diabetes(return_X_y=True, scaled=False)\n",
    "# X, y = make_regression(n_samples=1000, n_features=100)\n",
    "X, y = make_friedman1(n_samples=1000, noise=1e-2)\n",
    "\n",
    "X_scaler = MinMaxScaler()\n",
    "X = X_scaler.fit_transform(X)\n",
    "X = torch.Tensor(X).to(device)\n",
    "\n",
    "y_scaler = MinMaxScaler()\n",
    "y = y_scaler.fit_transform(y.reshape((-1, 1)))\n",
    "y = torch.Tensor(y).to(device)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "torch_data = TensorDataset(X_train, y_train)\n",
    "data_loader = DataLoader(torch_data, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.12176299840211868\n",
      "epoch:  1, loss: 0.08887650817632675\n",
      "epoch:  2, loss: 0.030889704823493958\n",
      "epoch:  3, loss: 0.029903989285230637\n",
      "epoch:  4, loss: 0.012602063827216625\n",
      "epoch:  5, loss: 0.009692499414086342\n",
      "epoch:  6, loss: 0.009599948301911354\n",
      "epoch:  7, loss: 0.007879713550209999\n",
      "epoch:  8, loss: 0.00784590095281601\n",
      "epoch:  9, loss: 0.006391721777617931\n",
      "epoch:  10, loss: 0.006210894789546728\n",
      "epoch:  11, loss: 0.005086940713226795\n",
      "epoch:  12, loss: 0.004753451328724623\n",
      "epoch:  13, loss: 0.004552618134766817\n",
      "epoch:  14, loss: 0.004539756570011377\n",
      "epoch:  15, loss: 0.00389763037674129\n",
      "epoch:  16, loss: 0.0037189736030995846\n",
      "epoch:  17, loss: 0.0031598759815096855\n",
      "epoch:  18, loss: 0.003077868605032563\n",
      "epoch:  19, loss: 0.0025409169029444456\n",
      "epoch:  20, loss: 0.002379290061071515\n",
      "epoch:  21, loss: 0.00228571193292737\n",
      "epoch:  22, loss: 0.0022808292414993048\n",
      "epoch:  23, loss: 0.0019331626826897264\n",
      "epoch:  24, loss: 0.0019268589094281197\n",
      "epoch:  25, loss: 0.001646330812945962\n",
      "epoch:  26, loss: 0.0016286320751532912\n",
      "epoch:  27, loss: 0.0014116668608039618\n",
      "epoch:  28, loss: 0.0013710695784538984\n",
      "epoch:  29, loss: 0.001215947326272726\n",
      "epoch:  30, loss: 0.0011845855042338371\n",
      "epoch:  31, loss: 0.001065016956999898\n",
      "epoch:  32, loss: 0.0010401718318462372\n",
      "epoch:  33, loss: 0.000944659928791225\n",
      "epoch:  34, loss: 0.0009301463724114001\n",
      "epoch:  35, loss: 0.0008534270455129445\n",
      "epoch:  36, loss: 0.0008368242415599525\n",
      "epoch:  37, loss: 0.0007755959522910416\n",
      "epoch:  38, loss: 0.0007577927899546921\n",
      "epoch:  39, loss: 0.0007130422163754702\n",
      "epoch:  40, loss: 0.0006953613483346999\n",
      "epoch:  41, loss: 0.0006623737281188369\n",
      "epoch:  42, loss: 0.0006437182310037315\n",
      "epoch:  43, loss: 0.0006199036142788827\n",
      "epoch:  44, loss: 0.0006044497131370008\n",
      "epoch:  45, loss: 0.000584153167437762\n",
      "epoch:  46, loss: 0.0005835089832544327\n",
      "epoch:  47, loss: 0.000535355182364583\n",
      "epoch:  48, loss: 0.0005309279658831656\n",
      "epoch:  49, loss: 0.0005307645187713206\n",
      "epoch:  50, loss: 0.0005174511461518705\n",
      "epoch:  51, loss: 0.0005166312912479043\n",
      "epoch:  52, loss: 0.0005050948821008205\n",
      "epoch:  53, loss: 0.0005028953310102224\n",
      "epoch:  54, loss: 0.0004934759344905615\n",
      "epoch:  55, loss: 0.0004910735297016799\n",
      "epoch:  56, loss: 0.0004836394509766251\n",
      "epoch:  57, loss: 0.00048121187137439847\n",
      "epoch:  58, loss: 0.0004754766123369336\n",
      "epoch:  59, loss: 0.00047283738967962563\n",
      "epoch:  60, loss: 0.0004684266459662467\n",
      "epoch:  61, loss: 0.0004662263090722263\n",
      "epoch:  62, loss: 0.0004658830293919891\n",
      "epoch:  63, loss: 0.0004591156612150371\n",
      "epoch:  64, loss: 0.00045689253602176905\n",
      "epoch:  65, loss: 0.00045255175791680813\n",
      "epoch:  66, loss: 0.00045020441757515073\n",
      "epoch:  67, loss: 0.0004463299410417676\n",
      "epoch:  68, loss: 0.0004454889858607203\n",
      "epoch:  69, loss: 0.0004350308736320585\n",
      "epoch:  70, loss: 0.00043280338286422193\n",
      "epoch:  71, loss: 0.0004313156823627651\n",
      "epoch:  72, loss: 0.0004309374780859798\n",
      "epoch:  73, loss: 0.00042636317084543407\n",
      "epoch:  74, loss: 0.0004252882790751755\n",
      "epoch:  75, loss: 0.0004211141786072403\n",
      "epoch:  76, loss: 0.00041848598630167544\n",
      "epoch:  77, loss: 0.0004154980124440044\n",
      "epoch:  78, loss: 0.00041530278394930065\n",
      "epoch:  79, loss: 0.0004058658960275352\n",
      "epoch:  80, loss: 0.0004039765044581145\n",
      "epoch:  81, loss: 0.0004025293164886534\n",
      "epoch:  82, loss: 0.00040187896229326725\n",
      "epoch:  83, loss: 0.00039838693919591606\n",
      "epoch:  84, loss: 0.00039664696669206023\n",
      "epoch:  85, loss: 0.0003963980416301638\n",
      "epoch:  86, loss: 0.00039105990435928106\n",
      "epoch:  87, loss: 0.0003879711148329079\n",
      "epoch:  88, loss: 0.00038661513826809824\n",
      "epoch:  89, loss: 0.0003791792260017246\n",
      "epoch:  90, loss: 0.00037681168760173023\n",
      "epoch:  91, loss: 0.00037138722836971283\n",
      "epoch:  92, loss: 0.00036959219141863286\n",
      "epoch:  93, loss: 0.0003654031897895038\n",
      "epoch:  94, loss: 0.0003633385640569031\n",
      "epoch:  95, loss: 0.00036312537849880755\n",
      "epoch:  96, loss: 0.0003571011184249073\n",
      "epoch:  97, loss: 0.0003556775627657771\n",
      "epoch:  98, loss: 0.0003509284579195082\n",
      "epoch:  99, loss: 0.0003476472629699856\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size=X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "opt = torch_numopt.AGD(\n",
    "    params=model.parameters(),\n",
    "    model=model,\n",
    "    lr=1,\n",
    "    mu=0.001,\n",
    "    mu_dec=0.1,\n",
    "    use_diagonal=False,\n",
    "    c1=1e-4,\n",
    "    tau=0.5,\n",
    "    line_search_method=\"backtrack\",\n",
    "    line_search_cond=\"armijo\",\n",
    ")\n",
    "\n",
    "all_loss = []\n",
    "patience = 0\n",
    "max_patience = 10\n",
    "for epoch in range(100):\n",
    "    print(\"epoch: \", epoch, end=\"\")\n",
    "    all_loss.append(0)\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y, loss_fn)\n",
    "\n",
    "        all_loss[epoch] += loss\n",
    "\n",
    "    all_loss[epoch] /= len(data_loader)\n",
    "\n",
    "    if epoch > 0 and all_loss[epoch - 1] <= all_loss[epoch]:\n",
    "        patience -= 1\n",
    "    else:\n",
    "        patience = max_patience\n",
    "\n",
    "    print(\", loss: {}\".format(all_loss[epoch].detach().cpu().numpy().item()))\n",
    "\n",
    "    if patience <= 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics: R2 = 0.9908385552463271\n",
      "Test metrics:  R2 = 0.9872556494552939\n"
     ]
    }
   ],
   "source": [
    "pred_train = model.forward(X_train).detach()\n",
    "pred_test = model.forward(X_test).detach()\n",
    "print(f\"Train metrics: R2 = {r2_score(pred_train, y_train)}\")\n",
    "print(f\"Test metrics:  R2 = {r2_score(pred_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.31840285658836365\n",
      "epoch:  1, loss: 0.0384487621486187\n",
      "epoch:  2, loss: 0.03130770102143288\n",
      "epoch:  3, loss: 0.020043374970555305\n",
      "epoch:  4, loss: 0.013425788842141628\n",
      "epoch:  5, loss: 0.011436599306762218\n",
      "epoch:  6, loss: 0.010700805112719536\n",
      "epoch:  7, loss: 0.010106268338859081\n",
      "epoch:  8, loss: 0.009780503809452057\n",
      "epoch:  9, loss: 0.009556412696838379\n",
      "epoch:  10, loss: 0.009382104501128197\n",
      "epoch:  11, loss: 0.009239071980118752\n",
      "epoch:  12, loss: 0.009087963961064816\n",
      "epoch:  13, loss: 0.008752881549298763\n",
      "epoch:  14, loss: 0.008117079734802246\n",
      "epoch:  15, loss: 0.007579416502267122\n",
      "epoch:  16, loss: 0.007348878309130669\n",
      "epoch:  17, loss: 0.006920448504388332\n",
      "epoch:  18, loss: 0.006698453798890114\n",
      "epoch:  19, loss: 0.006529805716127157\n",
      "epoch:  20, loss: 0.006390022113919258\n",
      "epoch:  21, loss: 0.006274827755987644\n",
      "epoch:  22, loss: 0.0061707948334515095\n",
      "epoch:  23, loss: 0.006086251698434353\n",
      "epoch:  24, loss: 0.005593189503997564\n",
      "epoch:  25, loss: 0.005515162833034992\n",
      "epoch:  26, loss: 0.005451759323477745\n",
      "epoch:  27, loss: 0.0049935318529605865\n",
      "epoch:  28, loss: 0.004780028015375137\n",
      "epoch:  29, loss: 0.004625972360372543\n",
      "epoch:  30, loss: 0.004522296134382486\n",
      "epoch:  31, loss: 0.004387358203530312\n",
      "epoch:  32, loss: 0.004256451036781073\n",
      "epoch:  33, loss: 0.004154494497925043\n",
      "epoch:  34, loss: 0.004069136455655098\n",
      "epoch:  35, loss: 0.004008153453469276\n",
      "epoch:  36, loss: 0.003807076718658209\n",
      "epoch:  37, loss: 0.0037292952183634043\n",
      "epoch:  38, loss: 0.0036949068307876587\n",
      "epoch:  39, loss: 0.0036747276317328215\n",
      "epoch:  40, loss: 0.0036604118067771196\n",
      "epoch:  41, loss: 0.003650283208116889\n",
      "epoch:  42, loss: 0.0036449420731514692\n",
      "epoch:  43, loss: 0.0036380707751959562\n",
      "epoch:  44, loss: 0.0036349212750792503\n",
      "epoch:  45, loss: 0.0036313750315457582\n",
      "epoch:  46, loss: 0.0036281307693570852\n",
      "epoch:  47, loss: 0.0036250269040465355\n",
      "epoch:  48, loss: 0.0036220932379364967\n",
      "epoch:  49, loss: 0.0036194876302033663\n",
      "epoch:  50, loss: 0.003619087627157569\n",
      "epoch:  51, loss: 0.0036144040059298277\n",
      "epoch:  52, loss: 0.003613126929849386\n",
      "epoch:  53, loss: 0.003611581865698099\n",
      "epoch:  54, loss: 0.003609956242144108\n",
      "epoch:  55, loss: 0.003607535269111395\n",
      "epoch:  56, loss: 0.003606617683544755\n",
      "epoch:  57, loss: 0.003602719632908702\n",
      "epoch:  58, loss: 0.0036020339466631413\n",
      "epoch:  59, loss: 0.003599385730922222\n",
      "epoch:  60, loss: 0.003598112380132079\n",
      "epoch:  61, loss: 0.0035954774357378483\n",
      "epoch:  62, loss: 0.003594649489969015\n",
      "epoch:  63, loss: 0.003592106746509671\n",
      "epoch:  64, loss: 0.0035907882265746593\n",
      "epoch:  65, loss: 0.0035886731930077076\n",
      "epoch:  66, loss: 0.0035874545574188232\n",
      "epoch:  67, loss: 0.0035860813222825527\n",
      "epoch:  68, loss: 0.003585053374990821\n",
      "epoch:  69, loss: 0.003583765821531415\n",
      "epoch:  70, loss: 0.0035818922333419323\n",
      "epoch:  71, loss: 0.0035797900054603815\n",
      "epoch:  72, loss: 0.003577851690351963\n",
      "epoch:  73, loss: 0.003575243754312396\n",
      "epoch:  74, loss: 0.0035700576845556498\n",
      "epoch:  75, loss: 0.0035571488551795483\n",
      "epoch:  76, loss: 0.0035508389119058847\n",
      "epoch:  77, loss: 0.0035283886827528477\n",
      "epoch:  78, loss: 0.0035196267999708652\n",
      "epoch:  79, loss: 0.0035134865902364254\n",
      "epoch:  80, loss: 0.003507575485855341\n",
      "epoch:  81, loss: 0.0035031563602387905\n",
      "epoch:  82, loss: 0.0034939309116452932\n",
      "epoch:  83, loss: 0.0034872600808739662\n",
      "epoch:  84, loss: 0.003484434215351939\n",
      "epoch:  85, loss: 0.003482338273897767\n",
      "epoch:  86, loss: 0.0034795296378433704\n",
      "epoch:  87, loss: 0.0034718478564172983\n",
      "epoch:  88, loss: 0.003469448070973158\n",
      "epoch:  89, loss: 0.003467981703579426\n",
      "epoch:  90, loss: 0.0034664557315409184\n",
      "epoch:  91, loss: 0.003465846413746476\n",
      "epoch:  92, loss: 0.003464900888502598\n",
      "epoch:  93, loss: 0.003464722540229559\n",
      "epoch:  94, loss: 0.0034642897080630064\n",
      "epoch:  95, loss: 0.0034629390574991703\n",
      "epoch:  96, loss: 0.003462458960711956\n",
      "epoch:  97, loss: 0.0034617672208696604\n",
      "epoch:  98, loss: 0.00346163148060441\n",
      "epoch:  99, loss: 0.003461573040112853\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size=X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "opt = torch_numopt.AGD(\n",
    "    params=model.parameters(),\n",
    "    model=model,\n",
    "    lr=1,\n",
    "    mu=0.001,\n",
    "    mu_dec=0.1,\n",
    "    use_diagonal=True,\n",
    "    c1=1e-4,\n",
    "    tau=0.1,\n",
    "    line_search_method=\"backtrack\",\n",
    "    line_search_cond=\"armijo\",\n",
    ")\n",
    "\n",
    "all_loss = []\n",
    "patience = 0\n",
    "max_patience = 10\n",
    "for epoch in range(100):\n",
    "    print(\"epoch: \", epoch, end=\"\")\n",
    "    all_loss.append(0)\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y, loss_fn)\n",
    "\n",
    "        all_loss[epoch] += loss\n",
    "\n",
    "    all_loss[epoch] /= len(data_loader)\n",
    "\n",
    "    if epoch > 0 and all_loss[epoch - 1] <= all_loss[epoch]:\n",
    "        patience -= 1\n",
    "    else:\n",
    "        patience = max_patience\n",
    "\n",
    "    print(\", loss: {}\".format(all_loss[epoch].detach().cpu().numpy().item()))\n",
    "\n",
    "    if patience <= 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics: R2 = 0.9035798914664308\n",
      "Test metrics:  R2 = 0.8818791135552208\n"
     ]
    }
   ],
   "source": [
    "pred_train = model.forward(X_train).detach()\n",
    "pred_test = model.forward(X_test).detach()\n",
    "print(f\"Train metrics: R2 = {r2_score(pred_train, y_train)}\")\n",
    "print(f\"Test metrics:  R2 = {r2_score(pred_test, y_test)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
