{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch_numopt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import *\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.f1 = nn.Linear(input_size, 10, device=device)\n",
    "        self.f2 = nn.Linear(10, 20, device=device)\n",
    "        self.f3 = nn.Linear(20, 20, device=device)\n",
    "        self.f4 = nn.Linear(20, 10, device=device)\n",
    "        self.f5 = nn.Linear(10, 1, device=device)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        # self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.f1(x))\n",
    "        x = self.activation(self.f2(x))\n",
    "        x = self.activation(self.f3(x))\n",
    "        x = self.activation(self.f4(x))\n",
    "        x = self.f5(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([800, 10]) torch.Size([800, 1])\n",
      "torch.Size([200, 10]) torch.Size([200, 1])\n"
     ]
    }
   ],
   "source": [
    "# X, y = load_diabetes(return_X_y=True, scaled=False)\n",
    "# X, y = make_regression(n_samples=1000, n_features=100)\n",
    "X, y = make_friedman1(n_samples=1000, noise=1e-2)\n",
    "\n",
    "X_scaler = MinMaxScaler()\n",
    "X = X_scaler.fit_transform(X)\n",
    "X = torch.Tensor(X).to(device)\n",
    "\n",
    "y_scaler = MinMaxScaler()\n",
    "y = y_scaler.fit_transform(y.reshape((-1, 1)))\n",
    "y = torch.Tensor(y).to(device)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "torch_data = TensorDataset(X_train, y_train)\n",
    "data_loader = DataLoader(torch_data, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.6343417763710022\n",
      "epoch:  1, loss: 0.11095446348190308\n",
      "epoch:  2, loss: 0.04510517045855522\n",
      "epoch:  3, loss: 0.022239182144403458\n",
      "epoch:  4, loss: 0.0157375056296587\n",
      "epoch:  5, loss: 0.014483723789453506\n",
      "epoch:  6, loss: 0.009270602837204933\n",
      "epoch:  7, loss: 0.008906678296625614\n",
      "epoch:  8, loss: 0.007963468320667744\n",
      "epoch:  9, loss: 0.007742570713162422\n",
      "epoch:  10, loss: 0.00688402121886611\n",
      "epoch:  11, loss: 0.00653162831440568\n",
      "epoch:  12, loss: 0.0057039218954741955\n",
      "epoch:  13, loss: 0.005500299856066704\n",
      "epoch:  14, loss: 0.0046315062791109085\n",
      "epoch:  15, loss: 0.004435374401509762\n",
      "epoch:  16, loss: 0.0037464911583811045\n",
      "epoch:  17, loss: 0.003506216686218977\n",
      "epoch:  18, loss: 0.0033750480506569147\n",
      "epoch:  19, loss: 0.0030802267137914896\n",
      "epoch:  20, loss: 0.0029791456181555986\n",
      "epoch:  21, loss: 0.002701696939766407\n",
      "epoch:  22, loss: 0.002609194489195943\n",
      "epoch:  23, loss: 0.002557491883635521\n",
      "epoch:  24, loss: 0.0024495255202054977\n",
      "epoch:  25, loss: 0.002391377929598093\n",
      "epoch:  26, loss: 0.002290783915668726\n",
      "epoch:  27, loss: 0.0022555377800017595\n",
      "epoch:  28, loss: 0.002159643452614546\n",
      "epoch:  29, loss: 0.0021315657068043947\n",
      "epoch:  30, loss: 0.002043874468654394\n",
      "epoch:  31, loss: 0.0020156116224825382\n",
      "epoch:  32, loss: 0.0019350169459357858\n",
      "epoch:  33, loss: 0.00191045505926013\n",
      "epoch:  34, loss: 0.00183291535358876\n",
      "epoch:  35, loss: 0.0018037494737654924\n",
      "epoch:  36, loss: 0.0017320630140602589\n",
      "epoch:  37, loss: 0.0016988285351544619\n",
      "epoch:  38, loss: 0.0016329436330124736\n",
      "epoch:  39, loss: 0.0015892110532149673\n",
      "epoch:  40, loss: 0.0015291778836399317\n",
      "epoch:  41, loss: 0.0015010131755843759\n",
      "epoch:  42, loss: 0.00144039501901716\n",
      "epoch:  43, loss: 0.0014109202893450856\n",
      "epoch:  44, loss: 0.001359492540359497\n",
      "epoch:  45, loss: 0.0013359913136810064\n",
      "epoch:  46, loss: 0.0012908999342471361\n",
      "epoch:  47, loss: 0.0012766303261741996\n",
      "epoch:  48, loss: 0.0012315594358369708\n",
      "epoch:  49, loss: 0.0012145875953137875\n",
      "epoch:  50, loss: 0.0011746091768145561\n",
      "epoch:  51, loss: 0.0011616186238825321\n",
      "epoch:  52, loss: 0.0011248148512095213\n",
      "epoch:  53, loss: 0.0011174645042046905\n",
      "epoch:  54, loss: 0.0010821004398167133\n",
      "epoch:  55, loss: 0.0010761262383311987\n",
      "epoch:  56, loss: 0.0010415945434942842\n",
      "epoch:  57, loss: 0.0010399792809039354\n",
      "epoch:  58, loss: 0.0010065300157293677\n",
      "epoch:  59, loss: 0.0009960586903616786\n",
      "epoch:  60, loss: 0.0009890588698908687\n",
      "epoch:  61, loss: 0.0009875145042315125\n",
      "epoch:  62, loss: 0.0009641705546528101\n",
      "epoch:  63, loss: 0.0009593434515409172\n",
      "epoch:  64, loss: 0.0009371021296828985\n",
      "epoch:  65, loss: 0.0009330167667940259\n",
      "epoch:  66, loss: 0.0009132560808211565\n",
      "epoch:  67, loss: 0.000910947856027633\n",
      "epoch:  68, loss: 0.0008922133129090071\n",
      "epoch:  69, loss: 0.0008861738606356084\n",
      "epoch:  70, loss: 0.0008799833012744784\n",
      "epoch:  71, loss: 0.0008787449914962053\n",
      "epoch:  72, loss: 0.0008628147770650685\n",
      "epoch:  73, loss: 0.0008578079869039357\n",
      "epoch:  74, loss: 0.0008428860455751419\n",
      "epoch:  75, loss: 0.0008373372838832438\n",
      "epoch:  76, loss: 0.0008246477809734643\n",
      "epoch:  77, loss: 0.0008203217294067144\n",
      "epoch:  78, loss: 0.0008162573794834316\n",
      "epoch:  79, loss: 0.0007949224673211575\n",
      "epoch:  80, loss: 0.0007896610186435282\n",
      "epoch:  81, loss: 0.0007855665753595531\n",
      "epoch:  82, loss: 0.0007765971240587533\n",
      "epoch:  83, loss: 0.00076986791100353\n",
      "epoch:  84, loss: 0.000768299971241504\n",
      "epoch:  85, loss: 0.0007540218066424131\n",
      "epoch:  86, loss: 0.000748155580367893\n",
      "epoch:  87, loss: 0.0007443216163665056\n",
      "epoch:  88, loss: 0.0007427407545037568\n",
      "epoch:  89, loss: 0.0007294943206943572\n",
      "epoch:  90, loss: 0.0007288039196282625\n",
      "epoch:  91, loss: 0.0007123656687326729\n",
      "epoch:  92, loss: 0.0007059765048325062\n",
      "epoch:  93, loss: 0.0007005251245573163\n",
      "epoch:  94, loss: 0.0006959005258977413\n",
      "epoch:  95, loss: 0.0006841251160949469\n",
      "epoch:  96, loss: 0.0006784484721720219\n",
      "epoch:  97, loss: 0.0006626573158428073\n",
      "epoch:  98, loss: 0.0006559167522937059\n",
      "epoch:  99, loss: 0.000654541130643338\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size=X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "opt = torch_numopt.AGD(\n",
    "    model.parameters(),\n",
    "    lr=1,\n",
    "    mu=0.001,\n",
    "    mu_dec=0.1,\n",
    "    model=model,\n",
    "    use_diagonal=False,\n",
    "    c1=1e-4,\n",
    "    tau=0.5,\n",
    "    line_search_method=\"backtrack\",\n",
    "    line_search_cond=\"armijo\",\n",
    ")\n",
    "\n",
    "all_loss = []\n",
    "patience = 0\n",
    "max_patience = 10\n",
    "for epoch in range(100):\n",
    "    print(\"epoch: \", epoch, end=\"\")\n",
    "    all_loss.append(0)\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y, loss_fn)\n",
    "\n",
    "        all_loss[epoch] += loss\n",
    "\n",
    "    all_loss[epoch] /= len(data_loader)\n",
    "\n",
    "    if epoch > 0 and all_loss[epoch - 1] <= all_loss[epoch]:\n",
    "        patience -= 1\n",
    "    else:\n",
    "        patience = max_patience\n",
    "\n",
    "    print(\", loss: {}\".format(all_loss[epoch].detach().cpu().numpy().item()))\n",
    "\n",
    "    if patience <= 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics: R2 = 0.9813711866670917\n",
      "Test metrics:  R2 = 0.9699278589986875\n"
     ]
    }
   ],
   "source": [
    "pred_train = model.forward(X_train).detach()\n",
    "pred_test = model.forward(X_test).detach()\n",
    "print(f\"Train metrics: R2 = {r2_score(pred_train, y_train)}\")\n",
    "print(f\"Test metrics:  R2 = {r2_score(pred_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.39195582270622253\n",
      "epoch:  1, loss: 0.278076708316803\n",
      "epoch:  2, loss: 0.23507434129714966\n",
      "epoch:  3, loss: 0.19397634267807007\n",
      "epoch:  4, loss: 0.05593416839838028\n",
      "epoch:  5, loss: 0.014008278027176857\n",
      "epoch:  6, loss: 0.011346837505698204\n",
      "epoch:  7, loss: 0.010078780353069305\n",
      "epoch:  8, loss: 0.009671495296061039\n",
      "epoch:  9, loss: 0.008871881291270256\n",
      "epoch:  10, loss: 0.00828420277684927\n",
      "epoch:  11, loss: 0.008148652501404285\n",
      "epoch:  12, loss: 0.007974503561854362\n",
      "epoch:  13, loss: 0.007588700857013464\n",
      "epoch:  14, loss: 0.007204002235084772\n",
      "epoch:  15, loss: 0.006947745569050312\n",
      "epoch:  16, loss: 0.006455664522945881\n",
      "epoch:  17, loss: 0.0062392838299274445\n",
      "epoch:  18, loss: 0.006113196723163128\n",
      "epoch:  19, loss: 0.006031856406480074\n",
      "epoch:  20, loss: 0.005978474393486977\n",
      "epoch:  21, loss: 0.005813491530716419\n",
      "epoch:  22, loss: 0.0057672662660479546\n",
      "epoch:  23, loss: 0.005725297145545483\n",
      "epoch:  24, loss: 0.005696759093552828\n",
      "epoch:  25, loss: 0.005678044632077217\n",
      "epoch:  26, loss: 0.005664253607392311\n",
      "epoch:  27, loss: 0.005650905892252922\n",
      "epoch:  28, loss: 0.0056305862963199615\n",
      "epoch:  29, loss: 0.005614690016955137\n",
      "epoch:  30, loss: 0.0056013991124928\n",
      "epoch:  31, loss: 0.005589164327830076\n",
      "epoch:  32, loss: 0.005580815486609936\n",
      "epoch:  33, loss: 0.005564550869166851\n",
      "epoch:  34, loss: 0.005557585507631302\n",
      "epoch:  35, loss: 0.005552743095904589\n",
      "epoch:  36, loss: 0.005546499975025654\n",
      "epoch:  37, loss: 0.005543449893593788\n",
      "epoch:  38, loss: 0.0055365655571222305\n",
      "epoch:  39, loss: 0.005533974152058363\n",
      "epoch:  40, loss: 0.0055274502374231815\n",
      "epoch:  41, loss: 0.005523345898836851\n",
      "epoch:  42, loss: 0.00552035728469491\n",
      "epoch:  43, loss: 0.005519022233784199\n",
      "epoch:  44, loss: 0.005514456424862146\n",
      "epoch:  45, loss: 0.005510414019227028\n",
      "epoch:  46, loss: 0.0055035753175616264\n",
      "epoch:  47, loss: 0.005500456318259239\n",
      "epoch:  48, loss: 0.005498087499290705\n",
      "epoch:  49, loss: 0.005491908639669418\n",
      "epoch:  50, loss: 0.005489037837833166\n",
      "epoch:  51, loss: 0.005486597307026386\n",
      "epoch:  52, loss: 0.005484333261847496\n",
      "epoch:  53, loss: 0.00547458790242672\n",
      "epoch:  54, loss: 0.005469578318297863\n",
      "epoch:  55, loss: 0.005466985050588846\n",
      "epoch:  56, loss: 0.005462927278131247\n",
      "epoch:  57, loss: 0.005460156127810478\n",
      "epoch:  58, loss: 0.0054563493467867374\n",
      "epoch:  59, loss: 0.005453607067465782\n",
      "epoch:  60, loss: 0.005447504576295614\n",
      "epoch:  61, loss: 0.005442015361040831\n",
      "epoch:  62, loss: 0.00538580073043704\n",
      "epoch:  63, loss: 0.005221339408308268\n",
      "epoch:  64, loss: 0.0047259763814508915\n",
      "epoch:  65, loss: 0.004436762537807226\n",
      "epoch:  66, loss: 0.00416563032194972\n",
      "epoch:  67, loss: 0.004001114517450333\n",
      "epoch:  68, loss: 0.0038722895551472902\n",
      "epoch:  69, loss: 0.0037607380654662848\n",
      "epoch:  70, loss: 0.0032657915726304054\n",
      "epoch:  71, loss: 0.002980171935632825\n",
      "epoch:  72, loss: 0.0028918120078742504\n",
      "epoch:  73, loss: 0.0027900543063879013\n",
      "epoch:  74, loss: 0.002694183960556984\n",
      "epoch:  75, loss: 0.0026103020645678043\n",
      "epoch:  76, loss: 0.002535222563892603\n",
      "epoch:  77, loss: 0.002477081958204508\n",
      "epoch:  78, loss: 0.0024327675346285105\n",
      "epoch:  79, loss: 0.0024005805607885122\n",
      "epoch:  80, loss: 0.002372537739574909\n",
      "epoch:  81, loss: 0.002311736810952425\n",
      "epoch:  82, loss: 0.002252145204693079\n",
      "epoch:  83, loss: 0.0022271997295320034\n",
      "epoch:  84, loss: 0.0022147907875478268\n",
      "epoch:  85, loss: 0.0022069858387112617\n",
      "epoch:  86, loss: 0.0022023955825716257\n",
      "epoch:  87, loss: 0.002180940005928278\n",
      "epoch:  88, loss: 0.002174986060708761\n",
      "epoch:  89, loss: 0.002171857049688697\n",
      "epoch:  90, loss: 0.00216891267336905\n",
      "epoch:  91, loss: 0.002166177611798048\n",
      "epoch:  92, loss: 0.0021562797483056784\n",
      "epoch:  93, loss: 0.002152843866497278\n",
      "epoch:  94, loss: 0.002151788678020239\n",
      "epoch:  95, loss: 0.002149603795260191\n",
      "epoch:  96, loss: 0.00214711157605052\n",
      "epoch:  97, loss: 0.0021462016738951206\n",
      "epoch:  98, loss: 0.0021450244821608067\n",
      "epoch:  99, loss: 0.0021444237791001797\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size=X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "opt = torch_numopt.AGD(\n",
    "    model.parameters(),\n",
    "    lr=1,\n",
    "    mu=0.001,\n",
    "    mu_dec=0.1,\n",
    "    model=model,\n",
    "    use_diagonal=True,\n",
    "    c1=1e-4,\n",
    "    tau=0.1,\n",
    "    line_search_method=\"backtrack\",\n",
    "    line_search_cond=\"armijo\",\n",
    ")\n",
    "\n",
    "all_loss = []\n",
    "patience = 0\n",
    "max_patience = 10\n",
    "for epoch in range(100):\n",
    "    print(\"epoch: \", epoch, end=\"\")\n",
    "    all_loss.append(0)\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y, loss_fn)\n",
    "\n",
    "        all_loss[epoch] += loss\n",
    "\n",
    "    all_loss[epoch] /= len(data_loader)\n",
    "\n",
    "    if epoch > 0 and all_loss[epoch - 1] <= all_loss[epoch]:\n",
    "        patience -= 1\n",
    "    else:\n",
    "        patience = max_patience\n",
    "\n",
    "    print(\", loss: {}\".format(all_loss[epoch].detach().cpu().numpy().item()))\n",
    "\n",
    "    if patience <= 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics: R2 = 0.9327147819997506\n",
      "Test metrics:  R2 = 0.916334225814023\n"
     ]
    }
   ],
   "source": [
    "pred_train = model.forward(X_train).detach()\n",
    "pred_test = model.forward(X_test).detach()\n",
    "print(f\"Train metrics: R2 = {r2_score(pred_train, y_train)}\")\n",
    "print(f\"Test metrics:  R2 = {r2_score(pred_test, y_test)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
