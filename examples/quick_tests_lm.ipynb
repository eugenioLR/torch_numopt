{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pytorch_soom\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.f1 = nn.Linear(input_size, 10, device=device)\n",
    "        self.f2 = nn.Linear(10, 20, device=device)\n",
    "        self.f3 = nn.Linear(20, 20, device=device)\n",
    "        self.f4 = nn.Linear(20, 10, device=device)\n",
    "        self.f5 = nn.Linear(10, 1, device=device)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        # self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.f1(x))\n",
    "        x = self.activation(self.f2(x))\n",
    "        x = self.activation(self.f3(x))\n",
    "        x = self.activation(self.f4(x))\n",
    "        x = self.f5(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_diabetes(return_X_y = True, scaled=False)\n",
    "\n",
    "X_scaler = MinMaxScaler()\n",
    "X = X_scaler.fit_transform(X)\n",
    "\n",
    "y_scaler = MinMaxScaler()\n",
    "y = y_scaler.fit_transform(y.reshape((-1, 1)))\n",
    "\n",
    "torch_data = TensorDataset(torch.Tensor(X).to(device), torch.Tensor(y).to(device))\n",
    "data_loader = DataLoader(torch_data, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.1482836753129959\n",
      "epoch:  1, loss: 0.10641468316316605\n",
      "epoch:  2, loss: 0.04400528594851494\n",
      "epoch:  3, loss: 0.05026593059301376\n",
      "epoch:  4, loss: 0.030253615230321884\n",
      "epoch:  5, loss: 0.029083557426929474\n",
      "epoch:  6, loss: 0.02852673828601837\n",
      "epoch:  7, loss: 0.028158042579889297\n",
      "epoch:  8, loss: 0.027902113273739815\n",
      "epoch:  9, loss: 0.027728527784347534\n",
      "epoch:  10, loss: 0.02749103493988514\n",
      "epoch:  11, loss: 0.027259500697255135\n",
      "epoch:  12, loss: 0.02708412893116474\n",
      "epoch:  13, loss: 0.02694172039628029\n",
      "epoch:  14, loss: 0.026780983433127403\n",
      "epoch:  15, loss: 0.02665804512798786\n",
      "epoch:  16, loss: 0.02654181979596615\n",
      "epoch:  17, loss: 0.026432322338223457\n",
      "epoch:  18, loss: 0.02635086327791214\n",
      "epoch:  19, loss: 0.026273123919963837\n",
      "epoch:  20, loss: 0.02622329443693161\n",
      "epoch:  21, loss: 0.02617236040532589\n",
      "epoch:  22, loss: 0.026114413514733315\n",
      "epoch:  23, loss: 0.026037579402327538\n",
      "epoch:  24, loss: 0.025961006060242653\n",
      "epoch:  25, loss: 0.025877324864268303\n",
      "epoch:  26, loss: 0.025762904435396194\n",
      "epoch:  27, loss: 0.02562563866376877\n",
      "epoch:  28, loss: 0.025478076189756393\n",
      "epoch:  29, loss: 0.02535584568977356\n",
      "epoch:  30, loss: 0.0252094529569149\n",
      "epoch:  31, loss: 0.02502945438027382\n",
      "epoch:  32, loss: 0.024985296651721\n",
      "epoch:  33, loss: 0.024614781141281128\n",
      "epoch:  34, loss: 0.024441935122013092\n",
      "epoch:  35, loss: 0.02427639253437519\n",
      "epoch:  36, loss: 0.024321256205439568\n",
      "epoch:  37, loss: 0.024045640602707863\n",
      "epoch:  38, loss: 0.023917745798826218\n",
      "epoch:  39, loss: 0.02385362982749939\n",
      "epoch:  40, loss: 0.02376120537519455\n",
      "epoch:  41, loss: 0.023707520216703415\n",
      "epoch:  42, loss: 0.02364455722272396\n",
      "epoch:  43, loss: 0.023602109402418137\n",
      "epoch:  44, loss: 0.023557024076581\n",
      "epoch:  45, loss: 0.02354021556675434\n",
      "epoch:  46, loss: 0.023483766242861748\n",
      "epoch:  47, loss: 0.02345574088394642\n",
      "epoch:  48, loss: 0.02340794913470745\n",
      "epoch:  49, loss: 0.023374881595373154\n",
      "epoch:  50, loss: 0.023349028080701828\n",
      "epoch:  51, loss: 0.02331990748643875\n",
      "epoch:  52, loss: 0.02331707812845707\n",
      "epoch:  53, loss: 0.023286093026399612\n",
      "epoch:  54, loss: 0.023288358002901077\n",
      "epoch:  55, loss: 0.023218845948576927\n",
      "epoch:  56, loss: 0.023165103048086166\n",
      "epoch:  57, loss: 0.023114044219255447\n",
      "epoch:  58, loss: 0.02307373285293579\n",
      "epoch:  59, loss: 0.023038232699036598\n",
      "epoch:  60, loss: 0.02302836999297142\n",
      "epoch:  61, loss: 0.02305700071156025\n",
      "epoch:  62, loss: 0.022987741976976395\n",
      "epoch:  63, loss: 0.022953232750296593\n",
      "epoch:  64, loss: 0.022907450795173645\n",
      "epoch:  65, loss: 0.022863376885652542\n",
      "epoch:  66, loss: 0.022823331877589226\n",
      "epoch:  67, loss: 0.022805260494351387\n",
      "epoch:  68, loss: 0.022779881954193115\n",
      "epoch:  69, loss: 0.022757049649953842\n",
      "epoch:  70, loss: 0.022753192111849785\n",
      "epoch:  71, loss: 0.022756891325116158\n",
      "epoch:  72, loss: 0.022957537323236465\n",
      "epoch:  73, loss: 0.022683676332235336\n",
      "epoch:  74, loss: 0.022696327418088913\n",
      "epoch:  75, loss: 0.02265842631459236\n",
      "epoch:  76, loss: 0.022676581516861916\n",
      "epoch:  77, loss: 0.02266329899430275\n",
      "epoch:  78, loss: 0.022674644365906715\n",
      "epoch:  79, loss: 0.022647259756922722\n",
      "epoch:  80, loss: 0.0226230900734663\n",
      "epoch:  81, loss: 0.022606784477829933\n",
      "epoch:  82, loss: 0.022601129487156868\n",
      "epoch:  83, loss: 0.02261807955801487\n",
      "epoch:  84, loss: 0.022637702524662018\n",
      "epoch:  85, loss: 0.022601081058382988\n",
      "epoch:  86, loss: 0.022583317011594772\n",
      "epoch:  87, loss: 0.022580422461032867\n",
      "epoch:  88, loss: 0.022599393501877785\n",
      "epoch:  89, loss: 0.02259642817080021\n",
      "epoch:  90, loss: 0.02264867164194584\n",
      "epoch:  91, loss: 0.02261565625667572\n",
      "epoch:  92, loss: 0.022644737735390663\n",
      "epoch:  93, loss: 0.022566920146346092\n",
      "epoch:  94, loss: 0.022547028958797455\n",
      "epoch:  95, loss: 0.022539382800459862\n",
      "epoch:  96, loss: 0.022542046383023262\n",
      "epoch:  97, loss: 0.022541984915733337\n",
      "epoch:  98, loss: 0.0225729588419199\n",
      "epoch:  99, loss: 0.022578051313757896\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size = X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "opt = pytorch_soom.LM(model.parameters(), lr=0.1, mu=0.001, mu_dec=0.1, model=model, use_diagonal=True, hessian_approx=False)\n",
    "\n",
    "all_loss = []\n",
    "patience = 0\n",
    "max_patience = 10\n",
    "for epoch in range(100):\n",
    "    print('epoch: ', epoch, end='')\n",
    "    all_loss.append(0)\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y, loss_fn)\n",
    "        # opt.update(loss)\n",
    "\n",
    "        all_loss[epoch] += loss\n",
    "    \n",
    "    all_loss[epoch] /= len(data_loader)\n",
    "\n",
    "    if epoch > 0 and all_loss[epoch-1] <= all_loss[epoch]:\n",
    "        patience -= 1\n",
    "    else:\n",
    "        patience = max_patience\n",
    "        \n",
    "\n",
    "    print(', loss: {}'.format(all_loss[epoch].detach().cpu().numpy().item()))\n",
    "\n",
    "    if patience <= 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.1305222064256668\n",
      "epoch:  1, loss: 0.11669120192527771\n",
      "epoch:  2, loss: 0.10032882541418076\n",
      "epoch:  3, loss: 0.08487207442522049\n",
      "epoch:  4, loss: 0.07070424407720566\n",
      "epoch:  5, loss: 0.05979813262820244\n",
      "epoch:  6, loss: 0.05690736323595047\n",
      "epoch:  7, loss: 0.06218424066901207\n",
      "epoch:  8, loss: 0.05523960292339325\n",
      "epoch:  9, loss: 0.0583680085837841\n",
      "epoch:  10, loss: 0.05610703304409981\n",
      "epoch:  11, loss: 0.05842607095837593\n",
      "epoch:  12, loss: 0.05478939041495323\n",
      "epoch:  13, loss: 0.05956759303808212\n",
      "epoch:  14, loss: 0.05291202664375305\n",
      "epoch:  15, loss: 0.061596136540174484\n",
      "epoch:  16, loss: 0.05139549449086189\n",
      "epoch:  17, loss: 0.05025597661733627\n",
      "epoch:  18, loss: 0.06040771305561066\n",
      "epoch:  19, loss: 0.04878214746713638\n",
      "epoch:  20, loss: 0.0475023090839386\n",
      "epoch:  21, loss: 0.06275629252195358\n",
      "epoch:  22, loss: 0.04690377414226532\n",
      "epoch:  23, loss: 0.06682387739419937\n",
      "epoch:  24, loss: 0.048548270016908646\n",
      "epoch:  25, loss: 0.05345550924539566\n",
      "epoch:  26, loss: 0.04623441398143768\n",
      "epoch:  27, loss: 0.06013825908303261\n",
      "epoch:  28, loss: 0.04486202076077461\n",
      "epoch:  29, loss: 0.06719370186328888\n",
      "epoch:  30, loss: 0.04796196520328522\n",
      "epoch:  31, loss: 0.0494040846824646\n",
      "epoch:  32, loss: 0.047708507627248764\n",
      "epoch:  33, loss: 0.04912653937935829\n",
      "epoch:  34, loss: 0.047483935952186584\n",
      "epoch:  35, loss: 0.048813577741384506\n",
      "epoch:  36, loss: 0.0473446324467659\n",
      "epoch:  37, loss: 0.04835602641105652\n",
      "epoch:  38, loss: 0.047501884400844574\n",
      "epoch:  39, loss: 0.04738496243953705\n",
      "epoch:  40, loss: 0.04884998872876167\n",
      "epoch:  41, loss: 0.04493911191821098\n",
      "epoch:  42, loss: 0.056238286197185516\n",
      "epoch:  43, loss: 0.042368948459625244\n",
      "epoch:  44, loss: 0.04997933655977249\n",
      "epoch:  45, loss: 0.040522146970033646\n",
      "epoch:  46, loss: 0.05872653052210808\n",
      "epoch:  47, loss: 0.040753722190856934\n",
      "epoch:  48, loss: 0.059014324098825455\n",
      "epoch:  49, loss: 0.04080396145582199\n",
      "epoch:  50, loss: 0.05347685143351555\n",
      "epoch:  51, loss: 0.03850231319665909\n",
      "epoch:  52, loss: 0.04991215094923973\n",
      "epoch:  53, loss: 0.03641780838370323\n",
      "epoch:  54, loss: 0.03535957634449005\n",
      "epoch:  55, loss: 0.06454499065876007\n",
      "epoch:  56, loss: 0.04249243438243866\n",
      "epoch:  57, loss: 0.03454894572496414\n",
      "epoch:  58, loss: 0.07052310556173325\n",
      "epoch:  59, loss: 0.047269873321056366\n",
      "epoch:  60, loss: 0.03293866664171219\n",
      "epoch:  61, loss: 0.039994340389966965\n",
      "epoch:  62, loss: 0.033698055893182755\n",
      "epoch:  63, loss: 0.05215179920196533\n",
      "epoch:  64, loss: 0.032862357795238495\n",
      "epoch:  65, loss: 0.06744850426912308\n",
      "epoch:  66, loss: 0.04421231150627136\n",
      "epoch:  67, loss: 0.030749058350920677\n",
      "epoch:  68, loss: 0.03026287630200386\n",
      "epoch:  69, loss: 0.078140027821064\n",
      "epoch:  70, loss: 0.05247855558991432\n",
      "epoch:  71, loss: 0.03231736645102501\n",
      "epoch:  72, loss: 0.06005284935235977\n",
      "epoch:  73, loss: 0.038030385971069336\n",
      "epoch:  74, loss: 0.03203589469194412\n",
      "epoch:  75, loss: 0.06260856986045837\n",
      "epoch:  76, loss: 0.04000634327530861\n",
      "epoch:  77, loss: 0.03022262454032898\n",
      "epoch:  78, loss: 0.09223492443561554\n",
      "epoch:  79, loss: 0.0662873387336731\n",
      "epoch:  80, loss: 0.04296877235174179\n",
      "epoch:  81, loss: 0.029228245839476585\n",
      "epoch:  82, loss: 0.02904611825942993\n",
      "epoch:  83, loss: 0.06303667277097702\n",
      "epoch:  84, loss: 0.039428357034921646\n",
      "epoch:  85, loss: 0.030320415273308754\n",
      "epoch:  86, loss: 0.07060547173023224\n",
      "epoch:  87, loss: 0.04572920873761177\n",
      "epoch:  88, loss: 0.02910693921148777\n",
      "epoch:  89, loss: 0.09147333353757858\n",
      "epoch:  90, loss: 0.06552242487668991\n",
      "epoch:  91, loss: 0.04222024232149124\n",
      "epoch:  92, loss: 0.02868690900504589\n",
      "epoch:  93, loss: 0.03325632959604263\n",
      "epoch:  94, loss: 0.04083016514778137\n",
      "epoch:  95, loss: 0.029024763032794\n",
      "epoch:  96, loss: 0.0843210443854332\n",
      "epoch:  97, loss: 0.057963963598012924\n",
      "epoch:  98, loss: 0.03546600416302681\n",
      "epoch:  99, loss: 0.03595579415559769\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size = X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "opt = pytorch_soom.LM(model.parameters(), lr=0.003, mu=0.001, mu_dec=0.1, model=model, use_diagonal=False, hessian_approx=True)\n",
    "\n",
    "all_loss = []\n",
    "patience = 0\n",
    "max_patience = 10\n",
    "for epoch in range(100):\n",
    "    print('epoch: ', epoch, end='')\n",
    "    all_loss.append(0)\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y, loss_fn)\n",
    "        # opt.update(loss)\n",
    "\n",
    "        all_loss[epoch] += loss\n",
    "    \n",
    "    all_loss[epoch] /= len(data_loader)\n",
    "\n",
    "    if epoch > 0 and all_loss[epoch-1] <= all_loss[epoch]:\n",
    "        patience -= 1\n",
    "    else:\n",
    "        patience = max_patience\n",
    "        \n",
    "\n",
    "    print(', loss: {}'.format(all_loss[epoch].detach().cpu().numpy().item()))\n",
    "\n",
    "    if patience <= 0:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
