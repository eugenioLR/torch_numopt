{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pytorch_soom\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.f1 = nn.Linear(input_size, 10, device=device)\n",
    "        self.f2 = nn.Linear(10, 20, device=device)\n",
    "        self.f3 = nn.Linear(20, 20, device=device)\n",
    "        self.f4 = nn.Linear(20, 10, device=device)\n",
    "        self.f5 = nn.Linear(10, 1, device=device)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        # self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.f1(x))\n",
    "        x = self.activation(self.f2(x))\n",
    "        x = self.activation(self.f3(x))\n",
    "        x = self.activation(self.f4(x))\n",
    "        x = self.f5(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_diabetes(return_X_y = True, scaled=False)\n",
    "\n",
    "X_scaler = MinMaxScaler()\n",
    "X = X_scaler.fit_transform(X)\n",
    "\n",
    "y_scaler = MinMaxScaler()\n",
    "y = y_scaler.fit_transform(y.reshape((-1, 1)))\n",
    "\n",
    "torch_data = TensorDataset(torch.Tensor(X).to(device), torch.Tensor(y).to(device))\n",
    "data_loader = DataLoader(torch_data, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.06355388462543488\n",
      "epoch:  1, loss: 0.057262152433395386\n",
      "epoch:  2, loss: 0.052650053054094315\n",
      "epoch:  3, loss: 0.04714050516486168\n",
      "epoch:  4, loss: 0.0416850671172142\n",
      "epoch:  5, loss: 0.04126954451203346\n",
      "epoch:  6, loss: 0.04087800160050392\n",
      "epoch:  7, loss: 0.04046721011400223\n",
      "epoch:  8, loss: 0.04004017636179924\n",
      "epoch:  9, loss: 0.03952330723404884\n",
      "epoch:  10, loss: 0.03948298469185829\n",
      "epoch:  11, loss: 0.0394793264567852\n",
      "epoch:  12, loss: 0.03947528451681137\n",
      "epoch:  13, loss: 0.03946743533015251\n",
      "epoch:  14, loss: 0.039466943591833115\n",
      "epoch:  15, loss: 0.03926545009016991\n",
      "epoch:  16, loss: 0.03789546713232994\n",
      "epoch:  17, loss: 0.036974452435970306\n",
      "epoch:  18, loss: 0.036449089646339417\n",
      "epoch:  19, loss: 0.03594626113772392\n",
      "epoch:  20, loss: 0.0319615975022316\n",
      "epoch:  21, loss: 0.03195788338780403\n",
      "epoch:  22, loss: 0.031917694956064224\n",
      "epoch:  23, loss: 0.02953372523188591\n",
      "epoch:  24, loss: 0.028581518679857254\n",
      "epoch:  25, loss: 0.028549907729029655\n",
      "epoch:  26, loss: 0.02641286328434944\n",
      "epoch:  27, loss: 0.024846453219652176\n",
      "epoch:  28, loss: 0.02354826219379902\n",
      "epoch:  29, loss: 0.02275010384619236\n",
      "epoch:  30, loss: 0.021331459283828735\n",
      "epoch:  31, loss: 0.021138979122042656\n",
      "epoch:  32, loss: 0.02068241313099861\n",
      "epoch:  33, loss: 0.02057601511478424\n",
      "epoch:  34, loss: 0.02053612284362316\n",
      "epoch:  35, loss: 0.01976938173174858\n",
      "epoch:  36, loss: 0.019747376441955566\n",
      "epoch:  37, loss: 0.019113954156637192\n",
      "epoch:  38, loss: 0.01892567239701748\n",
      "epoch:  39, loss: 0.018760381266474724\n",
      "epoch:  40, loss: 0.018200810998678207\n",
      "epoch:  41, loss: 0.01818086951971054\n",
      "epoch:  42, loss: 0.0178193598985672\n",
      "epoch:  43, loss: 0.017788702622056007\n",
      "epoch:  44, loss: 0.016894593834877014\n",
      "epoch:  45, loss: 0.01672797091305256\n",
      "epoch:  46, loss: 0.01658540964126587\n",
      "epoch:  47, loss: 0.01631125435233116\n",
      "epoch:  48, loss: 0.01618005894124508\n",
      "epoch:  49, loss: 0.015834713354706764\n",
      "epoch:  50, loss: 0.015677835792303085\n",
      "epoch:  51, loss: 0.015556694939732552\n",
      "epoch:  52, loss: 0.015384526923298836\n",
      "epoch:  53, loss: 0.015281721018254757\n",
      "epoch:  54, loss: 0.01520018745213747\n",
      "epoch:  55, loss: 0.015126788057386875\n",
      "epoch:  56, loss: 0.015043608844280243\n",
      "epoch:  57, loss: 0.014967947266995907\n",
      "epoch:  58, loss: 0.014887920580804348\n",
      "epoch:  59, loss: 0.014859335497021675\n",
      "epoch:  60, loss: 0.014793171547353268\n",
      "epoch:  61, loss: 0.01475035585463047\n",
      "epoch:  62, loss: 0.014749144203960896\n",
      "epoch:  63, loss: 0.014747927896678448\n",
      "epoch:  64, loss: 0.01474676001816988\n",
      "epoch:  65, loss: 0.014746398665010929\n",
      "epoch:  66, loss: 0.014743662439286709\n",
      "epoch:  67, loss: 0.014732965268194675\n",
      "epoch:  68, loss: 0.014728512614965439\n",
      "epoch:  69, loss: 0.014704869128763676\n",
      "epoch:  70, loss: 0.014695425517857075\n",
      "epoch:  71, loss: 0.014685411937534809\n",
      "epoch:  72, loss: 0.014675844460725784\n",
      "epoch:  73, loss: 0.014651807956397533\n",
      "epoch:  74, loss: 0.014641794376075268\n",
      "epoch:  75, loss: 0.014633111655712128\n",
      "epoch:  76, loss: 0.01462481264024973\n",
      "epoch:  77, loss: 0.014616427943110466\n",
      "epoch:  78, loss: 0.014615658670663834\n",
      "epoch:  79, loss: 0.014614752493798733\n",
      "epoch:  80, loss: 0.01461353711783886\n",
      "epoch:  81, loss: 0.014583456330001354\n",
      "epoch:  82, loss: 0.014560619369149208\n",
      "epoch:  83, loss: 0.014550195075571537\n",
      "epoch:  84, loss: 0.014545856043696404\n",
      "epoch:  85, loss: 0.01449378952383995\n",
      "epoch:  86, loss: 0.014463872648775578\n",
      "epoch:  87, loss: 0.014423413202166557\n",
      "epoch:  88, loss: 0.014396211132407188\n",
      "epoch:  89, loss: 0.01433002483099699\n",
      "epoch:  90, loss: 0.014317837543785572\n",
      "epoch:  91, loss: 0.014240577816963196\n",
      "epoch:  92, loss: 0.014179641380906105\n",
      "epoch:  93, loss: 0.014120824635028839\n",
      "epoch:  94, loss: 0.014089141972362995\n",
      "epoch:  95, loss: 0.014042980968952179\n",
      "epoch:  96, loss: 0.01400687638670206\n",
      "epoch:  97, loss: 0.0139447171241045\n",
      "epoch:  98, loss: 0.013903179205954075\n",
      "epoch:  99, loss: 0.013863854110240936\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size = X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "opt = pytorch_soom.LM(model.parameters(), lr=1, mu=0.001, mu_dec=0.1, model=model, use_diagonal=False, c1=1e-4, tau=0.1, line_search_method='backtrack')\n",
    "\n",
    "all_loss = []\n",
    "patience = 0\n",
    "max_patience = 10\n",
    "for epoch in range(100):\n",
    "    print('epoch: ', epoch, end='')\n",
    "    all_loss.append(0)\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y, loss_fn)\n",
    "        opt.update(loss)\n",
    "\n",
    "        all_loss[epoch] += loss\n",
    "    \n",
    "    all_loss[epoch] /= len(data_loader)\n",
    "\n",
    "    if epoch > 0 and all_loss[epoch-1] <= all_loss[epoch]:\n",
    "        patience -= 1\n",
    "    else:\n",
    "        patience = max_patience\n",
    "        \n",
    "\n",
    "    print(', loss: {}'.format(all_loss[epoch].detach().cpu().numpy().item()))\n",
    "\n",
    "    if patience <= 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", loss: 0.6663558483123779\n",
      "epoch:  1, loss: 0.6427651047706604\n",
      "epoch:  2, loss: 0.6177672147750854\n",
      "epoch:  3, loss: 0.6006138920783997\n",
      "epoch:  4, loss: 0.5790088772773743\n",
      "epoch:  5, loss: 0.5591574907302856\n",
      "epoch:  6, loss: 0.5352960228919983\n",
      "epoch:  7, loss: 0.5129629969596863\n",
      "epoch:  8, loss: 0.4914267957210541\n",
      "epoch:  9, loss: 0.46960267424583435\n",
      "epoch:  10, loss: 0.4477505087852478\n",
      "epoch:  11, loss: 0.4249090254306793\n",
      "epoch:  12, loss: 0.40400296449661255\n",
      "epoch:  13, loss: 0.3837655484676361\n",
      "epoch:  14, loss: 0.363253116607666\n",
      "epoch:  15, loss: 0.34323811531066895\n",
      "epoch:  16, loss: 0.30751436948776245\n",
      "epoch:  17, loss: 0.29105401039123535\n",
      "epoch:  18, loss: 0.2733418643474579\n",
      "epoch:  19, loss: 0.25357678532600403\n",
      "epoch:  20, loss: 0.2394772320985794\n",
      "epoch:  21, loss: 0.2242192029953003\n",
      "epoch:  22, loss: 0.21001604199409485\n",
      "epoch:  23, loss: 0.19806869328022003\n",
      "epoch:  24, loss: 0.1884024292230606\n",
      "epoch:  25, loss: 0.17727850377559662\n",
      "epoch:  26, loss: 0.1647949367761612\n",
      "epoch:  27, loss: 0.1516755223274231\n",
      "epoch:  28, loss: 0.14163319766521454\n",
      "epoch:  29, loss: 0.13148953020572662\n",
      "epoch:  30, loss: 0.1222514733672142\n",
      "epoch:  31, loss: 0.11388452351093292\n",
      "epoch:  32, loss: 0.10601625591516495\n",
      "epoch:  33, loss: 0.09864772111177444\n",
      "epoch:  34, loss: 0.0925818458199501\n",
      "epoch:  35, loss: 0.08594010025262833\n",
      "epoch:  36, loss: 0.07933367788791656\n",
      "epoch:  37, loss: 0.07342769205570221\n",
      "epoch:  38, loss: 0.06785500049591064\n",
      "epoch:  39, loss: 0.06271698325872421\n",
      "epoch:  40, loss: 0.057722751051187515\n",
      "epoch:  41, loss: 0.053151242434978485\n",
      "epoch:  42, loss: 0.0486099049448967\n",
      "epoch:  43, loss: 0.04482748731970787\n",
      "epoch:  44, loss: 0.041136857122182846\n",
      "epoch:  45, loss: 0.03769741579890251\n",
      "epoch:  46, loss: 0.034527488052845\n",
      "epoch:  47, loss: 0.03156927973031998\n",
      "epoch:  48, loss: 0.02890772372484207\n",
      "epoch:  49, loss: 0.02668928913772106\n",
      "epoch:  50, loss: 0.024630548432469368\n",
      "epoch:  51, loss: 0.02300918661057949\n",
      "epoch:  52, loss: 0.021599596366286278\n",
      "epoch:  53, loss: 0.020259784534573555\n",
      "epoch:  54, loss: 0.018963584676384926\n",
      "epoch:  55, loss: 0.018003053963184357\n",
      "epoch:  56, loss: 0.01736241951584816\n",
      "epoch:  57, loss: 0.016402006149291992\n",
      "epoch:  58, loss: 0.015705382451415062\n",
      "epoch:  59, loss: 0.015134517103433609\n",
      "epoch:  60, loss: 0.014713031239807606\n",
      "epoch:  61, loss: 0.014130021445453167\n",
      "epoch:  62, loss: 0.013509625568985939\n",
      "epoch:  63, loss: 0.012813488952815533\n",
      "epoch:  64, loss: 0.012328317388892174\n",
      "epoch:  65, loss: 0.012172025628387928\n",
      "epoch:  66, loss: 0.012050415389239788\n",
      "epoch:  67, loss: 0.01178587693721056\n",
      "epoch:  68, loss: 0.01162825245410204\n",
      "epoch:  69, loss: 0.011570353992283344\n",
      "epoch:  70, loss: 0.011386392638087273\n",
      "epoch:  71, loss: 0.011249039322137833\n",
      "epoch:  72, loss: 0.011145160533487797\n",
      "epoch:  73, loss: 0.010944655165076256\n",
      "epoch:  74, loss: 0.010787874460220337\n",
      "epoch:  75, loss: 0.010781629011034966\n",
      "epoch:  76, loss: 0.010604565031826496\n",
      "epoch:  77, loss: 0.010454670526087284\n",
      "epoch:  78, loss: 0.010428211651742458\n",
      "epoch:  79, loss: 0.010295641608536243\n",
      "epoch:  80, loss: 0.010003729723393917\n",
      "epoch:  81, loss: 0.009814808145165443\n",
      "epoch:  82, loss: 0.009665735065937042\n",
      "epoch:  83, loss: 0.009538345970213413\n",
      "epoch:  84, loss: 0.00945210736244917\n",
      "epoch:  85, loss: 0.009306436404585838\n",
      "epoch:  86, loss: 0.009168023243546486\n",
      "epoch:  87, loss: 0.00907156988978386\n",
      "epoch:  88, loss: 0.008997340686619282\n",
      "epoch:  89, loss: 0.008921569213271141\n",
      "epoch:  90, loss: 0.008852602913975716\n",
      "epoch:  91, loss: 0.008791441097855568\n",
      "epoch:  92, loss: 0.008765787817537785\n",
      "epoch:  93, loss: 0.00871602538973093\n",
      "epoch:  94, loss: 0.008663590997457504\n",
      "epoch:  95, loss: 0.00863622035831213\n",
      "epoch:  96, loss: 0.008601808920502663\n",
      "epoch:  97, loss: 0.008560769259929657\n",
      "epoch:  98, loss: 0.008528199046850204\n",
      "epoch:  99, loss: 0.008490744046866894\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size = X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "opt = pytorch_soom.LM(model.parameters(), lr=1, mu=0.001, mu_dec=0.1, model=model, use_diagonal=True, c1=1e-4, tau=0.1, line_search_method='backtrack')\n",
    "\n",
    "all_loss = []\n",
    "patience = 0\n",
    "max_patience = 10\n",
    "for epoch in range(100):\n",
    "    print('epoch: ', epoch, end='')\n",
    "    all_loss.append(0)\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y, loss_fn)\n",
    "        opt.update(loss)\n",
    "\n",
    "        all_loss[epoch] += loss\n",
    "    \n",
    "    all_loss[epoch] /= len(data_loader)\n",
    "\n",
    "    if epoch > 0 and all_loss[epoch-1] <= all_loss[epoch]:\n",
    "        patience -= 1\n",
    "    else:\n",
    "        patience = max_patience\n",
    "        \n",
    "\n",
    "    print(', loss: {}'.format(all_loss[epoch].detach().cpu().numpy().item()))\n",
    "\n",
    "    if patience <= 0:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
