{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch_numopt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import *\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.f1 = nn.Linear(input_size, 10, device=device)\n",
    "        self.f2 = nn.Linear(10, 20, device=device)\n",
    "        self.f3 = nn.Linear(20, 20, device=device)\n",
    "        self.f4 = nn.Linear(20, 10, device=device)\n",
    "        self.f5 = nn.Linear(10, 1, device=device)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        # self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.f1(x))\n",
    "        x = self.activation(self.f2(x))\n",
    "        x = self.activation(self.f3(x))\n",
    "        x = self.activation(self.f4(x))\n",
    "        x = self.f5(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([800, 10]) torch.Size([800, 1])\n",
      "torch.Size([200, 10]) torch.Size([200, 1])\n"
     ]
    }
   ],
   "source": [
    "# X, y = load_diabetes(return_X_y=True, scaled=False)\n",
    "# X, y = make_regression(n_samples=1000, n_features=100)\n",
    "X, y = make_friedman1(n_samples=1000, noise=1e-2)\n",
    "\n",
    "X_scaler = MinMaxScaler()\n",
    "X = X_scaler.fit_transform(X)\n",
    "X = torch.Tensor(X).to(device)\n",
    "\n",
    "y_scaler = MinMaxScaler()\n",
    "y = y_scaler.fit_transform(y.reshape((-1, 1)))\n",
    "y = torch.Tensor(y).to(device)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "torch_data = TensorDataset(X_train, y_train)\n",
    "data_loader = DataLoader(torch_data, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.4964022636413574\n",
      "epoch:  1, loss: 0.48567652702331543\n",
      "epoch:  2, loss: 0.4739667475223541\n",
      "epoch:  3, loss: 0.46143224835395813\n",
      "epoch:  4, loss: 0.44932645559310913\n",
      "epoch:  5, loss: 0.4369351863861084\n",
      "epoch:  6, loss: 0.42492401599884033\n",
      "epoch:  7, loss: 0.4127388596534729\n",
      "epoch:  8, loss: 0.4008500874042511\n",
      "epoch:  9, loss: 0.3888336718082428\n",
      "epoch:  10, loss: 0.37683746218681335\n",
      "epoch:  11, loss: 0.3654038906097412\n",
      "epoch:  12, loss: 0.35359007120132446\n",
      "epoch:  13, loss: 0.3420681357383728\n",
      "epoch:  14, loss: 0.33061736822128296\n",
      "epoch:  15, loss: 0.31905990839004517\n",
      "epoch:  16, loss: 0.3076438009738922\n",
      "epoch:  17, loss: 0.2958626449108124\n",
      "epoch:  18, loss: 0.284471720457077\n",
      "epoch:  19, loss: 0.2734847664833069\n",
      "epoch:  20, loss: 0.2625884711742401\n",
      "epoch:  21, loss: 0.2518167197704315\n",
      "epoch:  22, loss: 0.24116618931293488\n",
      "epoch:  23, loss: 0.2306242287158966\n",
      "epoch:  24, loss: 0.22034727036952972\n",
      "epoch:  25, loss: 0.21011443436145782\n",
      "epoch:  26, loss: 0.200198233127594\n",
      "epoch:  27, loss: 0.19042541086673737\n",
      "epoch:  28, loss: 0.18080712854862213\n",
      "epoch:  29, loss: 0.17095082998275757\n",
      "epoch:  30, loss: 0.16179095208644867\n",
      "epoch:  31, loss: 0.15295527875423431\n",
      "epoch:  32, loss: 0.1440534144639969\n",
      "epoch:  33, loss: 0.13590730726718903\n",
      "epoch:  34, loss: 0.1274348646402359\n",
      "epoch:  35, loss: 0.11952554434537888\n",
      "epoch:  36, loss: 0.11208651214838028\n",
      "epoch:  37, loss: 0.1050274446606636\n",
      "epoch:  38, loss: 0.09782477468252182\n",
      "epoch:  39, loss: 0.09137514233589172\n",
      "epoch:  40, loss: 0.08489231020212173\n",
      "epoch:  41, loss: 0.07865768671035767\n",
      "epoch:  42, loss: 0.0730520635843277\n",
      "epoch:  43, loss: 0.06801758706569672\n",
      "epoch:  44, loss: 0.06319702416658401\n",
      "epoch:  45, loss: 0.058204010128974915\n",
      "epoch:  46, loss: 0.054025448858737946\n",
      "epoch:  47, loss: 0.04952685162425041\n",
      "epoch:  48, loss: 0.045439183712005615\n",
      "epoch:  49, loss: 0.04183744266629219\n",
      "epoch:  50, loss: 0.03864670172333717\n",
      "epoch:  51, loss: 0.03546881675720215\n",
      "epoch:  52, loss: 0.032612089067697525\n",
      "epoch:  53, loss: 0.02982376515865326\n",
      "epoch:  54, loss: 0.02724377065896988\n",
      "epoch:  55, loss: 0.0248368289321661\n",
      "epoch:  56, loss: 0.02265857718884945\n",
      "epoch:  57, loss: 0.020743174478411674\n",
      "epoch:  58, loss: 0.01883728615939617\n",
      "epoch:  59, loss: 0.01711856760084629\n",
      "epoch:  60, loss: 0.015494401566684246\n",
      "epoch:  61, loss: 0.014013465493917465\n",
      "epoch:  62, loss: 0.0126364566385746\n",
      "epoch:  63, loss: 0.011401908472180367\n",
      "epoch:  64, loss: 0.010226096957921982\n",
      "epoch:  65, loss: 0.009183705784380436\n",
      "epoch:  66, loss: 0.008244513534009457\n",
      "epoch:  67, loss: 0.00767858000472188\n",
      "epoch:  68, loss: 0.007184405345469713\n",
      "epoch:  69, loss: 0.006809151731431484\n",
      "epoch:  70, loss: 0.006013492122292519\n",
      "epoch:  71, loss: 0.005806594155728817\n",
      "epoch:  72, loss: 0.005193120799958706\n",
      "epoch:  73, loss: 0.0048497747629880905\n",
      "epoch:  74, loss: 0.004451635759323835\n",
      "epoch:  75, loss: 0.004293194971978664\n",
      "epoch:  76, loss: 0.004166383761912584\n",
      "epoch:  77, loss: 0.003980876877903938\n",
      "epoch:  78, loss: 0.0035861199721693993\n",
      "epoch:  79, loss: 0.003290843917056918\n",
      "epoch:  80, loss: 0.0030848002061247826\n",
      "epoch:  81, loss: 0.0029442505910992622\n",
      "epoch:  82, loss: 0.002845176961272955\n",
      "epoch:  83, loss: 0.002731525572016835\n",
      "epoch:  84, loss: 0.002529672347009182\n",
      "epoch:  85, loss: 0.0024088239297270775\n",
      "epoch:  86, loss: 0.0023360480554401875\n",
      "epoch:  87, loss: 0.0021368786692619324\n",
      "epoch:  88, loss: 0.0019791957456618547\n",
      "epoch:  89, loss: 0.0018971628742292523\n",
      "epoch:  90, loss: 0.001811278983950615\n",
      "epoch:  91, loss: 0.001598646747879684\n",
      "epoch:  92, loss: 0.0015028136549517512\n",
      "epoch:  93, loss: 0.0014504697173833847\n",
      "epoch:  94, loss: 0.001276160590350628\n",
      "epoch:  95, loss: 0.0011768157128244638\n",
      "epoch:  96, loss: 0.0011271743569523096\n",
      "epoch:  97, loss: 0.0010595435742288828\n",
      "epoch:  98, loss: 0.000938848708756268\n",
      "epoch:  99, loss: 0.0008832082967273891\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size=X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "opt = torch_numopt.LM(\n",
    "    params=model.parameters(),\n",
    "    model=model,\n",
    "    lr=1,\n",
    "    mu=0.001,\n",
    "    mu_dec=0.1,\n",
    "    use_diagonal=False,\n",
    "    c1=1e-4,\n",
    "    tau=0.1,\n",
    "    line_search_method=\"backtrack\",\n",
    "    line_search_cond=\"armijo\",\n",
    ")\n",
    "\n",
    "times = []\n",
    "\n",
    "all_loss = {}\n",
    "for epoch in range(100):\n",
    "    print(\"epoch: \", epoch, end=\"\")\n",
    "    all_loss[epoch + 1] = 0\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y, loss_fn)\n",
    "\n",
    "        all_loss[epoch + 1] += loss\n",
    "    all_loss[epoch + 1] /= len(data_loader)\n",
    "    print(\", loss: {}\".format(all_loss[epoch + 1].cpu().detach().numpy().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics: R2 = 0.9750874518780936\n",
      "Test metrics:  R2 = 0.9620878160190012\n"
     ]
    }
   ],
   "source": [
    "pred_train = model.forward(X_train).cpu().detach()\n",
    "pred_test = model.forward(X_test).cpu().detach()\n",
    "print(f\"Train metrics: R2 = {r2_score(pred_train, y_train.cpu())}\")\n",
    "print(f\"Test metrics:  R2 = {r2_score(pred_test, y_test.cpu())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.100897878408432\n",
      "epoch:  1, loss: 0.09353119879961014\n",
      "epoch:  2, loss: 0.08680525422096252\n",
      "epoch:  3, loss: 0.0791642814874649\n",
      "epoch:  4"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eugeniolr/Documents/invAI/torch_numopt/src/torch_numopt/utils.py:33: UserWarning: torch.linalg.svd: During SVD computation with the selected cusolver driver, batches 0 failed to converge. A more accurate method will be used to compute the SVD as a fallback. Check doc at https://pytorch.org/docs/stable/generated/torch.linalg.svd.html (Triggered internally at /pytorch/aten/src/ATen/native/cuda/linalg/BatchLinearAlgebraLib.cpp:690.)\n",
      "  U, S, Vt = torch.linalg.svd(mat)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", loss: 0.07399028539657593\n",
      "epoch:  5, loss: 0.06732853502035141\n",
      "epoch:  6, loss: 0.06676404923200607\n",
      "epoch:  7, loss: 0.06487033516168594\n",
      "epoch:  8, loss: 0.05817955732345581\n",
      "epoch:  9, loss: 0.053729791194200516\n",
      "epoch:  10, loss: 0.04843919351696968\n",
      "epoch:  11, loss: 0.04360366612672806\n",
      "epoch:  12, loss: 0.038669612258672714\n",
      "epoch:  13, loss: 0.034483425319194794\n",
      "epoch:  14, loss: 0.030952658504247665\n",
      "epoch:  15, loss: 0.02741706557571888\n",
      "epoch:  16, loss: 0.024480389431118965\n",
      "epoch:  17, loss: 0.021885279566049576\n",
      "epoch:  18, loss: 0.019750535488128662\n",
      "epoch:  19, loss: 0.017851192504167557\n",
      "epoch:  20, loss: 0.016019117087125778\n",
      "epoch:  21, loss: 0.014269902370870113\n",
      "epoch:  22, loss: 0.01269958820194006\n",
      "epoch:  23, loss: 0.011302256025373936\n",
      "epoch:  24, loss: 0.010109484195709229\n",
      "epoch:  25, loss: 0.00909316074103117\n",
      "epoch:  26, loss: 0.008269531652331352\n",
      "epoch:  27, loss: 0.007492590229958296\n",
      "epoch:  28, loss: 0.006822254508733749\n",
      "epoch:  29, loss: 0.006216709036380053\n",
      "epoch:  30, loss: 0.00570286437869072\n",
      "epoch:  31, loss: 0.005236596800386906\n",
      "epoch:  32, loss: 0.004791694227606058\n",
      "epoch:  33, loss: 0.004392518661916256\n",
      "epoch:  34, loss: 0.003979820758104324\n",
      "epoch:  35, loss: 0.003585794707760215\n",
      "epoch:  36, loss: 0.003231968265026808\n",
      "epoch:  37, loss: 0.002848011674359441\n",
      "epoch:  38, loss: 0.0025961047504097223\n",
      "epoch:  39, loss: 0.0025754510425031185\n",
      "epoch:  40, loss: 0.002391234040260315\n",
      "epoch:  41, loss: 0.0022714228834956884\n",
      "epoch:  42, loss: 0.002196235116571188\n",
      "epoch:  43, loss: 0.002172759035602212\n",
      "epoch:  44, loss: 0.002005899092182517\n",
      "epoch:  45, loss: 0.001911737839691341\n",
      "epoch:  46, loss: 0.0018579032039269805\n",
      "epoch:  47, loss: 0.0018247677944600582\n",
      "epoch:  48, loss: 0.001693311845883727\n",
      "epoch:  49, loss: 0.0016344297910109162\n",
      "epoch:  50, loss: 0.0016014722641557455\n",
      "epoch:  51, loss: 0.0014964068541303277\n",
      "epoch:  52, loss: 0.0014293568674474955\n",
      "epoch:  53, loss: 0.0013970505679026246\n",
      "epoch:  54, loss: 0.0013067045947536826\n",
      "epoch:  55, loss: 0.001243220642209053\n",
      "epoch:  56, loss: 0.0012124248314648867\n",
      "epoch:  57, loss: 0.0012053119717165828\n",
      "epoch:  58, loss: 0.0011376498732715845\n",
      "epoch:  59, loss: 0.0011004101252183318\n",
      "epoch:  60, loss: 0.0010284192394465208\n",
      "epoch:  61, loss: 0.0009759565000422299\n",
      "epoch:  62, loss: 0.0009429868659935892\n",
      "epoch:  63, loss: 0.000921555794775486\n",
      "epoch:  64, loss: 0.0009138640016317368\n",
      "epoch:  65, loss: 0.0008616378763690591\n",
      "epoch:  66, loss: 0.0008309403783641756\n",
      "epoch:  67, loss: 0.0008122714934870601\n",
      "epoch:  68, loss: 0.0007734184036962688\n",
      "epoch:  69, loss: 0.0007338188588619232\n",
      "epoch:  70, loss: 0.0007122204406186938\n",
      "epoch:  71, loss: 0.0006985539221204817\n",
      "epoch:  72, loss: 0.0006571200210601091\n",
      "epoch:  73, loss: 0.0006289557786658406\n",
      "epoch:  74, loss: 0.0006149969412945211\n",
      "epoch:  75, loss: 0.0005932975327596068\n",
      "epoch:  76, loss: 0.0005645614000968635\n",
      "epoch:  77, loss: 0.0005476998630911112\n",
      "epoch:  78, loss: 0.0005361508810892701\n",
      "epoch:  79, loss: 0.0005117147811688483\n",
      "epoch:  80, loss: 0.0004907336551696062\n",
      "epoch:  81, loss: 0.00048099138075485826\n",
      "epoch:  82, loss: 0.0004742890887428075\n",
      "epoch:  83, loss: 0.00046826578909531236\n",
      "epoch:  84, loss: 0.00045782854431308806\n",
      "epoch:  85, loss: 0.0004349280789028853\n",
      "epoch:  86, loss: 0.00042195848072879016\n",
      "epoch:  87, loss: 0.00041241987491957843\n",
      "epoch:  88, loss: 0.0004048403352499008\n",
      "epoch:  89, loss: 0.0003993780701421201\n",
      "epoch:  90, loss: 0.000394492584746331\n",
      "epoch:  91, loss: 0.0003906588244717568\n",
      "epoch:  92, loss: 0.0003866514889523387\n",
      "epoch:  93, loss: 0.00038388706161640584\n",
      "epoch:  94, loss: 0.0003809997288044542\n",
      "epoch:  95, loss: 0.0003788038739003241\n",
      "epoch:  96, loss: 0.0003765432338695973\n",
      "epoch:  97, loss: 0.00037445611087605357\n",
      "epoch:  98, loss: 0.00037238121149130166\n",
      "epoch:  99, loss: 0.0003701836394611746\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size=X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "opt = torch_numopt.LM(\n",
    "    params=model.parameters(),\n",
    "    model=model,\n",
    "    lr=1,\n",
    "    mu=0.001,\n",
    "    mu_dec=0.1,\n",
    "    use_diagonal=True,\n",
    "    c1=1e-4,\n",
    "    tau=0.1,\n",
    "    line_search_method=\"backtrack\",\n",
    "    line_search_cond=\"armijo\",\n",
    ")\n",
    "\n",
    "all_loss = {}\n",
    "for epoch in range(100):\n",
    "    print(\"epoch: \", epoch, end=\"\")\n",
    "    all_loss[epoch + 1] = 0\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y, loss_fn)\n",
    "\n",
    "        all_loss[epoch + 1] += loss\n",
    "    all_loss[epoch + 1] /= len(data_loader)\n",
    "    print(\", loss: {}\".format(all_loss[epoch + 1].cpu().detach().numpy().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics: R2 = 0.9895769623159759\n",
      "Test metrics:  R2 = 0.9821004910719348\n"
     ]
    }
   ],
   "source": [
    "pred_train = model.forward(X_train).cpu().detach()\n",
    "pred_test = model.forward(X_test).cpu().detach()\n",
    "print(f\"Train metrics: R2 = {r2_score(pred_train, y_train.cpu())}\")\n",
    "print(f\"Test metrics:  R2 = {r2_score(pred_test, y_test.cpu())}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_numopt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
