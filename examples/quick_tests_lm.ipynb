{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pytorch_soom\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.f1 = nn.Linear(1, 10, device=device)\n",
    "        self.f2 = nn.Linear(10, 20, device=device)\n",
    "        self.f3 = nn.Linear(20, 20, device=device)\n",
    "        self.f4 = nn.Linear(20, 10, device=device)\n",
    "        self.f5 = nn.Linear(10, 1, device=device)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        # self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.f1(x))\n",
    "        x = self.activation(self.f2(x))\n",
    "        x = self.activation(self.f3(x))\n",
    "        x = self.activation(self.f4(x))\n",
    "        x = self.f5(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.uniform(0, 1, size=(1000, 1), )\n",
    "# y = X[:, 0] - X[:, 1]**2 + 2 * X[:, 2] * X[:, 3] + (1 / ((1 + X[:, 4]) ** 6))\n",
    "y = np.sinc(X).sum(axis=1, keepdims=True)\n",
    "\n",
    "torch_data = TensorDataset(torch.Tensor(X).to(device), torch.Tensor(y).to(device))\n",
    "data_loader = DataLoader(torch_data, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.5460023283958435\n",
      "epoch:  1, loss: 0.5388656854629517\n",
      "epoch:  2, loss: 0.5317680239677429\n",
      "epoch:  3, loss: 0.5247076749801636\n",
      "epoch:  4, loss: 0.5176854133605957\n",
      "epoch:  5, loss: 0.5102195143699646\n",
      "epoch:  6, loss: 0.5034319162368774\n",
      "epoch:  7, loss: 0.4967721700668335\n",
      "epoch:  8, loss: 0.49019044637680054\n",
      "epoch:  9, loss: 0.48401814699172974\n",
      "epoch:  10, loss: 0.47814521193504333\n",
      "epoch:  11, loss: 0.4717056453227997\n",
      "epoch:  12, loss: 0.4656820297241211\n",
      "epoch:  13, loss: 0.45930030941963196\n",
      "epoch:  14, loss: 0.4529972970485687\n",
      "epoch:  15, loss: 0.44826504588127136\n",
      "epoch:  16, loss: 0.44304463267326355\n",
      "epoch:  17, loss: 0.4381445348262787\n",
      "epoch:  18, loss: 0.4325650930404663\n",
      "epoch:  19, loss: 0.4270698130130768\n",
      "epoch:  20, loss: 0.4212556779384613\n",
      "epoch:  21, loss: 0.4158148467540741\n",
      "epoch:  22, loss: 0.41040951013565063\n",
      "epoch:  23, loss: 0.40490931272506714\n",
      "epoch:  24, loss: 0.3995688855648041\n",
      "epoch:  25, loss: 0.39403975009918213\n",
      "epoch:  26, loss: 0.38878437876701355\n",
      "epoch:  27, loss: 0.3834085762500763\n",
      "epoch:  28, loss: 0.3781566917896271\n",
      "epoch:  29, loss: 0.372888445854187\n",
      "epoch:  30, loss: 0.3677885830402374\n",
      "epoch:  31, loss: 0.36261165142059326\n",
      "epoch:  32, loss: 0.3575778901576996\n",
      "epoch:  33, loss: 0.35252994298934937\n",
      "epoch:  34, loss: 0.34756866097450256\n",
      "epoch:  35, loss: 0.34266093373298645\n",
      "epoch:  36, loss: 0.33779090642929077\n",
      "epoch:  37, loss: 0.33302485942840576\n",
      "epoch:  38, loss: 0.32823628187179565\n",
      "epoch:  39, loss: 0.3236221373081207\n",
      "epoch:  40, loss: 0.31892096996307373\n",
      "epoch:  41, loss: 0.3142608106136322\n",
      "epoch:  42, loss: 0.3098539113998413\n",
      "epoch:  43, loss: 0.30533796548843384\n",
      "epoch:  44, loss: 0.300916463136673\n",
      "epoch:  45, loss: 0.29651686549186707\n",
      "epoch:  46, loss: 0.2922893166542053\n",
      "epoch:  47, loss: 0.2879888415336609\n",
      "epoch:  48, loss: 0.2837509512901306\n",
      "epoch:  49, loss: 0.2795692980289459\n",
      "epoch:  50, loss: 0.2754580080509186\n",
      "epoch:  51, loss: 0.2715360224246979\n",
      "epoch:  52, loss: 0.26753470301628113\n",
      "epoch:  53, loss: 0.26359155774116516\n",
      "epoch:  54, loss: 0.2597091794013977\n",
      "epoch:  55, loss: 0.2558857798576355\n",
      "epoch:  56, loss: 0.2521231770515442\n",
      "epoch:  57, loss: 0.24843724071979523\n",
      "epoch:  58, loss: 0.24484729766845703\n",
      "epoch:  59, loss: 0.24130766093730927\n",
      "epoch:  60, loss: 0.23784461617469788\n",
      "epoch:  61, loss: 0.2345796674489975\n",
      "epoch:  62, loss: 0.23152999579906464\n",
      "epoch:  63, loss: 0.2283504605293274\n",
      "epoch:  64, loss: 0.2251216471195221\n",
      "epoch:  65, loss: 0.22194166481494904\n",
      "epoch:  66, loss: 0.21880674362182617\n",
      "epoch:  67, loss: 0.21573778986930847\n",
      "epoch:  68, loss: 0.21271412074565887\n",
      "epoch:  69, loss: 0.20973660051822662\n",
      "epoch:  70, loss: 0.20680323243141174\n",
      "epoch:  71, loss: 0.20391511917114258\n",
      "epoch:  72, loss: 0.20106978714466095\n",
      "epoch:  73, loss: 0.19826757907867432\n",
      "epoch:  74, loss: 0.19550685584545135\n",
      "epoch:  75, loss: 0.1927887350320816\n",
      "epoch:  76, loss: 0.19010798633098602\n",
      "epoch:  77, loss: 0.18749506771564484\n",
      "epoch:  78, loss: 0.18490241467952728\n",
      "epoch:  79, loss: 0.1823454648256302\n",
      "epoch:  80, loss: 0.1798260360956192\n",
      "epoch:  81, loss: 0.1773443967103958\n",
      "epoch:  82, loss: 0.17489971220493317\n",
      "epoch:  83, loss: 0.17249161005020142\n",
      "epoch:  84, loss: 0.17011918127536774\n",
      "epoch:  85, loss: 0.16778212785720825\n",
      "epoch:  86, loss: 0.16548000276088715\n",
      "epoch:  87, loss: 0.16321223974227905\n",
      "epoch:  88, loss: 0.16097842156887054\n",
      "epoch:  89, loss: 0.15877829492092133\n",
      "epoch:  90, loss: 0.1566106677055359\n",
      "epoch:  91, loss: 0.1544756293296814\n",
      "epoch:  92, loss: 0.1523723155260086\n",
      "epoch:  93, loss: 0.15030066668987274\n",
      "epoch:  94, loss: 0.1482597440481186\n",
      "epoch:  95, loss: 0.14624914526939392\n",
      "epoch:  96, loss: 0.14426857233047485\n",
      "epoch:  97, loss: 0.1423172801733017\n",
      "epoch:  98, loss: 0.14039510488510132\n",
      "epoch:  99, loss: 0.1385016292333603\n"
     ]
    }
   ],
   "source": [
    "model = Net(device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "opt = pytorch_soom.LM(model.parameters(), lr=1, ld=1, model=model, use_diagonal=False)\n",
    "\n",
    "all_loss = []\n",
    "patience = 0\n",
    "max_patience = 10\n",
    "for epoch in range(100):\n",
    "    print('epoch: ', epoch, end='')\n",
    "    all_loss.append(0)\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x)\n",
    "        # opt.update(loss)\n",
    "\n",
    "        all_loss[epoch] += loss\n",
    "    \n",
    "    all_loss[epoch] /= len(data_loader)\n",
    "\n",
    "    if epoch > 0 and all_loss[epoch-1] < all_loss[epoch]:\n",
    "        patience -= 1\n",
    "    else:\n",
    "        patience = max_patience\n",
    "        \n",
    "\n",
    "    print(', loss: {}'.format(all_loss[epoch].detach().cpu().numpy().item()))\n",
    "\n",
    "    if patience <= 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_56890/1234295546.py:3: UserWarning: Using the diagonal of the Hessian instead of the Identity matrix might lead to numerical instablity (it's probably just a bug on my part).\n",
      "  opt = pytorch_soom.LM(model.parameters(), lr=1, ld=1, model=model, use_diagonal=True, debug_stability=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", loss: 0.3595387935638428\n",
      "epoch:  1, loss: 0.35622265934944153\n",
      "epoch:  2, loss: 0.3526589572429657\n",
      "epoch:  3, loss: 0.35100114345550537\n",
      "epoch:  4, loss: 0.3460403382778168\n",
      "epoch:  5, loss: 0.34098583459854126\n",
      "epoch:  6, loss: 0.33836400508880615\n",
      "epoch:  7, loss: 0.33150574564933777\n",
      "epoch:  8, loss: 0.3263603150844574\n",
      "epoch:  9, loss: 0.3205708861351013\n",
      "epoch:  10, loss: 0.3160203695297241\n",
      "epoch:  11, loss: 0.31031689047813416\n",
      "epoch:  12, loss: 0.3050231337547302\n",
      "epoch:  13, loss: 0.300214946269989\n",
      "epoch:  14, loss: 0.2954219877719879\n",
      "epoch:  15, loss: 0.29049327969551086\n",
      "epoch:  16, loss: 0.28598761558532715\n",
      "epoch:  17, loss: 0.2814864218235016\n",
      "epoch:  18, loss: 0.27658283710479736\n",
      "epoch:  19, loss: 0.2722622752189636\n",
      "epoch:  20, loss: 0.26798656582832336\n",
      "epoch:  21, loss: 0.26362207531929016\n",
      "epoch:  22, loss: 0.2598413825035095\n",
      "epoch:  23, loss: 0.2556151747703552\n",
      "epoch:  24, loss: 0.25531333684921265\n",
      "epoch:  25, loss: 0.25091052055358887\n",
      "epoch:  26, loss: 0.24656879901885986\n",
      "epoch:  27, loss: 0.24267135560512543\n",
      "epoch:  28, loss: 0.2383757382631302\n",
      "epoch:  29, loss: 0.23430271446704865\n",
      "epoch:  30, loss: 0.23055775463581085\n",
      "epoch:  31, loss: 0.2485598921775818\n",
      "epoch:  32, loss: 0.24423857033252716\n",
      "epoch:  33, loss: 0.23973482847213745\n",
      "epoch:  34, loss: 0.23543141782283783\n",
      "epoch:  35, loss: 0.23103612661361694\n",
      "epoch:  36, loss: 0.22667661309242249\n",
      "epoch:  37, loss: 0.2226635366678238\n",
      "epoch:  38, loss: 0.21878935396671295\n",
      "epoch:  39, loss: 0.21487073600292206\n",
      "epoch:  40, loss: 0.21099963784217834\n",
      "epoch:  41, loss: 0.20772573351860046\n",
      "epoch:  42, loss: 0.2042965292930603\n",
      "epoch:  43, loss: 0.2014005482196808\n",
      "epoch:  44, loss: 0.19841687381267548\n",
      "epoch:  45, loss: 0.19545148313045502\n",
      "epoch:  46, loss: 0.1924663931131363\n",
      "epoch:  47, loss: 0.18929658830165863\n",
      "epoch:  48, loss: 0.18666721880435944\n",
      "epoch:  49, loss: 0.18368412554264069\n",
      "epoch:  50, loss: 0.18116123974323273\n",
      "epoch:  51, loss: 0.178249791264534\n",
      "epoch:  52, loss: 0.17620320618152618\n",
      "epoch:  53, loss: 0.17325428128242493\n",
      "epoch:  54, loss: 0.17055374383926392\n",
      "epoch:  55, loss: 0.1682952642440796\n",
      "epoch:  56, loss: 0.16545051336288452\n",
      "epoch:  57, loss: 0.16289517283439636\n",
      "epoch:  58, loss: 0.16046437621116638\n",
      "epoch:  59, loss: 0.1580960750579834\n",
      "epoch:  60, loss: 0.15569064021110535\n",
      "epoch:  61, loss: 0.15314020216464996\n",
      "epoch:  62, loss: 0.15122559666633606\n",
      "epoch:  63, loss: 0.1486063301563263\n",
      "epoch:  64, loss: 0.14619626104831696\n",
      "epoch:  65, loss: 0.14416107535362244\n",
      "epoch:  66, loss: 0.14189045131206512\n",
      "epoch:  67, loss: 0.1400877833366394\n",
      "epoch:  68, loss: 0.13794276118278503\n",
      "epoch:  69, loss: 0.13684280216693878\n",
      "epoch:  70, loss: 0.13454867899417877\n",
      "epoch:  71, loss: 0.13244354724884033\n",
      "epoch:  72, loss: 0.13054324686527252\n",
      "epoch:  73, loss: 0.12865480780601501\n",
      "epoch:  74, loss: 0.12666185200214386\n",
      "epoch:  75, loss: 0.1254895031452179\n",
      "epoch:  76, loss: 0.12340163439512253\n",
      "epoch:  77, loss: 0.12134575098752975\n",
      "epoch:  78, loss: 0.11961741000413895\n",
      "epoch:  79, loss: 0.1179678812623024\n",
      "epoch:  80, loss: 0.11644729226827621\n",
      "epoch:  81, loss: 0.11466661840677261\n",
      "epoch:  82, loss: 0.11340047419071198\n",
      "epoch:  83, loss: 0.11151131987571716\n",
      "epoch:  84, loss: 0.11070635914802551\n",
      "epoch:  85, loss: 0.10900764912366867\n",
      "epoch:  86, loss: 0.10824532061815262\n",
      "epoch:  87, loss: 0.10648403316736221\n",
      "epoch:  88, loss: 0.10476388782262802\n",
      "epoch:  89, loss: 0.10308825969696045\n",
      "epoch:  90, loss: 0.10222388803958893\n",
      "epoch:  91, loss: 0.10072917491197586\n",
      "epoch:  92, loss: 0.09914769232273102\n",
      "epoch:  93, loss: 0.09770037978887558\n",
      "epoch:  94, loss: 0.09623952209949493\n",
      "epoch:  95, loss: 0.09485384821891785\n",
      "epoch:  96, loss: 0.09363237768411636\n",
      "epoch:  97, loss: 0.092232346534729\n",
      "epoch:  98, loss: 0.09097852557897568\n",
      "epoch:  99, loss: 0.0898313820362091\n"
     ]
    }
   ],
   "source": [
    "model = Net(device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "opt = pytorch_soom.LM(model.parameters(), lr=1, ld=1, model=model, use_diagonal=True, debug_stability=True)\n",
    "\n",
    "all_loss = {}\n",
    "patience = 0\n",
    "max_patience = 10\n",
    "for epoch in range(100):\n",
    "    print('epoch: ', epoch, end='')\n",
    "    all_loss[epoch+1] = 0\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x)\n",
    "        opt.update(loss)\n",
    "\n",
    "        all_loss[epoch+1] += loss\n",
    "    \n",
    "    all_loss[epoch+1] /= len(data_loader)\n",
    "\n",
    "    if epoch > 0 and all_loss[epoch] < all_loss[epoch+1]:\n",
    "        patience -= 1\n",
    "    else:\n",
    "        patience = max_patience\n",
    "        \n",
    "\n",
    "    print(', loss: {}'.format(all_loss[epoch+1].detach().cpu().numpy().item()))\n",
    "\n",
    "    if patience <= 0:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
