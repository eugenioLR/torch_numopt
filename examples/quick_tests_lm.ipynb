{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pytorch_soom\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.f1 = nn.Linear(input_size, 10, device=device)\n",
    "        self.f2 = nn.Linear(10, 20, device=device)\n",
    "        self.f3 = nn.Linear(20, 20, device=device)\n",
    "        self.f4 = nn.Linear(20, 10, device=device)\n",
    "        self.f5 = nn.Linear(10, 1, device=device)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        # self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.f1(x))\n",
    "        x = self.activation(self.f2(x))\n",
    "        x = self.activation(self.f3(x))\n",
    "        x = self.activation(self.f4(x))\n",
    "        x = self.f5(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_diabetes(return_X_y = True, scaled=False)\n",
    "\n",
    "X_scaler = MinMaxScaler()\n",
    "X = X_scaler.fit_transform(X)\n",
    "\n",
    "y_scaler = MinMaxScaler()\n",
    "y = y_scaler.fit_transform(y.reshape((-1, 1)))\n",
    "\n",
    "torch_data = TensorDataset(torch.Tensor(X).to(device), torch.Tensor(y).to(device))\n",
    "data_loader = DataLoader(torch_data, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.10293138772249222\n",
      "epoch:  1, loss: 0.09468388557434082\n",
      "epoch:  2, loss: 0.08654583245515823\n",
      "epoch:  3, loss: 0.07858307659626007\n",
      "epoch:  4, loss: 0.0778888612985611\n",
      "epoch:  5, loss: 0.073574960231781\n",
      "epoch:  6, loss: 0.06216760352253914\n",
      "epoch:  7, loss: 0.05486954376101494\n",
      "epoch:  8, loss: 0.048601049929857254\n",
      "epoch:  9, loss: 0.04106307402253151\n",
      "epoch:  10, loss: 0.03831474483013153\n",
      "epoch:  11, loss: 0.03490950167179108\n",
      "epoch:  12, loss: 0.03080679476261139\n",
      "epoch:  13, loss: 0.02905493788421154\n",
      "epoch:  14, loss: 0.028696918860077858\n",
      "epoch:  15, loss: 0.026741649955511093\n",
      "epoch:  16, loss: 0.024706117808818817\n",
      "epoch:  17, loss: 0.02330246940255165\n",
      "epoch:  18, loss: 0.021804850548505783\n",
      "epoch:  19, loss: 0.020625069737434387\n",
      "epoch:  20, loss: 0.019669316709041595\n",
      "epoch:  21, loss: 0.01820002682507038\n",
      "epoch:  22, loss: 0.017799872905015945\n",
      "epoch:  23, loss: 0.01752820611000061\n",
      "epoch:  24, loss: 0.017249366268515587\n",
      "epoch:  25, loss: 0.01700097881257534\n",
      "epoch:  26, loss: 0.01671580970287323\n",
      "epoch:  27, loss: 0.01648493856191635\n",
      "epoch:  28, loss: 0.016279326751828194\n",
      "epoch:  29, loss: 0.01605978235602379\n",
      "epoch:  30, loss: 0.0158478245139122\n",
      "epoch:  31, loss: 0.01565927267074585\n",
      "epoch:  32, loss: 0.015494580380618572\n",
      "epoch:  33, loss: 0.015340290032327175\n",
      "epoch:  34, loss: 0.01516570895910263\n",
      "epoch:  35, loss: 0.015013358555734158\n",
      "epoch:  36, loss: 0.014885206706821918\n",
      "epoch:  37, loss: 0.014863193035125732\n",
      "epoch:  38, loss: 0.014718747697770596\n",
      "epoch:  39, loss: 0.014556493610143661\n",
      "epoch:  40, loss: 0.0144467419013381\n",
      "epoch:  41, loss: 0.014337575063109398\n",
      "epoch:  42, loss: 0.014263478107750416\n",
      "epoch:  43, loss: 0.014166628941893578\n",
      "epoch:  44, loss: 0.014051084406673908\n",
      "epoch:  45, loss: 0.01396622508764267\n",
      "epoch:  46, loss: 0.01385125145316124\n",
      "epoch:  47, loss: 0.013842480257153511\n",
      "epoch:  48, loss: 0.013694066554307938\n",
      "epoch:  49, loss: 0.01360881794244051\n",
      "epoch:  50, loss: 0.013478380627930164\n",
      "epoch:  51, loss: 0.013369901105761528\n",
      "epoch:  52, loss: 0.01323926355689764\n",
      "epoch:  53, loss: 0.013131508603692055\n",
      "epoch:  54, loss: 0.013007291592657566\n",
      "epoch:  55, loss: 0.012924146838486195\n",
      "epoch:  56, loss: 0.012851006351411343\n",
      "epoch:  57, loss: 0.012749165296554565\n",
      "epoch:  58, loss: 0.012703664600849152\n",
      "epoch:  59, loss: 0.012601892463862896\n",
      "epoch:  60, loss: 0.01254046056419611\n",
      "epoch:  61, loss: 0.012398268096148968\n",
      "epoch:  62, loss: 0.012359599582850933\n",
      "epoch:  63, loss: 0.012257776223123074\n",
      "epoch:  64, loss: 0.012108745984733105\n",
      "epoch:  65, loss: 0.011966072022914886\n",
      "epoch:  66, loss: 0.011869649402797222\n",
      "epoch:  67, loss: 0.011778516694903374\n",
      "epoch:  68, loss: 0.01165445614606142\n",
      "epoch:  69, loss: 0.011586260050535202\n",
      "epoch:  70, loss: 0.01145830936729908\n",
      "epoch:  71, loss: 0.011352826841175556\n",
      "epoch:  72, loss: 0.011252581141889095\n",
      "epoch:  73, loss: 0.01116016786545515\n",
      "epoch:  74, loss: 0.011142081581056118\n",
      "epoch:  75, loss: 0.01112719438970089\n",
      "epoch:  76, loss: 0.011062159202992916\n",
      "epoch:  77, loss: 0.010970257222652435\n",
      "epoch:  78, loss: 0.01086494605988264\n",
      "epoch:  79, loss: 0.010828501544892788\n",
      "epoch:  80, loss: 0.010751493275165558\n",
      "epoch:  81, loss: 0.010731777176260948\n",
      "epoch:  82, loss: 0.01067524403333664\n",
      "epoch:  83, loss: 0.010560081340372562\n",
      "epoch:  84, loss: 0.010466717183589935\n",
      "epoch:  85, loss: 0.010436427779495716\n",
      "epoch:  86, loss: 0.0103073101490736\n",
      "epoch:  87, loss: 0.01026409212499857\n",
      "epoch:  88, loss: 0.010169574059545994\n",
      "epoch:  89, loss: 0.010115795768797398\n",
      "epoch:  90, loss: 0.01004223246127367\n",
      "epoch:  91, loss: 0.009937944822013378\n",
      "epoch:  92, loss: 0.0098922960460186\n",
      "epoch:  93, loss: 0.009873120114207268\n",
      "epoch:  94, loss: 0.009817567653954029\n",
      "epoch:  95, loss: 0.00979140680283308\n",
      "epoch:  96, loss: 0.009684152901172638\n",
      "epoch:  97, loss: 0.009614487178623676\n",
      "epoch:  98, loss: 0.009561922401189804\n",
      "epoch:  99, loss: 0.009545164182782173\n",
      "0.04713420145213604\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size = X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "opt = pytorch_soom.LM(model.parameters(), lr=1, mu=0.001, mu_dec=0.1, model=model, use_diagonal=False, c1=1e-4, tau=0.1, line_search_method='backtrack')\n",
    "\n",
    "times = []\n",
    "\n",
    "all_loss = []\n",
    "patience = 0\n",
    "max_patience = 10\n",
    "for epoch in range(100):\n",
    "    start = time.perf_counter()\n",
    "    print('epoch: ', epoch, end='')\n",
    "    all_loss.append(0)\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y, loss_fn)\n",
    "        opt.update(loss)\n",
    "\n",
    "        all_loss[epoch] += loss\n",
    "    end = time.perf_counter()\n",
    "    \n",
    "    all_loss[epoch] /= len(data_loader)\n",
    "\n",
    "    if epoch > 0 and all_loss[epoch-1] <= all_loss[epoch]:\n",
    "        patience -= 1\n",
    "    else:\n",
    "        patience = max_patience\n",
    "        \n",
    "\n",
    "    print(', loss: {}'.format(all_loss[epoch].detach().cpu().numpy().item()))\n",
    "    # print(', time spent {}'.format(end-start))\n",
    "    times.append(end-start)\n",
    "\n",
    "    if patience <= 0:\n",
    "        break\n",
    "\n",
    "print(sum(times)/len(times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.27584630250930786\n",
      "epoch:  1, loss: 0.258060485124588\n",
      "epoch:  2, loss: 0.24565142393112183\n",
      "epoch:  3, loss: 0.22019362449645996\n",
      "epoch:  4, loss: 0.19657878577709198\n",
      "epoch:  5, loss: 0.17744438350200653\n",
      "epoch:  6, loss: 0.15840564668178558\n",
      "epoch:  7, loss: 0.14702625572681427\n",
      "epoch:  8, loss: 0.13465726375579834\n",
      "epoch:  9, loss: 0.12330587208271027\n",
      "epoch:  10, loss: 0.11314216256141663\n",
      "epoch:  11, loss: 0.10373230278491974\n",
      "epoch:  12, loss: 0.09607663005590439\n",
      "epoch:  13, loss: 0.08806784451007843\n",
      "epoch:  14, loss: 0.0817742869257927\n",
      "epoch:  15, loss: 0.07486937940120697\n",
      "epoch:  16, loss: 0.068750761449337\n",
      "epoch:  17, loss: 0.063807912170887\n",
      "epoch:  18, loss: 0.05851135402917862\n",
      "epoch:  19, loss: 0.053602393716573715\n",
      "epoch:  20, loss: 0.04966318607330322\n",
      "epoch:  21, loss: 0.045875340700149536\n",
      "epoch:  22, loss: 0.04324166104197502\n",
      "epoch:  23, loss: 0.040094904601573944\n",
      "epoch:  24, loss: 0.03666651248931885\n",
      "epoch:  25, loss: 0.03529784828424454\n",
      "epoch:  26, loss: 0.03270470350980759\n",
      "epoch:  27, loss: 0.030447501689195633\n",
      "epoch:  28, loss: 0.027895847335457802\n",
      "epoch:  29, loss: 0.026375697925686836\n",
      "epoch:  30, loss: 0.02446291595697403\n",
      "epoch:  31, loss: 0.022585775703191757\n",
      "epoch:  32, loss: 0.021076031029224396\n",
      "epoch:  33, loss: 0.019916152581572533\n",
      "epoch:  34, loss: 0.018678393214941025\n",
      "epoch:  35, loss: 0.017905835062265396\n",
      "epoch:  36, loss: 0.017578603699803352\n",
      "epoch:  37, loss: 0.01693422719836235\n",
      "epoch:  38, loss: 0.01596258021891117\n",
      "epoch:  39, loss: 0.015267303213477135\n",
      "epoch:  40, loss: 0.01479261927306652\n",
      "epoch:  41, loss: 0.014528730884194374\n",
      "epoch:  42, loss: 0.01397816650569439\n",
      "epoch:  43, loss: 0.013712512329220772\n",
      "epoch:  44, loss: 0.013403771445155144\n",
      "epoch:  45, loss: 0.013049731962382793\n",
      "epoch:  46, loss: 0.012790365144610405\n",
      "epoch:  47, loss: 0.012389115057885647\n",
      "epoch:  48, loss: 0.012243473902344704\n",
      "epoch:  49, loss: 0.012146245688199997\n",
      "epoch:  50, loss: 0.012042896822094917\n",
      "epoch:  51, loss: 0.011946857906877995\n",
      "epoch:  52, loss: 0.011871874332427979\n",
      "epoch:  53, loss: 0.011790729127824306\n",
      "epoch:  54, loss: 0.011744293384253979\n",
      "epoch:  55, loss: 0.011670618318021297\n",
      "epoch:  56, loss: 0.011608118191361427\n",
      "epoch:  57, loss: 0.011542430147528648\n",
      "epoch:  58, loss: 0.011494404636323452\n",
      "epoch:  59, loss: 0.011438736692070961\n",
      "epoch:  60, loss: 0.011389201506972313\n",
      "epoch:  61, loss: 0.011326238512992859\n",
      "epoch:  62, loss: 0.011272430419921875\n",
      "epoch:  63, loss: 0.011224211193621159\n",
      "epoch:  64, loss: 0.011168358847498894\n",
      "epoch:  65, loss: 0.011114023625850677\n",
      "epoch:  66, loss: 0.011064334772527218\n",
      "epoch:  67, loss: 0.011020472273230553\n",
      "epoch:  68, loss: 0.010987143963575363\n",
      "epoch:  69, loss: 0.010954625904560089\n",
      "epoch:  70, loss: 0.01091136783361435\n",
      "epoch:  71, loss: 0.010867011733353138\n",
      "epoch:  72, loss: 0.01082882285118103\n",
      "epoch:  73, loss: 0.010807624086737633\n",
      "epoch:  74, loss: 0.010770702734589577\n",
      "epoch:  75, loss: 0.010724196210503578\n",
      "epoch:  76, loss: 0.01070057600736618\n",
      "epoch:  77, loss: 0.010697083547711372\n",
      "epoch:  78, loss: 0.0106620192527771\n",
      "epoch:  79, loss: 0.010624449700117111\n",
      "epoch:  80, loss: 0.010576635599136353\n",
      "epoch:  81, loss: 0.010542296804487705\n",
      "epoch:  82, loss: 0.010513360612094402\n",
      "epoch:  83, loss: 0.010468937456607819\n",
      "epoch:  84, loss: 0.01043319795280695\n",
      "epoch:  85, loss: 0.01039515994489193\n",
      "epoch:  86, loss: 0.010352167300879955\n",
      "epoch:  87, loss: 0.01031193695962429\n",
      "epoch:  88, loss: 0.010277921333909035\n",
      "epoch:  89, loss: 0.010243730619549751\n",
      "epoch:  90, loss: 0.01021635066717863\n",
      "epoch:  91, loss: 0.010189322754740715\n",
      "epoch:  92, loss: 0.010136938653886318\n",
      "epoch:  93, loss: 0.010088427923619747\n",
      "epoch:  94, loss: 0.010057421401143074\n",
      "epoch:  95, loss: 0.01001544389873743\n",
      "epoch:  96, loss: 0.009999681264162064\n",
      "epoch:  97, loss: 0.009969028644263744\n",
      "epoch:  98, loss: 0.009932140819728374\n",
      "epoch:  99, loss: 0.0099154282361269\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size = X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "opt = pytorch_soom.LM(model.parameters(), lr=1, mu=0.001, mu_dec=0.1, model=model, use_diagonal=True, c1=1e-4, tau=0.1, line_search_method='backtrack')\n",
    "\n",
    "all_loss = []\n",
    "patience = 0\n",
    "max_patience = 10\n",
    "for epoch in range(100):\n",
    "    print('epoch: ', epoch, end='')\n",
    "    all_loss.append(0)\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y, loss_fn)\n",
    "        opt.update(loss)\n",
    "\n",
    "        all_loss[epoch] += loss\n",
    "    \n",
    "    all_loss[epoch] /= len(data_loader)\n",
    "\n",
    "    if epoch > 0 and all_loss[epoch-1] <= all_loss[epoch]:\n",
    "        patience -= 1\n",
    "    else:\n",
    "        patience = max_patience\n",
    "        \n",
    "\n",
    "    print(', loss: {}'.format(all_loss[epoch].detach().cpu().numpy().item()))\n",
    "\n",
    "    if patience <= 0:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
