{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch_numopt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import *\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.f1 = nn.Linear(input_size, 10, device=device)\n",
    "        self.f2 = nn.Linear(10, 20, device=device)\n",
    "        self.f3 = nn.Linear(20, 20, device=device)\n",
    "        self.f4 = nn.Linear(20, 10, device=device)\n",
    "        self.f5 = nn.Linear(10, 1, device=device)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        # self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.f1(x))\n",
    "        x = self.activation(self.f2(x))\n",
    "        x = self.activation(self.f3(x))\n",
    "        x = self.activation(self.f4(x))\n",
    "        x = self.f5(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([800, 10]) torch.Size([800, 1])\n",
      "torch.Size([200, 10]) torch.Size([200, 1])\n"
     ]
    }
   ],
   "source": [
    "# X, y = load_diabetes(return_X_y=True, scaled=False)\n",
    "# X, y = make_regression(n_samples=1000, n_features=100)\n",
    "X, y = make_friedman1(n_samples=1000, noise=1e-2)\n",
    "\n",
    "X_scaler = MinMaxScaler()\n",
    "X = X_scaler.fit_transform(X)\n",
    "X = torch.Tensor(X).to(device)\n",
    "\n",
    "y_scaler = MinMaxScaler()\n",
    "y = y_scaler.fit_transform(y.reshape((-1, 1)))\n",
    "y = torch.Tensor(y).to(device)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "torch_data = TensorDataset(X_train, y_train)\n",
    "data_loader = DataLoader(torch_data, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.218982994556427\n",
      "epoch:  1, loss: 0.20870982110500336\n",
      "epoch:  2, loss: 0.19838444888591766\n",
      "epoch:  3, loss: 0.18788135051727295\n",
      "epoch:  4, loss: 0.17770732939243317\n",
      "epoch:  5, loss: 0.1675848513841629\n",
      "epoch:  6, loss: 0.15772496163845062\n",
      "epoch:  7, loss: 0.14778293669223785\n",
      "epoch:  8, loss: 0.1381731927394867\n",
      "epoch:  9, loss: 0.1289830505847931\n",
      "epoch:  10, loss: 0.12015390396118164\n",
      "epoch:  11, loss: 0.11167602986097336\n",
      "epoch:  12, loss: 0.10355976223945618\n",
      "epoch:  13, loss: 0.09589988738298416\n",
      "epoch:  14, loss: 0.08874791115522385\n",
      "epoch:  15, loss: 0.08205219358205795\n",
      "epoch:  16, loss: 0.07606885582208633\n",
      "epoch:  17, loss: 0.07027136534452438\n",
      "epoch:  18, loss: 0.06508006155490875\n",
      "epoch:  19, loss: 0.060216471552848816\n",
      "epoch:  20, loss: 0.05584419146180153\n",
      "epoch:  21, loss: 0.05175312981009483\n",
      "epoch:  22, loss: 0.04785919189453125\n",
      "epoch:  23, loss: 0.04424922913312912\n",
      "epoch:  24, loss: 0.04080038145184517\n",
      "epoch:  25, loss: 0.03757404908537865\n",
      "epoch:  26, loss: 0.034718021750450134\n",
      "epoch:  27, loss: 0.0319671630859375\n",
      "epoch:  28, loss: 0.02937529981136322\n",
      "epoch:  29, loss: 0.027131762355566025\n",
      "epoch:  30, loss: 0.024984726682305336\n",
      "epoch:  31, loss: 0.02304447628557682\n",
      "epoch:  32, loss: 0.021184170618653297\n",
      "epoch:  33, loss: 0.019344959408044815\n",
      "epoch:  34, loss: 0.017726177349686623\n",
      "epoch:  35, loss: 0.016269752755761147\n",
      "epoch:  36, loss: 0.014931676909327507\n",
      "epoch:  37, loss: 0.013737314380705357\n",
      "epoch:  38, loss: 0.012669620104134083\n",
      "epoch:  39, loss: 0.011539063416421413\n",
      "epoch:  40, loss: 0.010576351545751095\n",
      "epoch:  41, loss: 0.009652786888182163\n",
      "epoch:  42, loss: 0.008836448192596436\n",
      "epoch:  43, loss: 0.008033995516598225\n",
      "epoch:  44, loss: 0.007361155468970537\n",
      "epoch:  45, loss: 0.006756842602044344\n",
      "epoch:  46, loss: 0.006206403952091932\n",
      "epoch:  47, loss: 0.005707687698304653\n",
      "epoch:  48, loss: 0.0052801924757659435\n",
      "epoch:  49, loss: 0.004830936901271343\n",
      "epoch:  50, loss: 0.0044267144985497\n",
      "epoch:  51, loss: 0.004029400181025267\n",
      "epoch:  52, loss: 0.0036577971186488867\n",
      "epoch:  53, loss: 0.0033084505703300238\n",
      "epoch:  54, loss: 0.0031313507352024317\n",
      "epoch:  55, loss: 0.0028469632379710674\n",
      "epoch:  56, loss: 0.002580035012215376\n",
      "epoch:  57, loss: 0.0025085799861699343\n",
      "epoch:  58, loss: 0.0023720350582152605\n",
      "epoch:  59, loss: 0.0022759693674743176\n",
      "epoch:  60, loss: 0.002237198641523719\n",
      "epoch:  61, loss: 0.002093323739245534\n",
      "epoch:  62, loss: 0.001993610057979822\n",
      "epoch:  63, loss: 0.0019282088615000248\n",
      "epoch:  64, loss: 0.0018847495084628463\n",
      "epoch:  65, loss: 0.0017798412591218948\n",
      "epoch:  66, loss: 0.0016960710054263473\n",
      "epoch:  67, loss: 0.001647027675062418\n",
      "epoch:  68, loss: 0.001608563121408224\n",
      "epoch:  69, loss: 0.0014994335360825062\n",
      "epoch:  70, loss: 0.0014428716385737062\n",
      "epoch:  71, loss: 0.0014129658229649067\n",
      "epoch:  72, loss: 0.0013334586983546615\n",
      "epoch:  73, loss: 0.001268609194085002\n",
      "epoch:  74, loss: 0.001236766460351646\n",
      "epoch:  75, loss: 0.0012140915496274829\n",
      "epoch:  76, loss: 0.0011229884112253785\n",
      "epoch:  77, loss: 0.0010861322516575456\n",
      "epoch:  78, loss: 0.0010678911348804832\n",
      "epoch:  79, loss: 0.001007198472507298\n",
      "epoch:  80, loss: 0.0009613318252377212\n",
      "epoch:  81, loss: 0.0009363304125145078\n",
      "epoch:  82, loss: 0.0009068757062777877\n",
      "epoch:  83, loss: 0.0008493027416989207\n",
      "epoch:  84, loss: 0.0008150356588885188\n",
      "epoch:  85, loss: 0.0008027221192605793\n",
      "epoch:  86, loss: 0.0007377424044534564\n",
      "epoch:  87, loss: 0.0006986315129324794\n",
      "epoch:  88, loss: 0.0006736571667715907\n",
      "epoch:  89, loss: 0.0006599288899451494\n",
      "epoch:  90, loss: 0.0006003460730426013\n",
      "epoch:  91, loss: 0.0005684510688297451\n",
      "epoch:  92, loss: 0.000549431424587965\n",
      "epoch:  93, loss: 0.0005189753137528896\n",
      "epoch:  94, loss: 0.00047786100185476243\n",
      "epoch:  95, loss: 0.0004573255719151348\n",
      "epoch:  96, loss: 0.000445924059022218\n",
      "epoch:  97, loss: 0.00042235589353367686\n",
      "epoch:  98, loss: 0.00039669990655966103\n",
      "epoch:  99, loss: 0.0003853569214697927\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size=X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "opt = torch_numopt.LM(\n",
    "    params=model.parameters(),\n",
    "    model=model,\n",
    "    lr=1,\n",
    "    mu=0.001,\n",
    "    mu_dec=0.1,\n",
    "    use_diagonal=False,\n",
    "    c1=1e-4,\n",
    "    tau=0.1,\n",
    "    line_search_method=\"backtrack\",\n",
    "    line_search_cond=\"armijo\",\n",
    ")\n",
    "\n",
    "times = []\n",
    "\n",
    "all_loss = {}\n",
    "for epoch in range(100):\n",
    "    print(\"epoch: \", epoch, end=\"\")\n",
    "    all_loss[epoch + 1] = 0\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y, loss_fn)\n",
    "\n",
    "        all_loss[epoch + 1] += loss\n",
    "    all_loss[epoch + 1] /= len(data_loader)\n",
    "    print(\", loss: {}\".format(all_loss[epoch + 1].cpu().detach().numpy().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics: R2 = 0.9900502405031074\n",
      "Test metrics:  R2 = 0.9793472135066519\n"
     ]
    }
   ],
   "source": [
    "pred_train = model.forward(X_train).cpu().detach()\n",
    "pred_test = model.forward(X_test).cpu().detach()\n",
    "print(f\"Train metrics: R2 = {r2_score(pred_train, y_train.cpu())}\")\n",
    "print(f\"Test metrics:  R2 = {r2_score(pred_test, y_test.cpu())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eugeniolr/Documents/invAI/torch_numopt/src/torch_numopt/utils.py:33: UserWarning: torch.linalg.svd: During SVD computation with the selected cusolver driver, batches 0 failed to converge. A more accurate method will be used to compute the SVD as a fallback. Check doc at https://pytorch.org/docs/stable/generated/torch.linalg.svd.html (Triggered internally at /pytorch/aten/src/ATen/native/cuda/linalg/BatchLinearAlgebraLib.cpp:690.)\n",
      "  U, S, Vt = torch.linalg.svd(mat)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", loss: 0.09479362517595291\n",
      "epoch:  1, loss: 0.08853869885206223\n",
      "epoch:  2, loss: 0.08481962233781815\n",
      "epoch:  3, loss: 0.07500425726175308\n",
      "epoch:  4, loss: 0.0682247132062912\n",
      "epoch:  5, loss: 0.062481839209795\n",
      "epoch:  6, loss: 0.05768628790974617\n",
      "epoch:  7, loss: 0.04894156754016876\n",
      "epoch:  8, loss: 0.0448191836476326\n",
      "epoch:  9, loss: 0.04052858427166939\n",
      "epoch:  10, loss: 0.037106938660144806\n",
      "epoch:  11, loss: 0.03317578509449959\n",
      "epoch:  12, loss: 0.029414257034659386\n",
      "epoch:  13, loss: 0.025448020547628403\n",
      "epoch:  14, loss: 0.022308597341179848\n",
      "epoch:  15, loss: 0.01982753723859787\n",
      "epoch:  16, loss: 0.017529431730508804\n",
      "epoch:  17, loss: 0.0155929084867239\n",
      "epoch:  18, loss: 0.01389321032911539\n",
      "epoch:  19, loss: 0.01195856649428606\n",
      "epoch:  20, loss: 0.010467266663908958\n",
      "epoch:  21, loss: 0.009076748974621296\n",
      "epoch:  22, loss: 0.0079647833481431\n",
      "epoch:  23, loss: 0.007162810303270817\n",
      "epoch:  24, loss: 0.00660259835422039\n",
      "epoch:  25, loss: 0.006085315719246864\n",
      "epoch:  26, loss: 0.005669840611517429\n",
      "epoch:  27, loss: 0.0052634417079389095\n",
      "epoch:  28, loss: 0.004899962339550257\n",
      "epoch:  29, loss: 0.004649428650736809\n",
      "epoch:  30, loss: 0.004325997084379196\n",
      "epoch:  31, loss: 0.004168516956269741\n",
      "epoch:  32, loss: 0.0038853005971759558\n",
      "epoch:  33, loss: 0.0037234670016914606\n",
      "epoch:  34, loss: 0.003517702454701066\n",
      "epoch:  35, loss: 0.0034748464822769165\n",
      "epoch:  36, loss: 0.003435047110542655\n",
      "epoch:  37, loss: 0.0031938247848302126\n",
      "epoch:  38, loss: 0.003121060086414218\n",
      "epoch:  39, loss: 0.0030798385851085186\n",
      "epoch:  40, loss: 0.00297773745842278\n",
      "epoch:  41, loss: 0.00290219415910542\n",
      "epoch:  42, loss: 0.0028486403170973063\n",
      "epoch:  43, loss: 0.0028102772776037455\n",
      "epoch:  44, loss: 0.002735853660851717\n",
      "epoch:  45, loss: 0.002732653170824051\n",
      "epoch:  46, loss: 0.002625560387969017\n",
      "epoch:  47, loss: 0.0025520860217511654\n",
      "epoch:  48, loss: 0.002504601376131177\n",
      "epoch:  49, loss: 0.0024710914585739374\n",
      "epoch:  50, loss: 0.0024026234168559313\n",
      "epoch:  51, loss: 0.0023586302995681763\n",
      "epoch:  52, loss: 0.002327942755073309\n",
      "epoch:  53, loss: 0.0023046424612402916\n",
      "epoch:  54, loss: 0.002208919497206807\n",
      "epoch:  55, loss: 0.002174400258809328\n",
      "epoch:  56, loss: 0.002153201727196574\n",
      "epoch:  57, loss: 0.0020968480966985226\n",
      "epoch:  58, loss: 0.0020551825873553753\n",
      "epoch:  59, loss: 0.0020323707722127438\n",
      "epoch:  60, loss: 0.0019569394644349813\n",
      "epoch:  61, loss: 0.0019066202221438289\n",
      "epoch:  62, loss: 0.001876916503533721\n",
      "epoch:  63, loss: 0.001793045667000115\n",
      "epoch:  64, loss: 0.001743502914905548\n",
      "epoch:  65, loss: 0.0017174910753965378\n",
      "epoch:  66, loss: 0.0016043857904151082\n",
      "epoch:  67, loss: 0.001540417317301035\n",
      "epoch:  68, loss: 0.001501105958595872\n",
      "epoch:  69, loss: 0.0014073157217353582\n",
      "epoch:  70, loss: 0.0013331419322639704\n",
      "epoch:  71, loss: 0.0012864579912275076\n",
      "epoch:  72, loss: 0.0012205892708152533\n",
      "epoch:  73, loss: 0.0011357112089172006\n",
      "epoch:  74, loss: 0.0010874411091208458\n",
      "epoch:  75, loss: 0.0010537124471738935\n",
      "epoch:  76, loss: 0.0009951775427907705\n",
      "epoch:  77, loss: 0.000927193439565599\n",
      "epoch:  78, loss: 0.0008857747889123857\n",
      "epoch:  79, loss: 0.0008593560778535903\n",
      "epoch:  80, loss: 0.000818422413431108\n",
      "epoch:  81, loss: 0.0007757071871310472\n",
      "epoch:  82, loss: 0.0007542821695096791\n",
      "epoch:  83, loss: 0.0007428761455230415\n",
      "epoch:  84, loss: 0.0007234780350700021\n",
      "epoch:  85, loss: 0.0007017782190814614\n",
      "epoch:  86, loss: 0.0006898614810779691\n",
      "epoch:  87, loss: 0.000682033714838326\n",
      "epoch:  88, loss: 0.0006617159233428538\n",
      "epoch:  89, loss: 0.0006448141066357493\n",
      "epoch:  90, loss: 0.0006346555892378092\n",
      "epoch:  91, loss: 0.0006274515762925148\n",
      "epoch:  92, loss: 0.0006231382139958441\n",
      "epoch:  93, loss: 0.0006060308660380542\n",
      "epoch:  94, loss: 0.000596830912400037\n",
      "epoch:  95, loss: 0.000590204494073987\n",
      "epoch:  96, loss: 0.0005851588794030249\n",
      "epoch:  97, loss: 0.0005791124422103167\n",
      "epoch:  98, loss: 0.0005607864004559815\n",
      "epoch:  99, loss: 0.0005525731248781085\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size=X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "opt = torch_numopt.LM(\n",
    "    params=model.parameters(),\n",
    "    model=model,\n",
    "    lr=1,\n",
    "    mu=0.001,\n",
    "    mu_dec=0.1,\n",
    "    use_diagonal=True,\n",
    "    c1=1e-4,\n",
    "    tau=0.1,\n",
    "    line_search_method=\"backtrack\",\n",
    "    line_search_cond=\"armijo\",\n",
    ")\n",
    "\n",
    "all_loss = {}\n",
    "for epoch in range(100):\n",
    "    print(\"epoch: \", epoch, end=\"\")\n",
    "    all_loss[epoch + 1] = 0\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y, loss_fn)\n",
    "\n",
    "        all_loss[epoch + 1] += loss\n",
    "    all_loss[epoch + 1] /= len(data_loader)\n",
    "    print(\", loss: {}\".format(all_loss[epoch + 1].cpu().detach().numpy().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics: R2 = 0.9855669068830493\n",
      "Test metrics:  R2 = 0.9816192728662396\n"
     ]
    }
   ],
   "source": [
    "pred_train = model.forward(X_train).cpu().detach()\n",
    "pred_test = model.forward(X_test).cpu().detach()\n",
    "print(f\"Train metrics: R2 = {r2_score(pred_train, y_train.cpu())}\")\n",
    "print(f\"Test metrics:  R2 = {r2_score(pred_test, y_test.cpu())}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_numopt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
