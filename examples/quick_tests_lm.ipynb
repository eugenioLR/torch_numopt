{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch_numopt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.f1 = nn.Linear(input_size, 10, device=device)\n",
    "        self.f2 = nn.Linear(10, 20, device=device)\n",
    "        self.f3 = nn.Linear(20, 20, device=device)\n",
    "        self.f4 = nn.Linear(20, 10, device=device)\n",
    "        self.f5 = nn.Linear(10, 1, device=device)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        # self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.f1(x))\n",
    "        x = self.activation(self.f2(x))\n",
    "        x = self.activation(self.f3(x))\n",
    "        x = self.activation(self.f4(x))\n",
    "        x = self.f5(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_diabetes(return_X_y = True, scaled=False)\n",
    "\n",
    "X_scaler = MinMaxScaler()\n",
    "X = X_scaler.fit_transform(X)\n",
    "\n",
    "y_scaler = MinMaxScaler()\n",
    "y = y_scaler.fit_transform(y.reshape((-1, 1)))\n",
    "\n",
    "torch_data = TensorDataset(torch.Tensor(X).to(device), torch.Tensor(y).to(device))\n",
    "data_loader = DataLoader(torch_data, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.14730283617973328\n",
      "epoch:  1, loss: 0.1328197419643402\n",
      "epoch:  2, loss: 0.11953799426555634\n",
      "epoch:  3, loss: 0.10804057866334915\n",
      "epoch:  4, loss: 0.09924285113811493\n",
      "epoch:  5, loss: 0.09785141050815582\n",
      "epoch:  6, loss: 0.09741376340389252\n",
      "epoch:  7, loss: 0.09637270122766495\n",
      "epoch:  8, loss: 0.0953952893614769\n",
      "epoch:  9, loss: 0.09487812221050262\n",
      "epoch:  10, loss: 0.08663635700941086\n",
      "epoch:  11, loss: 0.0865328460931778\n",
      "epoch:  12, loss: 0.08645948022603989\n",
      "epoch:  13, loss: 0.07620896399021149\n",
      "epoch:  14, loss: 0.07614918798208237\n",
      "epoch:  15, loss: 0.0753786489367485\n",
      "epoch:  16, loss: 0.07462982088327408\n",
      "epoch:  17, loss: 0.07427847385406494\n",
      "epoch:  18, loss: 0.06533383578062057\n",
      "epoch:  19, loss: 0.06434480845928192\n",
      "epoch:  20, loss: 0.055998388677835464\n",
      "epoch:  21, loss: 0.0513995923101902\n",
      "epoch:  22, loss: 0.04815226420760155\n",
      "epoch:  23, loss: 0.04434782266616821\n",
      "epoch:  24, loss: 0.040134381502866745\n",
      "epoch:  25, loss: 0.03988191857933998\n",
      "epoch:  26, loss: 0.037301283329725266\n",
      "epoch:  27, loss: 0.0351475328207016\n",
      "epoch:  28, loss: 0.03364226967096329\n",
      "epoch:  29, loss: 0.03131136670708656\n",
      "epoch:  30, loss: 0.030999761074781418\n",
      "epoch:  31, loss: 0.02949610725045204\n",
      "epoch:  32, loss: 0.02792206220328808\n",
      "epoch:  33, loss: 0.02761724777519703\n",
      "epoch:  34, loss: 0.02681165561079979\n",
      "epoch:  35, loss: 0.025539787486195564\n",
      "epoch:  36, loss: 0.023934893310070038\n",
      "epoch:  37, loss: 0.02377007156610489\n",
      "epoch:  38, loss: 0.02264934591948986\n",
      "epoch:  39, loss: 0.022621184587478638\n",
      "epoch:  40, loss: 0.021191973239183426\n",
      "epoch:  41, loss: 0.02076633833348751\n",
      "epoch:  42, loss: 0.020583156496286392\n",
      "epoch:  43, loss: 0.019752880558371544\n",
      "epoch:  44, loss: 0.01972448080778122\n",
      "epoch:  45, loss: 0.018998844549059868\n",
      "epoch:  46, loss: 0.017975566908717155\n",
      "epoch:  47, loss: 0.01773565076291561\n",
      "epoch:  48, loss: 0.01755497418344021\n",
      "epoch:  49, loss: 0.016910428181290627\n",
      "epoch:  50, loss: 0.01673845387995243\n",
      "epoch:  51, loss: 0.016203725710511208\n",
      "epoch:  52, loss: 0.01604234054684639\n",
      "epoch:  53, loss: 0.01583123579621315\n",
      "epoch:  54, loss: 0.015633130446076393\n",
      "epoch:  55, loss: 0.015489928424358368\n",
      "epoch:  56, loss: 0.015487730503082275\n",
      "epoch:  57, loss: 0.015454689040780067\n",
      "epoch:  58, loss: 0.015271566808223724\n",
      "epoch:  59, loss: 0.015122503973543644\n",
      "epoch:  60, loss: 0.01503054890781641\n",
      "epoch:  61, loss: 0.01488384511321783\n",
      "epoch:  62, loss: 0.014758728444576263\n",
      "epoch:  63, loss: 0.014623521827161312\n",
      "epoch:  64, loss: 0.014484012499451637\n",
      "epoch:  65, loss: 0.014425312168896198\n",
      "epoch:  66, loss: 0.014410683885216713\n",
      "epoch:  67, loss: 0.014308656565845013\n",
      "epoch:  68, loss: 0.014291735365986824\n",
      "epoch:  69, loss: 0.014277123846113682\n",
      "epoch:  70, loss: 0.014258982613682747\n",
      "epoch:  71, loss: 0.014134394004940987\n",
      "epoch:  72, loss: 0.014022348448634148\n",
      "epoch:  73, loss: 0.014006078243255615\n",
      "epoch:  74, loss: 0.013945585116744041\n",
      "epoch:  75, loss: 0.013791405595839024\n",
      "epoch:  76, loss: 0.01365652121603489\n",
      "epoch:  77, loss: 0.013638954609632492\n",
      "epoch:  78, loss: 0.013626460917294025\n",
      "epoch:  79, loss: 0.013440215028822422\n",
      "epoch:  80, loss: 0.013422721065580845\n",
      "epoch:  81, loss: 0.013413023203611374\n",
      "epoch:  82, loss: 0.013330072164535522\n",
      "epoch:  83, loss: 0.013298118487000465\n",
      "epoch:  84, loss: 0.013284451328217983\n",
      "epoch:  85, loss: 0.01327480748295784\n",
      "epoch:  86, loss: 0.01325842086225748\n",
      "epoch:  87, loss: 0.013243020512163639\n",
      "epoch:  88, loss: 0.013230876065790653\n",
      "epoch:  89, loss: 0.013218608684837818\n",
      "epoch:  90, loss: 0.01320446003228426\n",
      "epoch:  91, loss: 0.013192463666200638\n",
      "epoch:  92, loss: 0.013166239485144615\n",
      "epoch:  93, loss: 0.013165710493922234\n",
      "epoch:  94, loss: 0.013108363375067711\n",
      "epoch:  95, loss: 0.013083478435873985\n",
      "epoch:  96, loss: 0.013065055944025517\n",
      "epoch:  97, loss: 0.013048593886196613\n",
      "epoch:  98, loss: 0.013046730309724808\n",
      "epoch:  99, loss: 0.012969241477549076\n",
      "0.04755968402139842\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size = X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "opt = torch_numopt.LM(model.parameters(), lr=1, mu=0.001, mu_dec=0.1, model=model, use_diagonal=False, c1=1e-4, tau=0.1, line_search_method='backtrack')\n",
    "\n",
    "times = []\n",
    "\n",
    "all_loss = []\n",
    "patience = 0\n",
    "max_patience = 10\n",
    "for epoch in range(100):\n",
    "    start = time.perf_counter()\n",
    "    print('epoch: ', epoch, end='')\n",
    "    all_loss.append(0)\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y, loss_fn)\n",
    "        opt.update(loss)\n",
    "\n",
    "        all_loss[epoch] += loss\n",
    "    end = time.perf_counter()\n",
    "    \n",
    "    all_loss[epoch] /= len(data_loader)\n",
    "\n",
    "    if epoch > 0 and all_loss[epoch-1] <= all_loss[epoch]:\n",
    "        patience -= 1\n",
    "    else:\n",
    "        patience = max_patience\n",
    "        \n",
    "\n",
    "    print(', loss: {}'.format(all_loss[epoch].detach().cpu().numpy().item()))\n",
    "    # print(', time spent {}'.format(end-start))\n",
    "    times.append(end-start)\n",
    "\n",
    "    if patience <= 0:\n",
    "        break\n",
    "\n",
    "print(sum(times)/len(times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.5393333435058594\n",
      "epoch:  1, loss: 0.5313586592674255\n",
      "epoch:  2, loss: 0.5293454527854919\n",
      "epoch:  3, loss: 0.5273312926292419\n",
      "epoch:  4, loss: 0.5253179669380188\n",
      "epoch:  5, loss: 0.5233054757118225\n",
      "epoch:  6, loss: 0.521294116973877\n",
      "epoch:  7, loss: 0.5192835330963135\n",
      "epoch:  8, loss: 0.517274022102356\n",
      "epoch:  9, loss: 0.5152654647827148\n",
      "epoch:  10, loss: 0.5132580399513245\n",
      "epoch:  11, loss: 0.5112513899803162\n",
      "epoch:  12, loss: 0.5092458128929138\n",
      "epoch:  13, loss: 0.5072413086891174\n",
      "epoch:  14, loss: 0.5052377581596375\n",
      "epoch:  15, loss: 0.5032352209091187\n",
      "epoch:  16, loss: 0.5012338161468506\n",
      "epoch:  17, loss: 0.4992333948612213\n",
      "epoch:  18, loss: 0.4972340166568756\n",
      "epoch:  19, loss: 0.49523571133613586\n",
      "epoch:  20, loss: 0.4932384788990021\n",
      "epoch:  21, loss: 0.49124228954315186\n",
      "epoch:  22, loss: 0.48924720287323\n",
      "epoch:  23, loss: 0.48725321888923645\n",
      "epoch:  24, loss: 0.4852602481842041\n",
      "epoch:  25, loss: 0.48326846957206726\n",
      "epoch:  26, loss: 0.48127782344818115\n",
      "epoch:  27, loss: 0.4792881906032562\n",
      "epoch:  28, loss: 0.4772997796535492\n",
      "epoch:  29, loss: 0.4753124415874481\n",
      "epoch:  30, loss: 0.47332632541656494\n",
      "epoch:  31, loss: 0.4713413715362549\n",
      "epoch:  32, loss: 0.4693574905395508\n",
      "epoch:  33, loss: 0.4673748314380646\n",
      "epoch:  34, loss: 0.4653933644294739\n",
      "epoch:  35, loss: 0.4634130895137787\n",
      "epoch:  36, loss: 0.4614340364933014\n",
      "epoch:  37, loss: 0.4594561457633972\n",
      "epoch:  38, loss: 0.4574795663356781\n",
      "epoch:  39, loss: 0.4555042088031769\n",
      "epoch:  40, loss: 0.4535300135612488\n",
      "epoch:  41, loss: 0.45155707001686096\n",
      "epoch:  42, loss: 0.44958552718162537\n",
      "epoch:  43, loss: 0.4476151168346405\n",
      "epoch:  44, loss: 0.4456460475921631\n",
      "epoch:  45, loss: 0.44367825984954834\n",
      "epoch:  46, loss: 0.44171181321144104\n",
      "epoch:  47, loss: 0.439746618270874\n",
      "epoch:  48, loss: 0.43778279423713684\n",
      "epoch:  49, loss: 0.4358202815055847\n",
      "epoch:  50, loss: 0.4338591694831848\n",
      "epoch:  51, loss: 0.4318993389606476\n",
      "epoch:  52, loss: 0.42994093894958496\n",
      "epoch:  53, loss: 0.4279839098453522\n",
      "epoch:  54, loss: 0.4260282516479492\n",
      "epoch:  55, loss: 0.4240740239620209\n",
      "epoch:  56, loss: 0.4221212565898895\n",
      "epoch:  57, loss: 0.420169860124588\n",
      "epoch:  58, loss: 0.4182199239730835\n",
      "epoch:  59, loss: 0.416271448135376\n",
      "epoch:  60, loss: 0.41432446241378784\n",
      "epoch:  61, loss: 0.41237887740135193\n",
      "epoch:  62, loss: 0.4104348421096802\n",
      "epoch:  63, loss: 0.4084922969341278\n",
      "epoch:  64, loss: 0.4065512716770172\n",
      "epoch:  65, loss: 0.40461182594299316\n",
      "epoch:  66, loss: 0.4026739001274109\n",
      "epoch:  67, loss: 0.4007374942302704\n",
      "epoch:  68, loss: 0.39880266785621643\n",
      "epoch:  69, loss: 0.3968695104122162\n",
      "epoch:  70, loss: 0.3949378430843353\n",
      "epoch:  71, loss: 0.3930078446865082\n",
      "epoch:  72, loss: 0.39107951521873474\n",
      "epoch:  73, loss: 0.3891528248786926\n",
      "epoch:  74, loss: 0.38722771406173706\n",
      "epoch:  75, loss: 0.3853043019771576\n",
      "epoch:  76, loss: 0.3833826184272766\n",
      "epoch:  77, loss: 0.3814626634120941\n",
      "epoch:  78, loss: 0.37954431772232056\n",
      "epoch:  79, loss: 0.37762781977653503\n",
      "epoch:  80, loss: 0.3757130205631256\n",
      "epoch:  81, loss: 0.37380000948905945\n",
      "epoch:  82, loss: 0.37188875675201416\n",
      "epoch:  83, loss: 0.3699793517589569\n",
      "epoch:  84, loss: 0.3680717647075653\n",
      "epoch:  85, loss: 0.36616596579551697\n",
      "epoch:  86, loss: 0.36426204442977905\n",
      "epoch:  87, loss: 0.36236000061035156\n",
      "epoch:  88, loss: 0.3604598343372345\n",
      "epoch:  89, loss: 0.35856154561042786\n",
      "epoch:  90, loss: 0.3566652536392212\n",
      "epoch:  91, loss: 0.3547708988189697\n",
      "epoch:  92, loss: 0.3528784513473511\n",
      "epoch:  93, loss: 0.3509880304336548\n",
      "epoch:  94, loss: 0.34909960627555847\n",
      "epoch:  95, loss: 0.34721314907073975\n",
      "epoch:  96, loss: 0.34532877802848816\n",
      "epoch:  97, loss: 0.34344643354415894\n",
      "epoch:  98, loss: 0.3415662348270416\n",
      "epoch:  99, loss: 0.33968809247016907\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size = X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "opt = torch_numopt.LM(model.parameters(), lr=1, mu=0.001, mu_dec=0.1, model=model, use_diagonal=True, c1=1e-4, tau=0.1, line_search_method='backtrack')\n",
    "\n",
    "all_loss = []\n",
    "patience = 0\n",
    "max_patience = 10\n",
    "for epoch in range(100):\n",
    "    print('epoch: ', epoch, end='')\n",
    "    all_loss.append(0)\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y, loss_fn)\n",
    "        opt.update(loss)\n",
    "\n",
    "        all_loss[epoch] += loss\n",
    "    \n",
    "    all_loss[epoch] /= len(data_loader)\n",
    "\n",
    "    if epoch > 0 and all_loss[epoch-1] <= all_loss[epoch]:\n",
    "        patience -= 1\n",
    "    else:\n",
    "        patience = max_patience\n",
    "        \n",
    "\n",
    "    print(', loss: {}'.format(all_loss[epoch].detach().cpu().numpy().item()))\n",
    "\n",
    "    if patience <= 0:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
