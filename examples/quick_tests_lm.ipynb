{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pytorch_soom\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.f1 = nn.Linear(input_size, 10, device=device)\n",
    "        self.f2 = nn.Linear(10, 20, device=device)\n",
    "        self.f3 = nn.Linear(20, 20, device=device)\n",
    "        self.f4 = nn.Linear(20, 10, device=device)\n",
    "        self.f5 = nn.Linear(10, 1, device=device)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        # self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.f1(x))\n",
    "        x = self.activation(self.f2(x))\n",
    "        x = self.activation(self.f3(x))\n",
    "        x = self.activation(self.f4(x))\n",
    "        x = self.f5(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_diabetes(return_X_y = True, scaled=False)\n",
    "\n",
    "X_scaler = MinMaxScaler()\n",
    "X = X_scaler.fit_transform(X)\n",
    "\n",
    "y_scaler = MinMaxScaler()\n",
    "y = y_scaler.fit_transform(y.reshape((-1, 1)))\n",
    "\n",
    "torch_data = TensorDataset(torch.Tensor(X).to(device), torch.Tensor(y).to(device))\n",
    "data_loader = DataLoader(torch_data, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.13202518224716187\n",
      "epoch:  1, loss: 0.11094743758440018\n",
      "epoch:  2, loss: 0.10617230832576752\n",
      "epoch:  3, loss: 0.0832819864153862\n",
      "epoch:  4, loss: 0.07656904309988022\n",
      "epoch:  5, loss: 0.07209958136081696\n",
      "epoch:  6, loss: 0.0697375163435936\n",
      "epoch:  7, loss: 0.06518661975860596\n",
      "epoch:  8, loss: 0.06255731731653214\n",
      "epoch:  9, loss: 0.061671335250139236\n",
      "epoch:  10, loss: 0.061360765248537064\n",
      "epoch:  11, loss: 0.05896953120827675\n",
      "epoch:  12, loss: 0.05733287334442139\n",
      "epoch:  13, loss: 0.05582023039460182\n",
      "epoch:  14, loss: 0.05564546212553978\n",
      "epoch:  15, loss: 0.05479069799184799\n",
      "epoch:  16, loss: 0.053807780146598816\n",
      "epoch:  17, loss: 0.05325029045343399\n",
      "epoch:  18, loss: 0.05275009199976921\n",
      "epoch:  19, loss: 0.05250280350446701\n",
      "epoch:  20, loss: 0.05248160660266876\n",
      "epoch:  21, loss: 0.05153047665953636\n",
      "epoch:  22, loss: 0.051360055804252625\n",
      "epoch:  23, loss: 0.05064285174012184\n",
      "epoch:  24, loss: 0.050195902585983276\n",
      "epoch:  25, loss: 0.04992435500025749\n",
      "epoch:  26, loss: 0.04903459921479225\n",
      "epoch:  27, loss: 0.048256684094667435\n",
      "epoch:  28, loss: 0.04771773889660835\n",
      "epoch:  29, loss: 0.047541599720716476\n",
      "epoch:  30, loss: 0.04751322418451309\n",
      "epoch:  31, loss: 0.04659675061702728\n",
      "epoch:  32, loss: 0.04645465686917305\n",
      "epoch:  33, loss: 0.045679301023483276\n",
      "epoch:  34, loss: 0.04542844742536545\n",
      "epoch:  35, loss: 0.044773515313863754\n",
      "epoch:  36, loss: 0.04442112520337105\n",
      "epoch:  37, loss: 0.0439208447933197\n",
      "epoch:  38, loss: 0.04343562573194504\n",
      "epoch:  39, loss: 0.04301271215081215\n",
      "epoch:  40, loss: 0.04243697226047516\n",
      "epoch:  41, loss: 0.04210243001580238\n",
      "epoch:  42, loss: 0.0414317287504673\n",
      "epoch:  43, loss: 0.04120270907878876\n",
      "epoch:  44, loss: 0.040394075214862823\n",
      "epoch:  45, loss: 0.03895096853375435\n",
      "epoch:  46, loss: 0.03843020275235176\n",
      "epoch:  47, loss: 0.037943992763757706\n",
      "epoch:  48, loss: 0.037607286125421524\n",
      "epoch:  49, loss: 0.03694555535912514\n",
      "epoch:  50, loss: 0.03589901700615883\n",
      "epoch:  51, loss: 0.0353984497487545\n",
      "epoch:  52, loss: 0.03525085002183914\n",
      "epoch:  53, loss: 0.03450912982225418\n",
      "epoch:  54, loss: 0.034030281007289886\n",
      "epoch:  55, loss: 0.03388315066695213\n",
      "epoch:  56, loss: 0.033645424991846085\n",
      "epoch:  57, loss: 0.033210042864084244\n",
      "epoch:  58, loss: 0.032723985612392426\n",
      "epoch:  59, loss: 0.0320417582988739\n",
      "epoch:  60, loss: 0.031934432685375214\n",
      "epoch:  61, loss: 0.03151816874742508\n",
      "epoch:  62, loss: 0.03127841278910637\n",
      "epoch:  63, loss: 0.03108927607536316\n",
      "epoch:  64, loss: 0.030748657882213593\n",
      "epoch:  65, loss: 0.030676886439323425\n",
      "epoch:  66, loss: 0.030331050977110863\n",
      "epoch:  67, loss: 0.030281608924269676\n",
      "epoch:  68, loss: 0.029997404664754868\n",
      "epoch:  69, loss: 0.02992727793753147\n",
      "epoch:  70, loss: 0.029713662341237068\n",
      "epoch:  71, loss: 0.0296151302754879\n",
      "epoch:  72, loss: 0.02945338375866413\n",
      "epoch:  73, loss: 0.02934412658214569\n",
      "epoch:  74, loss: 0.029206667095422745\n",
      "epoch:  75, loss: 0.029106756672263145\n",
      "epoch:  76, loss: 0.028995733708143234\n",
      "epoch:  77, loss: 0.028898485004901886\n",
      "epoch:  78, loss: 0.028892910107970238\n",
      "epoch:  79, loss: 0.028795350342988968\n",
      "epoch:  80, loss: 0.02872195653617382\n",
      "epoch:  81, loss: 0.028718525543808937\n",
      "epoch:  82, loss: 0.028659123927354813\n",
      "epoch:  83, loss: 0.028656138107180595\n",
      "epoch:  84, loss: 0.02859537862241268\n",
      "epoch:  85, loss: 0.02851266972720623\n",
      "epoch:  86, loss: 0.028466511517763138\n",
      "epoch:  87, loss: 0.028449585661292076\n",
      "epoch:  88, loss: 0.028411438688635826\n",
      "epoch:  89, loss: 0.028403563424944878\n",
      "epoch:  90, loss: 0.028366388753056526\n",
      "epoch:  91, loss: 0.02835867926478386\n",
      "epoch:  92, loss: 0.028326524421572685\n",
      "epoch:  93, loss: 0.028314342722296715\n",
      "epoch:  94, loss: 0.02828872948884964\n",
      "epoch:  95, loss: 0.028272228315472603\n",
      "epoch:  96, loss: 0.028252191841602325\n",
      "epoch:  97, loss: 0.028234247118234634\n",
      "epoch:  98, loss: 0.028218921273946762\n",
      "epoch:  99, loss: 0.028195854276418686\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size = X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "opt = pytorch_soom.LM(model.parameters(), lr=1, mu=0.001, mu_dec=0.1, model=model, use_diagonal=False, c1=1e-4, tau=0.1, line_search_method='backtrack')\n",
    "\n",
    "all_loss = []\n",
    "patience = 0\n",
    "max_patience = 10\n",
    "for epoch in range(100):\n",
    "    print('epoch: ', epoch, end='')\n",
    "    all_loss.append(0)\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y, loss_fn)\n",
    "        # opt.update(loss)\n",
    "\n",
    "        all_loss[epoch] += loss\n",
    "    \n",
    "    all_loss[epoch] /= len(data_loader)\n",
    "\n",
    "    if epoch > 0 and all_loss[epoch-1] <= all_loss[epoch]:\n",
    "        patience -= 1\n",
    "    else:\n",
    "        patience = max_patience\n",
    "        \n",
    "\n",
    "    print(', loss: {}'.format(all_loss[epoch].detach().cpu().numpy().item()))\n",
    "\n",
    "    if patience <= 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.28115880489349365\n",
      "epoch:  1, loss: 0.20888052880764008\n",
      "epoch:  2, loss: 0.17400911450386047\n",
      "epoch:  3, loss: 0.17035219073295593\n",
      "epoch:  4, loss: 0.1455872505903244\n",
      "epoch:  5, loss: 0.12844522297382355\n",
      "epoch:  6, loss: 0.12219702452421188\n",
      "epoch:  7, loss: 0.10740651190280914\n",
      "epoch:  8, loss: 0.09864925593137741\n",
      "epoch:  9, loss: 0.09577640891075134\n",
      "epoch:  10, loss: 0.09394504129886627\n",
      "epoch:  11, loss: 0.08519385010004044\n",
      "epoch:  12, loss: 0.07421094924211502\n",
      "epoch:  13, loss: 0.06746806204319\n",
      "epoch:  14, loss: 0.06649968773126602\n",
      "epoch:  15, loss: 0.06016349792480469\n",
      "epoch:  16, loss: 0.05801587551832199\n",
      "epoch:  17, loss: 0.05683813616633415\n",
      "epoch:  18, loss: 0.05558540299534798\n",
      "epoch:  19, loss: 0.052858076989650726\n",
      "epoch:  20, loss: 0.05063335970044136\n",
      "epoch:  21, loss: 0.04964904859662056\n",
      "epoch:  22, loss: 0.048448000103235245\n",
      "epoch:  23, loss: 0.04830684885382652\n",
      "epoch:  24, loss: 0.04705002158880234\n",
      "epoch:  25, loss: 0.04650012403726578\n",
      "epoch:  26, loss: 0.045735884457826614\n",
      "epoch:  27, loss: 0.04497663676738739\n",
      "epoch:  28, loss: 0.04478783160448074\n",
      "epoch:  29, loss: 0.0443582758307457\n",
      "epoch:  30, loss: 0.04426667466759682\n",
      "epoch:  31, loss: 0.04412205144762993\n",
      "epoch:  32, loss: 0.04384727030992508\n",
      "epoch:  33, loss: 0.04381294921040535\n",
      "epoch:  34, loss: 0.04372727498412132\n",
      "epoch:  35, loss: 0.04362551495432854\n",
      "epoch:  36, loss: 0.04329619184136391\n",
      "epoch:  37, loss: 0.04305864870548248\n",
      "epoch:  38, loss: 0.042758285999298096\n",
      "epoch:  39, loss: 0.04248705133795738\n",
      "epoch:  40, loss: 0.042323559522628784\n",
      "epoch:  41, loss: 0.0421234667301178\n",
      "epoch:  42, loss: 0.04198016598820686\n",
      "epoch:  43, loss: 0.04173901677131653\n",
      "epoch:  44, loss: 0.041724793612957\n",
      "epoch:  45, loss: 0.04155104234814644\n",
      "epoch:  46, loss: 0.041459646075963974\n",
      "epoch:  47, loss: 0.04128040000796318\n",
      "epoch:  48, loss: 0.041224170476198196\n",
      "epoch:  49, loss: 0.04095736891031265\n",
      "epoch:  50, loss: 0.04086753726005554\n",
      "epoch:  51, loss: 0.04071304574608803\n",
      "epoch:  52, loss: 0.040636807680130005\n",
      "epoch:  53, loss: 0.040452197194099426\n",
      "epoch:  54, loss: 0.040225639939308167\n",
      "epoch:  55, loss: 0.04009948670864105\n",
      "epoch:  56, loss: 0.039885301142930984\n",
      "epoch:  57, loss: 0.039744872599840164\n",
      "epoch:  58, loss: 0.03956422582268715\n",
      "epoch:  59, loss: 0.03950843960046768\n",
      "epoch:  60, loss: 0.0392964668571949\n",
      "epoch:  61, loss: 0.039125896990299225\n",
      "epoch:  62, loss: 0.03896673023700714\n",
      "epoch:  63, loss: 0.0387846864759922\n",
      "epoch:  64, loss: 0.038651950657367706\n",
      "epoch:  65, loss: 0.038602083921432495\n",
      "epoch:  66, loss: 0.038383599370718\n",
      "epoch:  67, loss: 0.03826884180307388\n",
      "epoch:  68, loss: 0.038267575204372406\n",
      "epoch:  69, loss: 0.038144759833812714\n",
      "epoch:  70, loss: 0.03806430473923683\n",
      "epoch:  71, loss: 0.03798067942261696\n",
      "epoch:  72, loss: 0.037815775722265244\n",
      "epoch:  73, loss: 0.03757227212190628\n",
      "epoch:  74, loss: 0.03748779743909836\n",
      "epoch:  75, loss: 0.03744278848171234\n",
      "epoch:  76, loss: 0.03733178228139877\n",
      "epoch:  77, loss: 0.03717346116900444\n",
      "epoch:  78, loss: 0.037111181765794754\n",
      "epoch:  79, loss: 0.036892663687467575\n",
      "epoch:  80, loss: 0.03675452619791031\n",
      "epoch:  81, loss: 0.03668542951345444\n",
      "epoch:  82, loss: 0.03662201017141342\n",
      "epoch:  83, loss: 0.036537617444992065\n",
      "epoch:  84, loss: 0.036313220858573914\n",
      "epoch:  85, loss: 0.036116838455200195\n",
      "epoch:  86, loss: 0.0359811969101429\n",
      "epoch:  87, loss: 0.03588943928480148\n",
      "epoch:  88, loss: 0.03585882484912872\n",
      "epoch:  89, loss: 0.03582904860377312\n",
      "epoch:  90, loss: 0.035685986280441284\n",
      "epoch:  91, loss: 0.03562263399362564\n",
      "epoch:  92, loss: 0.03556849807500839\n",
      "epoch:  93, loss: 0.03532249107956886\n",
      "epoch:  94, loss: 0.03522321209311485\n",
      "epoch:  95, loss: 0.03518957272171974\n",
      "epoch:  96, loss: 0.035004448145627975\n",
      "epoch:  97, loss: 0.03477560728788376\n",
      "epoch:  98, loss: 0.03464687615633011\n",
      "epoch:  99, loss: 0.03450680896639824\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size = X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "opt = pytorch_soom.LM(model.parameters(), lr=1, mu=0.001, mu_dec=0.1, model=model, use_diagonal=True, c1=1e-4, tau=0.1, line_search_method='backtrack')\n",
    "\n",
    "all_loss = []\n",
    "patience = 0\n",
    "max_patience = 10\n",
    "for epoch in range(100):\n",
    "    print('epoch: ', epoch, end='')\n",
    "    all_loss.append(0)\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y, loss_fn)\n",
    "        # opt.update(loss)\n",
    "\n",
    "        all_loss[epoch] += loss\n",
    "    \n",
    "    all_loss[epoch] /= len(data_loader)\n",
    "\n",
    "    if epoch > 0 and all_loss[epoch-1] <= all_loss[epoch]:\n",
    "        patience -= 1\n",
    "    else:\n",
    "        patience = max_patience\n",
    "        \n",
    "\n",
    "    print(', loss: {}'.format(all_loss[epoch].detach().cpu().numpy().item()))\n",
    "\n",
    "    if patience <= 0:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
