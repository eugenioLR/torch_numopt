{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pytorch_soom\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.f1 = nn.Linear(1, 10, device=device)\n",
    "        self.f2 = nn.Linear(10, 20, device=device)\n",
    "        self.f3 = nn.Linear(20, 20, device=device)\n",
    "        self.f4 = nn.Linear(20, 10, device=device)\n",
    "        self.f5 = nn.Linear(10, 1, device=device)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        # self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.f1(x))\n",
    "        x = self.activation(self.f2(x))\n",
    "        x = self.activation(self.f3(x))\n",
    "        x = self.activation(self.f4(x))\n",
    "        x = self.f5(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.uniform(0, 1, size=(1000, 1))\n",
    "# y = X[:, 0] - X[:, 1]**2 + 2 * X[:, 2] * X[:, 3] + (1 / ((1 + X[:, 4]) ** 6))\n",
    "y = np.sinc(X).sum(axis=1, keepdims=True)\n",
    "\n",
    "torch_data = TensorDataset(torch.Tensor(X).to(device), torch.Tensor(y).to(device))\n",
    "data_loader = DataLoader(torch_data, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.6124632954597473\n",
      "epoch:  1, loss: 0.5835167765617371\n",
      "epoch:  2, loss: 0.5563561320304871\n",
      "epoch:  3, loss: 0.5308499932289124\n",
      "epoch:  4, loss: 0.5068796277046204\n",
      "epoch:  5, loss: 0.4843372702598572\n",
      "epoch:  6, loss: 0.4631248414516449\n",
      "epoch:  7, loss: 0.44315293431282043\n",
      "epoch:  8, loss: 0.42433974146842957\n",
      "epoch:  9, loss: 0.4066101312637329\n",
      "epoch:  10, loss: 0.3898950219154358\n",
      "epoch:  11, loss: 0.3741307556629181\n",
      "epoch:  12, loss: 0.3592585623264313\n",
      "epoch:  13, loss: 0.345223993062973\n",
      "epoch:  14, loss: 0.33197659254074097\n",
      "epoch:  15, loss: 0.31946951150894165\n",
      "epoch:  16, loss: 0.3076590299606323\n",
      "epoch:  17, loss: 0.2965046465396881\n",
      "epoch:  18, loss: 0.2859683334827423\n",
      "epoch:  19, loss: 0.27601465582847595\n",
      "epoch:  20, loss: 0.2666104733943939\n",
      "epoch:  21, loss: 0.25772473216056824\n",
      "epoch:  22, loss: 0.24932830035686493\n",
      "epoch:  23, loss: 0.2413937896490097\n",
      "epoch:  24, loss: 0.23389555513858795\n",
      "epoch:  25, loss: 0.22680939733982086\n",
      "epoch:  26, loss: 0.22011259198188782\n",
      "epoch:  27, loss: 0.21378374099731445\n",
      "epoch:  28, loss: 0.2078026831150055\n",
      "epoch:  29, loss: 0.2021504044532776\n",
      "epoch:  30, loss: 0.19680890440940857\n",
      "epoch:  31, loss: 0.1917613446712494\n",
      "epoch:  32, loss: 0.18699076771736145\n",
      "epoch:  33, loss: 0.18243812024593353\n",
      "epoch:  34, loss: 0.1780644655227661\n",
      "epoch:  35, loss: 0.17392896115779877\n",
      "epoch:  36, loss: 0.17005780339241028\n",
      "epoch:  37, loss: 0.16643153131008148\n",
      "epoch:  38, loss: 0.16303296387195587\n",
      "epoch:  39, loss: 0.15984287858009338\n",
      "epoch:  40, loss: 0.15684445202350616\n",
      "epoch:  41, loss: 0.15402954816818237\n",
      "epoch:  42, loss: 0.15138526260852814\n",
      "epoch:  43, loss: 0.1488926112651825\n",
      "epoch:  44, loss: 0.14654523134231567\n",
      "epoch:  45, loss: 0.14432892203330994\n",
      "epoch:  46, loss: 0.1422319859266281\n",
      "epoch:  47, loss: 0.14024631679058075\n",
      "epoch:  48, loss: 0.13836634159088135\n",
      "epoch:  49, loss: 0.13658715784549713\n",
      "epoch:  50, loss: 0.13490353524684906\n",
      "epoch:  51, loss: 0.1333112120628357\n",
      "epoch:  52, loss: 0.13180305063724518\n",
      "epoch:  53, loss: 0.13037554919719696\n",
      "epoch:  54, loss: 0.1290241777896881\n",
      "epoch:  55, loss: 0.12774428725242615\n",
      "epoch:  56, loss: 0.12653158605098724\n",
      "epoch:  57, loss: 0.1253831833600998\n",
      "epoch:  58, loss: 0.12429594993591309\n",
      "epoch:  59, loss: 0.12326669692993164\n",
      "epoch:  60, loss: 0.12229228019714355\n",
      "epoch:  61, loss: 0.12136980146169662\n",
      "epoch:  62, loss: 0.12049590051174164\n",
      "epoch:  63, loss: 0.11966820061206818\n",
      "epoch:  64, loss: 0.11888481676578522\n",
      "epoch:  65, loss: 0.11814295500516891\n",
      "epoch:  66, loss: 0.11744046211242676\n",
      "epoch:  67, loss: 0.1167752593755722\n",
      "epoch:  68, loss: 0.11614536494016647\n",
      "epoch:  69, loss: 0.11554915457963943\n",
      "epoch:  70, loss: 0.1149846687912941\n",
      "epoch:  71, loss: 0.11445016413927078\n",
      "epoch:  72, loss: 0.1139441579580307\n",
      "epoch:  73, loss: 0.11346500366926193\n",
      "epoch:  74, loss: 0.11301127076148987\n",
      "epoch:  75, loss: 0.11258155107498169\n",
      "epoch:  76, loss: 0.11217465251684189\n",
      "epoch:  77, loss: 0.1117895171046257\n",
      "epoch:  78, loss: 0.11142485588788986\n",
      "epoch:  79, loss: 0.11107964813709259\n",
      "epoch:  80, loss: 0.11075282841920853\n",
      "epoch:  81, loss: 0.11044338345527649\n",
      "epoch:  82, loss: 0.11015038192272186\n",
      "epoch:  83, loss: 0.10987293720245361\n",
      "epoch:  84, loss: 0.10961020737886429\n",
      "epoch:  85, loss: 0.10936146974563599\n",
      "epoch:  86, loss: 0.10912590473890305\n",
      "epoch:  87, loss: 0.10890283435583115\n",
      "epoch:  88, loss: 0.10869159549474716\n",
      "epoch:  89, loss: 0.10849157720804214\n",
      "epoch:  90, loss: 0.10830216109752655\n",
      "epoch:  91, loss: 0.10812276601791382\n",
      "epoch:  92, loss: 0.10795285552740097\n",
      "epoch:  93, loss: 0.10779189318418503\n",
      "epoch:  94, loss: 0.10763940960168839\n",
      "epoch:  95, loss: 0.10749495774507523\n",
      "epoch:  96, loss: 0.10735808312892914\n",
      "epoch:  97, loss: 0.10722840577363968\n",
      "epoch:  98, loss: 0.10710549354553223\n",
      "epoch:  99, loss: 0.10698901861906052\n"
     ]
    }
   ],
   "source": [
    "model = Net(device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "opt = optim.SGD(model.parameters(), lr = 1e-2)\n",
    "\n",
    "all_loss = {}\n",
    "for epoch in range(100):\n",
    "    print('epoch: ', epoch, end='')\n",
    "    all_loss[epoch+1] = 0\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step()\n",
    "\n",
    "        all_loss[epoch+1] += loss\n",
    "    all_loss[epoch+1] /= len(data_loader)\n",
    "    print(', loss: {}'.format(all_loss[epoch+1].detach().cpu().numpy().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.21514053642749786\n",
      "epoch:  1, loss: 0.17482998967170715\n",
      "epoch:  2, loss: 0.14475904405117035\n",
      "epoch:  3, loss: 0.12503978610038757\n",
      "epoch:  4, loss: 0.11315830051898956\n",
      "epoch:  5, loss: 0.11294684559106827\n",
      "epoch:  6, loss: 0.11638039350509644\n",
      "epoch:  7, loss: 0.11348729580640793\n",
      "epoch:  8, loss: 0.10554010421037674\n",
      "epoch:  9, loss: 0.097958043217659\n",
      "epoch:  10, loss: 0.09447843581438065\n",
      "epoch:  11, loss: 0.09197928011417389\n",
      "epoch:  12, loss: 0.09057636559009552\n",
      "epoch:  13, loss: 0.08858001977205276\n",
      "epoch:  14, loss: 0.08416341245174408\n",
      "epoch:  15, loss: 0.07735150307416916\n",
      "epoch:  16, loss: 0.07017824798822403\n",
      "epoch:  17, loss: 0.06418316811323166\n",
      "epoch:  18, loss: 0.05908946692943573\n",
      "epoch:  19, loss: 0.053061045706272125\n",
      "epoch:  20, loss: 0.04469316452741623\n",
      "epoch:  21, loss: 0.035440560430288315\n",
      "epoch:  22, loss: 0.02774355188012123\n",
      "epoch:  23, loss: 0.022123951464891434\n",
      "epoch:  24, loss: 0.016693610697984695\n",
      "epoch:  25, loss: 0.011359634809195995\n",
      "epoch:  26, loss: 0.007849341258406639\n",
      "epoch:  27, loss: 0.00781260896474123\n",
      "epoch:  28, loss: 0.009455718100070953\n",
      "epoch:  29, loss: 0.010858436115086079\n",
      "epoch:  30, loss: 0.01258802879601717\n",
      "epoch:  31, loss: 0.013617484830319881\n",
      "epoch:  32, loss: 0.01315467618405819\n",
      "epoch:  33, loss: 0.012001188471913338\n",
      "epoch:  34, loss: 0.010469552129507065\n",
      "epoch:  35, loss: 0.008236339315772057\n",
      "epoch:  36, loss: 0.006039605475962162\n",
      "epoch:  37, loss: 0.004550147335976362\n",
      "epoch:  38, loss: 0.003625233191996813\n",
      "epoch:  39, loss: 0.003440657863393426\n",
      "epoch:  40, loss: 0.00400708569213748\n",
      "epoch:  41, loss: 0.004571443889290094\n",
      "epoch:  42, loss: 0.0049383630976080894\n",
      "epoch:  43, loss: 0.0049916161224246025\n",
      "epoch:  44, loss: 0.004379284102469683\n",
      "epoch:  45, loss: 0.0035011013969779015\n",
      "epoch:  46, loss: 0.0025869242381304502\n",
      "epoch:  47, loss: 0.0016772974049672484\n",
      "epoch:  48, loss: 0.0011954422807320952\n",
      "epoch:  49, loss: 0.001043407479301095\n",
      "epoch:  50, loss: 0.0011475702049210668\n",
      "epoch:  51, loss: 0.001482266467064619\n",
      "epoch:  52, loss: 0.001674778526648879\n",
      "epoch:  53, loss: 0.0017366455867886543\n",
      "epoch:  54, loss: 0.0015232571167871356\n",
      "epoch:  55, loss: 0.0011362178483977914\n",
      "epoch:  56, loss: 0.0007611053297296166\n",
      "epoch:  57, loss: 0.00044044991955161095\n",
      "epoch:  58, loss: 0.00036099180579185486\n",
      "epoch:  59, loss: 0.00040670461021363735\n",
      "epoch:  60, loss: 0.000577602069824934\n",
      "epoch:  61, loss: 0.000701352022588253\n",
      "epoch:  62, loss: 0.0007406664080917835\n",
      "epoch:  63, loss: 0.0006609367555938661\n",
      "epoch:  64, loss: 0.0004955953918397427\n",
      "epoch:  65, loss: 0.0003379580448381603\n",
      "epoch:  66, loss: 0.00022427238582167774\n",
      "epoch:  67, loss: 0.0002170911175198853\n",
      "epoch:  68, loss: 0.0002602181339170784\n",
      "epoch:  69, loss: 0.00034256427898071706\n",
      "epoch:  70, loss: 0.0003755773650482297\n",
      "epoch:  71, loss: 0.00036521704168990254\n",
      "epoch:  72, loss: 0.000292119657387957\n",
      "epoch:  73, loss: 0.00021721432858612388\n",
      "epoch:  74, loss: 0.00015342952974606305\n",
      "epoch:  75, loss: 0.0001395585568388924\n",
      "epoch:  76, loss: 0.0001526963897049427\n",
      "epoch:  77, loss: 0.0001846346858656034\n",
      "epoch:  78, loss: 0.0001988040457945317\n",
      "epoch:  79, loss: 0.0001914562308229506\n",
      "epoch:  80, loss: 0.00015768686716910452\n",
      "epoch:  81, loss: 0.00011770578566938639\n",
      "epoch:  82, loss: 8.504002471454442e-05\n",
      "epoch:  83, loss: 7.050924614304677e-05\n",
      "epoch:  84, loss: 7.391363033093512e-05\n",
      "epoch:  85, loss: 8.22844376671128e-05\n",
      "epoch:  86, loss: 8.83859975147061e-05\n",
      "epoch:  87, loss: 7.978195208124816e-05\n",
      "epoch:  88, loss: 6.523038609884679e-05\n",
      "epoch:  89, loss: 4.568242729874328e-05\n",
      "epoch:  90, loss: 3.5079345252597705e-05\n",
      "epoch:  91, loss: 3.209195347153582e-05\n",
      "epoch:  92, loss: 3.767459566006437e-05\n",
      "epoch:  93, loss: 4.416310184751637e-05\n",
      "epoch:  94, loss: 4.634978176909499e-05\n",
      "epoch:  95, loss: 4.339282531873323e-05\n",
      "epoch:  96, loss: 3.551836562110111e-05\n",
      "epoch:  97, loss: 2.9373781217145734e-05\n",
      "epoch:  98, loss: 2.5612192985136062e-05\n",
      "epoch:  99, loss: 2.714083166210912e-05\n"
     ]
    }
   ],
   "source": [
    "model = Net(device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "opt = optim.Adam(model.parameters(), lr = 1e-2)\n",
    "\n",
    "all_loss = {}\n",
    "for epoch in range(100):\n",
    "    print('epoch: ', epoch, end='')\n",
    "    all_loss[epoch+1] = 0\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step()\n",
    "\n",
    "        all_loss[epoch+1] += loss\n",
    "    all_loss[epoch+1] /= len(data_loader)\n",
    "    print(', loss: {}'.format(all_loss[epoch+1].detach().cpu().numpy().item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
