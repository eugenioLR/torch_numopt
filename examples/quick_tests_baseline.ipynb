{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch_numopt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import *\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.f1 = nn.Linear(input_size, 10, device=device)\n",
    "        self.f2 = nn.Linear(10, 20, device=device)\n",
    "        self.f3 = nn.Linear(20, 20, device=device)\n",
    "        self.f4 = nn.Linear(20, 10, device=device)\n",
    "        self.f5 = nn.Linear(10, 1, device=device)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        # self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.f1(x))\n",
    "        x = self.activation(self.f2(x))\n",
    "        x = self.activation(self.f3(x))\n",
    "        x = self.activation(self.f4(x))\n",
    "        x = self.f5(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([800, 10]) torch.Size([800, 1])\n",
      "torch.Size([200, 10]) torch.Size([200, 1])\n"
     ]
    }
   ],
   "source": [
    "# X, y = load_diabetes(return_X_y=True, scaled=False)\n",
    "# X, y = make_regression(n_samples=1000, n_features=100)\n",
    "X, y = make_friedman1(n_samples=1000, noise=1e-2)\n",
    "\n",
    "X_scaler = MinMaxScaler()\n",
    "X = X_scaler.fit_transform(X)\n",
    "X = torch.Tensor(X).to(device)\n",
    "\n",
    "y_scaler = MinMaxScaler()\n",
    "y = y_scaler.fit_transform(y.reshape((-1, 1)))\n",
    "y = torch.Tensor(y).to(device)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "torch_data = TensorDataset(X_train, y_train)\n",
    "data_loader = DataLoader(torch_data, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.06713506579399109\n",
      "epoch:  1, loss: 0.06430155783891678\n",
      "epoch:  2, loss: 0.06176654249429703\n",
      "epoch:  3, loss: 0.05949873477220535\n",
      "epoch:  4, loss: 0.05747276172041893\n",
      "epoch:  5, loss: 0.05569075420498848\n",
      "epoch:  6, loss: 0.05414021387696266\n",
      "epoch:  7, loss: 0.05276952683925629\n",
      "epoch:  8, loss: 0.05154668167233467\n",
      "epoch:  9, loss: 0.0504477396607399\n",
      "epoch:  10, loss: 0.049459800124168396\n",
      "epoch:  11, loss: 0.04856991767883301\n",
      "epoch:  12, loss: 0.047768134623765945\n",
      "epoch:  13, loss: 0.04704546183347702\n",
      "epoch:  14, loss: 0.046393983066082\n",
      "epoch:  15, loss: 0.04580668359994888\n",
      "epoch:  16, loss: 0.045277222990989685\n",
      "epoch:  17, loss: 0.04479983076453209\n",
      "epoch:  18, loss: 0.04436938092112541\n",
      "epoch:  19, loss: 0.04398125037550926\n",
      "epoch:  20, loss: 0.043631263077259064\n",
      "epoch:  21, loss: 0.04331570491194725\n",
      "epoch:  22, loss: 0.04303119704127312\n",
      "epoch:  23, loss: 0.04277460649609566\n",
      "epoch:  24, loss: 0.0425431951880455\n",
      "epoch:  25, loss: 0.04233445227146149\n",
      "epoch:  26, loss: 0.04214615747332573\n",
      "epoch:  27, loss: 0.041976261883974075\n",
      "epoch:  28, loss: 0.041823070496320724\n",
      "epoch:  29, loss: 0.041684720665216446\n",
      "epoch:  30, loss: 0.04155983030796051\n",
      "epoch:  31, loss: 0.04144703596830368\n",
      "epoch:  32, loss: 0.04134520888328552\n",
      "epoch:  33, loss: 0.041253332048654556\n",
      "epoch:  34, loss: 0.041170310229063034\n",
      "epoch:  35, loss: 0.04109501838684082\n",
      "epoch:  36, loss: 0.041026704013347626\n",
      "epoch:  37, loss: 0.04096479341387749\n",
      "epoch:  38, loss: 0.04090861603617668\n",
      "epoch:  39, loss: 0.04085804894566536\n",
      "epoch:  40, loss: 0.04081219434738159\n",
      "epoch:  41, loss: 0.04077061638236046\n",
      "epoch:  42, loss: 0.04073299467563629\n",
      "epoch:  43, loss: 0.04069898650050163\n",
      "epoch:  44, loss: 0.04066808521747589\n",
      "epoch:  45, loss: 0.040640078485012054\n",
      "epoch:  46, loss: 0.040614739060401917\n",
      "epoch:  47, loss: 0.040591850876808167\n",
      "epoch:  48, loss: 0.04057113453745842\n",
      "epoch:  49, loss: 0.04055243358016014\n",
      "epoch:  50, loss: 0.04053545743227005\n",
      "epoch:  51, loss: 0.04052005708217621\n",
      "epoch:  52, loss: 0.04050604626536369\n",
      "epoch:  53, loss: 0.04049323871731758\n",
      "epoch:  54, loss: 0.040481582283973694\n",
      "epoch:  55, loss: 0.04047081246972084\n",
      "epoch:  56, loss: 0.0404609777033329\n",
      "epoch:  57, loss: 0.040452029556035995\n",
      "epoch:  58, loss: 0.04044385999441147\n",
      "epoch:  59, loss: 0.04043637216091156\n",
      "epoch:  60, loss: 0.040429458022117615\n",
      "epoch:  61, loss: 0.0404231920838356\n",
      "epoch:  62, loss: 0.04041748121380806\n",
      "epoch:  63, loss: 0.04041225463151932\n",
      "epoch:  64, loss: 0.040407467633485794\n",
      "epoch:  65, loss: 0.04040304943919182\n",
      "epoch:  66, loss: 0.04039900377392769\n",
      "epoch:  67, loss: 0.04039527848362923\n",
      "epoch:  68, loss: 0.04039182513952255\n",
      "epoch:  69, loss: 0.04038864001631737\n",
      "epoch:  70, loss: 0.04038567841053009\n",
      "epoch:  71, loss: 0.04038294404745102\n",
      "epoch:  72, loss: 0.040380384773015976\n",
      "epoch:  73, loss: 0.040378015488386154\n",
      "epoch:  74, loss: 0.04037578031420708\n",
      "epoch:  75, loss: 0.04037371650338173\n",
      "epoch:  76, loss: 0.04037179425358772\n",
      "epoch:  77, loss: 0.040369994938373566\n",
      "epoch:  78, loss: 0.040368299931287766\n",
      "epoch:  79, loss: 0.040366705507040024\n",
      "epoch:  80, loss: 0.040365200489759445\n",
      "epoch:  81, loss: 0.04036378860473633\n",
      "epoch:  82, loss: 0.04036245122551918\n",
      "epoch:  83, loss: 0.040361203253269196\n",
      "epoch:  84, loss: 0.04036002233624458\n",
      "epoch:  85, loss: 0.040358882397413254\n",
      "epoch:  86, loss: 0.040357790887355804\n",
      "epoch:  87, loss: 0.040356721729040146\n",
      "epoch:  88, loss: 0.040355682373046875\n",
      "epoch:  89, loss: 0.04035466909408569\n",
      "epoch:  90, loss: 0.0403536893427372\n",
      "epoch:  91, loss: 0.040352724492549896\n",
      "epoch:  92, loss: 0.04035179689526558\n",
      "epoch:  93, loss: 0.04035087674856186\n",
      "epoch:  94, loss: 0.04034997522830963\n",
      "epoch:  95, loss: 0.040349092334508896\n",
      "epoch:  96, loss: 0.040348224341869354\n",
      "epoch:  97, loss: 0.040347374975681305\n",
      "epoch:  98, loss: 0.04034653678536415\n",
      "epoch:  99, loss: 0.04034570977091789\n",
      "epoch:  100, loss: 0.04034489393234253\n",
      "epoch:  101, loss: 0.04034408926963806\n",
      "epoch:  102, loss: 0.04034329205751419\n",
      "epoch:  103, loss: 0.040342509746551514\n",
      "epoch:  104, loss: 0.04034171998500824\n",
      "epoch:  105, loss: 0.04034093767404556\n",
      "epoch:  106, loss: 0.040340155363082886\n",
      "epoch:  107, loss: 0.04033936932682991\n",
      "epoch:  108, loss: 0.04033857211470604\n",
      "epoch:  109, loss: 0.04033777117729187\n",
      "epoch:  110, loss: 0.040336981415748596\n",
      "epoch:  111, loss: 0.04033619910478592\n",
      "epoch:  112, loss: 0.04033540561795235\n",
      "epoch:  113, loss: 0.04033462330698967\n",
      "epoch:  114, loss: 0.04033385589718819\n",
      "epoch:  115, loss: 0.0403330996632576\n",
      "epoch:  116, loss: 0.04033234342932701\n",
      "epoch:  117, loss: 0.04033157229423523\n",
      "epoch:  118, loss: 0.04033080115914345\n",
      "epoch:  119, loss: 0.04033001884818077\n",
      "epoch:  120, loss: 0.040329255163669586\n",
      "epoch:  121, loss: 0.04032852128148079\n",
      "epoch:  122, loss: 0.04032779112458229\n",
      "epoch:  123, loss: 0.0403270460665226\n",
      "epoch:  124, loss: 0.0403263121843338\n",
      "epoch:  125, loss: 0.040325604379177094\n",
      "epoch:  126, loss: 0.040324896574020386\n",
      "epoch:  127, loss: 0.04032420367002487\n",
      "epoch:  128, loss: 0.04032352194190025\n",
      "epoch:  129, loss: 0.040322836488485336\n",
      "epoch:  130, loss: 0.04032215103507042\n",
      "epoch:  131, loss: 0.040321458131074905\n",
      "epoch:  132, loss: 0.04032076150178909\n",
      "epoch:  133, loss: 0.040320053696632385\n",
      "epoch:  134, loss: 0.04031934589147568\n",
      "epoch:  135, loss: 0.04031864181160927\n",
      "epoch:  136, loss: 0.040317945182323456\n",
      "epoch:  137, loss: 0.04031723365187645\n",
      "epoch:  138, loss: 0.04031653329730034\n",
      "epoch:  139, loss: 0.04031582921743393\n",
      "epoch:  140, loss: 0.04031513258814812\n",
      "epoch:  141, loss: 0.040314435958862305\n",
      "epoch:  142, loss: 0.04031374305486679\n",
      "epoch:  143, loss: 0.04031306132674217\n",
      "epoch:  144, loss: 0.040312375873327255\n",
      "epoch:  145, loss: 0.040311697870492935\n",
      "epoch:  146, loss: 0.04031102731823921\n",
      "epoch:  147, loss: 0.04031035676598549\n",
      "epoch:  148, loss: 0.04030971601605415\n",
      "epoch:  149, loss: 0.040309078991413116\n",
      "epoch:  150, loss: 0.04030844196677208\n",
      "epoch:  151, loss: 0.040307797491550446\n",
      "epoch:  152, loss: 0.04030715301632881\n",
      "epoch:  153, loss: 0.04030649736523628\n",
      "epoch:  154, loss: 0.04030584171414375\n",
      "epoch:  155, loss: 0.040305186063051224\n",
      "epoch:  156, loss: 0.04030454158782959\n",
      "epoch:  157, loss: 0.04030388593673706\n",
      "epoch:  158, loss: 0.040303248912096024\n",
      "epoch:  159, loss: 0.04030262678861618\n",
      "epoch:  160, loss: 0.040302008390426636\n",
      "epoch:  161, loss: 0.04030139744281769\n",
      "epoch:  162, loss: 0.04030077904462814\n",
      "epoch:  163, loss: 0.0403001494705677\n",
      "epoch:  164, loss: 0.04029952362179756\n",
      "epoch:  165, loss: 0.04029889032244682\n",
      "epoch:  166, loss: 0.04029826447367668\n",
      "epoch:  167, loss: 0.04029764235019684\n",
      "epoch:  168, loss: 0.040297020226716995\n",
      "epoch:  169, loss: 0.04029640182852745\n",
      "epoch:  170, loss: 0.04029577970504761\n",
      "epoch:  171, loss: 0.04029515013098717\n",
      "epoch:  172, loss: 0.04029451310634613\n",
      "epoch:  173, loss: 0.04029388353228569\n",
      "epoch:  174, loss: 0.040293242782354355\n",
      "epoch:  175, loss: 0.04029260575771332\n",
      "epoch:  176, loss: 0.04029196500778198\n",
      "epoch:  177, loss: 0.04029133543372154\n",
      "epoch:  178, loss: 0.040290717035532\n",
      "epoch:  179, loss: 0.04029010236263275\n",
      "epoch:  180, loss: 0.04028948396444321\n",
      "epoch:  181, loss: 0.04028886556625366\n",
      "epoch:  182, loss: 0.04028824716806412\n",
      "epoch:  183, loss: 0.04028762876987457\n",
      "epoch:  184, loss: 0.04028702154755592\n",
      "epoch:  185, loss: 0.04028642550110817\n",
      "epoch:  186, loss: 0.04028584063053131\n",
      "epoch:  187, loss: 0.04028526321053505\n",
      "epoch:  188, loss: 0.04028468579053879\n",
      "epoch:  189, loss: 0.040284108370542526\n",
      "epoch:  190, loss: 0.040283527225255966\n",
      "epoch:  191, loss: 0.0402829647064209\n",
      "epoch:  192, loss: 0.04028240218758583\n",
      "epoch:  193, loss: 0.04028183966875076\n",
      "epoch:  194, loss: 0.040281280875205994\n",
      "epoch:  195, loss: 0.040280718356370926\n",
      "epoch:  196, loss: 0.04028015211224556\n",
      "epoch:  197, loss: 0.0402795635163784\n",
      "epoch:  198, loss: 0.04027898609638214\n",
      "epoch:  199, loss: 0.04027840495109558\n",
      "epoch:  200, loss: 0.04027782380580902\n",
      "epoch:  201, loss: 0.04027725011110306\n",
      "epoch:  202, loss: 0.04027668759226799\n",
      "epoch:  203, loss: 0.040276117622852325\n",
      "epoch:  204, loss: 0.04027554765343666\n",
      "epoch:  205, loss: 0.04027498513460159\n",
      "epoch:  206, loss: 0.04027441889047623\n",
      "epoch:  207, loss: 0.040273867547512054\n",
      "epoch:  208, loss: 0.040273312479257584\n",
      "epoch:  209, loss: 0.04027276113629341\n",
      "epoch:  210, loss: 0.04027220606803894\n",
      "epoch:  211, loss: 0.040271662175655365\n",
      "epoch:  212, loss: 0.04027111455798149\n",
      "epoch:  213, loss: 0.040270570665597916\n",
      "epoch:  214, loss: 0.04027002304792404\n",
      "epoch:  215, loss: 0.040269479155540466\n",
      "epoch:  216, loss: 0.04026893526315689\n",
      "epoch:  217, loss: 0.040268391370773315\n",
      "epoch:  218, loss: 0.04026784002780914\n",
      "epoch:  219, loss: 0.04026729613542557\n",
      "epoch:  220, loss: 0.04026675224304199\n",
      "epoch:  221, loss: 0.04026619717478752\n",
      "epoch:  222, loss: 0.04026564955711365\n",
      "epoch:  223, loss: 0.04026510566473007\n",
      "epoch:  224, loss: 0.0402645580470562\n",
      "epoch:  225, loss: 0.04026401787996292\n",
      "epoch:  226, loss: 0.04026348143815994\n",
      "epoch:  227, loss: 0.04026293754577637\n",
      "epoch:  228, loss: 0.04026239365339279\n",
      "epoch:  229, loss: 0.040261849761009216\n",
      "epoch:  230, loss: 0.04026130586862564\n",
      "epoch:  231, loss: 0.04026075825095177\n",
      "epoch:  232, loss: 0.04026021063327789\n",
      "epoch:  233, loss: 0.04025966674089432\n",
      "epoch:  234, loss: 0.04025911167263985\n",
      "epoch:  235, loss: 0.04025855287909508\n",
      "epoch:  236, loss: 0.04025799781084061\n",
      "epoch:  237, loss: 0.040257442742586136\n",
      "epoch:  238, loss: 0.040256891399621964\n",
      "epoch:  239, loss: 0.04025634750723839\n",
      "epoch:  240, loss: 0.04025580361485481\n",
      "epoch:  241, loss: 0.04025524854660034\n",
      "epoch:  242, loss: 0.04025469720363617\n",
      "epoch:  243, loss: 0.0402541309595108\n",
      "epoch:  244, loss: 0.040253568440675735\n",
      "epoch:  245, loss: 0.04025300592184067\n",
      "epoch:  246, loss: 0.040252435952425\n",
      "epoch:  247, loss: 0.040251873433589935\n",
      "epoch:  248, loss: 0.04025130346417427\n",
      "epoch:  249, loss: 0.0402507409453392\n",
      "epoch:  250, loss: 0.040250182151794434\n",
      "epoch:  251, loss: 0.04024961218237877\n",
      "epoch:  252, loss: 0.040249042212963104\n",
      "epoch:  253, loss: 0.04024846479296684\n",
      "epoch:  254, loss: 0.04024788737297058\n",
      "epoch:  255, loss: 0.04024730622768402\n",
      "epoch:  256, loss: 0.040246713906526566\n",
      "epoch:  257, loss: 0.04024611786007881\n",
      "epoch:  258, loss: 0.04024552181363106\n",
      "epoch:  259, loss: 0.040244925767183304\n",
      "epoch:  260, loss: 0.04024432972073555\n",
      "epoch:  261, loss: 0.040243737399578094\n",
      "epoch:  262, loss: 0.04024314880371094\n",
      "epoch:  263, loss: 0.04024256765842438\n",
      "epoch:  264, loss: 0.040241971611976624\n",
      "epoch:  265, loss: 0.04024137556552887\n",
      "epoch:  266, loss: 0.04024076461791992\n",
      "epoch:  267, loss: 0.040240149945020676\n",
      "epoch:  268, loss: 0.04023953527212143\n",
      "epoch:  269, loss: 0.04023892432451248\n",
      "epoch:  270, loss: 0.04023832827806473\n",
      "epoch:  271, loss: 0.040237732231616974\n",
      "epoch:  272, loss: 0.04023713618516922\n",
      "epoch:  273, loss: 0.040236540138721466\n",
      "epoch:  274, loss: 0.04023594409227371\n",
      "epoch:  275, loss: 0.04023534059524536\n",
      "epoch:  276, loss: 0.04023473709821701\n",
      "epoch:  277, loss: 0.04023413732647896\n",
      "epoch:  278, loss: 0.04023352637887001\n",
      "epoch:  279, loss: 0.04023292288184166\n",
      "epoch:  280, loss: 0.04023232311010361\n",
      "epoch:  281, loss: 0.040231723338365555\n",
      "epoch:  282, loss: 0.040231116116046906\n",
      "epoch:  283, loss: 0.04023051634430885\n",
      "epoch:  284, loss: 0.0402299165725708\n",
      "epoch:  285, loss: 0.04022932052612305\n",
      "epoch:  286, loss: 0.040228717029094696\n",
      "epoch:  287, loss: 0.040228135883808136\n",
      "epoch:  288, loss: 0.040227554738521576\n",
      "epoch:  289, loss: 0.04022696241736412\n",
      "epoch:  290, loss: 0.04022637754678726\n",
      "epoch:  291, loss: 0.0402257926762104\n",
      "epoch:  292, loss: 0.04022521525621414\n",
      "epoch:  293, loss: 0.040224626660346985\n",
      "epoch:  294, loss: 0.04022405669093132\n",
      "epoch:  295, loss: 0.04022347927093506\n",
      "epoch:  296, loss: 0.0402229018509388\n",
      "epoch:  297, loss: 0.04022232070565224\n",
      "epoch:  298, loss: 0.040221743285655975\n",
      "epoch:  299, loss: 0.040221162140369415\n",
      "epoch:  300, loss: 0.040220584720373154\n",
      "epoch:  301, loss: 0.040220003575086594\n",
      "epoch:  302, loss: 0.04021942615509033\n",
      "epoch:  303, loss: 0.04021884500980377\n",
      "epoch:  304, loss: 0.04021826758980751\n",
      "epoch:  305, loss: 0.04021767899394035\n",
      "epoch:  306, loss: 0.040217094123363495\n",
      "epoch:  307, loss: 0.04021650552749634\n",
      "epoch:  308, loss: 0.04021592065691948\n",
      "epoch:  309, loss: 0.04021533951163292\n",
      "epoch:  310, loss: 0.04021475091576576\n",
      "epoch:  311, loss: 0.0402141697704792\n",
      "epoch:  312, loss: 0.040213584899902344\n",
      "epoch:  313, loss: 0.04021299630403519\n",
      "epoch:  314, loss: 0.04021241143345833\n",
      "epoch:  315, loss: 0.040211815387010574\n",
      "epoch:  316, loss: 0.04021121934056282\n",
      "epoch:  317, loss: 0.04021061956882477\n",
      "epoch:  318, loss: 0.04021001607179642\n",
      "epoch:  319, loss: 0.040209416300058365\n",
      "epoch:  320, loss: 0.040208809077739716\n",
      "epoch:  321, loss: 0.040208205580711365\n",
      "epoch:  322, loss: 0.04020760580897331\n",
      "epoch:  323, loss: 0.040206994861364365\n",
      "epoch:  324, loss: 0.040206387639045715\n",
      "epoch:  325, loss: 0.040205784142017365\n",
      "epoch:  326, loss: 0.04020516201853752\n",
      "epoch:  327, loss: 0.04020454362034798\n",
      "epoch:  328, loss: 0.040203921496868134\n",
      "epoch:  329, loss: 0.04020329937338829\n",
      "epoch:  330, loss: 0.04020267352461815\n",
      "epoch:  331, loss: 0.04020204022526741\n",
      "epoch:  332, loss: 0.04020141437649727\n",
      "epoch:  333, loss: 0.040200792253017426\n",
      "epoch:  334, loss: 0.040200166404247284\n",
      "epoch:  335, loss: 0.040199536830186844\n",
      "epoch:  336, loss: 0.0401989109814167\n",
      "epoch:  337, loss: 0.04019828885793686\n",
      "epoch:  338, loss: 0.04019765928387642\n",
      "epoch:  339, loss: 0.040197018533945084\n",
      "epoch:  340, loss: 0.04019638150930405\n",
      "epoch:  341, loss: 0.04019574448466301\n",
      "epoch:  342, loss: 0.04019510746002197\n",
      "epoch:  343, loss: 0.04019446298480034\n",
      "epoch:  344, loss: 0.040193814784288406\n",
      "epoch:  345, loss: 0.040193162858486176\n",
      "epoch:  346, loss: 0.04019251838326454\n",
      "epoch:  347, loss: 0.04019187018275261\n",
      "epoch:  348, loss: 0.04019122198224068\n",
      "epoch:  349, loss: 0.040190570056438446\n",
      "epoch:  350, loss: 0.04018993303179741\n",
      "epoch:  351, loss: 0.04018930345773697\n",
      "epoch:  352, loss: 0.04018867015838623\n",
      "epoch:  353, loss: 0.04018803685903549\n",
      "epoch:  354, loss: 0.040187396109104156\n",
      "epoch:  355, loss: 0.04018675535917282\n",
      "epoch:  356, loss: 0.04018612951040268\n",
      "epoch:  357, loss: 0.04018549993634224\n",
      "epoch:  358, loss: 0.0401848629117012\n",
      "epoch:  359, loss: 0.040184229612350464\n",
      "epoch:  360, loss: 0.040183596312999725\n",
      "epoch:  361, loss: 0.040182966738939285\n",
      "epoch:  362, loss: 0.04018232226371765\n",
      "epoch:  363, loss: 0.04018169268965721\n",
      "epoch:  364, loss: 0.04018105939030647\n",
      "epoch:  365, loss: 0.04018041864037514\n",
      "epoch:  366, loss: 0.0401797778904438\n",
      "epoch:  367, loss: 0.04017913341522217\n",
      "epoch:  368, loss: 0.040178485214710236\n",
      "epoch:  369, loss: 0.040177829563617706\n",
      "epoch:  370, loss: 0.040177181363105774\n",
      "epoch:  371, loss: 0.040176525712013245\n",
      "epoch:  372, loss: 0.04017585515975952\n",
      "epoch:  373, loss: 0.040175192058086395\n",
      "epoch:  374, loss: 0.04017453268170357\n",
      "epoch:  375, loss: 0.04017387330532074\n",
      "epoch:  376, loss: 0.04017321392893791\n",
      "epoch:  377, loss: 0.040172550827264786\n",
      "epoch:  378, loss: 0.04017188772559166\n",
      "epoch:  379, loss: 0.040171220898628235\n",
      "epoch:  380, loss: 0.04017055407166481\n",
      "epoch:  381, loss: 0.04016988351941109\n",
      "epoch:  382, loss: 0.040169212967157364\n",
      "epoch:  383, loss: 0.04016854241490364\n",
      "epoch:  384, loss: 0.04016786441206932\n",
      "epoch:  385, loss: 0.0401671826839447\n",
      "epoch:  386, loss: 0.040166497230529785\n",
      "epoch:  387, loss: 0.04016581550240517\n",
      "epoch:  388, loss: 0.04016513377428055\n",
      "epoch:  389, loss: 0.04016445204615593\n",
      "epoch:  390, loss: 0.04016377031803131\n",
      "epoch:  391, loss: 0.04016308858990669\n",
      "epoch:  392, loss: 0.04016241058707237\n",
      "epoch:  393, loss: 0.04016172140836716\n",
      "epoch:  394, loss: 0.04016103968024254\n",
      "epoch:  395, loss: 0.04016035795211792\n",
      "epoch:  396, loss: 0.0401596836745739\n",
      "epoch:  397, loss: 0.04015899449586868\n",
      "epoch:  398, loss: 0.040158312767744064\n",
      "epoch:  399, loss: 0.04015762358903885\n",
      "epoch:  400, loss: 0.04015693441033363\n",
      "epoch:  401, loss: 0.040156248956918716\n",
      "epoch:  402, loss: 0.0401555560529232\n",
      "epoch:  403, loss: 0.040154874324798584\n",
      "epoch:  404, loss: 0.04015420749783516\n",
      "epoch:  405, loss: 0.04015352949500084\n",
      "epoch:  406, loss: 0.040152858942747116\n",
      "epoch:  407, loss: 0.040152180939912796\n",
      "epoch:  408, loss: 0.04015149921178818\n",
      "epoch:  409, loss: 0.04015081375837326\n",
      "epoch:  410, loss: 0.04015013575553894\n",
      "epoch:  411, loss: 0.04014945030212402\n",
      "epoch:  412, loss: 0.04014876112341881\n",
      "epoch:  413, loss: 0.040148068219423294\n",
      "epoch:  414, loss: 0.04014737159013748\n",
      "epoch:  415, loss: 0.04014667496085167\n",
      "epoch:  416, loss: 0.040145982056856155\n",
      "epoch:  417, loss: 0.040145281702280045\n",
      "epoch:  418, loss: 0.04014458507299423\n",
      "epoch:  419, loss: 0.04014388471841812\n",
      "epoch:  420, loss: 0.04014318436384201\n",
      "epoch:  421, loss: 0.0401424840092659\n",
      "epoch:  422, loss: 0.04014178365468979\n",
      "epoch:  423, loss: 0.040141087025403976\n",
      "epoch:  424, loss: 0.040140390396118164\n",
      "epoch:  425, loss: 0.04013969376683235\n",
      "epoch:  426, loss: 0.04013899713754654\n",
      "epoch:  427, loss: 0.04013830050826073\n",
      "epoch:  428, loss: 0.04013761132955551\n",
      "epoch:  429, loss: 0.0401369072496891\n",
      "epoch:  430, loss: 0.04013621062040329\n",
      "epoch:  431, loss: 0.04013551026582718\n",
      "epoch:  432, loss: 0.04013480991125107\n",
      "epoch:  433, loss: 0.040134113281965256\n",
      "epoch:  434, loss: 0.04013341665267944\n",
      "epoch:  435, loss: 0.04013271629810333\n",
      "epoch:  436, loss: 0.04013201221823692\n",
      "epoch:  437, loss: 0.04013131186366081\n",
      "epoch:  438, loss: 0.040130604058504105\n",
      "epoch:  439, loss: 0.0401298813521862\n",
      "epoch:  440, loss: 0.0401291698217392\n",
      "epoch:  441, loss: 0.04012845829129219\n",
      "epoch:  442, loss: 0.040127743035554886\n",
      "epoch:  443, loss: 0.04012702777981758\n",
      "epoch:  444, loss: 0.040126312524080276\n",
      "epoch:  445, loss: 0.04012559354305267\n",
      "epoch:  446, loss: 0.04012487456202507\n",
      "epoch:  447, loss: 0.04012414813041687\n",
      "epoch:  448, loss: 0.04012342914938927\n",
      "epoch:  449, loss: 0.040122710168361664\n",
      "epoch:  450, loss: 0.040121983736753464\n",
      "epoch:  451, loss: 0.04012126475572586\n",
      "epoch:  452, loss: 0.040120530873537064\n",
      "epoch:  453, loss: 0.04011978954076767\n",
      "epoch:  454, loss: 0.04011905565857887\n",
      "epoch:  455, loss: 0.040118321776390076\n",
      "epoch:  456, loss: 0.04011759161949158\n",
      "epoch:  457, loss: 0.04011686146259308\n",
      "epoch:  458, loss: 0.04011612758040428\n",
      "epoch:  459, loss: 0.040115393698215485\n",
      "epoch:  460, loss: 0.04011465981602669\n",
      "epoch:  461, loss: 0.04011393338441849\n",
      "epoch:  462, loss: 0.04011319950222969\n",
      "epoch:  463, loss: 0.040112461894750595\n",
      "epoch:  464, loss: 0.0401117168366909\n",
      "epoch:  465, loss: 0.04011096805334091\n",
      "epoch:  466, loss: 0.04011021554470062\n",
      "epoch:  467, loss: 0.04010946676135063\n",
      "epoch:  468, loss: 0.04010871425271034\n",
      "epoch:  469, loss: 0.040107954293489456\n",
      "epoch:  470, loss: 0.04010720178484917\n",
      "epoch:  471, loss: 0.04010644927620888\n",
      "epoch:  472, loss: 0.04010568559169769\n",
      "epoch:  473, loss: 0.040104933083057404\n",
      "epoch:  474, loss: 0.040104176849126816\n",
      "epoch:  475, loss: 0.04010342061519623\n",
      "epoch:  476, loss: 0.040102653205394745\n",
      "epoch:  477, loss: 0.04010188952088356\n",
      "epoch:  478, loss: 0.04010111838579178\n",
      "epoch:  479, loss: 0.0401003435254097\n",
      "epoch:  480, loss: 0.04009956866502762\n",
      "epoch:  481, loss: 0.04009878635406494\n",
      "epoch:  482, loss: 0.04009799286723137\n",
      "epoch:  483, loss: 0.0400971882045269\n",
      "epoch:  484, loss: 0.04009638726711273\n",
      "epoch:  485, loss: 0.04009558632969856\n",
      "epoch:  486, loss: 0.040094781666994095\n",
      "epoch:  487, loss: 0.04009397327899933\n",
      "epoch:  488, loss: 0.04009317606687546\n",
      "epoch:  489, loss: 0.04009237512946129\n",
      "epoch:  490, loss: 0.04009157046675682\n",
      "epoch:  491, loss: 0.04009076952934265\n",
      "epoch:  492, loss: 0.04008996859192848\n",
      "epoch:  493, loss: 0.040089163929224014\n",
      "epoch:  494, loss: 0.040088362991809845\n",
      "epoch:  495, loss: 0.04008755460381508\n",
      "epoch:  496, loss: 0.04008674994111061\n",
      "epoch:  497, loss: 0.04008594900369644\n",
      "epoch:  498, loss: 0.040085144340991974\n",
      "epoch:  499, loss: 0.040084343403577805\n",
      "epoch:  500, loss: 0.04008354991674423\n",
      "epoch:  501, loss: 0.04008276388049126\n",
      "epoch:  502, loss: 0.04008196294307709\n",
      "epoch:  503, loss: 0.04008116573095322\n",
      "epoch:  504, loss: 0.040080368518829346\n",
      "epoch:  505, loss: 0.04007957503199577\n",
      "epoch:  506, loss: 0.040078774094581604\n",
      "epoch:  507, loss: 0.04007796198129654\n",
      "epoch:  508, loss: 0.040077149868011475\n",
      "epoch:  509, loss: 0.04007633030414581\n",
      "epoch:  510, loss: 0.04007550701498985\n",
      "epoch:  511, loss: 0.040074680000543594\n",
      "epoch:  512, loss: 0.040073852986097336\n",
      "epoch:  513, loss: 0.040073029696941376\n",
      "epoch:  514, loss: 0.04007220268249512\n",
      "epoch:  515, loss: 0.040071386843919754\n",
      "epoch:  516, loss: 0.04007057100534439\n",
      "epoch:  517, loss: 0.04006974771618843\n",
      "epoch:  518, loss: 0.04006892070174217\n",
      "epoch:  519, loss: 0.040068089962005615\n",
      "epoch:  520, loss: 0.04006726294755936\n",
      "epoch:  521, loss: 0.0400664322078228\n",
      "epoch:  522, loss: 0.04006561264395714\n",
      "epoch:  523, loss: 0.04006478190422058\n",
      "epoch:  524, loss: 0.040063951164484024\n",
      "epoch:  525, loss: 0.04006313160061836\n",
      "epoch:  526, loss: 0.040062300860881805\n",
      "epoch:  527, loss: 0.04006147384643555\n",
      "epoch:  528, loss: 0.040060654282569885\n",
      "epoch:  529, loss: 0.040059830993413925\n",
      "epoch:  530, loss: 0.040059011429548264\n",
      "epoch:  531, loss: 0.0400582030415535\n",
      "epoch:  532, loss: 0.040057387202978134\n",
      "epoch:  533, loss: 0.04005657136440277\n",
      "epoch:  534, loss: 0.04005575180053711\n",
      "epoch:  535, loss: 0.040054935961961746\n",
      "epoch:  536, loss: 0.04005412012338638\n",
      "epoch:  537, loss: 0.04005330801010132\n",
      "epoch:  538, loss: 0.04005249962210655\n",
      "epoch:  539, loss: 0.04005169868469238\n",
      "epoch:  540, loss: 0.04005088657140732\n",
      "epoch:  541, loss: 0.04005008563399315\n",
      "epoch:  542, loss: 0.04004928097128868\n",
      "epoch:  543, loss: 0.04004848003387451\n",
      "epoch:  544, loss: 0.04004766792058945\n",
      "epoch:  545, loss: 0.04004685953259468\n",
      "epoch:  546, loss: 0.040046047419309616\n",
      "epoch:  547, loss: 0.04004523530602455\n",
      "epoch:  548, loss: 0.040044430643320084\n",
      "epoch:  549, loss: 0.04004361107945442\n",
      "epoch:  550, loss: 0.04004279151558876\n",
      "epoch:  551, loss: 0.0400419645011425\n",
      "epoch:  552, loss: 0.04004113748669624\n",
      "epoch:  553, loss: 0.040040310472249985\n",
      "epoch:  554, loss: 0.04003947973251343\n",
      "epoch:  555, loss: 0.04003864526748657\n",
      "epoch:  556, loss: 0.04003782197833061\n",
      "epoch:  557, loss: 0.040036991238594055\n",
      "epoch:  558, loss: 0.040036167949438095\n",
      "epoch:  559, loss: 0.04003534093499184\n",
      "epoch:  560, loss: 0.04003450274467468\n",
      "epoch:  561, loss: 0.040033672004938126\n",
      "epoch:  562, loss: 0.04003283381462097\n",
      "epoch:  563, loss: 0.040031980723142624\n",
      "epoch:  564, loss: 0.04003113508224487\n",
      "epoch:  565, loss: 0.04003029316663742\n",
      "epoch:  566, loss: 0.04002944752573967\n",
      "epoch:  567, loss: 0.04002860561013222\n",
      "epoch:  568, loss: 0.04002775251865387\n",
      "epoch:  569, loss: 0.04002689570188522\n",
      "epoch:  570, loss: 0.04002605006098747\n",
      "epoch:  571, loss: 0.040025196969509125\n",
      "epoch:  572, loss: 0.04002434015274048\n",
      "epoch:  573, loss: 0.04002348706126213\n",
      "epoch:  574, loss: 0.04002264514565468\n",
      "epoch:  575, loss: 0.04002179950475693\n",
      "epoch:  576, loss: 0.040020957589149475\n",
      "epoch:  577, loss: 0.040020111948251724\n",
      "epoch:  578, loss: 0.04001926630735397\n",
      "epoch:  579, loss: 0.04001842811703682\n",
      "epoch:  580, loss: 0.04001758620142937\n",
      "epoch:  581, loss: 0.04001675173640251\n",
      "epoch:  582, loss: 0.04001590609550476\n",
      "epoch:  583, loss: 0.04001506417989731\n",
      "epoch:  584, loss: 0.04001421853899956\n",
      "epoch:  585, loss: 0.04001336917281151\n",
      "epoch:  586, loss: 0.04001251235604286\n",
      "epoch:  587, loss: 0.04001166298985481\n",
      "epoch:  588, loss: 0.04001080244779587\n",
      "epoch:  589, loss: 0.04000994935631752\n",
      "epoch:  590, loss: 0.04000909626483917\n",
      "epoch:  591, loss: 0.04000824689865112\n",
      "epoch:  592, loss: 0.04000740498304367\n",
      "epoch:  593, loss: 0.04000655934214592\n",
      "epoch:  594, loss: 0.04000570997595787\n",
      "epoch:  595, loss: 0.04000485688447952\n",
      "epoch:  596, loss: 0.040004003793001175\n",
      "epoch:  597, loss: 0.040003154426813126\n",
      "epoch:  598, loss: 0.040002308785915375\n",
      "epoch:  599, loss: 0.04000145196914673\n",
      "epoch:  600, loss: 0.04000059887766838\n",
      "epoch:  601, loss: 0.03999974951148033\n",
      "epoch:  602, loss: 0.039998896420001984\n",
      "epoch:  603, loss: 0.03999803960323334\n",
      "epoch:  604, loss: 0.03999719023704529\n",
      "epoch:  605, loss: 0.03999634087085724\n",
      "epoch:  606, loss: 0.039995498955249786\n",
      "epoch:  607, loss: 0.039994657039642334\n",
      "epoch:  608, loss: 0.03999381139874458\n",
      "epoch:  609, loss: 0.03999297693371773\n",
      "epoch:  610, loss: 0.039992135018110275\n",
      "epoch:  611, loss: 0.03999129682779312\n",
      "epoch:  612, loss: 0.039990462362766266\n",
      "epoch:  613, loss: 0.03998962789773941\n",
      "epoch:  614, loss: 0.039988789707422256\n",
      "epoch:  615, loss: 0.039987947791814804\n",
      "epoch:  616, loss: 0.03998710960149765\n",
      "epoch:  617, loss: 0.039986275136470795\n",
      "epoch:  618, loss: 0.03998544067144394\n",
      "epoch:  619, loss: 0.039984606206417084\n",
      "epoch:  620, loss: 0.03998377174139023\n",
      "epoch:  621, loss: 0.03998293727636337\n",
      "epoch:  622, loss: 0.03998209536075592\n",
      "epoch:  623, loss: 0.039981257170438766\n",
      "epoch:  624, loss: 0.039980415254831314\n",
      "epoch:  625, loss: 0.03997958078980446\n",
      "epoch:  626, loss: 0.0399787500500679\n",
      "epoch:  627, loss: 0.039977915585041046\n",
      "epoch:  628, loss: 0.039977092295885086\n",
      "epoch:  629, loss: 0.039976272732019424\n",
      "epoch:  630, loss: 0.039975449442863464\n",
      "epoch:  631, loss: 0.039974622428417206\n",
      "epoch:  632, loss: 0.03997379541397095\n",
      "epoch:  633, loss: 0.03997297212481499\n",
      "epoch:  634, loss: 0.03997214511036873\n",
      "epoch:  635, loss: 0.03997131437063217\n",
      "epoch:  636, loss: 0.039970483630895615\n",
      "epoch:  637, loss: 0.03996965289115906\n",
      "epoch:  638, loss: 0.0399688221514225\n",
      "epoch:  639, loss: 0.039967987686395645\n",
      "epoch:  640, loss: 0.039967138320207596\n",
      "epoch:  641, loss: 0.03996628522872925\n",
      "epoch:  642, loss: 0.0399654321372509\n",
      "epoch:  643, loss: 0.03996457904577255\n",
      "epoch:  644, loss: 0.039963725954294205\n",
      "epoch:  645, loss: 0.03996286913752556\n",
      "epoch:  646, loss: 0.03996201977133751\n",
      "epoch:  647, loss: 0.03996116667985916\n",
      "epoch:  648, loss: 0.039960309863090515\n",
      "epoch:  649, loss: 0.039959460496902466\n",
      "epoch:  650, loss: 0.03995860368013382\n",
      "epoch:  651, loss: 0.03995775431394577\n",
      "epoch:  652, loss: 0.03995690122246742\n",
      "epoch:  653, loss: 0.03995604068040848\n",
      "epoch:  654, loss: 0.039955172687768936\n",
      "epoch:  655, loss: 0.03995431214570999\n",
      "epoch:  656, loss: 0.03995344042778015\n",
      "epoch:  657, loss: 0.03995256498456001\n",
      "epoch:  658, loss: 0.03995169326663017\n",
      "epoch:  659, loss: 0.039950814098119736\n",
      "epoch:  660, loss: 0.0399499386548996\n",
      "epoch:  661, loss: 0.03994905948638916\n",
      "epoch:  662, loss: 0.03994818776845932\n",
      "epoch:  663, loss: 0.039947301149368286\n",
      "epoch:  664, loss: 0.03994641453027725\n",
      "epoch:  665, loss: 0.03994552418589592\n",
      "epoch:  666, loss: 0.03994463011622429\n",
      "epoch:  667, loss: 0.03994372859597206\n",
      "epoch:  668, loss: 0.039942823350429535\n",
      "epoch:  669, loss: 0.03994191065430641\n",
      "epoch:  670, loss: 0.03994099423289299\n",
      "epoch:  671, loss: 0.03994007781147957\n",
      "epoch:  672, loss: 0.03993915393948555\n",
      "epoch:  673, loss: 0.03993822634220123\n",
      "epoch:  674, loss: 0.039937298744916916\n",
      "epoch:  675, loss: 0.0399363711476326\n",
      "epoch:  676, loss: 0.03993543982505798\n",
      "epoch:  677, loss: 0.03993450850248337\n",
      "epoch:  678, loss: 0.03993358090519905\n",
      "epoch:  679, loss: 0.03993265703320503\n",
      "epoch:  680, loss: 0.03993172571063042\n",
      "epoch:  681, loss: 0.0399307981133461\n",
      "epoch:  682, loss: 0.03992987051606178\n",
      "epoch:  683, loss: 0.039928942918777466\n",
      "epoch:  684, loss: 0.03992801159620285\n",
      "epoch:  685, loss: 0.03992708772420883\n",
      "epoch:  686, loss: 0.039926156401634216\n",
      "epoch:  687, loss: 0.0399252250790596\n",
      "epoch:  688, loss: 0.039924297481775284\n",
      "epoch:  689, loss: 0.03992336988449097\n",
      "epoch:  690, loss: 0.03992244973778725\n",
      "epoch:  691, loss: 0.03992151841521263\n",
      "epoch:  692, loss: 0.03992059454321861\n",
      "epoch:  693, loss: 0.03991967439651489\n",
      "epoch:  694, loss: 0.039918750524520874\n",
      "epoch:  695, loss: 0.039917826652526855\n",
      "epoch:  696, loss: 0.03991689532995224\n",
      "epoch:  697, loss: 0.03991597145795822\n",
      "epoch:  698, loss: 0.039915040135383606\n",
      "epoch:  699, loss: 0.03991411253809929\n",
      "epoch:  700, loss: 0.03991318494081497\n",
      "epoch:  701, loss: 0.039912257343530655\n",
      "epoch:  702, loss: 0.03991132974624634\n",
      "epoch:  703, loss: 0.03991040214896202\n",
      "epoch:  704, loss: 0.039909474551677704\n",
      "epoch:  705, loss: 0.03990854695439339\n",
      "epoch:  706, loss: 0.03990761563181877\n",
      "epoch:  707, loss: 0.039906688034534454\n",
      "epoch:  708, loss: 0.03990574926137924\n",
      "epoch:  709, loss: 0.03990481421351433\n",
      "epoch:  710, loss: 0.03990386426448822\n",
      "epoch:  711, loss: 0.03990291804075241\n",
      "epoch:  712, loss: 0.0399019718170166\n",
      "epoch:  713, loss: 0.03990102559328079\n",
      "epoch:  714, loss: 0.039900071918964386\n",
      "epoch:  715, loss: 0.03989911451935768\n",
      "epoch:  716, loss: 0.03989815711975098\n",
      "epoch:  717, loss: 0.03989719972014427\n",
      "epoch:  718, loss: 0.03989624232053757\n",
      "epoch:  719, loss: 0.03989528864622116\n",
      "epoch:  720, loss: 0.03989434242248535\n",
      "epoch:  721, loss: 0.03989339619874954\n",
      "epoch:  722, loss: 0.039892442524433136\n",
      "epoch:  723, loss: 0.03989149630069733\n",
      "epoch:  724, loss: 0.03989054262638092\n",
      "epoch:  725, loss: 0.03988959640264511\n",
      "epoch:  726, loss: 0.0398886501789093\n",
      "epoch:  727, loss: 0.03988770768046379\n",
      "epoch:  728, loss: 0.03988676145672798\n",
      "epoch:  729, loss: 0.039885811507701874\n",
      "epoch:  730, loss: 0.039884861558675766\n",
      "epoch:  731, loss: 0.039883919060230255\n",
      "epoch:  732, loss: 0.039882972836494446\n",
      "epoch:  733, loss: 0.039882026612758636\n",
      "epoch:  734, loss: 0.03988108038902283\n",
      "epoch:  735, loss: 0.039880137890577316\n",
      "epoch:  736, loss: 0.03987918421626091\n",
      "epoch:  737, loss: 0.0398782379925251\n",
      "epoch:  738, loss: 0.03987728804349899\n",
      "epoch:  739, loss: 0.039876341819763184\n",
      "epoch:  740, loss: 0.03987539932131767\n",
      "epoch:  741, loss: 0.03987445682287216\n",
      "epoch:  742, loss: 0.039873506873846054\n",
      "epoch:  743, loss: 0.03987256437540054\n",
      "epoch:  744, loss: 0.039871614426374435\n",
      "epoch:  745, loss: 0.039870671927928925\n",
      "epoch:  746, loss: 0.039869725704193115\n",
      "epoch:  747, loss: 0.039868783205747604\n",
      "epoch:  748, loss: 0.039867840707302094\n",
      "epoch:  749, loss: 0.03986689820885658\n",
      "epoch:  750, loss: 0.03986595943570137\n",
      "epoch:  751, loss: 0.03986501693725586\n",
      "epoch:  752, loss: 0.039864081889390945\n",
      "epoch:  753, loss: 0.03986314311623573\n",
      "epoch:  754, loss: 0.03986221179366112\n",
      "epoch:  755, loss: 0.039861276745796204\n",
      "epoch:  756, loss: 0.03986034169793129\n",
      "epoch:  757, loss: 0.039859410375356674\n",
      "epoch:  758, loss: 0.03985847905278206\n",
      "epoch:  759, loss: 0.03985754773020744\n",
      "epoch:  760, loss: 0.03985661268234253\n",
      "epoch:  761, loss: 0.039855681359767914\n",
      "epoch:  762, loss: 0.0398547425866127\n",
      "epoch:  763, loss: 0.039853811264038086\n",
      "epoch:  764, loss: 0.03985287994146347\n",
      "epoch:  765, loss: 0.03985194116830826\n",
      "epoch:  766, loss: 0.039851006120443344\n",
      "epoch:  767, loss: 0.03985007107257843\n",
      "epoch:  768, loss: 0.03984914347529411\n",
      "epoch:  769, loss: 0.039848215878009796\n",
      "epoch:  770, loss: 0.03984728455543518\n",
      "epoch:  771, loss: 0.039846353232860565\n",
      "epoch:  772, loss: 0.03984542191028595\n",
      "epoch:  773, loss: 0.03984449431300163\n",
      "epoch:  774, loss: 0.03984355926513672\n",
      "epoch:  775, loss: 0.039842624217271805\n",
      "epoch:  776, loss: 0.03984168544411659\n",
      "epoch:  777, loss: 0.03984073922038078\n",
      "epoch:  778, loss: 0.03983979672193527\n",
      "epoch:  779, loss: 0.03983885049819946\n",
      "epoch:  780, loss: 0.039837904274463654\n",
      "epoch:  781, loss: 0.03983696177601814\n",
      "epoch:  782, loss: 0.03983601555228233\n",
      "epoch:  783, loss: 0.039835069328546524\n",
      "epoch:  784, loss: 0.03983412683010101\n",
      "epoch:  785, loss: 0.0398331880569458\n",
      "epoch:  786, loss: 0.03983224555850029\n",
      "epoch:  787, loss: 0.03983129933476448\n",
      "epoch:  788, loss: 0.039830345660448074\n",
      "epoch:  789, loss: 0.039829399436712265\n",
      "epoch:  790, loss: 0.039828456938266754\n",
      "epoch:  791, loss: 0.039827506989240646\n",
      "epoch:  792, loss: 0.039826564490795135\n",
      "epoch:  793, loss: 0.03982561081647873\n",
      "epoch:  794, loss: 0.03982465714216232\n",
      "epoch:  795, loss: 0.039823707193136215\n",
      "epoch:  796, loss: 0.03982275724411011\n",
      "epoch:  797, loss: 0.0398218035697937\n",
      "epoch:  798, loss: 0.039820849895477295\n",
      "epoch:  799, loss: 0.03981988877058029\n",
      "epoch:  800, loss: 0.03981892764568329\n",
      "epoch:  801, loss: 0.039817966520786285\n",
      "epoch:  802, loss: 0.03981700912117958\n",
      "epoch:  803, loss: 0.03981604427099228\n",
      "epoch:  804, loss: 0.039815083146095276\n",
      "epoch:  805, loss: 0.039814118295907974\n",
      "epoch:  806, loss: 0.03981315717101097\n",
      "epoch:  807, loss: 0.03981219604611397\n",
      "epoch:  808, loss: 0.039811234921216965\n",
      "epoch:  809, loss: 0.03981027007102966\n",
      "epoch:  810, loss: 0.03980930894613266\n",
      "epoch:  811, loss: 0.03980834409594536\n",
      "epoch:  812, loss: 0.03980737924575806\n",
      "epoch:  813, loss: 0.039806414395570755\n",
      "epoch:  814, loss: 0.03980545327067375\n",
      "epoch:  815, loss: 0.03980448842048645\n",
      "epoch:  816, loss: 0.03980352729558945\n",
      "epoch:  817, loss: 0.039802566170692444\n",
      "epoch:  818, loss: 0.039801597595214844\n",
      "epoch:  819, loss: 0.03980064392089844\n",
      "epoch:  820, loss: 0.039799679070711136\n",
      "epoch:  821, loss: 0.03979871794581413\n",
      "epoch:  822, loss: 0.03979774937033653\n",
      "epoch:  823, loss: 0.03979678452014923\n",
      "epoch:  824, loss: 0.03979582339525223\n",
      "epoch:  825, loss: 0.03979485109448433\n",
      "epoch:  826, loss: 0.03979388251900673\n",
      "epoch:  827, loss: 0.03979291394352913\n",
      "epoch:  828, loss: 0.03979194164276123\n",
      "epoch:  829, loss: 0.03979097306728363\n",
      "epoch:  830, loss: 0.03979000449180603\n",
      "epoch:  831, loss: 0.03978903219103813\n",
      "epoch:  832, loss: 0.039788056164979935\n",
      "epoch:  833, loss: 0.03978708013892174\n",
      "epoch:  834, loss: 0.03978610783815384\n",
      "epoch:  835, loss: 0.039785124361515045\n",
      "epoch:  836, loss: 0.03978414088487625\n",
      "epoch:  837, loss: 0.03978315740823746\n",
      "epoch:  838, loss: 0.03978217765688896\n",
      "epoch:  839, loss: 0.03978119418025017\n",
      "epoch:  840, loss: 0.03978021442890167\n",
      "epoch:  841, loss: 0.03977923467755318\n",
      "epoch:  842, loss: 0.03977825492620468\n",
      "epoch:  843, loss: 0.039777275174856186\n",
      "epoch:  844, loss: 0.03977629542350769\n",
      "epoch:  845, loss: 0.03977531939744949\n",
      "epoch:  846, loss: 0.039774343371391296\n",
      "epoch:  847, loss: 0.0397733673453331\n",
      "epoch:  848, loss: 0.039772387593984604\n",
      "epoch:  849, loss: 0.03977140784263611\n",
      "epoch:  850, loss: 0.039770424365997314\n",
      "epoch:  851, loss: 0.03976944833993912\n",
      "epoch:  852, loss: 0.039768464863300323\n",
      "epoch:  853, loss: 0.039767492562532425\n",
      "epoch:  854, loss: 0.03976650536060333\n",
      "epoch:  855, loss: 0.03976552560925484\n",
      "epoch:  856, loss: 0.03976454213261604\n",
      "epoch:  857, loss: 0.03976356238126755\n",
      "epoch:  858, loss: 0.03976258262991905\n",
      "epoch:  859, loss: 0.03976160287857056\n",
      "epoch:  860, loss: 0.03976062312722206\n",
      "epoch:  861, loss: 0.039759643375873566\n",
      "epoch:  862, loss: 0.03975866734981537\n",
      "epoch:  863, loss: 0.03975768759846687\n",
      "epoch:  864, loss: 0.03975670784711838\n",
      "epoch:  865, loss: 0.039755724370479584\n",
      "epoch:  866, loss: 0.03975474834442139\n",
      "epoch:  867, loss: 0.03975376859307289\n",
      "epoch:  868, loss: 0.039752792567014694\n",
      "epoch:  869, loss: 0.0397518165409565\n",
      "epoch:  870, loss: 0.039750836789608\n",
      "epoch:  871, loss: 0.039749860763549805\n",
      "epoch:  872, loss: 0.03974887728691101\n",
      "epoch:  873, loss: 0.03974789381027222\n",
      "epoch:  874, loss: 0.03974691033363342\n",
      "epoch:  875, loss: 0.03974592685699463\n",
      "epoch:  876, loss: 0.039744939655065536\n",
      "epoch:  877, loss: 0.039743952453136444\n",
      "epoch:  878, loss: 0.03974296525120735\n",
      "epoch:  879, loss: 0.03974197804927826\n",
      "epoch:  880, loss: 0.03974098712205887\n",
      "epoch:  881, loss: 0.03973999246954918\n",
      "epoch:  882, loss: 0.03973900526762009\n",
      "epoch:  883, loss: 0.0397380106151104\n",
      "epoch:  884, loss: 0.03973701223731041\n",
      "epoch:  885, loss: 0.03973602503538132\n",
      "epoch:  886, loss: 0.03973502665758133\n",
      "epoch:  887, loss: 0.03973403200507164\n",
      "epoch:  888, loss: 0.03973303362727165\n",
      "epoch:  889, loss: 0.039732035249471664\n",
      "epoch:  890, loss: 0.039731040596961975\n",
      "epoch:  891, loss: 0.039730045944452286\n",
      "epoch:  892, loss: 0.0397290475666523\n",
      "epoch:  893, loss: 0.03972804918885231\n",
      "epoch:  894, loss: 0.03972705453634262\n",
      "epoch:  895, loss: 0.039726052433252335\n",
      "epoch:  896, loss: 0.03972505033016205\n",
      "epoch:  897, loss: 0.03972404822707176\n",
      "epoch:  898, loss: 0.039723046123981476\n",
      "epoch:  899, loss: 0.03972204029560089\n",
      "epoch:  900, loss: 0.039721034467220306\n",
      "epoch:  901, loss: 0.03972002491354942\n",
      "epoch:  902, loss: 0.03971901908516884\n",
      "epoch:  903, loss: 0.03971800580620766\n",
      "epoch:  904, loss: 0.039716992527246475\n",
      "epoch:  905, loss: 0.039715979248285294\n",
      "epoch:  906, loss: 0.039714958518743515\n",
      "epoch:  907, loss: 0.03971394523978233\n",
      "epoch:  908, loss: 0.039712924510240555\n",
      "epoch:  909, loss: 0.03971190005540848\n",
      "epoch:  910, loss: 0.0397108793258667\n",
      "epoch:  911, loss: 0.03970985859632492\n",
      "epoch:  912, loss: 0.03970883786678314\n",
      "epoch:  913, loss: 0.039707813411951065\n",
      "epoch:  914, loss: 0.039706792682409286\n",
      "epoch:  915, loss: 0.03970576450228691\n",
      "epoch:  916, loss: 0.039704740047454834\n",
      "epoch:  917, loss: 0.03970371186733246\n",
      "epoch:  918, loss: 0.039702679961919785\n",
      "epoch:  919, loss: 0.03970164433121681\n",
      "epoch:  920, loss: 0.03970060870051384\n",
      "epoch:  921, loss: 0.03969956934452057\n",
      "epoch:  922, loss: 0.0396985299885273\n",
      "epoch:  923, loss: 0.03969749063253403\n",
      "epoch:  924, loss: 0.039696451276540756\n",
      "epoch:  925, loss: 0.039695415645837784\n",
      "epoch:  926, loss: 0.03969438001513481\n",
      "epoch:  927, loss: 0.03969334810972214\n",
      "epoch:  928, loss: 0.039692316204309464\n",
      "epoch:  929, loss: 0.03969127684831619\n",
      "epoch:  930, loss: 0.03969024494290352\n",
      "epoch:  931, loss: 0.03968920186161995\n",
      "epoch:  932, loss: 0.03968816250562668\n",
      "epoch:  933, loss: 0.03968712314963341\n",
      "epoch:  934, loss: 0.03968608379364014\n",
      "epoch:  935, loss: 0.03968503698706627\n",
      "epoch:  936, loss: 0.0396839939057827\n",
      "epoch:  937, loss: 0.03968294709920883\n",
      "epoch:  938, loss: 0.039681900292634964\n",
      "epoch:  939, loss: 0.039680853486061096\n",
      "epoch:  940, loss: 0.03967979922890663\n",
      "epoch:  941, loss: 0.039678748697042465\n",
      "epoch:  942, loss: 0.0396777018904686\n",
      "epoch:  943, loss: 0.03967665508389473\n",
      "epoch:  944, loss: 0.03967560827732086\n",
      "epoch:  945, loss: 0.0396745540201664\n",
      "epoch:  946, loss: 0.03967349976301193\n",
      "epoch:  947, loss: 0.039672449231147766\n",
      "epoch:  948, loss: 0.039671391248703\n",
      "epoch:  949, loss: 0.03967033326625824\n",
      "epoch:  950, loss: 0.03966927528381348\n",
      "epoch:  951, loss: 0.03966821730136871\n",
      "epoch:  952, loss: 0.03966715931892395\n",
      "epoch:  953, loss: 0.039666105061769485\n",
      "epoch:  954, loss: 0.039665043354034424\n",
      "epoch:  955, loss: 0.03966398537158966\n",
      "epoch:  956, loss: 0.0396629236638546\n",
      "epoch:  957, loss: 0.03966186195611954\n",
      "epoch:  958, loss: 0.039660800248384476\n",
      "epoch:  959, loss: 0.039659738540649414\n",
      "epoch:  960, loss: 0.039658673107624054\n",
      "epoch:  961, loss: 0.039657607674598694\n",
      "epoch:  962, loss: 0.039656538516283035\n",
      "epoch:  963, loss: 0.03965546935796738\n",
      "epoch:  964, loss: 0.03965440019965172\n",
      "epoch:  965, loss: 0.03965333476662636\n",
      "epoch:  966, loss: 0.0396522656083107\n",
      "epoch:  967, loss: 0.03965119644999504\n",
      "epoch:  968, loss: 0.03965013101696968\n",
      "epoch:  969, loss: 0.03964906185865402\n",
      "epoch:  970, loss: 0.03964799642562866\n",
      "epoch:  971, loss: 0.039646923542022705\n",
      "epoch:  972, loss: 0.03964585065841675\n",
      "epoch:  973, loss: 0.03964477404952049\n",
      "epoch:  974, loss: 0.03964369744062424\n",
      "epoch:  975, loss: 0.03964262083172798\n",
      "epoch:  976, loss: 0.03964153677225113\n",
      "epoch:  977, loss: 0.039640456438064575\n",
      "epoch:  978, loss: 0.03963937237858772\n",
      "epoch:  979, loss: 0.03963829204440117\n",
      "epoch:  980, loss: 0.039637207984924316\n",
      "epoch:  981, loss: 0.039636120200157166\n",
      "epoch:  982, loss: 0.039635028690099716\n",
      "epoch:  983, loss: 0.039633940905332565\n",
      "epoch:  984, loss: 0.039632849395275116\n",
      "epoch:  985, loss: 0.03963175788521767\n",
      "epoch:  986, loss: 0.03963066637516022\n",
      "epoch:  987, loss: 0.03962957113981247\n",
      "epoch:  988, loss: 0.03962847590446472\n",
      "epoch:  989, loss: 0.039627380669116974\n",
      "epoch:  990, loss: 0.039626285433769226\n",
      "epoch:  991, loss: 0.03962519019842148\n",
      "epoch:  992, loss: 0.03962409123778343\n",
      "epoch:  993, loss: 0.039622996002435684\n",
      "epoch:  994, loss: 0.03962189704179764\n",
      "epoch:  995, loss: 0.03962079808115959\n",
      "epoch:  996, loss: 0.039619699120521545\n",
      "epoch:  997, loss: 0.0396185964345932\n",
      "epoch:  998, loss: 0.039617493748664856\n",
      "epoch:  999, loss: 0.03961638733744621\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size=X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "opt = optim.SGD(model.parameters(), lr=2e-2)\n",
    "\n",
    "all_loss = {}\n",
    "for epoch in range(1000):\n",
    "    print(\"epoch: \", epoch, end=\"\")\n",
    "    all_loss[epoch + 1] = 0\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step()\n",
    "\n",
    "        all_loss[epoch + 1] += loss\n",
    "    all_loss[epoch + 1] /= len(data_loader)\n",
    "    print(\", loss: {}\".format(all_loss[epoch + 1].cpu().detach().numpy().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics: R2 = -919.1025453992007\n",
      "Test metrics:  R2 = -936.8878815968839\n"
     ]
    }
   ],
   "source": [
    "pred_train = model.forward(X_train).cpu().detach()\n",
    "pred_test = model.forward(X_test).cpu().detach()\n",
    "print(f\"Train metrics: R2 = {r2_score(pred_train, y_train.cpu())}\")\n",
    "print(f\"Test metrics:  R2 = {r2_score(pred_test, y_test.cpu())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.2655571699142456\n",
      "epoch:  1, loss: 0.20283934473991394\n",
      "epoch:  2, loss: 0.1419653743505478\n",
      "epoch:  3, loss: 0.08759559690952301\n",
      "epoch:  4, loss: 0.04829483851790428\n",
      "epoch:  5, loss: 0.043578069657087326\n",
      "epoch:  6, loss: 0.07290425151586533\n",
      "epoch:  7, loss: 0.07765365391969681\n",
      "epoch:  8, loss: 0.06205832213163376\n",
      "epoch:  9, loss: 0.04667859524488449\n",
      "epoch:  10, loss: 0.03957419842481613\n",
      "epoch:  11, loss: 0.0394400916993618\n",
      "epoch:  12, loss: 0.04261208325624466\n",
      "epoch:  13, loss: 0.04610878974199295\n",
      "epoch:  14, loss: 0.048309653997421265\n",
      "epoch:  15, loss: 0.048936519771814346\n",
      "epoch:  16, loss: 0.048118770122528076\n",
      "epoch:  17, loss: 0.04618070647120476\n",
      "epoch:  18, loss: 0.04358457401394844\n",
      "epoch:  19, loss: 0.04085351526737213\n",
      "epoch:  20, loss: 0.038514621555805206\n",
      "epoch:  21, loss: 0.03702387586236\n",
      "epoch:  22, loss: 0.0366700254380703\n",
      "epoch:  23, loss: 0.03737964481115341\n",
      "epoch:  24, loss: 0.038638636469841\n",
      "epoch:  25, loss: 0.039584722369909286\n",
      "epoch:  26, loss: 0.03949665278196335\n",
      "epoch:  27, loss: 0.03830467537045479\n",
      "epoch:  28, loss: 0.0365639254450798\n",
      "epoch:  29, loss: 0.03497660532593727\n",
      "epoch:  30, loss: 0.033972326666116714\n",
      "epoch:  31, loss: 0.033584438264369965\n",
      "epoch:  32, loss: 0.03355598449707031\n",
      "epoch:  33, loss: 0.03353869542479515\n",
      "epoch:  34, loss: 0.03324895724654198\n",
      "epoch:  35, loss: 0.0325491838157177\n",
      "epoch:  36, loss: 0.03149239346385002\n",
      "epoch:  37, loss: 0.030300315469503403\n",
      "epoch:  38, loss: 0.029287852346897125\n",
      "epoch:  39, loss: 0.028693687170743942\n",
      "epoch:  40, loss: 0.028463374823331833\n",
      "epoch:  41, loss: 0.028185047209262848\n",
      "epoch:  42, loss: 0.027429761365056038\n",
      "epoch:  43, loss: 0.026186078786849976\n",
      "epoch:  44, loss: 0.024808820337057114\n",
      "epoch:  45, loss: 0.02362249232828617\n",
      "epoch:  46, loss: 0.02269355207681656\n",
      "epoch:  47, loss: 0.021795377135276794\n",
      "epoch:  48, loss: 0.020715657621622086\n",
      "epoch:  49, loss: 0.019497446715831757\n",
      "epoch:  50, loss: 0.018393689766526222\n",
      "epoch:  51, loss: 0.017512187361717224\n",
      "epoch:  52, loss: 0.016631988808512688\n",
      "epoch:  53, loss: 0.015614975243806839\n",
      "epoch:  54, loss: 0.014772932976484299\n",
      "epoch:  55, loss: 0.014254975132644176\n",
      "epoch:  56, loss: 0.01375829428434372\n",
      "epoch:  57, loss: 0.01325848326086998\n",
      "epoch:  58, loss: 0.013017475605010986\n",
      "epoch:  59, loss: 0.012794209644198418\n",
      "epoch:  60, loss: 0.012414779514074326\n",
      "epoch:  61, loss: 0.012163680978119373\n",
      "epoch:  62, loss: 0.011905123479664326\n",
      "epoch:  63, loss: 0.011506477370858192\n",
      "epoch:  64, loss: 0.01119378674775362\n",
      "epoch:  65, loss: 0.010925034061074257\n",
      "epoch:  66, loss: 0.010605217888951302\n",
      "epoch:  67, loss: 0.01038187276571989\n",
      "epoch:  68, loss: 0.010246302001178265\n",
      "epoch:  69, loss: 0.01007807906717062\n",
      "epoch:  70, loss: 0.009998244233429432\n",
      "epoch:  71, loss: 0.009973577223718166\n",
      "epoch:  72, loss: 0.009908550418913364\n",
      "epoch:  73, loss: 0.009873893111944199\n",
      "epoch:  74, loss: 0.009855585172772408\n",
      "epoch:  75, loss: 0.009790943004190922\n",
      "epoch:  76, loss: 0.009744478389620781\n",
      "epoch:  77, loss: 0.009712533093988895\n",
      "epoch:  78, loss: 0.009650061838328838\n",
      "epoch:  79, loss: 0.00960623286664486\n",
      "epoch:  80, loss: 0.009567873552441597\n",
      "epoch:  81, loss: 0.009502599015831947\n",
      "epoch:  82, loss: 0.009454051032662392\n",
      "epoch:  83, loss: 0.009406119585037231\n",
      "epoch:  84, loss: 0.00935196690261364\n",
      "epoch:  85, loss: 0.009328092448413372\n",
      "epoch:  86, loss: 0.009297591634094715\n",
      "epoch:  87, loss: 0.009270341135561466\n",
      "epoch:  88, loss: 0.00925392284989357\n",
      "epoch:  89, loss: 0.00921943411231041\n",
      "epoch:  90, loss: 0.00919503066688776\n",
      "epoch:  91, loss: 0.009168688207864761\n",
      "epoch:  92, loss: 0.00913858599960804\n",
      "epoch:  93, loss: 0.009116134606301785\n",
      "epoch:  94, loss: 0.0090833380818367\n",
      "epoch:  95, loss: 0.009055498987436295\n",
      "epoch:  96, loss: 0.00902523659169674\n",
      "epoch:  97, loss: 0.008994481526315212\n",
      "epoch:  98, loss: 0.008971928618848324\n",
      "epoch:  99, loss: 0.00894487276673317\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size=X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "opt = optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "all_loss = {}\n",
    "for epoch in range(100):\n",
    "    print(\"epoch: \", epoch, end=\"\")\n",
    "    all_loss[epoch + 1] = 0\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step()\n",
    "\n",
    "        all_loss[epoch + 1] += loss\n",
    "    all_loss[epoch + 1] /= len(data_loader)\n",
    "    print(\", loss: {}\".format(all_loss[epoch + 1].cpu().detach().cpu().numpy().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics: R2 = 0.7060751516593458\n",
      "Test metrics:  R2 = 0.6821113031185273\n"
     ]
    }
   ],
   "source": [
    "pred_train = model.forward(X_train).cpu().detach()\n",
    "pred_test = model.forward(X_test).cpu().detach()\n",
    "print(f\"Train metrics: R2 = {r2_score(pred_train, y_train.cpu())}\")\n",
    "print(f\"Test metrics:  R2 = {r2_score(pred_test, y_test.cpu())}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_numopt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
