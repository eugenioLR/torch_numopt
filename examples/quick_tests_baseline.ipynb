{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pytorch_soom\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.f1 = nn.Linear(input_size, 10, device=device)\n",
    "        self.f2 = nn.Linear(10, 20, device=device)\n",
    "        self.f3 = nn.Linear(20, 20, device=device)\n",
    "        self.f4 = nn.Linear(20, 10, device=device)\n",
    "        self.f5 = nn.Linear(10, 1, device=device)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        # self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.f1(x))\n",
    "        x = self.activation(self.f2(x))\n",
    "        x = self.activation(self.f3(x))\n",
    "        x = self.activation(self.f4(x))\n",
    "        x = self.f5(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_diabetes(return_X_y = True, scaled=False)\n",
    "\n",
    "X_scaler = MinMaxScaler()\n",
    "X = X_scaler.fit_transform(X)\n",
    "\n",
    "y_scaler = MinMaxScaler()\n",
    "y = y_scaler.fit_transform(y.reshape((-1, 1)))\n",
    "\n",
    "torch_data = TensorDataset(torch.Tensor(X).to(device), torch.Tensor(y).to(device))\n",
    "data_loader = DataLoader(torch_data, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.24149444699287415\n",
      "epoch:  1, loss: 0.23211970925331116\n",
      "epoch:  2, loss: 0.22325003147125244\n",
      "epoch:  3, loss: 0.21485504508018494\n",
      "epoch:  4, loss: 0.20690634846687317\n",
      "epoch:  5, loss: 0.1993778944015503\n",
      "epoch:  6, loss: 0.1922454535961151\n",
      "epoch:  7, loss: 0.1854863464832306\n",
      "epoch:  8, loss: 0.17907896637916565\n",
      "epoch:  9, loss: 0.1730034500360489\n",
      "epoch:  10, loss: 0.16724146902561188\n",
      "epoch:  11, loss: 0.16177572309970856\n",
      "epoch:  12, loss: 0.1565900295972824\n",
      "epoch:  13, loss: 0.15166905522346497\n",
      "epoch:  14, loss: 0.14699888229370117\n",
      "epoch:  15, loss: 0.14256924390792847\n",
      "epoch:  16, loss: 0.13837528228759766\n",
      "epoch:  17, loss: 0.1344127207994461\n",
      "epoch:  18, loss: 0.1307019740343094\n",
      "epoch:  19, loss: 0.1272537112236023\n",
      "epoch:  20, loss: 0.12399578839540482\n",
      "epoch:  21, loss: 0.12089867889881134\n",
      "epoch:  22, loss: 0.11795064061880112\n",
      "epoch:  23, loss: 0.11514674872159958\n",
      "epoch:  24, loss: 0.11247842758893967\n",
      "epoch:  25, loss: 0.10994468629360199\n",
      "epoch:  26, loss: 0.10753866285085678\n",
      "epoch:  27, loss: 0.1052531972527504\n",
      "epoch:  28, loss: 0.10307956486940384\n",
      "epoch:  29, loss: 0.10100878030061722\n",
      "epoch:  30, loss: 0.09903443604707718\n",
      "epoch:  31, loss: 0.09715288132429123\n",
      "epoch:  32, loss: 0.09535834193229675\n",
      "epoch:  33, loss: 0.09364648163318634\n",
      "epoch:  34, loss: 0.09201318770647049\n",
      "epoch:  35, loss: 0.09045464545488358\n",
      "epoch:  36, loss: 0.0889669731259346\n",
      "epoch:  37, loss: 0.08754688501358032\n",
      "epoch:  38, loss: 0.08619112521409988\n",
      "epoch:  39, loss: 0.0848969966173172\n",
      "epoch:  40, loss: 0.08366146683692932\n",
      "epoch:  41, loss: 0.08248177915811539\n",
      "epoch:  42, loss: 0.08135530352592468\n",
      "epoch:  43, loss: 0.08027967065572739\n",
      "epoch:  44, loss: 0.07925238460302353\n",
      "epoch:  45, loss: 0.07827124744653702\n",
      "epoch:  46, loss: 0.0773341953754425\n",
      "epoch:  47, loss: 0.07643923908472061\n",
      "epoch:  48, loss: 0.07558441162109375\n",
      "epoch:  49, loss: 0.07476790249347687\n",
      "epoch:  50, loss: 0.07398798316717148\n",
      "epoch:  51, loss: 0.07324303686618805\n",
      "epoch:  52, loss: 0.07253143936395645\n",
      "epoch:  53, loss: 0.07185190916061401\n",
      "epoch:  54, loss: 0.07120322436094284\n",
      "epoch:  55, loss: 0.07058396190404892\n",
      "epoch:  56, loss: 0.06999340653419495\n",
      "epoch:  57, loss: 0.06943157315254211\n",
      "epoch:  58, loss: 0.06889690458774567\n",
      "epoch:  59, loss: 0.06838850677013397\n",
      "epoch:  60, loss: 0.06790473312139511\n",
      "epoch:  61, loss: 0.06744398921728134\n",
      "epoch:  62, loss: 0.06700430065393448\n",
      "epoch:  63, loss: 0.06658471375703812\n",
      "epoch:  64, loss: 0.06618393957614899\n",
      "epoch:  65, loss: 0.0658012256026268\n",
      "epoch:  66, loss: 0.06543546915054321\n",
      "epoch:  67, loss: 0.06508583575487137\n",
      "epoch:  68, loss: 0.06475211679935455\n",
      "epoch:  69, loss: 0.06443290412425995\n",
      "epoch:  70, loss: 0.06412747502326965\n",
      "epoch:  71, loss: 0.06383537501096725\n",
      "epoch:  72, loss: 0.06355631351470947\n",
      "epoch:  73, loss: 0.06328949332237244\n",
      "epoch:  74, loss: 0.06303419172763824\n",
      "epoch:  75, loss: 0.06279006600379944\n",
      "epoch:  76, loss: 0.06255640834569931\n",
      "epoch:  77, loss: 0.06233276054263115\n",
      "epoch:  78, loss: 0.06211869418621063\n",
      "epoch:  79, loss: 0.06191382557153702\n",
      "epoch:  80, loss: 0.06171771511435509\n",
      "epoch:  81, loss: 0.06152996048331261\n",
      "epoch:  82, loss: 0.061350323259830475\n",
      "epoch:  83, loss: 0.061178404837846756\n",
      "epoch:  84, loss: 0.06101390719413757\n",
      "epoch:  85, loss: 0.06085633859038353\n",
      "epoch:  86, loss: 0.06070544570684433\n",
      "epoch:  87, loss: 0.0605609267950058\n",
      "epoch:  88, loss: 0.060422614216804504\n",
      "epoch:  89, loss: 0.060290273278951645\n",
      "epoch:  90, loss: 0.06016334146261215\n",
      "epoch:  91, loss: 0.06004171818494797\n",
      "epoch:  92, loss: 0.05992510914802551\n",
      "epoch:  93, loss: 0.05981340631842613\n",
      "epoch:  94, loss: 0.059706415981054306\n",
      "epoch:  95, loss: 0.05960400775074959\n",
      "epoch:  96, loss: 0.05950599163770676\n",
      "epoch:  97, loss: 0.05941213294863701\n",
      "epoch:  98, loss: 0.0593222975730896\n",
      "epoch:  99, loss: 0.059236228466033936\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size = X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "opt = optim.SGD(model.parameters(), lr = 1e-2)\n",
    "\n",
    "all_loss = {}\n",
    "for epoch in range(100):\n",
    "    print('epoch: ', epoch, end='')\n",
    "    all_loss[epoch+1] = 0\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step()\n",
    "\n",
    "        all_loss[epoch+1] += loss\n",
    "    all_loss[epoch+1] /= len(data_loader)\n",
    "    print(', loss: {}'.format(all_loss[epoch+1].detach().cpu().numpy().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.0623062402009964\n",
      "epoch:  1, loss: 0.05777193233370781\n",
      "epoch:  2, loss: 0.05699910596013069\n",
      "epoch:  3, loss: 0.057509712874889374\n",
      "epoch:  4, loss: 0.057704027742147446\n",
      "epoch:  5, loss: 0.05710136517882347\n",
      "epoch:  6, loss: 0.05624129995703697\n",
      "epoch:  7, loss: 0.05549498274922371\n",
      "epoch:  8, loss: 0.054914798587560654\n",
      "epoch:  9, loss: 0.05434662103652954\n",
      "epoch:  10, loss: 0.05350053310394287\n",
      "epoch:  11, loss: 0.052188653498888016\n",
      "epoch:  12, loss: 0.050379347056150436\n",
      "epoch:  13, loss: 0.0482637956738472\n",
      "epoch:  14, loss: 0.046167828142642975\n",
      "epoch:  15, loss: 0.04404192417860031\n",
      "epoch:  16, loss: 0.0415063351392746\n",
      "epoch:  17, loss: 0.038924235850572586\n",
      "epoch:  18, loss: 0.03688272461295128\n",
      "epoch:  19, loss: 0.035059381276369095\n",
      "epoch:  20, loss: 0.033763617277145386\n",
      "epoch:  21, loss: 0.03339529410004616\n",
      "epoch:  22, loss: 0.033116746693849564\n",
      "epoch:  23, loss: 0.03317643702030182\n",
      "epoch:  24, loss: 0.03263682499527931\n",
      "epoch:  25, loss: 0.0319666787981987\n",
      "epoch:  26, loss: 0.03092791698873043\n",
      "epoch:  27, loss: 0.030022932216525078\n",
      "epoch:  28, loss: 0.029213745146989822\n",
      "epoch:  29, loss: 0.02877982147037983\n",
      "epoch:  30, loss: 0.02849888615310192\n",
      "epoch:  31, loss: 0.028402160853147507\n",
      "epoch:  32, loss: 0.02824941836297512\n",
      "epoch:  33, loss: 0.02816799283027649\n",
      "epoch:  34, loss: 0.027968795970082283\n",
      "epoch:  35, loss: 0.027895312756299973\n",
      "epoch:  36, loss: 0.027772922068834305\n",
      "epoch:  37, loss: 0.027778971940279007\n",
      "epoch:  38, loss: 0.0277244932949543\n",
      "epoch:  39, loss: 0.02774711139500141\n",
      "epoch:  40, loss: 0.02768869884312153\n",
      "epoch:  41, loss: 0.02763066440820694\n",
      "epoch:  42, loss: 0.027504468336701393\n",
      "epoch:  43, loss: 0.02738393470644951\n",
      "epoch:  44, loss: 0.027285780757665634\n",
      "epoch:  45, loss: 0.027193766087293625\n",
      "epoch:  46, loss: 0.02717982605099678\n",
      "epoch:  47, loss: 0.0271340049803257\n",
      "epoch:  48, loss: 0.02714082971215248\n",
      "epoch:  49, loss: 0.027114052325487137\n",
      "epoch:  50, loss: 0.02708549052476883\n",
      "epoch:  51, loss: 0.027068229392170906\n",
      "epoch:  52, loss: 0.02703290618956089\n",
      "epoch:  53, loss: 0.027005953714251518\n",
      "epoch:  54, loss: 0.026962783187627792\n",
      "epoch:  55, loss: 0.026918156072497368\n",
      "epoch:  56, loss: 0.026876460760831833\n",
      "epoch:  57, loss: 0.02682667039334774\n",
      "epoch:  58, loss: 0.026782337576150894\n",
      "epoch:  59, loss: 0.026732610538601875\n",
      "epoch:  60, loss: 0.02667156048119068\n",
      "epoch:  61, loss: 0.026633737608790398\n",
      "epoch:  62, loss: 0.026591910049319267\n",
      "epoch:  63, loss: 0.026560580357909203\n",
      "epoch:  64, loss: 0.02653464488685131\n",
      "epoch:  65, loss: 0.026501649990677834\n",
      "epoch:  66, loss: 0.02647627517580986\n",
      "epoch:  67, loss: 0.026459436863660812\n",
      "epoch:  68, loss: 0.026430461555719376\n",
      "epoch:  69, loss: 0.026406241580843925\n",
      "epoch:  70, loss: 0.02638484351336956\n",
      "epoch:  71, loss: 0.026350121945142746\n",
      "epoch:  72, loss: 0.026320718228816986\n",
      "epoch:  73, loss: 0.026296252384781837\n",
      "epoch:  74, loss: 0.026266220957040787\n",
      "epoch:  75, loss: 0.026243126019835472\n",
      "epoch:  76, loss: 0.026224881410598755\n",
      "epoch:  77, loss: 0.026205670088529587\n",
      "epoch:  78, loss: 0.026173369958996773\n",
      "epoch:  79, loss: 0.02615257166326046\n",
      "epoch:  80, loss: 0.026141870766878128\n",
      "epoch:  81, loss: 0.026126369833946228\n",
      "epoch:  82, loss: 0.02610418014228344\n",
      "epoch:  83, loss: 0.026077676564455032\n",
      "epoch:  84, loss: 0.02605275996029377\n",
      "epoch:  85, loss: 0.026031946763396263\n",
      "epoch:  86, loss: 0.026019154116511345\n",
      "epoch:  87, loss: 0.02600616216659546\n",
      "epoch:  88, loss: 0.025991134345531464\n",
      "epoch:  89, loss: 0.025966307148337364\n",
      "epoch:  90, loss: 0.02593854069709778\n",
      "epoch:  91, loss: 0.025917014107108116\n",
      "epoch:  92, loss: 0.02590298466384411\n",
      "epoch:  93, loss: 0.025894995778799057\n",
      "epoch:  94, loss: 0.025890348479151726\n",
      "epoch:  95, loss: 0.025884509086608887\n",
      "epoch:  96, loss: 0.025883598253130913\n",
      "epoch:  97, loss: 0.02587105520069599\n",
      "epoch:  98, loss: 0.02585751749575138\n",
      "epoch:  99, loss: 0.02582838572561741\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size = X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "opt = optim.Adam(model.parameters(), lr = 1e-2)\n",
    "\n",
    "all_loss = {}\n",
    "for epoch in range(100):\n",
    "    print('epoch: ', epoch, end='')\n",
    "    all_loss[epoch+1] = 0\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step()\n",
    "\n",
    "        all_loss[epoch+1] += loss\n",
    "    all_loss[epoch+1] /= len(data_loader)\n",
    "    print(', loss: {}'.format(all_loss[epoch+1].detach().cpu().numpy().item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
