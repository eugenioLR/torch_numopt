{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pytorch_soom\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.f1 = nn.Linear(1, 10)\n",
    "        self.f2 = nn.Linear(10, 20)\n",
    "        self.f3 = nn.Linear(20, 20)\n",
    "        self.f4 = nn.Linear(20, 10)\n",
    "        self.f5 = nn.Linear(10, 1)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        # self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.f1(x))\n",
    "        x = self.activation(self.f2(x))\n",
    "        x = self.activation(self.f3(x))\n",
    "        x = self.activation(self.f4(x))\n",
    "        x = self.f5(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.uniform(0, 1, size=(300, 1))\n",
    "# y = X[:, 0] - X[:, 1]**2 + 2 * X[:, 2] * X[:, 3] + (1 / ((1 + X[:, 4]) ** 6))\n",
    "y = np.sinc(X).sum(axis=1, keepdims=True)\n",
    "\n",
    "torch_data = TensorDataset(torch.Tensor(X), torch.Tensor(y))\n",
    "data_loader = DataLoader(torch_data, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.7849858403205872\n",
      "epoch:  1, loss: 0.6961668133735657\n",
      "epoch:  2, loss: 0.6188426613807678\n",
      "epoch:  3, loss: 0.5514369010925293\n",
      "epoch:  4, loss: 0.49262356758117676\n",
      "epoch:  5, loss: 0.44128191471099854\n",
      "epoch:  6, loss: 0.3964559733867645\n",
      "epoch:  7, loss: 0.3573244512081146\n",
      "epoch:  8, loss: 0.32317742705345154\n",
      "epoch:  9, loss: 0.29339954257011414\n",
      "epoch:  10, loss: 0.26753804087638855\n",
      "epoch:  11, loss: 0.245152547955513\n",
      "epoch:  12, loss: 0.2257201075553894\n",
      "epoch:  13, loss: 0.2088526040315628\n",
      "epoch:  14, loss: 0.19421260058879852\n",
      "epoch:  15, loss: 0.1815105676651001\n",
      "epoch:  16, loss: 0.17049908638000488\n",
      "epoch:  17, loss: 0.1609625369310379\n",
      "epoch:  18, loss: 0.15271227061748505\n",
      "epoch:  19, loss: 0.14558278024196625\n",
      "epoch:  20, loss: 0.13942863047122955\n",
      "epoch:  21, loss: 0.13412193953990936\n",
      "epoch:  22, loss: 0.12955059111118317\n",
      "epoch:  23, loss: 0.12561637163162231\n",
      "epoch:  24, loss: 0.1222333088517189\n",
      "epoch:  25, loss: 0.11932641267776489\n",
      "epoch:  26, loss: 0.1168302670121193\n",
      "epoch:  27, loss: 0.11468800157308578\n",
      "epoch:  28, loss: 0.11285015195608139\n",
      "epoch:  29, loss: 0.11127383261919022\n",
      "epoch:  30, loss: 0.10992195457220078\n",
      "epoch:  31, loss: 0.10876256227493286\n",
      "epoch:  32, loss: 0.10776820033788681\n",
      "epoch:  33, loss: 0.10691529512405396\n",
      "epoch:  34, loss: 0.1061837300658226\n",
      "epoch:  35, loss: 0.10555668920278549\n",
      "epoch:  36, loss: 0.1050187423825264\n",
      "epoch:  37, loss: 0.1045568585395813\n",
      "epoch:  38, loss: 0.10416027903556824\n",
      "epoch:  39, loss: 0.10381942987442017\n",
      "epoch:  40, loss: 0.10352569818496704\n",
      "epoch:  41, loss: 0.10327207297086716\n",
      "epoch:  42, loss: 0.10305348038673401\n",
      "epoch:  43, loss: 0.10286438465118408\n",
      "epoch:  44, loss: 0.10270001739263535\n",
      "epoch:  45, loss: 0.10255710035562515\n",
      "epoch:  46, loss: 0.10243264585733414\n",
      "epoch:  47, loss: 0.10232425481081009\n",
      "epoch:  48, loss: 0.10222967714071274\n",
      "epoch:  49, loss: 0.10214638710021973\n",
      "epoch:  50, loss: 0.10207268595695496\n",
      "epoch:  51, loss: 0.10200739651918411\n",
      "epoch:  52, loss: 0.10194900631904602\n",
      "epoch:  53, loss: 0.1018964871764183\n",
      "epoch:  54, loss: 0.10184893757104874\n",
      "epoch:  55, loss: 0.10180547833442688\n",
      "epoch:  56, loss: 0.10176554322242737\n",
      "epoch:  57, loss: 0.1017286479473114\n",
      "epoch:  58, loss: 0.1016944944858551\n",
      "epoch:  59, loss: 0.10166255384683609\n",
      "epoch:  60, loss: 0.10163261741399765\n",
      "epoch:  61, loss: 0.10160420089960098\n",
      "epoch:  62, loss: 0.10157709568738937\n",
      "epoch:  63, loss: 0.10155101865530014\n",
      "epoch:  64, loss: 0.10152582079172134\n",
      "epoch:  65, loss: 0.10150134563446045\n",
      "epoch:  66, loss: 0.10147744417190552\n",
      "epoch:  67, loss: 0.10145408660173416\n",
      "epoch:  68, loss: 0.10143116861581802\n",
      "epoch:  69, loss: 0.10140857100486755\n",
      "epoch:  70, loss: 0.10138624161481857\n",
      "epoch:  71, loss: 0.10136416554450989\n",
      "epoch:  72, loss: 0.10134225338697433\n",
      "epoch:  73, loss: 0.10132045298814774\n",
      "epoch:  74, loss: 0.1012987494468689\n",
      "epoch:  75, loss: 0.10127713531255722\n",
      "epoch:  76, loss: 0.10125555843114853\n",
      "epoch:  77, loss: 0.10123399645090103\n",
      "epoch:  78, loss: 0.10121241956949234\n",
      "epoch:  79, loss: 0.10119084268808365\n",
      "epoch:  80, loss: 0.10116923600435257\n",
      "epoch:  81, loss: 0.10114756971597672\n",
      "epoch:  82, loss: 0.10112582892179489\n",
      "epoch:  83, loss: 0.1011040136218071\n",
      "epoch:  84, loss: 0.10108217597007751\n",
      "epoch:  85, loss: 0.10106021165847778\n",
      "epoch:  86, loss: 0.10103818029165268\n",
      "epoch:  87, loss: 0.10101604461669922\n",
      "epoch:  88, loss: 0.100993812084198\n",
      "epoch:  89, loss: 0.10097149014472961\n",
      "epoch:  90, loss: 0.10094905644655228\n",
      "epoch:  91, loss: 0.10092654079198837\n",
      "epoch:  92, loss: 0.10090389847755432\n",
      "epoch:  93, loss: 0.10088115185499191\n",
      "epoch:  94, loss: 0.10085827112197876\n",
      "epoch:  95, loss: 0.10083526372909546\n",
      "epoch:  96, loss: 0.10081213712692261\n",
      "epoch:  97, loss: 0.10078888386487961\n",
      "epoch:  98, loss: 0.10076551884412766\n",
      "epoch:  99, loss: 0.10074201971292496\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "loss_fn = nn.MSELoss()\n",
    "opt = optim.SGD(model.parameters(), lr = 1e-2)\n",
    "\n",
    "all_loss = {}\n",
    "for epoch in range(100):\n",
    "    print('epoch: ', epoch, end='')\n",
    "    all_loss[epoch+1] = 0\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step()\n",
    "\n",
    "        all_loss[epoch+1] += loss\n",
    "    all_loss[epoch+1] /= len(data_loader)\n",
    "    print(', loss: {}'.format(all_loss[epoch+1].detach().numpy().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.5276046395301819\n",
      "epoch:  1, loss: 0.3672352135181427\n",
      "epoch:  2, loss: 0.20673155784606934\n",
      "epoch:  3, loss: 0.16143134236335754\n",
      "epoch:  4, loss: 0.15939025580883026\n",
      "epoch:  5, loss: 0.10734068602323532\n",
      "epoch:  6, loss: 0.09610405564308167\n",
      "epoch:  7, loss: 0.08673232793807983\n",
      "epoch:  8, loss: 0.06293223053216934\n",
      "epoch:  9, loss: 0.04226066544651985\n",
      "epoch:  10, loss: 0.025672635063529015\n",
      "epoch:  11, loss: 0.007858662866055965\n",
      "epoch:  12, loss: 0.004108670633286238\n",
      "epoch:  13, loss: 0.004339640494436026\n",
      "epoch:  14, loss: 0.007878604345023632\n",
      "epoch:  15, loss: 0.0061804247088730335\n",
      "epoch:  16, loss: 0.0032874392345547676\n",
      "epoch:  17, loss: 0.0011900911340489984\n",
      "epoch:  18, loss: 0.0012900785077363253\n",
      "epoch:  19, loss: 0.0019112005829811096\n",
      "epoch:  20, loss: 0.0017971588531509042\n",
      "epoch:  21, loss: 0.0012107500806450844\n",
      "epoch:  22, loss: 0.0005990276695229113\n",
      "epoch:  23, loss: 0.0005925684818066657\n",
      "epoch:  24, loss: 0.0006833143415860832\n",
      "epoch:  25, loss: 0.0006406173924915493\n",
      "epoch:  26, loss: 0.00044131276081316173\n",
      "epoch:  27, loss: 0.0003152702411171049\n",
      "epoch:  28, loss: 0.00033186411019414663\n",
      "epoch:  29, loss: 0.00033246984821744263\n",
      "epoch:  30, loss: 0.00028315719100646675\n",
      "epoch:  31, loss: 0.00021609645045828074\n",
      "epoch:  32, loss: 0.0002033597556874156\n",
      "epoch:  33, loss: 0.00019396610150579363\n",
      "epoch:  34, loss: 0.00017094983195420355\n",
      "epoch:  35, loss: 0.00014591576473321766\n",
      "epoch:  36, loss: 0.00013100549404043704\n",
      "epoch:  37, loss: 0.0001226712774951011\n",
      "epoch:  38, loss: 0.00011086495942436159\n",
      "epoch:  39, loss: 9.796023368835449e-05\n",
      "epoch:  40, loss: 8.838433859637007e-05\n",
      "epoch:  41, loss: 8.130310015985742e-05\n",
      "epoch:  42, loss: 7.414710125885904e-05\n",
      "epoch:  43, loss: 6.670023140031844e-05\n",
      "epoch:  44, loss: 6.0900503740413114e-05\n",
      "epoch:  45, loss: 5.637803769786842e-05\n",
      "epoch:  46, loss: 5.1360020734136924e-05\n",
      "epoch:  47, loss: 4.683998486143537e-05\n",
      "epoch:  48, loss: 4.3268042645649984e-05\n",
      "epoch:  49, loss: 4.0007416828302667e-05\n",
      "epoch:  50, loss: 3.7040143070043996e-05\n",
      "epoch:  51, loss: 3.453266981523484e-05\n",
      "epoch:  52, loss: 3.2242362067336217e-05\n",
      "epoch:  53, loss: 3.022066994162742e-05\n",
      "epoch:  54, loss: 2.8399526854627766e-05\n",
      "epoch:  55, loss: 2.6810987037606537e-05\n",
      "epoch:  56, loss: 2.5503342840238474e-05\n",
      "epoch:  57, loss: 2.435204260109458e-05\n",
      "epoch:  58, loss: 2.332436815777328e-05\n",
      "epoch:  59, loss: 2.2471585907624103e-05\n",
      "epoch:  60, loss: 2.1725232727476396e-05\n",
      "epoch:  61, loss: 2.1106898202560842e-05\n",
      "epoch:  62, loss: 2.059106009255629e-05\n",
      "epoch:  63, loss: 2.0148519979557022e-05\n",
      "epoch:  64, loss: 1.9782452000072226e-05\n",
      "epoch:  65, loss: 1.9466104276943952e-05\n",
      "epoch:  66, loss: 1.9188644728274085e-05\n",
      "epoch:  67, loss: 1.8942599126603454e-05\n",
      "epoch:  68, loss: 1.8722086679190397e-05\n",
      "epoch:  69, loss: 1.8525113773648627e-05\n",
      "epoch:  70, loss: 1.834629802033305e-05\n",
      "epoch:  71, loss: 1.8180164261138998e-05\n",
      "epoch:  72, loss: 1.8027036276180297e-05\n",
      "epoch:  73, loss: 1.788377994671464e-05\n",
      "epoch:  74, loss: 1.7749742255546153e-05\n",
      "epoch:  75, loss: 1.7624495740165003e-05\n",
      "epoch:  76, loss: 1.7505682990304194e-05\n",
      "epoch:  77, loss: 1.7393036614521407e-05\n",
      "epoch:  78, loss: 1.728506504150573e-05\n",
      "epoch:  79, loss: 1.7181320799863897e-05\n",
      "epoch:  80, loss: 1.708173476799857e-05\n",
      "epoch:  81, loss: 1.69862323673442e-05\n",
      "epoch:  82, loss: 1.6894528016564436e-05\n",
      "epoch:  83, loss: 1.6807089195935987e-05\n",
      "epoch:  84, loss: 1.6721789506846108e-05\n",
      "epoch:  85, loss: 1.663905095483642e-05\n",
      "epoch:  86, loss: 1.6559324649279006e-05\n",
      "epoch:  87, loss: 1.6482392311445437e-05\n",
      "epoch:  88, loss: 1.6408746887464076e-05\n",
      "epoch:  89, loss: 1.6337338820449077e-05\n",
      "epoch:  90, loss: 1.6268300896626897e-05\n",
      "epoch:  91, loss: 1.6201651305891573e-05\n",
      "epoch:  92, loss: 1.6136158592416905e-05\n",
      "epoch:  93, loss: 1.6074778613983653e-05\n",
      "epoch:  94, loss: 1.6015401342883706e-05\n",
      "epoch:  95, loss: 1.5958026779117063e-05\n",
      "epoch:  96, loss: 1.590207284607459e-05\n",
      "epoch:  97, loss: 1.5847565009607933e-05\n",
      "epoch:  98, loss: 1.5795203580637462e-05\n",
      "epoch:  99, loss: 1.574483212607447e-05\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "loss_fn = nn.MSELoss()\n",
    "opt = optim.Adam(model.parameters(), lr = 1e-2)\n",
    "\n",
    "all_loss = {}\n",
    "for epoch in range(100):\n",
    "    print('epoch: ', epoch, end='')\n",
    "    all_loss[epoch+1] = 0\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step()\n",
    "\n",
    "        all_loss[epoch+1] += loss\n",
    "    all_loss[epoch+1] /= len(data_loader)\n",
    "    print(', loss: {}'.format(all_loss[epoch+1].detach().numpy().item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
