{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pytorch_soom\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.f1 = nn.Linear(input_size, 10, device=device)\n",
    "        self.f2 = nn.Linear(10, 20, device=device)\n",
    "        self.f3 = nn.Linear(20, 20, device=device)\n",
    "        self.f4 = nn.Linear(20, 10, device=device)\n",
    "        self.f5 = nn.Linear(10, 1, device=device)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        # self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.f1(x))\n",
    "        x = self.activation(self.f2(x))\n",
    "        x = self.activation(self.f3(x))\n",
    "        x = self.activation(self.f4(x))\n",
    "        x = self.f5(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_diabetes(return_X_y = True, scaled=False)\n",
    "\n",
    "X_scaler = MinMaxScaler()\n",
    "X = X_scaler.fit_transform(X)\n",
    "\n",
    "y_scaler = MinMaxScaler()\n",
    "y = y_scaler.fit_transform(y.reshape((-1, 1)))\n",
    "\n",
    "torch_data = TensorDataset(torch.Tensor(X).to(device), torch.Tensor(y).to(device))\n",
    "data_loader = DataLoader(torch_data, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.2699007987976074\n",
      "epoch:  1, loss: 0.25835636258125305\n",
      "epoch:  2, loss: 0.24751296639442444\n",
      "epoch:  3, loss: 0.2373473048210144\n",
      "epoch:  4, loss: 0.22782915830612183\n",
      "epoch:  5, loss: 0.2188768982887268\n",
      "epoch:  6, loss: 0.2104426771402359\n",
      "epoch:  7, loss: 0.20253190398216248\n",
      "epoch:  8, loss: 0.1951756775379181\n",
      "epoch:  9, loss: 0.18834707140922546\n",
      "epoch:  10, loss: 0.18198102712631226\n",
      "epoch:  11, loss: 0.1759854108095169\n",
      "epoch:  12, loss: 0.17031608521938324\n",
      "epoch:  13, loss: 0.16496112942695618\n",
      "epoch:  14, loss: 0.15989555418491364\n",
      "epoch:  15, loss: 0.15509456396102905\n",
      "epoch:  16, loss: 0.1505279690027237\n",
      "epoch:  17, loss: 0.14618365466594696\n",
      "epoch:  18, loss: 0.14205381274223328\n",
      "epoch:  19, loss: 0.13812384009361267\n",
      "epoch:  20, loss: 0.13438251614570618\n",
      "epoch:  21, loss: 0.13081848621368408\n",
      "epoch:  22, loss: 0.12742391228675842\n",
      "epoch:  23, loss: 0.1241883635520935\n",
      "epoch:  24, loss: 0.12110395729541779\n",
      "epoch:  25, loss: 0.11816363036632538\n",
      "epoch:  26, loss: 0.11536228656768799\n",
      "epoch:  27, loss: 0.11269423365592957\n",
      "epoch:  28, loss: 0.11015494167804718\n",
      "epoch:  29, loss: 0.10774146020412445\n",
      "epoch:  30, loss: 0.10544897615909576\n",
      "epoch:  31, loss: 0.10326999425888062\n",
      "epoch:  32, loss: 0.10119614005088806\n",
      "epoch:  33, loss: 0.09922055155038834\n",
      "epoch:  34, loss: 0.09733929485082626\n",
      "epoch:  35, loss: 0.09554635733366013\n",
      "epoch:  36, loss: 0.09383883327245712\n",
      "epoch:  37, loss: 0.09220898896455765\n",
      "epoch:  38, loss: 0.09065251052379608\n",
      "epoch:  39, loss: 0.08916627615690231\n",
      "epoch:  40, loss: 0.08774751424789429\n",
      "epoch:  41, loss: 0.0863925963640213\n",
      "epoch:  42, loss: 0.08509848266839981\n",
      "epoch:  43, loss: 0.08386251330375671\n",
      "epoch:  44, loss: 0.08268281817436218\n",
      "epoch:  45, loss: 0.08155714720487595\n",
      "epoch:  46, loss: 0.08048631995916367\n",
      "epoch:  47, loss: 0.07946553081274033\n",
      "epoch:  48, loss: 0.07849476486444473\n",
      "epoch:  49, loss: 0.07756970077753067\n",
      "epoch:  50, loss: 0.07669039815664291\n",
      "epoch:  51, loss: 0.07585488259792328\n",
      "epoch:  52, loss: 0.07506192475557327\n",
      "epoch:  53, loss: 0.07430773228406906\n",
      "epoch:  54, loss: 0.07358928769826889\n",
      "epoch:  55, loss: 0.07290531694889069\n",
      "epoch:  56, loss: 0.07225260883569717\n",
      "epoch:  57, loss: 0.07162948697805405\n",
      "epoch:  58, loss: 0.07103534042835236\n",
      "epoch:  59, loss: 0.07046731561422348\n",
      "epoch:  60, loss: 0.06992355734109879\n",
      "epoch:  61, loss: 0.06940355896949768\n",
      "epoch:  62, loss: 0.06890713423490524\n",
      "epoch:  63, loss: 0.06843183934688568\n",
      "epoch:  64, loss: 0.06797634065151215\n",
      "epoch:  65, loss: 0.06754004955291748\n",
      "epoch:  66, loss: 0.06712184101343155\n",
      "epoch:  67, loss: 0.06672130525112152\n",
      "epoch:  68, loss: 0.06633736193180084\n",
      "epoch:  69, loss: 0.06596925854682922\n",
      "epoch:  70, loss: 0.06561634689569473\n",
      "epoch:  71, loss: 0.06527815759181976\n",
      "epoch:  72, loss: 0.06495384126901627\n",
      "epoch:  73, loss: 0.06464280188083649\n",
      "epoch:  74, loss: 0.06434448808431625\n",
      "epoch:  75, loss: 0.06405840069055557\n",
      "epoch:  76, loss: 0.06378412246704102\n",
      "epoch:  77, loss: 0.06352099031209946\n",
      "epoch:  78, loss: 0.06326856464147568\n",
      "epoch:  79, loss: 0.06302640587091446\n",
      "epoch:  80, loss: 0.06279408186674118\n",
      "epoch:  81, loss: 0.06257114559412003\n",
      "epoch:  82, loss: 0.06235719472169876\n",
      "epoch:  83, loss: 0.06215185672044754\n",
      "epoch:  84, loss: 0.061954766511917114\n",
      "epoch:  85, loss: 0.06176553666591644\n",
      "epoch:  86, loss: 0.06158384308218956\n",
      "epoch:  87, loss: 0.061409421265125275\n",
      "epoch:  88, loss: 0.06124204397201538\n",
      "epoch:  89, loss: 0.06108136102557182\n",
      "epoch:  90, loss: 0.06092706695199013\n",
      "epoch:  91, loss: 0.06077890843153\n",
      "epoch:  92, loss: 0.060636527836322784\n",
      "epoch:  93, loss: 0.060499705374240875\n",
      "epoch:  94, loss: 0.060368284583091736\n",
      "epoch:  95, loss: 0.06024201959371567\n",
      "epoch:  96, loss: 0.06012070178985596\n",
      "epoch:  97, loss: 0.06000414118170738\n",
      "epoch:  98, loss: 0.059892043471336365\n",
      "epoch:  99, loss: 0.059784047305583954\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size = X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "opt = optim.SGD(model.parameters(), lr = 1e-2)\n",
    "\n",
    "all_loss = {}\n",
    "for epoch in range(100):\n",
    "    print('epoch: ', epoch, end='')\n",
    "    all_loss[epoch+1] = 0\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step()\n",
    "\n",
    "        all_loss[epoch+1] += loss\n",
    "    all_loss[epoch+1] /= len(data_loader)\n",
    "    print(', loss: {}'.format(all_loss[epoch+1].detach().cpu().numpy().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.13254565000534058\n",
      "epoch:  1, loss: 0.11314540356397629\n",
      "epoch:  2, loss: 0.09315424412488937\n",
      "epoch:  3, loss: 0.07415223121643066\n",
      "epoch:  4, loss: 0.05929703637957573\n",
      "epoch:  5, loss: 0.055472757667303085\n",
      "epoch:  6, loss: 0.06621158123016357\n",
      "epoch:  7, loss: 0.07040335983037949\n",
      "epoch:  8, loss: 0.06514700502157211\n",
      "epoch:  9, loss: 0.058726005256175995\n",
      "epoch:  10, loss: 0.05507079139351845\n",
      "epoch:  11, loss: 0.05429603531956673\n",
      "epoch:  12, loss: 0.05515581741929054\n",
      "epoch:  13, loss: 0.05646618828177452\n",
      "epoch:  14, loss: 0.05750930309295654\n",
      "epoch:  15, loss: 0.05796803906559944\n",
      "epoch:  16, loss: 0.057780053466558456\n",
      "epoch:  17, loss: 0.05703849717974663\n",
      "epoch:  18, loss: 0.05593316629528999\n",
      "epoch:  19, loss: 0.05470661073923111\n",
      "epoch:  20, loss: 0.05362262204289436\n",
      "epoch:  21, loss: 0.052906572818756104\n",
      "epoch:  22, loss: 0.052679792046546936\n",
      "epoch:  23, loss: 0.05288742482662201\n",
      "epoch:  24, loss: 0.05328083783388138\n",
      "epoch:  25, loss: 0.053513988852500916\n",
      "epoch:  26, loss: 0.05333676189184189\n",
      "epoch:  27, loss: 0.05274021252989769\n",
      "epoch:  28, loss: 0.05193367227911949\n",
      "epoch:  29, loss: 0.05118051916360855\n",
      "epoch:  30, loss: 0.050648923963308334\n",
      "epoch:  31, loss: 0.05035540461540222\n",
      "epoch:  32, loss: 0.050202008336782455\n",
      "epoch:  33, loss: 0.050044555217027664\n",
      "epoch:  34, loss: 0.049760811030864716\n",
      "epoch:  35, loss: 0.049288101494312286\n",
      "epoch:  36, loss: 0.04864208772778511\n",
      "epoch:  37, loss: 0.04790312051773071\n",
      "epoch:  38, loss: 0.04718036204576492\n",
      "epoch:  39, loss: 0.04656233265995979\n",
      "epoch:  40, loss: 0.04605284705758095\n",
      "epoch:  41, loss: 0.04554244130849838\n",
      "epoch:  42, loss: 0.04488759860396385\n",
      "epoch:  43, loss: 0.044010601937770844\n",
      "epoch:  44, loss: 0.042990293353796005\n",
      "epoch:  45, loss: 0.04199366271495819\n",
      "epoch:  46, loss: 0.04109564423561096\n",
      "epoch:  47, loss: 0.04025766998529434\n",
      "epoch:  48, loss: 0.03932400792837143\n",
      "epoch:  49, loss: 0.03819519281387329\n",
      "epoch:  50, loss: 0.0369592010974884\n",
      "epoch:  51, loss: 0.03581304848194122\n",
      "epoch:  52, loss: 0.03484085202217102\n",
      "epoch:  53, loss: 0.033884212374687195\n",
      "epoch:  54, loss: 0.03283090144395828\n",
      "epoch:  55, loss: 0.031919196248054504\n",
      "epoch:  56, loss: 0.03133218735456467\n",
      "epoch:  57, loss: 0.03084745816886425\n",
      "epoch:  58, loss: 0.030367618426680565\n",
      "epoch:  59, loss: 0.030198926106095314\n",
      "epoch:  60, loss: 0.030232921242713928\n",
      "epoch:  61, loss: 0.030072087422013283\n",
      "epoch:  62, loss: 0.0300642941147089\n",
      "epoch:  63, loss: 0.030063534155488014\n",
      "epoch:  64, loss: 0.029828345403075218\n",
      "epoch:  65, loss: 0.029664188623428345\n",
      "epoch:  66, loss: 0.02946462295949459\n",
      "epoch:  67, loss: 0.02911531925201416\n",
      "epoch:  68, loss: 0.028868170455098152\n",
      "epoch:  69, loss: 0.02865070104598999\n",
      "epoch:  70, loss: 0.02844112738966942\n",
      "epoch:  71, loss: 0.02834935486316681\n",
      "epoch:  72, loss: 0.02825893461704254\n",
      "epoch:  73, loss: 0.028242068365216255\n",
      "epoch:  74, loss: 0.028252463787794113\n",
      "epoch:  75, loss: 0.02824925258755684\n",
      "epoch:  76, loss: 0.028274940326809883\n",
      "epoch:  77, loss: 0.028264716267585754\n",
      "epoch:  78, loss: 0.028245214372873306\n",
      "epoch:  79, loss: 0.02822323888540268\n",
      "epoch:  80, loss: 0.028171947225928307\n",
      "epoch:  81, loss: 0.028132930397987366\n",
      "epoch:  82, loss: 0.028094084933400154\n",
      "epoch:  83, loss: 0.028048571199178696\n",
      "epoch:  84, loss: 0.02802443318068981\n",
      "epoch:  85, loss: 0.027999337762594223\n",
      "epoch:  86, loss: 0.027979165315628052\n",
      "epoch:  87, loss: 0.027972910553216934\n",
      "epoch:  88, loss: 0.027959773316979408\n",
      "epoch:  89, loss: 0.02795298956334591\n",
      "epoch:  90, loss: 0.02794678509235382\n",
      "epoch:  91, loss: 0.02793257310986519\n",
      "epoch:  92, loss: 0.027923334389925003\n",
      "epoch:  93, loss: 0.027907436713576317\n",
      "epoch:  94, loss: 0.027890808880329132\n",
      "epoch:  95, loss: 0.027878284454345703\n",
      "epoch:  96, loss: 0.02786267176270485\n",
      "epoch:  97, loss: 0.02785239741206169\n",
      "epoch:  98, loss: 0.027844011783599854\n",
      "epoch:  99, loss: 0.027835944667458534\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size = X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "opt = optim.Adam(model.parameters(), lr = 1e-2)\n",
    "\n",
    "all_loss = {}\n",
    "for epoch in range(100):\n",
    "    print('epoch: ', epoch, end='')\n",
    "    all_loss[epoch+1] = 0\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step()\n",
    "\n",
    "        all_loss[epoch+1] += loss\n",
    "    all_loss[epoch+1] /= len(data_loader)\n",
    "    print(', loss: {}'.format(all_loss[epoch+1].detach().cpu().numpy().item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
