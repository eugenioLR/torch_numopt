{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch_numopt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import *\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.f1 = nn.Linear(input_size, 10, device=device)\n",
    "        self.f2 = nn.Linear(10, 20, device=device)\n",
    "        self.f3 = nn.Linear(20, 20, device=device)\n",
    "        self.f4 = nn.Linear(20, 10, device=device)\n",
    "        self.f5 = nn.Linear(10, 1, device=device)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        # self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.f1(x))\n",
    "        x = self.activation(self.f2(x))\n",
    "        x = self.activation(self.f3(x))\n",
    "        x = self.activation(self.f4(x))\n",
    "        x = self.f5(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([800, 10]) torch.Size([800, 1])\n",
      "torch.Size([200, 10]) torch.Size([200, 1])\n"
     ]
    }
   ],
   "source": [
    "# X, y = load_diabetes(return_X_y=True, scaled=False)\n",
    "# X, y = make_regression(n_samples=1000, n_features=100)\n",
    "X, y = make_friedman1(n_samples=1000, noise=1e-2)\n",
    "\n",
    "X_scaler = MinMaxScaler()\n",
    "X = X_scaler.fit_transform(X)\n",
    "X = torch.Tensor(X).to(device)\n",
    "\n",
    "y_scaler = MinMaxScaler()\n",
    "y = y_scaler.fit_transform(y.reshape((-1, 1)))\n",
    "y = torch.Tensor(y).to(device)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "torch_data = TensorDataset(X_train, y_train)\n",
    "data_loader = DataLoader(torch_data, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.1778826117515564\n",
      "epoch:  1, loss: 0.16626380383968353\n",
      "epoch:  2, loss: 0.155645951628685\n",
      "epoch:  3, loss: 0.14594502747058868\n",
      "epoch:  4, loss: 0.13708345592021942\n",
      "epoch:  5, loss: 0.12899428606033325\n",
      "epoch:  6, loss: 0.12160565704107285\n",
      "epoch:  7, loss: 0.11485477536916733\n",
      "epoch:  8, loss: 0.10868828743696213\n",
      "epoch:  9, loss: 0.10305129736661911\n",
      "epoch:  10, loss: 0.09789412468671799\n",
      "epoch:  11, loss: 0.09317369014024734\n",
      "epoch:  12, loss: 0.08884841948747635\n",
      "epoch:  13, loss: 0.08488387614488602\n",
      "epoch:  14, loss: 0.08124840259552002\n",
      "epoch:  15, loss: 0.07791347056627274\n",
      "epoch:  16, loss: 0.07485339045524597\n",
      "epoch:  17, loss: 0.07204516977071762\n",
      "epoch:  18, loss: 0.06946806609630585\n",
      "epoch:  19, loss: 0.06710252910852432\n",
      "epoch:  20, loss: 0.06493081897497177\n",
      "epoch:  21, loss: 0.06293690204620361\n",
      "epoch:  22, loss: 0.0611058734357357\n",
      "epoch:  23, loss: 0.05942436680197716\n",
      "epoch:  24, loss: 0.05788009613752365\n",
      "epoch:  25, loss: 0.05646181106567383\n",
      "epoch:  26, loss: 0.055159084498882294\n",
      "epoch:  27, loss: 0.05396242439746857\n",
      "epoch:  28, loss: 0.05286315828561783\n",
      "epoch:  29, loss: 0.051853302866220474\n",
      "epoch:  30, loss: 0.05092554911971092\n",
      "epoch:  31, loss: 0.05007316544651985\n",
      "epoch:  32, loss: 0.04929003491997719\n",
      "epoch:  33, loss: 0.048570483922958374\n",
      "epoch:  34, loss: 0.04790933430194855\n",
      "epoch:  35, loss: 0.047301825135946274\n",
      "epoch:  36, loss: 0.04674358665943146\n",
      "epoch:  37, loss: 0.046230606734752655\n",
      "epoch:  38, loss: 0.04575920104980469\n",
      "epoch:  39, loss: 0.04532596468925476\n",
      "epoch:  40, loss: 0.04492781683802605\n",
      "epoch:  41, loss: 0.04456188157200813\n",
      "epoch:  42, loss: 0.04422555863857269\n",
      "epoch:  43, loss: 0.04391643404960632\n",
      "epoch:  44, loss: 0.043632302433252335\n",
      "epoch:  45, loss: 0.043371107429265976\n",
      "epoch:  46, loss: 0.04313100874423981\n",
      "epoch:  47, loss: 0.04291027784347534\n",
      "epoch:  48, loss: 0.04270733892917633\n",
      "epoch:  49, loss: 0.042520731687545776\n",
      "epoch:  50, loss: 0.042349155992269516\n",
      "epoch:  51, loss: 0.04219137132167816\n",
      "epoch:  52, loss: 0.04204624891281128\n",
      "epoch:  53, loss: 0.041912760585546494\n",
      "epoch:  54, loss: 0.04178997501730919\n",
      "epoch:  55, loss: 0.041677024215459824\n",
      "epoch:  56, loss: 0.041573088616132736\n",
      "epoch:  57, loss: 0.041477467864751816\n",
      "epoch:  58, loss: 0.04138951376080513\n",
      "epoch:  59, loss: 0.04130853712558746\n",
      "epoch:  60, loss: 0.04123394936323166\n",
      "epoch:  61, loss: 0.04116528853774071\n",
      "epoch:  62, loss: 0.041102085262537\n",
      "epoch:  63, loss: 0.041043877601623535\n",
      "epoch:  64, loss: 0.04099027439951897\n",
      "epoch:  65, loss: 0.040940940380096436\n",
      "epoch:  66, loss: 0.04089551419019699\n",
      "epoch:  67, loss: 0.040853604674339294\n",
      "epoch:  68, loss: 0.04081498831510544\n",
      "epoch:  69, loss: 0.04077945277094841\n",
      "epoch:  70, loss: 0.04074671119451523\n",
      "epoch:  71, loss: 0.04071652889251709\n",
      "epoch:  72, loss: 0.04068870097398758\n",
      "epoch:  73, loss: 0.04066307097673416\n",
      "epoch:  74, loss: 0.04063943400979042\n",
      "epoch:  75, loss: 0.04061764478683472\n",
      "epoch:  76, loss: 0.04059750959277153\n",
      "epoch:  77, loss: 0.04057883471250534\n",
      "epoch:  78, loss: 0.040561575442552567\n",
      "epoch:  79, loss: 0.040545642375946045\n",
      "epoch:  80, loss: 0.040530890226364136\n",
      "epoch:  81, loss: 0.04051720350980759\n",
      "epoch:  82, loss: 0.04050448164343834\n",
      "epoch:  83, loss: 0.04049267992377281\n",
      "epoch:  84, loss: 0.040481746196746826\n",
      "epoch:  85, loss: 0.040471576154232025\n",
      "epoch:  86, loss: 0.04046209156513214\n",
      "epoch:  87, loss: 0.04045325145125389\n",
      "epoch:  88, loss: 0.040444985032081604\n",
      "epoch:  89, loss: 0.04043731093406677\n",
      "epoch:  90, loss: 0.040430136024951935\n",
      "epoch:  91, loss: 0.04042350500822067\n",
      "epoch:  92, loss: 0.04041736572980881\n",
      "epoch:  93, loss: 0.04041164368391037\n",
      "epoch:  94, loss: 0.040406327694654465\n",
      "epoch:  95, loss: 0.04040138050913811\n",
      "epoch:  96, loss: 0.04039674252271652\n",
      "epoch:  97, loss: 0.04039240628480911\n",
      "epoch:  98, loss: 0.04038836061954498\n",
      "epoch:  99, loss: 0.04038451984524727\n",
      "epoch:  100, loss: 0.04038090631365776\n",
      "epoch:  101, loss: 0.040377482771873474\n",
      "epoch:  102, loss: 0.0403742678463459\n",
      "epoch:  103, loss: 0.040371205657720566\n",
      "epoch:  104, loss: 0.04036833345890045\n",
      "epoch:  105, loss: 0.04036569967865944\n",
      "epoch:  106, loss: 0.04036317765712738\n",
      "epoch:  107, loss: 0.040360745042562485\n",
      "epoch:  108, loss: 0.040358416736125946\n",
      "epoch:  109, loss: 0.04035628214478493\n",
      "epoch:  110, loss: 0.040354304015636444\n",
      "epoch:  111, loss: 0.04035245627164841\n",
      "epoch:  112, loss: 0.04035070165991783\n",
      "epoch:  113, loss: 0.04034903272986412\n",
      "epoch:  114, loss: 0.04034740477800369\n",
      "epoch:  115, loss: 0.04034584388136864\n",
      "epoch:  116, loss: 0.040344346314668655\n",
      "epoch:  117, loss: 0.04034291207790375\n",
      "epoch:  118, loss: 0.040341511368751526\n",
      "epoch:  119, loss: 0.04034017026424408\n",
      "epoch:  120, loss: 0.0403389073908329\n",
      "epoch:  121, loss: 0.04033772274851799\n",
      "epoch:  122, loss: 0.04033661633729935\n",
      "epoch:  123, loss: 0.04033557325601578\n",
      "epoch:  124, loss: 0.040334552526474\n",
      "epoch:  125, loss: 0.0403335802257061\n",
      "epoch:  126, loss: 0.04033263027667999\n",
      "epoch:  127, loss: 0.040331725031137466\n",
      "epoch:  128, loss: 0.04033083841204643\n",
      "epoch:  129, loss: 0.0403299480676651\n",
      "epoch:  130, loss: 0.040329113602638245\n",
      "epoch:  131, loss: 0.04032832011580467\n",
      "epoch:  132, loss: 0.040327537804841995\n",
      "epoch:  133, loss: 0.0403268001973629\n",
      "epoch:  134, loss: 0.0403260812163353\n",
      "epoch:  135, loss: 0.04032536596059799\n",
      "epoch:  136, loss: 0.04032467305660248\n",
      "epoch:  137, loss: 0.040323998779058456\n",
      "epoch:  138, loss: 0.040323369204998016\n",
      "epoch:  139, loss: 0.040322765707969666\n",
      "epoch:  140, loss: 0.04032216593623161\n",
      "epoch:  141, loss: 0.040321577340364456\n",
      "epoch:  142, loss: 0.040320996195077896\n",
      "epoch:  143, loss: 0.04032043740153313\n",
      "epoch:  144, loss: 0.04031990095973015\n",
      "epoch:  145, loss: 0.040319375693798065\n",
      "epoch:  146, loss: 0.04031885042786598\n",
      "epoch:  147, loss: 0.040318332612514496\n",
      "epoch:  148, loss: 0.04031781107187271\n",
      "epoch:  149, loss: 0.04031731188297272\n",
      "epoch:  150, loss: 0.04031682759523392\n",
      "epoch:  151, loss: 0.04031636193394661\n",
      "epoch:  152, loss: 0.040315885096788406\n",
      "epoch:  153, loss: 0.0403154119849205\n",
      "epoch:  154, loss: 0.0403149388730526\n",
      "epoch:  155, loss: 0.04031447693705559\n",
      "epoch:  156, loss: 0.040314044803380966\n",
      "epoch:  157, loss: 0.040313635021448135\n",
      "epoch:  158, loss: 0.0403132326900959\n",
      "epoch:  159, loss: 0.040312837809324265\n",
      "epoch:  160, loss: 0.040312446653842926\n",
      "epoch:  161, loss: 0.04031205177307129\n",
      "epoch:  162, loss: 0.04031165689229965\n",
      "epoch:  163, loss: 0.04031125083565712\n",
      "epoch:  164, loss: 0.040310848504304886\n",
      "epoch:  165, loss: 0.04031044989824295\n",
      "epoch:  166, loss: 0.04031006619334221\n",
      "epoch:  167, loss: 0.04030968248844147\n",
      "epoch:  168, loss: 0.040309298783540726\n",
      "epoch:  169, loss: 0.040308915078639984\n",
      "epoch:  170, loss: 0.040308527648448944\n",
      "epoch:  171, loss: 0.04030812159180641\n",
      "epoch:  172, loss: 0.04030770808458328\n",
      "epoch:  173, loss: 0.04030730202794075\n",
      "epoch:  174, loss: 0.04030691832304001\n",
      "epoch:  175, loss: 0.040306538343429565\n",
      "epoch:  176, loss: 0.04030615836381912\n",
      "epoch:  177, loss: 0.040305785834789276\n",
      "epoch:  178, loss: 0.04030543193221092\n",
      "epoch:  179, loss: 0.04030507802963257\n",
      "epoch:  180, loss: 0.04030473530292511\n",
      "epoch:  181, loss: 0.04030439630150795\n",
      "epoch:  182, loss: 0.04030405357480049\n",
      "epoch:  183, loss: 0.040303707122802734\n",
      "epoch:  184, loss: 0.040303364396095276\n",
      "epoch:  185, loss: 0.04030301421880722\n",
      "epoch:  186, loss: 0.04030267521739006\n",
      "epoch:  187, loss: 0.0403023324906826\n",
      "epoch:  188, loss: 0.04030200466513634\n",
      "epoch:  189, loss: 0.04030168056488037\n",
      "epoch:  190, loss: 0.04030134901404381\n",
      "epoch:  191, loss: 0.04030102491378784\n",
      "epoch:  192, loss: 0.04030069708824158\n",
      "epoch:  193, loss: 0.04030036926269531\n",
      "epoch:  194, loss: 0.04030003398656845\n",
      "epoch:  195, loss: 0.04029970243573189\n",
      "epoch:  196, loss: 0.040299370884895325\n",
      "epoch:  197, loss: 0.04029904305934906\n",
      "epoch:  198, loss: 0.040298715233802795\n",
      "epoch:  199, loss: 0.04029839113354683\n",
      "epoch:  200, loss: 0.04029806703329086\n",
      "epoch:  201, loss: 0.040297746658325195\n",
      "epoch:  202, loss: 0.040297430008649826\n",
      "epoch:  203, loss: 0.040297120809555054\n",
      "epoch:  204, loss: 0.04029681533575058\n",
      "epoch:  205, loss: 0.0402965173125267\n",
      "epoch:  206, loss: 0.04029621556401253\n",
      "epoch:  207, loss: 0.04029591381549835\n",
      "epoch:  208, loss: 0.040295619517564774\n",
      "epoch:  209, loss: 0.0402953140437603\n",
      "epoch:  210, loss: 0.04029501974582672\n",
      "epoch:  211, loss: 0.040294717997312546\n",
      "epoch:  212, loss: 0.04029441624879837\n",
      "epoch:  213, loss: 0.04029412195086479\n",
      "epoch:  214, loss: 0.04029382765293121\n",
      "epoch:  215, loss: 0.040293529629707336\n",
      "epoch:  216, loss: 0.04029323533177376\n",
      "epoch:  217, loss: 0.04029293358325958\n",
      "epoch:  218, loss: 0.040292635560035706\n",
      "epoch:  219, loss: 0.04029233381152153\n",
      "epoch:  220, loss: 0.040292028337717056\n",
      "epoch:  221, loss: 0.04029172658920288\n",
      "epoch:  222, loss: 0.040291428565979004\n",
      "epoch:  223, loss: 0.04029112681746483\n",
      "epoch:  224, loss: 0.04029082506895065\n",
      "epoch:  225, loss: 0.04029052332043648\n",
      "epoch:  226, loss: 0.0402902215719223\n",
      "epoch:  227, loss: 0.04028991982340813\n",
      "epoch:  228, loss: 0.04028962180018425\n",
      "epoch:  229, loss: 0.04028932377696037\n",
      "epoch:  230, loss: 0.040289029479026794\n",
      "epoch:  231, loss: 0.040288735181093216\n",
      "epoch:  232, loss: 0.04028843715786934\n",
      "epoch:  233, loss: 0.04028814285993576\n",
      "epoch:  234, loss: 0.040287844836711884\n",
      "epoch:  235, loss: 0.04028754308819771\n",
      "epoch:  236, loss: 0.04028724133968353\n",
      "epoch:  237, loss: 0.040286947041749954\n",
      "epoch:  238, loss: 0.04028664901852608\n",
      "epoch:  239, loss: 0.0402863547205925\n",
      "epoch:  240, loss: 0.04028606414794922\n",
      "epoch:  241, loss: 0.04028576612472534\n",
      "epoch:  242, loss: 0.04028547182679176\n",
      "epoch:  243, loss: 0.040285177528858185\n",
      "epoch:  244, loss: 0.0402848906815052\n",
      "epoch:  245, loss: 0.040284596383571625\n",
      "epoch:  246, loss: 0.04028429836034775\n",
      "epoch:  247, loss: 0.04028400778770447\n",
      "epoch:  248, loss: 0.04028371721506119\n",
      "epoch:  249, loss: 0.04028342664241791\n",
      "epoch:  250, loss: 0.04028313234448433\n",
      "epoch:  251, loss: 0.04028283432126045\n",
      "epoch:  252, loss: 0.04028254374861717\n",
      "epoch:  253, loss: 0.040282249450683594\n",
      "epoch:  254, loss: 0.04028196260333061\n",
      "epoch:  255, loss: 0.04028167575597763\n",
      "epoch:  256, loss: 0.04028138518333435\n",
      "epoch:  257, loss: 0.04028109461069107\n",
      "epoch:  258, loss: 0.04028080031275749\n",
      "epoch:  259, loss: 0.040280502289533615\n",
      "epoch:  260, loss: 0.04028020799160004\n",
      "epoch:  261, loss: 0.040279921144247055\n",
      "epoch:  262, loss: 0.040279630571603775\n",
      "epoch:  263, loss: 0.04027934372425079\n",
      "epoch:  264, loss: 0.04027905687689781\n",
      "epoch:  265, loss: 0.040278781205415726\n",
      "epoch:  266, loss: 0.04027850180864334\n",
      "epoch:  267, loss: 0.040278222411870956\n",
      "epoch:  268, loss: 0.04027794301509857\n",
      "epoch:  269, loss: 0.040277667343616486\n",
      "epoch:  270, loss: 0.0402773879468441\n",
      "epoch:  271, loss: 0.04027711600065231\n",
      "epoch:  272, loss: 0.04027684032917023\n",
      "epoch:  273, loss: 0.04027656465768814\n",
      "epoch:  274, loss: 0.04027629643678665\n",
      "epoch:  275, loss: 0.04027603194117546\n",
      "epoch:  276, loss: 0.04027576372027397\n",
      "epoch:  277, loss: 0.040275491774082184\n",
      "epoch:  278, loss: 0.040275219827890396\n",
      "epoch:  279, loss: 0.04027495160698891\n",
      "epoch:  280, loss: 0.04027467966079712\n",
      "epoch:  281, loss: 0.04027440771460533\n",
      "epoch:  282, loss: 0.04027414321899414\n",
      "epoch:  283, loss: 0.04027387127280235\n",
      "epoch:  284, loss: 0.040273603051900864\n",
      "epoch:  285, loss: 0.040273331105709076\n",
      "epoch:  286, loss: 0.04027306288480759\n",
      "epoch:  287, loss: 0.0402727946639061\n",
      "epoch:  288, loss: 0.04027253016829491\n",
      "epoch:  289, loss: 0.04027225822210312\n",
      "epoch:  290, loss: 0.04027199745178223\n",
      "epoch:  291, loss: 0.04027172923088074\n",
      "epoch:  292, loss: 0.04027145728468895\n",
      "epoch:  293, loss: 0.04027118533849716\n",
      "epoch:  294, loss: 0.040270913392305374\n",
      "epoch:  295, loss: 0.040270641446113586\n",
      "epoch:  296, loss: 0.0402703657746315\n",
      "epoch:  297, loss: 0.04027009755373001\n",
      "epoch:  298, loss: 0.040269821882247925\n",
      "epoch:  299, loss: 0.04026954993605614\n",
      "epoch:  300, loss: 0.04026927426457405\n",
      "epoch:  301, loss: 0.04026900231838226\n",
      "epoch:  302, loss: 0.04026872664690018\n",
      "epoch:  303, loss: 0.04026845470070839\n",
      "epoch:  304, loss: 0.0402681827545166\n",
      "epoch:  305, loss: 0.040267907083034515\n",
      "epoch:  306, loss: 0.04026763513684273\n",
      "epoch:  307, loss: 0.04026736319065094\n",
      "epoch:  308, loss: 0.04026708006858826\n",
      "epoch:  309, loss: 0.04026680812239647\n",
      "epoch:  310, loss: 0.04026653245091438\n",
      "epoch:  311, loss: 0.0402662567794323\n",
      "epoch:  312, loss: 0.04026597738265991\n",
      "epoch:  313, loss: 0.040265705436468124\n",
      "epoch:  314, loss: 0.04026542976498604\n",
      "epoch:  315, loss: 0.04026515781879425\n",
      "epoch:  316, loss: 0.040264878422021866\n",
      "epoch:  317, loss: 0.04026460647583008\n",
      "epoch:  318, loss: 0.040264323353767395\n",
      "epoch:  319, loss: 0.04026404395699501\n",
      "epoch:  320, loss: 0.040263764560222626\n",
      "epoch:  321, loss: 0.04026348888874054\n",
      "epoch:  322, loss: 0.04026321321725845\n",
      "epoch:  323, loss: 0.04026293754577637\n",
      "epoch:  324, loss: 0.04026266559958458\n",
      "epoch:  325, loss: 0.04026239365339279\n",
      "epoch:  326, loss: 0.040262117981910706\n",
      "epoch:  327, loss: 0.04026184603571892\n",
      "epoch:  328, loss: 0.04026157408952713\n",
      "epoch:  329, loss: 0.040261294692754745\n",
      "epoch:  330, loss: 0.04026102274656296\n",
      "epoch:  331, loss: 0.04026075452566147\n",
      "epoch:  332, loss: 0.040260475128889084\n",
      "epoch:  333, loss: 0.040260199457407\n",
      "epoch:  334, loss: 0.04025992751121521\n",
      "epoch:  335, loss: 0.04025965556502342\n",
      "epoch:  336, loss: 0.040259379893541336\n",
      "epoch:  337, loss: 0.04025910422205925\n",
      "epoch:  338, loss: 0.04025883600115776\n",
      "epoch:  339, loss: 0.04025856405496597\n",
      "epoch:  340, loss: 0.040258292108774185\n",
      "epoch:  341, loss: 0.040258023887872696\n",
      "epoch:  342, loss: 0.04025774821639061\n",
      "epoch:  343, loss: 0.04025747999548912\n",
      "epoch:  344, loss: 0.04025721549987793\n",
      "epoch:  345, loss: 0.04025694355368614\n",
      "epoch:  346, loss: 0.040256671607494354\n",
      "epoch:  347, loss: 0.040256403386592865\n",
      "epoch:  348, loss: 0.04025612771511078\n",
      "epoch:  349, loss: 0.04025585949420929\n",
      "epoch:  350, loss: 0.0402555875480175\n",
      "epoch:  351, loss: 0.040255315601825714\n",
      "epoch:  352, loss: 0.04025505110621452\n",
      "epoch:  353, loss: 0.040254779160022736\n",
      "epoch:  354, loss: 0.04025450721383095\n",
      "epoch:  355, loss: 0.04025423526763916\n",
      "epoch:  356, loss: 0.04025396332144737\n",
      "epoch:  357, loss: 0.040253691375255585\n",
      "epoch:  358, loss: 0.0402534194290638\n",
      "epoch:  359, loss: 0.04025314748287201\n",
      "epoch:  360, loss: 0.04025287553668022\n",
      "epoch:  361, loss: 0.04025260731577873\n",
      "epoch:  362, loss: 0.040252335369586945\n",
      "epoch:  363, loss: 0.04025206342339516\n",
      "epoch:  364, loss: 0.04025178775191307\n",
      "epoch:  365, loss: 0.04025151953101158\n",
      "epoch:  366, loss: 0.040251247584819794\n",
      "epoch:  367, loss: 0.040250979363918304\n",
      "epoch:  368, loss: 0.04025070741772652\n",
      "epoch:  369, loss: 0.04025043547153473\n",
      "epoch:  370, loss: 0.040250156074762344\n",
      "epoch:  371, loss: 0.04024988412857056\n",
      "epoch:  372, loss: 0.04024961218237877\n",
      "epoch:  373, loss: 0.04024933651089668\n",
      "epoch:  374, loss: 0.0402490608394146\n",
      "epoch:  375, loss: 0.04024878889322281\n",
      "epoch:  376, loss: 0.04024851694703102\n",
      "epoch:  377, loss: 0.040248241275548935\n",
      "epoch:  378, loss: 0.04024796560406685\n",
      "epoch:  379, loss: 0.040247686207294464\n",
      "epoch:  380, loss: 0.04024741053581238\n",
      "epoch:  381, loss: 0.04024713486433029\n",
      "epoch:  382, loss: 0.04024685174226761\n",
      "epoch:  383, loss: 0.040246572345495224\n",
      "epoch:  384, loss: 0.04024628922343254\n",
      "epoch:  385, loss: 0.040246009826660156\n",
      "epoch:  386, loss: 0.04024572670459747\n",
      "epoch:  387, loss: 0.04024544730782509\n",
      "epoch:  388, loss: 0.040245167911052704\n",
      "epoch:  389, loss: 0.04024488851428032\n",
      "epoch:  390, loss: 0.040244605392217636\n",
      "epoch:  391, loss: 0.04024432227015495\n",
      "epoch:  392, loss: 0.04024403914809227\n",
      "epoch:  393, loss: 0.040243759751319885\n",
      "epoch:  394, loss: 0.0402434766292572\n",
      "epoch:  395, loss: 0.04024318978190422\n",
      "epoch:  396, loss: 0.040242910385131836\n",
      "epoch:  397, loss: 0.04024263471364975\n",
      "epoch:  398, loss: 0.040242355316877365\n",
      "epoch:  399, loss: 0.04024207592010498\n",
      "epoch:  400, loss: 0.0402417927980423\n",
      "epoch:  401, loss: 0.04024151712656021\n",
      "epoch:  402, loss: 0.04024123400449753\n",
      "epoch:  403, loss: 0.04024095460772514\n",
      "epoch:  404, loss: 0.04024067893624306\n",
      "epoch:  405, loss: 0.040240392088890076\n",
      "epoch:  406, loss: 0.04024010896682739\n",
      "epoch:  407, loss: 0.04023982584476471\n",
      "epoch:  408, loss: 0.04023953527212143\n",
      "epoch:  409, loss: 0.04023924842476845\n",
      "epoch:  410, loss: 0.040238961577415466\n",
      "epoch:  411, loss: 0.040238671004772186\n",
      "epoch:  412, loss: 0.040238380432128906\n",
      "epoch:  413, loss: 0.040238093584775925\n",
      "epoch:  414, loss: 0.04023780673742294\n",
      "epoch:  415, loss: 0.04023751616477966\n",
      "epoch:  416, loss: 0.04023722559213638\n",
      "epoch:  417, loss: 0.0402369387447834\n",
      "epoch:  418, loss: 0.04023664817214012\n",
      "epoch:  419, loss: 0.04023635759949684\n",
      "epoch:  420, loss: 0.04023606702685356\n",
      "epoch:  421, loss: 0.04023577645421028\n",
      "epoch:  422, loss: 0.040235485881567\n",
      "epoch:  423, loss: 0.04023519530892372\n",
      "epoch:  424, loss: 0.04023490473628044\n",
      "epoch:  425, loss: 0.04023461416363716\n",
      "epoch:  426, loss: 0.040234316140413284\n",
      "epoch:  427, loss: 0.0402340292930603\n",
      "epoch:  428, loss: 0.040233734995126724\n",
      "epoch:  429, loss: 0.040233440697193146\n",
      "epoch:  430, loss: 0.04023314267396927\n",
      "epoch:  431, loss: 0.04023284837603569\n",
      "epoch:  432, loss: 0.04023255780339241\n",
      "epoch:  433, loss: 0.04023226723074913\n",
      "epoch:  434, loss: 0.04023196920752525\n",
      "epoch:  435, loss: 0.040231674909591675\n",
      "epoch:  436, loss: 0.040231384336948395\n",
      "epoch:  437, loss: 0.040231093764305115\n",
      "epoch:  438, loss: 0.040230799466371536\n",
      "epoch:  439, loss: 0.04023050516843796\n",
      "epoch:  440, loss: 0.04023021087050438\n",
      "epoch:  441, loss: 0.040229909121990204\n",
      "epoch:  442, loss: 0.040229614824056625\n",
      "epoch:  443, loss: 0.04022932052612305\n",
      "epoch:  444, loss: 0.04022901877760887\n",
      "epoch:  445, loss: 0.04022872447967529\n",
      "epoch:  446, loss: 0.040228426456451416\n",
      "epoch:  447, loss: 0.04022812843322754\n",
      "epoch:  448, loss: 0.040227826684713364\n",
      "epoch:  449, loss: 0.04022752493619919\n",
      "epoch:  450, loss: 0.04022722691297531\n",
      "epoch:  451, loss: 0.040226925164461136\n",
      "epoch:  452, loss: 0.04022661969065666\n",
      "epoch:  453, loss: 0.040226321667432785\n",
      "epoch:  454, loss: 0.04022601619362831\n",
      "epoch:  455, loss: 0.040225714445114136\n",
      "epoch:  456, loss: 0.04022540524601936\n",
      "epoch:  457, loss: 0.04022510349750519\n",
      "epoch:  458, loss: 0.040224798023700714\n",
      "epoch:  459, loss: 0.04022449254989624\n",
      "epoch:  460, loss: 0.040224187076091766\n",
      "epoch:  461, loss: 0.04022388532757759\n",
      "epoch:  462, loss: 0.04022357985377312\n",
      "epoch:  463, loss: 0.04022327810525894\n",
      "epoch:  464, loss: 0.04022296890616417\n",
      "epoch:  465, loss: 0.040222663432359695\n",
      "epoch:  466, loss: 0.04022236168384552\n",
      "epoch:  467, loss: 0.04022205248475075\n",
      "epoch:  468, loss: 0.04022175073623657\n",
      "epoch:  469, loss: 0.0402214415371418\n",
      "epoch:  470, loss: 0.04022114351391792\n",
      "epoch:  471, loss: 0.04022083058953285\n",
      "epoch:  472, loss: 0.04022052511572838\n",
      "epoch:  473, loss: 0.0402202233672142\n",
      "epoch:  474, loss: 0.04021991789340973\n",
      "epoch:  475, loss: 0.04021960496902466\n",
      "epoch:  476, loss: 0.040219295769929886\n",
      "epoch:  477, loss: 0.040218986570835114\n",
      "epoch:  478, loss: 0.04021867737174034\n",
      "epoch:  479, loss: 0.04021836817264557\n",
      "epoch:  480, loss: 0.0402180552482605\n",
      "epoch:  481, loss: 0.04021774232387543\n",
      "epoch:  482, loss: 0.040217433124780655\n",
      "epoch:  483, loss: 0.04021712392568588\n",
      "epoch:  484, loss: 0.04021680727601051\n",
      "epoch:  485, loss: 0.04021649807691574\n",
      "epoch:  486, loss: 0.04021618142724037\n",
      "epoch:  487, loss: 0.0402158722281456\n",
      "epoch:  488, loss: 0.04021556302905083\n",
      "epoch:  489, loss: 0.040215253829956055\n",
      "epoch:  490, loss: 0.04021494463086128\n",
      "epoch:  491, loss: 0.04021462798118591\n",
      "epoch:  492, loss: 0.04021431878209114\n",
      "epoch:  493, loss: 0.04021400213241577\n",
      "epoch:  494, loss: 0.0402136892080307\n",
      "epoch:  495, loss: 0.04021338000893593\n",
      "epoch:  496, loss: 0.04021306335926056\n",
      "epoch:  497, loss: 0.04021275416016579\n",
      "epoch:  498, loss: 0.040212441235780716\n",
      "epoch:  499, loss: 0.04021212458610535\n",
      "epoch:  500, loss: 0.040211811661720276\n",
      "epoch:  501, loss: 0.04021149501204491\n",
      "epoch:  502, loss: 0.040211185812950134\n",
      "epoch:  503, loss: 0.040210869163274765\n",
      "epoch:  504, loss: 0.040210556238889694\n",
      "epoch:  505, loss: 0.04021024703979492\n",
      "epoch:  506, loss: 0.04020993039011955\n",
      "epoch:  507, loss: 0.04020962491631508\n",
      "epoch:  508, loss: 0.04020931199193001\n",
      "epoch:  509, loss: 0.04020899534225464\n",
      "epoch:  510, loss: 0.040208686143159866\n",
      "epoch:  511, loss: 0.040208373218774796\n",
      "epoch:  512, loss: 0.040208056569099426\n",
      "epoch:  513, loss: 0.040207747370004654\n",
      "epoch:  514, loss: 0.04020743444561958\n",
      "epoch:  515, loss: 0.040207117795944214\n",
      "epoch:  516, loss: 0.04020680487155914\n",
      "epoch:  517, loss: 0.040206488221883774\n",
      "epoch:  518, loss: 0.0402061752974987\n",
      "epoch:  519, loss: 0.040205858647823334\n",
      "epoch:  520, loss: 0.040205538272857666\n",
      "epoch:  521, loss: 0.040205225348472595\n",
      "epoch:  522, loss: 0.040204908698797226\n",
      "epoch:  523, loss: 0.04020459204912186\n",
      "epoch:  524, loss: 0.04020427539944649\n",
      "epoch:  525, loss: 0.04020396247506142\n",
      "epoch:  526, loss: 0.04020364210009575\n",
      "epoch:  527, loss: 0.04020332172513008\n",
      "epoch:  528, loss: 0.04020300880074501\n",
      "epoch:  529, loss: 0.04020268842577934\n",
      "epoch:  530, loss: 0.04020237177610397\n",
      "epoch:  531, loss: 0.040202055126428604\n",
      "epoch:  532, loss: 0.040201734751462936\n",
      "epoch:  533, loss: 0.04020141810178757\n",
      "epoch:  534, loss: 0.0402011014521122\n",
      "epoch:  535, loss: 0.04020077735185623\n",
      "epoch:  536, loss: 0.040200456976890564\n",
      "epoch:  537, loss: 0.040200136601924896\n",
      "epoch:  538, loss: 0.04019981250166893\n",
      "epoch:  539, loss: 0.04019949957728386\n",
      "epoch:  540, loss: 0.04019917547702789\n",
      "epoch:  541, loss: 0.040198855102062225\n",
      "epoch:  542, loss: 0.04019853472709656\n",
      "epoch:  543, loss: 0.04019821435213089\n",
      "epoch:  544, loss: 0.04019789770245552\n",
      "epoch:  545, loss: 0.04019757732748985\n",
      "epoch:  546, loss: 0.040197256952524185\n",
      "epoch:  547, loss: 0.04019693657755852\n",
      "epoch:  548, loss: 0.04019661992788315\n",
      "epoch:  549, loss: 0.040196292102336884\n",
      "epoch:  550, loss: 0.040195975452661514\n",
      "epoch:  551, loss: 0.04019565507769585\n",
      "epoch:  552, loss: 0.04019533470273018\n",
      "epoch:  553, loss: 0.04019501432776451\n",
      "epoch:  554, loss: 0.040194690227508545\n",
      "epoch:  555, loss: 0.040194373577833176\n",
      "epoch:  556, loss: 0.040194056928157806\n",
      "epoch:  557, loss: 0.04019373655319214\n",
      "epoch:  558, loss: 0.04019341990351677\n",
      "epoch:  559, loss: 0.0401930995285511\n",
      "epoch:  560, loss: 0.040192775428295135\n",
      "epoch:  561, loss: 0.040192458778619766\n",
      "epoch:  562, loss: 0.0401921421289444\n",
      "epoch:  563, loss: 0.04019182175397873\n",
      "epoch:  564, loss: 0.04019149765372276\n",
      "epoch:  565, loss: 0.0401911735534668\n",
      "epoch:  566, loss: 0.04019085317850113\n",
      "epoch:  567, loss: 0.04019053280353546\n",
      "epoch:  568, loss: 0.040190208703279495\n",
      "epoch:  569, loss: 0.04018988460302353\n",
      "epoch:  570, loss: 0.040189553052186966\n",
      "epoch:  571, loss: 0.0401892326772213\n",
      "epoch:  572, loss: 0.04018891230225563\n",
      "epoch:  573, loss: 0.040188588201999664\n",
      "epoch:  574, loss: 0.0401882603764534\n",
      "epoch:  575, loss: 0.040187936276197433\n",
      "epoch:  576, loss: 0.040187615901231766\n",
      "epoch:  577, loss: 0.0401872880756855\n",
      "epoch:  578, loss: 0.040186963975429535\n",
      "epoch:  579, loss: 0.04018663242459297\n",
      "epoch:  580, loss: 0.040186308324337006\n",
      "epoch:  581, loss: 0.04018598422408104\n",
      "epoch:  582, loss: 0.040185656398534775\n",
      "epoch:  583, loss: 0.04018532484769821\n",
      "epoch:  584, loss: 0.040185000747442245\n",
      "epoch:  585, loss: 0.04018466919660568\n",
      "epoch:  586, loss: 0.04018434137105942\n",
      "epoch:  587, loss: 0.04018401354551315\n",
      "epoch:  588, loss: 0.04018368571996689\n",
      "epoch:  589, loss: 0.04018336161971092\n",
      "epoch:  590, loss: 0.04018303006887436\n",
      "epoch:  591, loss: 0.040182698518037796\n",
      "epoch:  592, loss: 0.04018237069249153\n",
      "epoch:  593, loss: 0.04018204286694527\n",
      "epoch:  594, loss: 0.040181711316108704\n",
      "epoch:  595, loss: 0.04018138349056244\n",
      "epoch:  596, loss: 0.040181055665016174\n",
      "epoch:  597, loss: 0.04018072411417961\n",
      "epoch:  598, loss: 0.04018039256334305\n",
      "epoch:  599, loss: 0.040180061012506485\n",
      "epoch:  600, loss: 0.04017973318696022\n",
      "epoch:  601, loss: 0.040179405361413956\n",
      "epoch:  602, loss: 0.04017907381057739\n",
      "epoch:  603, loss: 0.04017874598503113\n",
      "epoch:  604, loss: 0.04017842188477516\n",
      "epoch:  605, loss: 0.0401780940592289\n",
      "epoch:  606, loss: 0.040177762508392334\n",
      "epoch:  607, loss: 0.04017743840813637\n",
      "epoch:  608, loss: 0.0401771143078804\n",
      "epoch:  609, loss: 0.04017678648233414\n",
      "epoch:  610, loss: 0.04017645865678787\n",
      "epoch:  611, loss: 0.040176134556531906\n",
      "epoch:  612, loss: 0.040175799280405045\n",
      "epoch:  613, loss: 0.04017547518014908\n",
      "epoch:  614, loss: 0.040175147354602814\n",
      "epoch:  615, loss: 0.04017481580376625\n",
      "epoch:  616, loss: 0.040174487978219986\n",
      "epoch:  617, loss: 0.04017416015267372\n",
      "epoch:  618, loss: 0.04017382860183716\n",
      "epoch:  619, loss: 0.040173500776290894\n",
      "epoch:  620, loss: 0.04017317295074463\n",
      "epoch:  621, loss: 0.040172841399908066\n",
      "epoch:  622, loss: 0.0401725098490715\n",
      "epoch:  623, loss: 0.04017218202352524\n",
      "epoch:  624, loss: 0.040171850472688675\n",
      "epoch:  625, loss: 0.04017151892185211\n",
      "epoch:  626, loss: 0.04017118364572525\n",
      "epoch:  627, loss: 0.04017084836959839\n",
      "epoch:  628, loss: 0.040170520544052124\n",
      "epoch:  629, loss: 0.04017018526792526\n",
      "epoch:  630, loss: 0.040169857442379\n",
      "epoch:  631, loss: 0.040169525891542435\n",
      "epoch:  632, loss: 0.04016919061541557\n",
      "epoch:  633, loss: 0.04016885161399841\n",
      "epoch:  634, loss: 0.04016851261258125\n",
      "epoch:  635, loss: 0.04016818106174469\n",
      "epoch:  636, loss: 0.04016784578561783\n",
      "epoch:  637, loss: 0.04016750678420067\n",
      "epoch:  638, loss: 0.04016716778278351\n",
      "epoch:  639, loss: 0.04016682505607605\n",
      "epoch:  640, loss: 0.04016648977994919\n",
      "epoch:  641, loss: 0.04016614705324173\n",
      "epoch:  642, loss: 0.04016581550240517\n",
      "epoch:  643, loss: 0.04016547277569771\n",
      "epoch:  644, loss: 0.04016513377428055\n",
      "epoch:  645, loss: 0.040164798498153687\n",
      "epoch:  646, loss: 0.040164463222026825\n",
      "epoch:  647, loss: 0.04016412794589996\n",
      "epoch:  648, loss: 0.0401637926697731\n",
      "epoch:  649, loss: 0.04016344994306564\n",
      "epoch:  650, loss: 0.04016311094164848\n",
      "epoch:  651, loss: 0.040162768214941025\n",
      "epoch:  652, loss: 0.040162429213523865\n",
      "epoch:  653, loss: 0.040162086486816406\n",
      "epoch:  654, loss: 0.04016174376010895\n",
      "epoch:  655, loss: 0.04016139358282089\n",
      "epoch:  656, loss: 0.040161047130823135\n",
      "epoch:  657, loss: 0.04016069695353508\n",
      "epoch:  658, loss: 0.04016035050153732\n",
      "epoch:  659, loss: 0.04016000032424927\n",
      "epoch:  660, loss: 0.04015965387225151\n",
      "epoch:  661, loss: 0.040159307420253754\n",
      "epoch:  662, loss: 0.040158960968256\n",
      "epoch:  663, loss: 0.04015861451625824\n",
      "epoch:  664, loss: 0.04015827178955078\n",
      "epoch:  665, loss: 0.04015791788697243\n",
      "epoch:  666, loss: 0.040157563984394073\n",
      "epoch:  667, loss: 0.04015721380710602\n",
      "epoch:  668, loss: 0.04015686362981796\n",
      "epoch:  669, loss: 0.04015650600194931\n",
      "epoch:  670, loss: 0.04015616327524185\n",
      "epoch:  671, loss: 0.040155816823244095\n",
      "epoch:  672, loss: 0.04015546664595604\n",
      "epoch:  673, loss: 0.04015512019395828\n",
      "epoch:  674, loss: 0.04015477001667023\n",
      "epoch:  675, loss: 0.04015441983938217\n",
      "epoch:  676, loss: 0.040154073387384415\n",
      "epoch:  677, loss: 0.04015372321009636\n",
      "epoch:  678, loss: 0.040153373032808304\n",
      "epoch:  679, loss: 0.04015302658081055\n",
      "epoch:  680, loss: 0.04015267267823219\n",
      "epoch:  681, loss: 0.040152326226234436\n",
      "epoch:  682, loss: 0.04015197232365608\n",
      "epoch:  683, loss: 0.04015162214636803\n",
      "epoch:  684, loss: 0.04015127196907997\n",
      "epoch:  685, loss: 0.04015091806650162\n",
      "epoch:  686, loss: 0.04015057161450386\n",
      "epoch:  687, loss: 0.04015021771192551\n",
      "epoch:  688, loss: 0.040149860084056854\n",
      "epoch:  689, loss: 0.0401495099067688\n",
      "epoch:  690, loss: 0.04014915972948074\n",
      "epoch:  691, loss: 0.04014880582690239\n",
      "epoch:  692, loss: 0.040148451924324036\n",
      "epoch:  693, loss: 0.04014810547232628\n",
      "epoch:  694, loss: 0.040147751569747925\n",
      "epoch:  695, loss: 0.04014739766716957\n",
      "epoch:  696, loss: 0.040147047489881516\n",
      "epoch:  697, loss: 0.04014669358730316\n",
      "epoch:  698, loss: 0.04014633968472481\n",
      "epoch:  699, loss: 0.04014599695801735\n",
      "epoch:  700, loss: 0.040145643055438995\n",
      "epoch:  701, loss: 0.04014529287815094\n",
      "epoch:  702, loss: 0.040144942700862885\n",
      "epoch:  703, loss: 0.04014458879828453\n",
      "epoch:  704, loss: 0.040144238620996475\n",
      "epoch:  705, loss: 0.04014388844370842\n",
      "epoch:  706, loss: 0.040143538266420364\n",
      "epoch:  707, loss: 0.04014318436384201\n",
      "epoch:  708, loss: 0.04014283046126366\n",
      "epoch:  709, loss: 0.0401424765586853\n",
      "epoch:  710, loss: 0.04014211893081665\n",
      "epoch:  711, loss: 0.040141768753528595\n",
      "epoch:  712, loss: 0.04014141112565994\n",
      "epoch:  713, loss: 0.04014105722308159\n",
      "epoch:  714, loss: 0.040140699595212936\n",
      "epoch:  715, loss: 0.040140341967344284\n",
      "epoch:  716, loss: 0.04013998806476593\n",
      "epoch:  717, loss: 0.040139637887477875\n",
      "epoch:  718, loss: 0.040139272809028625\n",
      "epoch:  719, loss: 0.04013892263174057\n",
      "epoch:  720, loss: 0.04013856500387192\n",
      "epoch:  721, loss: 0.040138211101293564\n",
      "epoch:  722, loss: 0.04013785347342491\n",
      "epoch:  723, loss: 0.04013749212026596\n",
      "epoch:  724, loss: 0.04013712704181671\n",
      "epoch:  725, loss: 0.04013676941394806\n",
      "epoch:  726, loss: 0.04013640433549881\n",
      "epoch:  727, loss: 0.04013603925704956\n",
      "epoch:  728, loss: 0.04013567790389061\n",
      "epoch:  729, loss: 0.04013531655073166\n",
      "epoch:  730, loss: 0.04013495892286301\n",
      "epoch:  731, loss: 0.04013459011912346\n",
      "epoch:  732, loss: 0.04013422876596451\n",
      "epoch:  733, loss: 0.04013385996222496\n",
      "epoch:  734, loss: 0.04013349488377571\n",
      "epoch:  735, loss: 0.04013312608003616\n",
      "epoch:  736, loss: 0.04013275355100632\n",
      "epoch:  737, loss: 0.04013238847255707\n",
      "epoch:  738, loss: 0.04013201966881752\n",
      "epoch:  739, loss: 0.04013165459036827\n",
      "epoch:  740, loss: 0.040131282061338425\n",
      "epoch:  741, loss: 0.04013090953230858\n",
      "epoch:  742, loss: 0.040130533277988434\n",
      "epoch:  743, loss: 0.04013015702366829\n",
      "epoch:  744, loss: 0.04012978449463844\n",
      "epoch:  745, loss: 0.0401294082403183\n",
      "epoch:  746, loss: 0.040129031985998154\n",
      "epoch:  747, loss: 0.04012865573167801\n",
      "epoch:  748, loss: 0.04012827202677727\n",
      "epoch:  749, loss: 0.04012789577245712\n",
      "epoch:  750, loss: 0.04012751579284668\n",
      "epoch:  751, loss: 0.040127139538526535\n",
      "epoch:  752, loss: 0.04012676328420639\n",
      "epoch:  753, loss: 0.04012638330459595\n",
      "epoch:  754, loss: 0.040126003324985504\n",
      "epoch:  755, loss: 0.04012562707066536\n",
      "epoch:  756, loss: 0.04012524336576462\n",
      "epoch:  757, loss: 0.04012486711144447\n",
      "epoch:  758, loss: 0.04012448713183403\n",
      "epoch:  759, loss: 0.040124110877513885\n",
      "epoch:  760, loss: 0.040123727172613144\n",
      "epoch:  761, loss: 0.0401233471930027\n",
      "epoch:  762, loss: 0.04012296721339226\n",
      "epoch:  763, loss: 0.04012257978320122\n",
      "epoch:  764, loss: 0.040122199803590775\n",
      "epoch:  765, loss: 0.040121812373399734\n",
      "epoch:  766, loss: 0.04012142866849899\n",
      "epoch:  767, loss: 0.04012104496359825\n",
      "epoch:  768, loss: 0.04012065753340721\n",
      "epoch:  769, loss: 0.04012027755379677\n",
      "epoch:  770, loss: 0.04011989012360573\n",
      "epoch:  771, loss: 0.04011950269341469\n",
      "epoch:  772, loss: 0.04011912643909454\n",
      "epoch:  773, loss: 0.0401187427341938\n",
      "epoch:  774, loss: 0.04011835902929306\n",
      "epoch:  775, loss: 0.04011797904968262\n",
      "epoch:  776, loss: 0.04011759161949158\n",
      "epoch:  777, loss: 0.04011720418930054\n",
      "epoch:  778, loss: 0.040116824209690094\n",
      "epoch:  779, loss: 0.04011644423007965\n",
      "epoch:  780, loss: 0.04011605307459831\n",
      "epoch:  781, loss: 0.04011566936969757\n",
      "epoch:  782, loss: 0.04011527821421623\n",
      "epoch:  783, loss: 0.04011489823460579\n",
      "epoch:  784, loss: 0.04011450707912445\n",
      "epoch:  785, loss: 0.04011411964893341\n",
      "epoch:  786, loss: 0.040113724768161774\n",
      "epoch:  787, loss: 0.040113333612680435\n",
      "epoch:  788, loss: 0.0401129424571991\n",
      "epoch:  789, loss: 0.04011254757642746\n",
      "epoch:  790, loss: 0.04011215269565582\n",
      "epoch:  791, loss: 0.04011175408959389\n",
      "epoch:  792, loss: 0.04011135920882225\n",
      "epoch:  793, loss: 0.04011096432805061\n",
      "epoch:  794, loss: 0.040110569447278976\n",
      "epoch:  795, loss: 0.04011016711592674\n",
      "epoch:  796, loss: 0.040109772235155106\n",
      "epoch:  797, loss: 0.04010937735438347\n",
      "epoch:  798, loss: 0.040108975023031235\n",
      "epoch:  799, loss: 0.0401085801422596\n",
      "epoch:  800, loss: 0.040108177810907364\n",
      "epoch:  801, loss: 0.04010777920484543\n",
      "epoch:  802, loss: 0.040107373148202896\n",
      "epoch:  803, loss: 0.04010697454214096\n",
      "epoch:  804, loss: 0.04010657221078873\n",
      "epoch:  805, loss: 0.040106162428855896\n",
      "epoch:  806, loss: 0.040105756372213364\n",
      "epoch:  807, loss: 0.04010535776615143\n",
      "epoch:  808, loss: 0.040104951709508896\n",
      "epoch:  809, loss: 0.040104545652866364\n",
      "epoch:  810, loss: 0.04010413959622383\n",
      "epoch:  811, loss: 0.040103729814291\n",
      "epoch:  812, loss: 0.04010332003235817\n",
      "epoch:  813, loss: 0.04010291025042534\n",
      "epoch:  814, loss: 0.04010250046849251\n",
      "epoch:  815, loss: 0.04010209068655968\n",
      "epoch:  816, loss: 0.040101680904626846\n",
      "epoch:  817, loss: 0.04010127857327461\n",
      "epoch:  818, loss: 0.04010086879134178\n",
      "epoch:  819, loss: 0.04010046273469925\n",
      "epoch:  820, loss: 0.040100060403347015\n",
      "epoch:  821, loss: 0.04009965434670448\n",
      "epoch:  822, loss: 0.040099237114191055\n",
      "epoch:  823, loss: 0.04009881988167763\n",
      "epoch:  824, loss: 0.0400984100997448\n",
      "epoch:  825, loss: 0.04009799286723137\n",
      "epoch:  826, loss: 0.04009757936000824\n",
      "epoch:  827, loss: 0.040097158402204514\n",
      "epoch:  828, loss: 0.040096744894981384\n",
      "epoch:  829, loss: 0.040096331387758255\n",
      "epoch:  830, loss: 0.04009591415524483\n",
      "epoch:  831, loss: 0.0400955006480217\n",
      "epoch:  832, loss: 0.04009507969021797\n",
      "epoch:  833, loss: 0.04009466618299484\n",
      "epoch:  834, loss: 0.040094245225191116\n",
      "epoch:  835, loss: 0.04009382799267769\n",
      "epoch:  836, loss: 0.04009340703487396\n",
      "epoch:  837, loss: 0.04009298235177994\n",
      "epoch:  838, loss: 0.04009256139397621\n",
      "epoch:  839, loss: 0.040092144161462784\n",
      "epoch:  840, loss: 0.04009171575307846\n",
      "epoch:  841, loss: 0.040091291069984436\n",
      "epoch:  842, loss: 0.04009086638689041\n",
      "epoch:  843, loss: 0.04009043425321579\n",
      "epoch:  844, loss: 0.04009000211954117\n",
      "epoch:  845, loss: 0.04008956998586655\n",
      "epoch:  846, loss: 0.040089137852191925\n",
      "epoch:  847, loss: 0.0400887094438076\n",
      "epoch:  848, loss: 0.04008828103542328\n",
      "epoch:  849, loss: 0.04008784517645836\n",
      "epoch:  850, loss: 0.04008741304278374\n",
      "epoch:  851, loss: 0.04008697718381882\n",
      "epoch:  852, loss: 0.040086545050144196\n",
      "epoch:  853, loss: 0.040086112916469574\n",
      "epoch:  854, loss: 0.040085677057504654\n",
      "epoch:  855, loss: 0.040085237473249435\n",
      "epoch:  856, loss: 0.04008480906486511\n",
      "epoch:  857, loss: 0.04008438065648079\n",
      "epoch:  858, loss: 0.04008394107222557\n",
      "epoch:  859, loss: 0.04008351266384125\n",
      "epoch:  860, loss: 0.04008307680487633\n",
      "epoch:  861, loss: 0.040082644671201706\n",
      "epoch:  862, loss: 0.040082208812236786\n",
      "epoch:  863, loss: 0.040081776678562164\n",
      "epoch:  864, loss: 0.040081337094306946\n",
      "epoch:  865, loss: 0.04008090868592262\n",
      "epoch:  866, loss: 0.040080469101667404\n",
      "epoch:  867, loss: 0.040080033242702484\n",
      "epoch:  868, loss: 0.040079593658447266\n",
      "epoch:  869, loss: 0.04007915407419205\n",
      "epoch:  870, loss: 0.040078721940517426\n",
      "epoch:  871, loss: 0.04007828235626221\n",
      "epoch:  872, loss: 0.04007784277200699\n",
      "epoch:  873, loss: 0.04007739946246147\n",
      "epoch:  874, loss: 0.04007695987820625\n",
      "epoch:  875, loss: 0.040076516568660736\n",
      "epoch:  876, loss: 0.04007607698440552\n",
      "epoch:  877, loss: 0.04007563367486\n",
      "epoch:  878, loss: 0.040075190365314484\n",
      "epoch:  879, loss: 0.04007474705576897\n",
      "epoch:  880, loss: 0.04007431119680405\n",
      "epoch:  881, loss: 0.04007387533783913\n",
      "epoch:  882, loss: 0.04007343202829361\n",
      "epoch:  883, loss: 0.04007298871874809\n",
      "epoch:  884, loss: 0.04007254168391228\n",
      "epoch:  885, loss: 0.04007209092378616\n",
      "epoch:  886, loss: 0.04007164388895035\n",
      "epoch:  887, loss: 0.040071189403533936\n",
      "epoch:  888, loss: 0.04007074236869812\n",
      "epoch:  889, loss: 0.040070295333862305\n",
      "epoch:  890, loss: 0.04006984084844589\n",
      "epoch:  891, loss: 0.04006939008831978\n",
      "epoch:  892, loss: 0.04006893187761307\n",
      "epoch:  893, loss: 0.040068481117486954\n",
      "epoch:  894, loss: 0.04006803035736084\n",
      "epoch:  895, loss: 0.04006757587194443\n",
      "epoch:  896, loss: 0.04006711766123772\n",
      "epoch:  897, loss: 0.0400666706264019\n",
      "epoch:  898, loss: 0.04006621241569519\n",
      "epoch:  899, loss: 0.04006574675440788\n",
      "epoch:  900, loss: 0.04006528854370117\n",
      "epoch:  901, loss: 0.04006483033299446\n",
      "epoch:  902, loss: 0.04006436839699745\n",
      "epoch:  903, loss: 0.04006390646100044\n",
      "epoch:  904, loss: 0.040063437074422836\n",
      "epoch:  905, loss: 0.04006297513842583\n",
      "epoch:  906, loss: 0.04006251320242882\n",
      "epoch:  907, loss: 0.040062036365270615\n",
      "epoch:  908, loss: 0.04006156325340271\n",
      "epoch:  909, loss: 0.0400610975921154\n",
      "epoch:  910, loss: 0.0400606207549572\n",
      "epoch:  911, loss: 0.04006015136837959\n",
      "epoch:  912, loss: 0.04005967825651169\n",
      "epoch:  913, loss: 0.04005920886993408\n",
      "epoch:  914, loss: 0.04005873203277588\n",
      "epoch:  915, loss: 0.040058258920907974\n",
      "epoch:  916, loss: 0.04005778208374977\n",
      "epoch:  917, loss: 0.040057308971881866\n",
      "epoch:  918, loss: 0.040056828409433365\n",
      "epoch:  919, loss: 0.04005635529756546\n",
      "epoch:  920, loss: 0.040055885910987854\n",
      "epoch:  921, loss: 0.04005540907382965\n",
      "epoch:  922, loss: 0.04005493223667145\n",
      "epoch:  923, loss: 0.04005444794893265\n",
      "epoch:  924, loss: 0.040053967386484146\n",
      "epoch:  925, loss: 0.040053486824035645\n",
      "epoch:  926, loss: 0.04005299508571625\n",
      "epoch:  927, loss: 0.04005250707268715\n",
      "epoch:  928, loss: 0.04005202278494835\n",
      "epoch:  929, loss: 0.04005153104662895\n",
      "epoch:  930, loss: 0.040051043033599854\n",
      "epoch:  931, loss: 0.040050555020570755\n",
      "epoch:  932, loss: 0.040050067007541656\n",
      "epoch:  933, loss: 0.04004957154393196\n",
      "epoch:  934, loss: 0.04004908353090286\n",
      "epoch:  935, loss: 0.040048591792583466\n",
      "epoch:  936, loss: 0.04004810377955437\n",
      "epoch:  937, loss: 0.04004761576652527\n",
      "epoch:  938, loss: 0.04004713147878647\n",
      "epoch:  939, loss: 0.04004664719104767\n",
      "epoch:  940, loss: 0.04004615545272827\n",
      "epoch:  941, loss: 0.04004567116498947\n",
      "epoch:  942, loss: 0.04004518315196037\n",
      "epoch:  943, loss: 0.040044691413640976\n",
      "epoch:  944, loss: 0.04004420340061188\n",
      "epoch:  945, loss: 0.04004371538758278\n",
      "epoch:  946, loss: 0.04004322364926338\n",
      "epoch:  947, loss: 0.040042731910943985\n",
      "epoch:  948, loss: 0.040042247623205185\n",
      "epoch:  949, loss: 0.040041759610176086\n",
      "epoch:  950, loss: 0.04004126414656639\n",
      "epoch:  951, loss: 0.040040772408246994\n",
      "epoch:  952, loss: 0.0400402806699276\n",
      "epoch:  953, loss: 0.0400397926568985\n",
      "epoch:  954, loss: 0.0400393009185791\n",
      "epoch:  955, loss: 0.040038805454969406\n",
      "epoch:  956, loss: 0.04003830999135971\n",
      "epoch:  957, loss: 0.04003781080245972\n",
      "epoch:  958, loss: 0.04003731533885002\n",
      "epoch:  959, loss: 0.040036819875240326\n",
      "epoch:  960, loss: 0.04003632441163063\n",
      "epoch:  961, loss: 0.040035828948020935\n",
      "epoch:  962, loss: 0.04003532975912094\n",
      "epoch:  963, loss: 0.04003482311964035\n",
      "epoch:  964, loss: 0.04003431648015976\n",
      "epoch:  965, loss: 0.04003380611538887\n",
      "epoch:  966, loss: 0.04003329575061798\n",
      "epoch:  967, loss: 0.040032777935266495\n",
      "epoch:  968, loss: 0.04003226011991501\n",
      "epoch:  969, loss: 0.04003175348043442\n",
      "epoch:  970, loss: 0.04003123193979263\n",
      "epoch:  971, loss: 0.04003070667386055\n",
      "epoch:  972, loss: 0.04003019258379936\n",
      "epoch:  973, loss: 0.04002967104315758\n",
      "epoch:  974, loss: 0.04002915322780609\n",
      "epoch:  975, loss: 0.040028639137744904\n",
      "epoch:  976, loss: 0.04002811759710312\n",
      "epoch:  977, loss: 0.040027592331171036\n",
      "epoch:  978, loss: 0.040027063339948654\n",
      "epoch:  979, loss: 0.04002653807401657\n",
      "epoch:  980, loss: 0.04002600908279419\n",
      "epoch:  981, loss: 0.04002548009157181\n",
      "epoch:  982, loss: 0.04002495855093002\n",
      "epoch:  983, loss: 0.04002443328499794\n",
      "epoch:  984, loss: 0.040023915469646454\n",
      "epoch:  985, loss: 0.04002339392900467\n",
      "epoch:  986, loss: 0.040022868663072586\n",
      "epoch:  987, loss: 0.0400223433971405\n",
      "epoch:  988, loss: 0.04002181813120842\n",
      "epoch:  989, loss: 0.04002129286527634\n",
      "epoch:  990, loss: 0.040020763874053955\n",
      "epoch:  991, loss: 0.040020231157541275\n",
      "epoch:  992, loss: 0.040019690990448\n",
      "epoch:  993, loss: 0.04001915454864502\n",
      "epoch:  994, loss: 0.040018610656261444\n",
      "epoch:  995, loss: 0.04001807048916817\n",
      "epoch:  996, loss: 0.04001753777265549\n",
      "epoch:  997, loss: 0.04001699388027191\n",
      "epoch:  998, loss: 0.04001646116375923\n",
      "epoch:  999, loss: 0.040015920996665955\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size=X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "opt = optim.SGD(model.parameters(), lr=2e-2)\n",
    "\n",
    "all_loss = {}\n",
    "for epoch in range(1000):\n",
    "    print(\"epoch: \", epoch, end=\"\")\n",
    "    all_loss[epoch + 1] = 0\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step()\n",
    "\n",
    "        all_loss[epoch + 1] += loss\n",
    "    all_loss[epoch + 1] /= len(data_loader)\n",
    "    print(\", loss: {}\".format(all_loss[epoch + 1].cpu().detach().numpy().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics: R2 = -6493.385020554838\n",
      "Test metrics:  R2 = -5726.414842743044\n"
     ]
    }
   ],
   "source": [
    "pred_train = model.forward(X_train).cpu().detach()\n",
    "pred_test = model.forward(X_test).cpu().detach()\n",
    "print(f\"Train metrics: R2 = {r2_score(pred_train, y_train.cpu())}\")\n",
    "print(f\"Test metrics:  R2 = {r2_score(pred_test, y_test.cpu())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.2209373116493225\n",
      "epoch:  1, loss: 0.18289239704608917\n",
      "epoch:  2, loss: 0.1485273241996765\n",
      "epoch:  3, loss: 0.11472073942422867\n",
      "epoch:  4, loss: 0.08132372051477432\n",
      "epoch:  5, loss: 0.05237060412764549\n",
      "epoch:  6, loss: 0.037233103066682816\n",
      "epoch:  7, loss: 0.049321211874485016\n",
      "epoch:  8, loss: 0.06692074984312057\n",
      "epoch:  9, loss: 0.06430836766958237\n",
      "epoch:  10, loss: 0.052060727030038834\n",
      "epoch:  11, loss: 0.041838426142930984\n",
      "epoch:  12, loss: 0.037303145974874496\n",
      "epoch:  13, loss: 0.03734647110104561\n",
      "epoch:  14, loss: 0.03955647721886635\n",
      "epoch:  15, loss: 0.04197460412979126\n",
      "epoch:  16, loss: 0.04361793026328087\n",
      "epoch:  17, loss: 0.044207241386175156\n",
      "epoch:  18, loss: 0.043829191476106644\n",
      "epoch:  19, loss: 0.04272680729627609\n",
      "epoch:  20, loss: 0.04120100289583206\n",
      "epoch:  21, loss: 0.03956182301044464\n",
      "epoch:  22, loss: 0.03809279948472977\n",
      "epoch:  23, loss: 0.03701317310333252\n",
      "epoch:  24, loss: 0.03644777089357376\n",
      "epoch:  25, loss: 0.036399804055690765\n",
      "epoch:  26, loss: 0.03674716502428055\n",
      "epoch:  27, loss: 0.03725982457399368\n",
      "epoch:  28, loss: 0.03767315670847893\n",
      "epoch:  29, loss: 0.03779274970293045\n",
      "epoch:  30, loss: 0.037558820098638535\n",
      "epoch:  31, loss: 0.03705121949315071\n",
      "epoch:  32, loss: 0.03643283620476723\n",
      "epoch:  33, loss: 0.03586927801370621\n",
      "epoch:  34, loss: 0.0354686938226223\n",
      "epoch:  35, loss: 0.03526070713996887\n",
      "epoch:  36, loss: 0.03520868346095085\n",
      "epoch:  37, loss: 0.0352381095290184\n",
      "epoch:  38, loss: 0.03526774421334267\n",
      "epoch:  39, loss: 0.03523227199912071\n",
      "epoch:  40, loss: 0.035093940794467926\n",
      "epoch:  41, loss: 0.03484683483839035\n",
      "epoch:  42, loss: 0.034514136612415314\n",
      "epoch:  43, loss: 0.03413930907845497\n",
      "epoch:  44, loss: 0.03377202898263931\n",
      "epoch:  45, loss: 0.033454641699790955\n",
      "epoch:  46, loss: 0.0332072451710701\n",
      "epoch:  47, loss: 0.03301985561847687\n",
      "epoch:  48, loss: 0.03285546228289604\n",
      "epoch:  49, loss: 0.032664790749549866\n",
      "epoch:  50, loss: 0.03240835294127464\n",
      "epoch:  51, loss: 0.03207406401634216\n",
      "epoch:  52, loss: 0.03168046101927757\n",
      "epoch:  53, loss: 0.03126375749707222\n",
      "epoch:  54, loss: 0.030858755111694336\n",
      "epoch:  55, loss: 0.030481718480587006\n",
      "epoch:  56, loss: 0.030124163255095482\n",
      "epoch:  57, loss: 0.029760155826807022\n",
      "epoch:  58, loss: 0.029361939057707787\n",
      "epoch:  59, loss: 0.028912177309393883\n",
      "epoch:  60, loss: 0.02841188572347164\n",
      "epoch:  61, loss: 0.02787867747247219\n",
      "epoch:  62, loss: 0.027335412800312042\n",
      "epoch:  63, loss: 0.02679636888206005\n",
      "epoch:  64, loss: 0.026256946846842766\n",
      "epoch:  65, loss: 0.025695929303765297\n",
      "epoch:  66, loss: 0.025089161470532417\n",
      "epoch:  67, loss: 0.02442961558699608\n",
      "epoch:  68, loss: 0.023730358108878136\n",
      "epoch:  69, loss: 0.023013973608613014\n",
      "epoch:  70, loss: 0.02229980193078518\n",
      "epoch:  71, loss: 0.021578660234808922\n",
      "epoch:  72, loss: 0.0208309143781662\n",
      "epoch:  73, loss: 0.02004862017929554\n",
      "epoch:  74, loss: 0.01924320124089718\n",
      "epoch:  75, loss: 0.018442032858729362\n",
      "epoch:  76, loss: 0.017662782222032547\n",
      "epoch:  77, loss: 0.01690611243247986\n",
      "epoch:  78, loss: 0.016159212216734886\n",
      "epoch:  79, loss: 0.015424945391714573\n",
      "epoch:  80, loss: 0.014729651622474194\n",
      "epoch:  81, loss: 0.014096538536250591\n",
      "epoch:  82, loss: 0.013529348187148571\n",
      "epoch:  83, loss: 0.01301233284175396\n",
      "epoch:  84, loss: 0.012548081576824188\n",
      "epoch:  85, loss: 0.012159381061792374\n",
      "epoch:  86, loss: 0.01184376236051321\n",
      "epoch:  87, loss: 0.011572315357625484\n",
      "epoch:  88, loss: 0.0113324373960495\n",
      "epoch:  89, loss: 0.011129911057651043\n",
      "epoch:  90, loss: 0.010956480167806149\n",
      "epoch:  91, loss: 0.01078751776367426\n",
      "epoch:  92, loss: 0.010613076388835907\n",
      "epoch:  93, loss: 0.010441015474498272\n",
      "epoch:  94, loss: 0.010266316123306751\n",
      "epoch:  95, loss: 0.010082319378852844\n",
      "epoch:  96, loss: 0.009891820140182972\n",
      "epoch:  97, loss: 0.00971207208931446\n",
      "epoch:  98, loss: 0.009542669169604778\n",
      "epoch:  99, loss: 0.009383357129991055\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size=X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "opt = optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "all_loss = {}\n",
    "for epoch in range(100):\n",
    "    print(\"epoch: \", epoch, end=\"\")\n",
    "    all_loss[epoch + 1] = 0\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step()\n",
    "\n",
    "        all_loss[epoch + 1] += loss\n",
    "    all_loss[epoch + 1] /= len(data_loader)\n",
    "    print(\", loss: {}\".format(all_loss[epoch + 1].cpu().detach().cpu().numpy().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics: R2 = 0.6688919425876207\n",
      "Test metrics:  R2 = 0.5650632959382436\n"
     ]
    }
   ],
   "source": [
    "pred_train = model.forward(X_train).cpu().detach()\n",
    "pred_test = model.forward(X_test).cpu().detach()\n",
    "print(f\"Train metrics: R2 = {r2_score(pred_train, y_train.cpu())}\")\n",
    "print(f\"Test metrics:  R2 = {r2_score(pred_test, y_test.cpu())}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_numopt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
