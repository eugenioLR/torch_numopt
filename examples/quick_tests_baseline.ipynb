{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.f1 = nn.Linear(input_size, 10, device=device)\n",
    "        self.f2 = nn.Linear(10, 20, device=device)\n",
    "        self.f3 = nn.Linear(20, 20, device=device)\n",
    "        self.f4 = nn.Linear(20, 10, device=device)\n",
    "        self.f5 = nn.Linear(10, 1, device=device)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        # self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.f1(x))\n",
    "        x = self.activation(self.f2(x))\n",
    "        x = self.activation(self.f3(x))\n",
    "        x = self.activation(self.f4(x))\n",
    "        x = self.f5(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_diabetes(return_X_y = True, scaled=False)\n",
    "\n",
    "X_scaler = MinMaxScaler()\n",
    "X = X_scaler.fit_transform(X)\n",
    "\n",
    "y_scaler = MinMaxScaler()\n",
    "y = y_scaler.fit_transform(y.reshape((-1, 1)))\n",
    "\n",
    "torch_data = TensorDataset(torch.Tensor(X).to(device), torch.Tensor(y).to(device))\n",
    "data_loader = DataLoader(torch_data, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.11247320473194122\n",
      "epoch:  1, loss: 0.10932402312755585\n",
      "epoch:  2, loss: 0.10634509474039078\n",
      "epoch:  3, loss: 0.10352645069360733\n",
      "epoch:  4, loss: 0.10086102783679962\n",
      "epoch:  5, loss: 0.09833139926195145\n",
      "epoch:  6, loss: 0.0959349051117897\n",
      "epoch:  7, loss: 0.09366614371538162\n",
      "epoch:  8, loss: 0.09151507914066315\n",
      "epoch:  9, loss: 0.08947668969631195\n",
      "epoch:  10, loss: 0.08755224198102951\n",
      "epoch:  11, loss: 0.08573557436466217\n",
      "epoch:  12, loss: 0.08402257412672043\n",
      "epoch:  13, loss: 0.08241210132837296\n",
      "epoch:  14, loss: 0.08089885115623474\n",
      "epoch:  15, loss: 0.0794769898056984\n",
      "epoch:  16, loss: 0.0781424343585968\n",
      "epoch:  17, loss: 0.07688989490270615\n",
      "epoch:  18, loss: 0.07571371644735336\n",
      "epoch:  19, loss: 0.07460970431566238\n",
      "epoch:  20, loss: 0.07357343286275864\n",
      "epoch:  21, loss: 0.07260128110647202\n",
      "epoch:  22, loss: 0.07168895751237869\n",
      "epoch:  23, loss: 0.07083295285701752\n",
      "epoch:  24, loss: 0.07002992182970047\n",
      "epoch:  25, loss: 0.06927642226219177\n",
      "epoch:  26, loss: 0.06856954842805862\n",
      "epoch:  27, loss: 0.06790649890899658\n",
      "epoch:  28, loss: 0.06728443503379822\n",
      "epoch:  29, loss: 0.06670111417770386\n",
      "epoch:  30, loss: 0.06615415960550308\n",
      "epoch:  31, loss: 0.06564128398895264\n",
      "epoch:  32, loss: 0.0651603415608406\n",
      "epoch:  33, loss: 0.06470945477485657\n",
      "epoch:  34, loss: 0.06428678333759308\n",
      "epoch:  35, loss: 0.06389053910970688\n",
      "epoch:  36, loss: 0.06351911276578903\n",
      "epoch:  37, loss: 0.06317096948623657\n",
      "epoch:  38, loss: 0.06284447759389877\n",
      "epoch:  39, loss: 0.06253854930400848\n",
      "epoch:  40, loss: 0.06225188076496124\n",
      "epoch:  41, loss: 0.06198317930102348\n",
      "epoch:  42, loss: 0.06173142045736313\n",
      "epoch:  43, loss: 0.061495568603277206\n",
      "epoch:  44, loss: 0.06127466633915901\n",
      "epoch:  45, loss: 0.06106765940785408\n",
      "epoch:  46, loss: 0.0608736127614975\n",
      "epoch:  47, loss: 0.06069177761673927\n",
      "epoch:  48, loss: 0.060521066188812256\n",
      "epoch:  49, loss: 0.06036077439785004\n",
      "epoch:  50, loss: 0.060210149735212326\n",
      "epoch:  51, loss: 0.06006857007741928\n",
      "epoch:  52, loss: 0.059935539960861206\n",
      "epoch:  53, loss: 0.059810373932123184\n",
      "epoch:  54, loss: 0.059693045914173126\n",
      "epoch:  55, loss: 0.059582848101854324\n",
      "epoch:  56, loss: 0.05947941541671753\n",
      "epoch:  57, loss: 0.05938224121928215\n",
      "epoch:  58, loss: 0.059290699660778046\n",
      "epoch:  59, loss: 0.059204015880823135\n",
      "epoch:  60, loss: 0.05912196636199951\n",
      "epoch:  61, loss: 0.059043724089860916\n",
      "epoch:  62, loss: 0.05896933376789093\n",
      "epoch:  63, loss: 0.05889827013015747\n",
      "epoch:  64, loss: 0.05883059278130531\n",
      "epoch:  65, loss: 0.05876583978533745\n",
      "epoch:  66, loss: 0.058703113347291946\n",
      "epoch:  67, loss: 0.05864263325929642\n",
      "epoch:  68, loss: 0.05858463793992996\n",
      "epoch:  69, loss: 0.058529332280159\n",
      "epoch:  70, loss: 0.058476630598306656\n",
      "epoch:  71, loss: 0.05842573568224907\n",
      "epoch:  72, loss: 0.0583762563765049\n",
      "epoch:  73, loss: 0.05832821503281593\n",
      "epoch:  74, loss: 0.05828016251325607\n",
      "epoch:  75, loss: 0.05823339894413948\n",
      "epoch:  76, loss: 0.05818800628185272\n",
      "epoch:  77, loss: 0.05814369022846222\n",
      "epoch:  78, loss: 0.058102045208215714\n",
      "epoch:  79, loss: 0.05806205794215202\n",
      "epoch:  80, loss: 0.05802379921078682\n",
      "epoch:  81, loss: 0.0579872652888298\n",
      "epoch:  82, loss: 0.05795278772711754\n",
      "epoch:  83, loss: 0.05791885033249855\n",
      "epoch:  84, loss: 0.057887233793735504\n",
      "epoch:  85, loss: 0.05785704031586647\n",
      "epoch:  86, loss: 0.05782786384224892\n",
      "epoch:  87, loss: 0.057799793779850006\n",
      "epoch:  88, loss: 0.05777198076248169\n",
      "epoch:  89, loss: 0.05774643272161484\n",
      "epoch:  90, loss: 0.057723499834537506\n",
      "epoch:  91, loss: 0.05770231410861015\n",
      "epoch:  92, loss: 0.05768235772848129\n",
      "epoch:  93, loss: 0.05766313523054123\n",
      "epoch:  94, loss: 0.057644594460725784\n",
      "epoch:  95, loss: 0.057626862078905106\n",
      "epoch:  96, loss: 0.057610124349594116\n",
      "epoch:  97, loss: 0.057594697922468185\n",
      "epoch:  98, loss: 0.057579997926950455\n",
      "epoch:  99, loss: 0.05756641924381256\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size = X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "opt = optim.SGD(model.parameters(), lr = 1e-2)\n",
    "\n",
    "all_loss = {}\n",
    "for epoch in range(100):\n",
    "    print('epoch: ', epoch, end='')\n",
    "    all_loss[epoch+1] = 0\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step()\n",
    "\n",
    "        all_loss[epoch+1] += loss\n",
    "    all_loss[epoch+1] /= len(data_loader)\n",
    "    print(', loss: {}'.format(all_loss[epoch+1].detach().cpu().numpy().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.1267888844013214\n",
      "epoch:  1, loss: 0.10387511551380157\n",
      "epoch:  2, loss: 0.08551064878702164\n",
      "epoch:  3, loss: 0.07332144677639008\n",
      "epoch:  4, loss: 0.06384661048650742\n",
      "epoch:  5, loss: 0.0581718385219574\n",
      "epoch:  6, loss: 0.057954635471105576\n",
      "epoch:  7, loss: 0.06091494485735893\n",
      "epoch:  8, loss: 0.06209683045744896\n",
      "epoch:  9, loss: 0.06029672175645828\n",
      "epoch:  10, loss: 0.057730723172426224\n",
      "epoch:  11, loss: 0.05653904378414154\n",
      "epoch:  12, loss: 0.05715442821383476\n",
      "epoch:  13, loss: 0.05821247398853302\n",
      "epoch:  14, loss: 0.05833446979522705\n",
      "epoch:  15, loss: 0.05738365650177002\n",
      "epoch:  16, loss: 0.0560438334941864\n",
      "epoch:  17, loss: 0.05506858229637146\n",
      "epoch:  18, loss: 0.054695162922143936\n",
      "epoch:  19, loss: 0.05465621128678322\n",
      "epoch:  20, loss: 0.054224248975515366\n",
      "epoch:  21, loss: 0.05296001210808754\n",
      "epoch:  22, loss: 0.05126926675438881\n",
      "epoch:  23, loss: 0.04992076754570007\n",
      "epoch:  24, loss: 0.04917405918240547\n",
      "epoch:  25, loss: 0.04830161854624748\n",
      "epoch:  26, loss: 0.04652475565671921\n",
      "epoch:  27, loss: 0.0442027747631073\n",
      "epoch:  28, loss: 0.042419616132974625\n",
      "epoch:  29, loss: 0.040810633450746536\n",
      "epoch:  30, loss: 0.03844065964221954\n",
      "epoch:  31, loss: 0.03610791638493538\n",
      "epoch:  32, loss: 0.03523512929677963\n",
      "epoch:  33, loss: 0.03444398194551468\n",
      "epoch:  34, loss: 0.03427300974726677\n",
      "epoch:  35, loss: 0.034747395664453506\n",
      "epoch:  36, loss: 0.03428588807582855\n",
      "epoch:  37, loss: 0.03424626216292381\n",
      "epoch:  38, loss: 0.03300386667251587\n",
      "epoch:  39, loss: 0.032507311552762985\n",
      "epoch:  40, loss: 0.03140872344374657\n",
      "epoch:  41, loss: 0.031109504401683807\n",
      "epoch:  42, loss: 0.03060549683868885\n",
      "epoch:  43, loss: 0.030489813536405563\n",
      "epoch:  44, loss: 0.030423691496253014\n",
      "epoch:  45, loss: 0.030190303921699524\n",
      "epoch:  46, loss: 0.03018932417035103\n",
      "epoch:  47, loss: 0.029874907806515694\n",
      "epoch:  48, loss: 0.029639869928359985\n",
      "epoch:  49, loss: 0.029371874406933784\n",
      "epoch:  50, loss: 0.02896398864686489\n",
      "epoch:  51, loss: 0.028789188712835312\n",
      "epoch:  52, loss: 0.02847946621477604\n",
      "epoch:  53, loss: 0.028409257531166077\n",
      "epoch:  54, loss: 0.028248850256204605\n",
      "epoch:  55, loss: 0.028230689465999603\n",
      "epoch:  56, loss: 0.028117651119828224\n",
      "epoch:  57, loss: 0.028055064380168915\n",
      "epoch:  58, loss: 0.02790619432926178\n",
      "epoch:  59, loss: 0.027801768854260445\n",
      "epoch:  60, loss: 0.027671031653881073\n",
      "epoch:  61, loss: 0.027591818943619728\n",
      "epoch:  62, loss: 0.02753724530339241\n",
      "epoch:  63, loss: 0.02751258946955204\n",
      "epoch:  64, loss: 0.027506433427333832\n",
      "epoch:  65, loss: 0.02747207321226597\n",
      "epoch:  66, loss: 0.02745320275425911\n",
      "epoch:  67, loss: 0.027382254600524902\n",
      "epoch:  68, loss: 0.027343815192580223\n",
      "epoch:  69, loss: 0.027262629941105843\n",
      "epoch:  70, loss: 0.027231493964791298\n",
      "epoch:  71, loss: 0.027171900495886803\n",
      "epoch:  72, loss: 0.02716260403394699\n",
      "epoch:  73, loss: 0.02712150849401951\n",
      "epoch:  74, loss: 0.02711539901793003\n",
      "epoch:  75, loss: 0.027066852897405624\n",
      "epoch:  76, loss: 0.027043184265494347\n",
      "epoch:  77, loss: 0.026986820623278618\n",
      "epoch:  78, loss: 0.026967104524374008\n",
      "epoch:  79, loss: 0.02691825106739998\n",
      "epoch:  80, loss: 0.026899343356490135\n",
      "epoch:  81, loss: 0.026858588680624962\n",
      "epoch:  82, loss: 0.02683446928858757\n",
      "epoch:  83, loss: 0.026797590777277946\n",
      "epoch:  84, loss: 0.026777515187859535\n",
      "epoch:  85, loss: 0.026747530326247215\n",
      "epoch:  86, loss: 0.026722831651568413\n",
      "epoch:  87, loss: 0.026700301095843315\n",
      "epoch:  88, loss: 0.026673877611756325\n",
      "epoch:  89, loss: 0.026648497208952904\n",
      "epoch:  90, loss: 0.026618408039212227\n",
      "epoch:  91, loss: 0.0265993420034647\n",
      "epoch:  92, loss: 0.02656978741288185\n",
      "epoch:  93, loss: 0.026548391208052635\n",
      "epoch:  94, loss: 0.026520075276494026\n",
      "epoch:  95, loss: 0.02649724669754505\n",
      "epoch:  96, loss: 0.02647421695291996\n",
      "epoch:  97, loss: 0.026451408863067627\n",
      "epoch:  98, loss: 0.02643127366900444\n",
      "epoch:  99, loss: 0.026403825730085373\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size = X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "opt = optim.Adam(model.parameters(), lr = 1e-2)\n",
    "\n",
    "all_loss = {}\n",
    "for epoch in range(100):\n",
    "    print('epoch: ', epoch, end='')\n",
    "    all_loss[epoch+1] = 0\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step()\n",
    "\n",
    "        all_loss[epoch+1] += loss\n",
    "    all_loss[epoch+1] /= len(data_loader)\n",
    "    print(', loss: {}'.format(all_loss[epoch+1].detach().cpu().numpy().item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
