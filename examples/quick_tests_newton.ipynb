{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pytorch_soom\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import *\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.f1 = nn.Linear(input_size, 10, device=device)\n",
    "        self.f2 = nn.Linear(10, 20, device=device)\n",
    "        self.f3 = nn.Linear(20, 20, device=device)\n",
    "        self.f4 = nn.Linear(20, 10, device=device)\n",
    "        self.f5 = nn.Linear(10, 1, device=device)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        # self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.f1(x))\n",
    "        x = self.activation(self.f2(x))\n",
    "        x = self.activation(self.f3(x))\n",
    "        x = self.activation(self.f4(x))\n",
    "        x = self.f5(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_diabetes(return_X_y = True, scaled=False)\n",
    "# X, y = make_regression(n_samples=1000, n_features=100)\n",
    "# print(X.shape)\n",
    "\n",
    "X_scaler = MinMaxScaler()\n",
    "X = X_scaler.fit_transform(X)\n",
    "\n",
    "y_scaler = MinMaxScaler()\n",
    "y = y_scaler.fit_transform(y.reshape((-1, 1)))\n",
    "\n",
    "torch_data = TensorDataset(torch.Tensor(X).to(device), torch.Tensor(y).to(device))\n",
    "data_loader = DataLoader(torch_data, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.5097523331642151\n",
      "epoch:  1, loss: 0.5009604692459106\n",
      "epoch:  2, loss: 0.4380524158477783\n",
      "epoch:  3, loss: 0.4298827648162842\n",
      "epoch:  4, loss: 0.4264882504940033\n",
      "epoch:  5, loss: 0.3951069116592407\n",
      "epoch:  6, loss: 0.27165091037750244\n",
      "epoch:  7, loss: 0.18259297311306\n",
      "epoch:  8, loss: 0.17886075377464294\n",
      "epoch:  9, loss: 0.1497703492641449\n",
      "epoch:  10, loss: 0.1490883082151413\n",
      "epoch:  11, loss: 0.11225183308124542\n",
      "epoch:  12, loss: 0.09590497612953186\n",
      "epoch:  13, loss: 0.0929986760020256\n",
      "epoch:  14, loss: 0.09023471176624298\n",
      "epoch:  15, loss: 0.07982666045427322\n",
      "epoch:  16, loss: 0.07568445056676865\n",
      "epoch:  17, loss: 0.07520782947540283\n",
      "epoch:  18, loss: 0.07335309684276581\n",
      "epoch:  19, loss: 0.07245758175849915\n",
      "epoch:  20, loss: 0.0706978440284729\n",
      "epoch:  21, loss: 0.06732102483510971\n",
      "epoch:  22, loss: 0.06428799033164978\n",
      "epoch:  23, loss: 0.058572329580783844\n",
      "epoch:  24, loss: 0.05370477959513664\n",
      "epoch:  25, loss: 0.049855269491672516\n",
      "epoch:  26, loss: 0.048164013773202896\n",
      "epoch:  27, loss: 0.04502119868993759\n",
      "epoch:  28, loss: 0.042282454669475555\n",
      "epoch:  29, loss: 0.03728071600198746\n",
      "epoch:  30, loss: 0.033559445291757584\n",
      "epoch:  31, loss: 0.030637118965387344\n",
      "epoch:  32, loss: 0.030041459947824478\n",
      "epoch:  33, loss: 0.02891664206981659\n",
      "epoch:  34, loss: 0.028409885242581367\n",
      "epoch:  35, loss: 0.02816792018711567\n",
      "epoch:  36, loss: 0.027712635695934296\n",
      "epoch:  37, loss: 0.02727530524134636\n",
      "epoch:  38, loss: 0.02685444802045822\n",
      "epoch:  39, loss: 0.026652678847312927\n",
      "epoch:  40, loss: 0.026265401393175125\n",
      "epoch:  41, loss: 0.026079487055540085\n",
      "epoch:  42, loss: 0.025899313390254974\n",
      "epoch:  43, loss: 0.025853648781776428\n",
      "epoch:  44, loss: 0.025678837671875954\n",
      "epoch:  45, loss: 0.025635061785578728\n",
      "epoch:  46, loss: 0.025468651205301285\n",
      "epoch:  47, loss: 0.025425981730222702\n",
      "epoch:  48, loss: 0.025343211367726326\n",
      "epoch:  49, loss: 0.02518196403980255\n",
      "epoch:  50, loss: 0.02514113113284111\n",
      "epoch:  51, loss: 0.024983668699860573\n",
      "epoch:  52, loss: 0.0249638669192791\n",
      "epoch:  53, loss: 0.024924151599407196\n",
      "epoch:  54, loss: 0.02477436512708664\n",
      "epoch:  55, loss: 0.02473476156592369\n",
      "epoch:  56, loss: 0.024659261107444763\n",
      "epoch:  57, loss: 0.02462214231491089\n",
      "epoch:  58, loss: 0.024547604843974113\n",
      "epoch:  59, loss: 0.02452884055674076\n",
      "epoch:  60, loss: 0.024493122473359108\n",
      "epoch:  61, loss: 0.024458404630422592\n",
      "epoch:  62, loss: 0.024422910064458847\n",
      "epoch:  63, loss: 0.024352792650461197\n",
      "epoch:  64, loss: 0.02431686781346798\n",
      "epoch:  65, loss: 0.02428182028234005\n",
      "epoch:  66, loss: 0.02421271987259388\n",
      "epoch:  67, loss: 0.024194512516260147\n",
      "epoch:  68, loss: 0.024176474660634995\n",
      "epoch:  69, loss: 0.024158913642168045\n",
      "epoch:  70, loss: 0.02415662258863449\n",
      "epoch:  71, loss: 0.02415441907942295\n",
      "epoch:  72, loss: 0.024120081216096878\n",
      "epoch:  73, loss: 0.02411557175219059\n",
      "epoch:  74, loss: 0.024106519296765327\n",
      "epoch:  75, loss: 0.0241021029651165\n",
      "epoch:  76, loss: 0.024097783491015434\n",
      "epoch:  77, loss: 0.024064408615231514\n",
      "epoch:  78, loss: 0.024059824645519257\n",
      "epoch:  79, loss: 0.024055467918515205\n",
      "epoch:  80, loss: 0.024046640843153\n",
      "epoch:  81, loss: 0.024029379710555077\n",
      "epoch:  82, loss: 0.02402500808238983\n",
      "epoch:  83, loss: 0.02400805428624153\n",
      "epoch:  84, loss: 0.02400582656264305\n",
      "epoch:  85, loss: 0.02400147169828415\n",
      "epoch:  86, loss: 0.024000385776162148\n",
      "epoch:  87, loss: 0.023991931229829788\n",
      "epoch:  88, loss: 0.023989761248230934\n",
      "epoch:  89, loss: 0.023973407223820686\n",
      "epoch:  90, loss: 0.023971134796738625\n",
      "epoch:  91, loss: 0.023966757580637932\n",
      "epoch:  92, loss: 0.023962687700986862\n",
      "epoch:  93, loss: 0.023960551247000694\n",
      "epoch:  94, loss: 0.02392781339585781\n",
      "epoch:  95, loss: 0.023923423141241074\n",
      "epoch:  96, loss: 0.023919185623526573\n",
      "epoch:  97, loss: 0.023910444229841232\n",
      "epoch:  98, loss: 0.023908311501145363\n",
      "epoch:  99, loss: 0.023900054395198822\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size = X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "# opt = pytorch_soom.Newton(model.parameters(), lr=1, model=model, hessian_approx=False, c1=1e-4, tau=0.5, line_search_method='backtrack', line_search_cond='armijo')\n",
    "opt = pytorch_soom.Newton(model.parameters(), lr=1, model=model, hessian_approx=False, c1=1e-4, tau=0.5, line_search_method='backtrack', line_search_cond='wolfe')\n",
    "# opt = pytorch_soom.Newton(model.parameters(), lr=1, model=model, hessian_approx=False, c1=1e-4, tau=0.5, line_search_method='backtrack', line_search_cond='strong-wolfe')\n",
    "# opt = pytorch_soom.Newton(model.parameters(), lr=1, model=model, hessian_approx=False, c1=1e-4, tau=0.5, line_search_method='backtrack', line_search_cond='goldstein')\n",
    "\n",
    "all_loss = {}\n",
    "for epoch in range(100):\n",
    "    print('epoch: ', epoch, end='')\n",
    "    all_loss[epoch+1] = 0\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y, loss_fn)\n",
    "\n",
    "        all_loss[epoch+1] += loss\n",
    "    all_loss[epoch+1] /= len(data_loader)\n",
    "    print(', loss: {}'.format(all_loss[epoch+1].detach().numpy().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.08656758069992065\n",
      "epoch:  1, loss: 0.08137121051549911\n",
      "epoch:  2, loss: 0.06291179358959198\n",
      "epoch:  3, loss: 0.06012265011668205\n",
      "epoch:  4, loss: 0.05838647484779358\n",
      "epoch:  5, loss: 0.058228667825460434\n",
      "epoch:  6, loss: 0.05772688612341881\n",
      "epoch:  7, loss: 0.057529449462890625\n",
      "epoch:  8, loss: 0.05750676989555359\n",
      "epoch:  9, loss: 0.05740099772810936\n",
      "epoch:  10, loss: 0.0573103241622448\n",
      "epoch:  11, loss: 0.057204682379961014\n",
      "epoch:  12, loss: 0.056979354470968246\n",
      "epoch:  13, loss: 0.056944865733385086\n",
      "epoch:  14, loss: 0.05679263547062874\n",
      "epoch:  15, loss: 0.05633760988712311\n",
      "epoch:  16, loss: 0.05442000553011894\n",
      "epoch:  17, loss: 0.05227173864841461\n",
      "epoch:  18, loss: 0.050003618001937866\n",
      "epoch:  19, loss: 0.04958706349134445\n",
      "epoch:  20, loss: 0.04880253225564957\n",
      "epoch:  21, loss: 0.03947758302092552\n",
      "epoch:  22, loss: 0.03696170821785927\n",
      "epoch:  23, loss: 0.03608682379126549\n",
      "epoch:  24, loss: 0.03570878878235817\n",
      "epoch:  25, loss: 0.033621177077293396\n",
      "epoch:  26, loss: 0.03346538916230202\n",
      "epoch:  27, loss: 0.033238161355257034\n",
      "epoch:  28, loss: 0.03278570994734764\n",
      "epoch:  29, loss: 0.03248923271894455\n",
      "epoch:  30, loss: 0.032022397965192795\n",
      "epoch:  31, loss: 0.03166402876377106\n",
      "epoch:  32, loss: 0.03155537694692612\n",
      "epoch:  33, loss: 0.031508900225162506\n",
      "epoch:  34, loss: 0.03149129077792168\n",
      "epoch:  35, loss: 0.031377341598272324\n",
      "epoch:  36, loss: 0.031331099569797516\n",
      "epoch:  37, loss: 0.0312734991312027\n",
      "epoch:  38, loss: 0.031251829117536545\n",
      "epoch:  39, loss: 0.03107016161084175\n",
      "epoch:  40, loss: 0.030979936942458153\n",
      "epoch:  41, loss: 0.03094402141869068\n",
      "epoch:  42, loss: 0.03062625043094158\n",
      "epoch:  43, loss: 0.03061121329665184\n",
      "epoch:  44, loss: 0.030549919232726097\n",
      "epoch:  45, loss: 0.03039422631263733\n",
      "epoch:  46, loss: 0.03033711388707161\n",
      "epoch:  47, loss: 0.030137453228235245\n",
      "epoch:  48, loss: 0.030131328850984573\n",
      "epoch:  49, loss: 0.03010438196361065\n",
      "epoch:  50, loss: 0.030079001560807228\n",
      "epoch:  51, loss: 0.03007296472787857\n",
      "epoch:  52, loss: 0.030003484338521957\n",
      "epoch:  53, loss: 0.02990507148206234\n",
      "epoch:  54, loss: 0.02989906072616577\n",
      "epoch:  55, loss: 0.029898138716816902\n",
      "epoch:  56, loss: 0.029882634058594704\n",
      "epoch:  57, loss: 0.029881492257118225\n",
      "epoch:  58, loss: 0.029865028336644173\n",
      "epoch:  59, loss: 0.029863964766263962\n",
      "epoch:  60, loss: 0.029856668785214424\n",
      "epoch:  61, loss: 0.02985459752380848\n",
      "epoch:  62, loss: 0.029847197234630585\n",
      "epoch:  63, loss: 0.02984565868973732\n",
      "epoch:  64, loss: 0.02984313666820526\n",
      "epoch:  65, loss: 0.02983850985765457\n",
      "epoch:  66, loss: 0.029832690954208374\n",
      "epoch:  67, loss: 0.029825516045093536\n",
      "epoch:  68, loss: 0.029820716008543968\n",
      "epoch:  69, loss: 0.02981526218354702\n",
      "epoch:  70, loss: 0.02981261909008026\n",
      "epoch:  71, loss: 0.02980639971792698\n",
      "epoch:  72, loss: 0.029804885387420654\n",
      "epoch:  73, loss: 0.02979763224720955\n",
      "epoch:  74, loss: 0.029795430600643158\n",
      "epoch:  75, loss: 0.02979397214949131\n",
      "epoch:  76, loss: 0.029792239889502525\n",
      "epoch:  77, loss: 0.029788905754685402\n",
      "epoch:  78, loss: 0.029787994921207428\n",
      "epoch:  79, loss: 0.02978472039103508\n",
      "epoch:  80, loss: 0.029783248901367188\n",
      "epoch:  81, loss: 0.029780400916934013\n",
      "epoch:  82, loss: 0.029777061194181442\n",
      "epoch:  83, loss: 0.02977549470961094\n",
      "epoch:  84, loss: 0.029771851375699043\n",
      "epoch:  85, loss: 0.029771095141768456\n",
      "epoch:  86, loss: 0.029769416898489\n",
      "epoch:  87, loss: 0.02976825460791588\n",
      "epoch:  88, loss: 0.029766477644443512\n",
      "epoch:  89, loss: 0.02976571023464203\n",
      "epoch:  90, loss: 0.029764782637357712\n",
      "epoch:  91, loss: 0.029762903228402138\n",
      "epoch:  92, loss: 0.02976207248866558\n",
      "epoch:  93, loss: 0.02976035699248314\n",
      "epoch:  94, loss: 0.02976030483841896\n",
      "epoch:  95, loss: 0.029758494347333908\n",
      "epoch:  96, loss: 0.02975573018193245\n",
      "epoch:  97, loss: 0.029751941561698914\n",
      "epoch:  98, loss: 0.029750535264611244\n",
      "epoch:  99, loss: 0.02974891848862171\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size = X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "opt = pytorch_soom.Newton(model.parameters(), lr=1, model=model, hessian_approx=True, c1=1e-4, tau=0.5, line_search_method='backtrack', line_search_cond=\"armijo\")\n",
    "# opt = pytorch_soom.Newton(model.parameters(), lr=1, model=model, hessian_approx=True, c1=1e-4, tau=0.5, line_search_method='backtrack', line_search_cond=\"wolfe\")\n",
    "# opt = pytorch_soom.Newton(model.parameters(), lr=1, model=model, hessian_approx=True, c1=1e-4, tau=0.5, line_search_method='backtrack', line_search_cond=\"strong-wolfe\")\n",
    "# opt = pytorch_soom.Newton(model.parameters(), lr=1, model=model, hessian_approx=True, c1=0.1, tau=0.99, line_search_method='backtrack', line_search_cond=\"goldstein\")\n",
    "\n",
    "all_loss = {}\n",
    "for epoch in range(100):\n",
    "    print('epoch: ', epoch, end='')\n",
    "    all_loss[epoch+1] = 0\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y, loss_fn)\n",
    "\n",
    "        all_loss[epoch+1] += loss\n",
    "    all_loss[epoch+1] /= len(data_loader)\n",
    "    print(', loss: {}'.format(all_loss[epoch+1].detach().numpy().item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
