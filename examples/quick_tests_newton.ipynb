{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pytorch_soom\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.f1 = nn.Linear(1, 10, device=device)\n",
    "        self.f2 = nn.Linear(10, 20, device=device)\n",
    "        self.f3 = nn.Linear(20, 20, device=device)\n",
    "        self.f4 = nn.Linear(20, 10, device=device)\n",
    "        self.f5 = nn.Linear(10, 1, device=device)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        # self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.f1(x))\n",
    "        x = self.activation(self.f2(x))\n",
    "        x = self.activation(self.f3(x))\n",
    "        x = self.activation(self.f4(x))\n",
    "        x = self.f5(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.uniform(0, 1, size=(1000, 1))\n",
    "# y = X[:, 0] - X[:, 1]**2 + 2 * X[:, 2] * X[:, 3] + (1 / ((1 + X[:, 4]) ** 6))\n",
    "y = np.sinc(X).sum(axis=1, keepdims=True)\n",
    "\n",
    "torch_data = TensorDataset(torch.Tensor(X).to(device), torch.Tensor(y).to(device))\n",
    "data_loader = DataLoader(torch_data, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.6696417331695557\n",
      "epoch:  1, loss: 0.6703011393547058\n",
      "epoch:  2, loss: 0.6636910438537598\n",
      "epoch:  3, loss: 0.6507073640823364\n",
      "epoch:  4, loss: 0.6428490281105042\n",
      "epoch:  5, loss: 0.6296328902244568\n",
      "epoch:  6, loss: 0.6126193404197693\n",
      "epoch:  7, loss: 0.6042999625205994\n",
      "epoch:  8, loss: 0.5903075933456421\n",
      "epoch:  9, loss: 0.57951420545578\n",
      "epoch:  10, loss: 0.569944441318512\n",
      "epoch:  11, loss: 0.5611693263053894\n",
      "epoch:  12, loss: 0.5530411601066589\n",
      "epoch:  13, loss: 0.5438578128814697\n",
      "epoch:  14, loss: 0.5330502390861511\n",
      "epoch:  15, loss: 0.5199289917945862\n",
      "epoch:  16, loss: 0.22097478806972504\n",
      "epoch:  17, loss: 0.21790513396263123\n",
      "epoch:  18, loss: 0.21519875526428223\n",
      "epoch:  19, loss: 0.21125604212284088\n",
      "epoch:  20, loss: 0.2070716768503189\n",
      "epoch:  21, loss: 0.19994871318340302\n",
      "epoch:  22, loss: 0.1974535435438156\n",
      "epoch:  23, loss: 0.1926138550043106\n",
      "epoch:  24, loss: 0.18849338591098785\n",
      "epoch:  25, loss: 0.18567220866680145\n",
      "epoch:  26, loss: 0.1822865605354309\n",
      "epoch:  27, loss: 0.17940451204776764\n",
      "epoch:  28, loss: 0.1767347902059555\n",
      "epoch:  29, loss: 0.17403694987297058\n",
      "epoch:  30, loss: 0.17127791047096252\n",
      "epoch:  31, loss: 0.16845466196537018\n",
      "epoch:  32, loss: 0.16578546166419983\n",
      "epoch:  33, loss: 0.16310571134090424\n",
      "epoch:  34, loss: 0.16125763952732086\n",
      "epoch:  35, loss: 0.15862177312374115\n",
      "epoch:  36, loss: 0.1561993956565857\n",
      "epoch:  37, loss: 0.1540878862142563\n",
      "epoch:  38, loss: 0.15193875133991241\n",
      "epoch:  39, loss: 0.15004649758338928\n",
      "epoch:  40, loss: 0.14785882830619812\n",
      "epoch:  41, loss: 0.14571797847747803\n",
      "epoch:  42, loss: 0.1436777412891388\n",
      "epoch:  43, loss: 0.1414518803358078\n",
      "epoch:  44, loss: 0.139566570520401\n",
      "epoch:  45, loss: 0.13734866678714752\n",
      "epoch:  46, loss: 0.13578476011753082\n",
      "epoch:  47, loss: 0.13392692804336548\n",
      "epoch:  48, loss: 0.13238340616226196\n",
      "epoch:  49, loss: 0.1306595802307129\n",
      "epoch:  50, loss: 0.12890243530273438\n",
      "epoch:  51, loss: 0.1290784478187561\n",
      "epoch:  52, loss: 0.1266755759716034\n",
      "epoch:  53, loss: 0.12459985911846161\n",
      "epoch:  54, loss: 0.12252482026815414\n",
      "epoch:  55, loss: 0.12155033648014069\n",
      "epoch:  56, loss: 0.11978115141391754\n",
      "epoch:  57, loss: 0.11814092099666595\n",
      "epoch:  58, loss: 0.11663010716438293\n",
      "epoch:  59, loss: 0.11488859355449677\n",
      "epoch:  60, loss: 0.11326342821121216\n",
      "epoch:  61, loss: 0.11147694289684296\n",
      "epoch:  62, loss: 0.10814342647790909\n",
      "epoch:  63, loss: 0.10686834156513214\n",
      "epoch:  64, loss: 0.13852503895759583\n",
      "epoch:  65, loss: 0.12867800891399384\n",
      "epoch:  66, loss: 0.12672549486160278\n",
      "epoch:  67, loss: 0.12302995473146439\n",
      "epoch:  68, loss: 0.12124674022197723\n",
      "epoch:  69, loss: 0.1188960000872612\n",
      "epoch:  70, loss: 0.11674980819225311\n",
      "epoch:  71, loss: 0.11475168168544769\n",
      "epoch:  72, loss: 0.11301654577255249\n",
      "epoch:  73, loss: 0.11105921864509583\n",
      "epoch:  74, loss: 0.10931900143623352\n",
      "epoch:  75, loss: 0.10705386847257614\n",
      "epoch:  76, loss: 0.10528217256069183\n",
      "epoch:  77, loss: 0.10350876301527023\n",
      "epoch:  78, loss: 0.10176020860671997\n",
      "epoch:  79, loss: 0.09535861760377884\n",
      "epoch:  80, loss: 0.09360600262880325\n",
      "epoch:  81, loss: 0.09212201088666916\n",
      "epoch:  82, loss: 0.09108933061361313\n",
      "epoch:  83, loss: 0.08957795053720474\n",
      "epoch:  84, loss: 0.08823429048061371\n",
      "epoch:  85, loss: 0.0864926353096962\n",
      "epoch:  86, loss: 0.08513474464416504\n",
      "epoch:  87, loss: 0.0839432030916214\n",
      "epoch:  88, loss: 0.08271889388561249\n",
      "epoch:  89, loss: 0.08105318993330002\n",
      "epoch:  90, loss: 0.07972511649131775\n",
      "epoch:  91, loss: 0.07844491302967072\n",
      "epoch:  92, loss: 0.07724559307098389\n",
      "epoch:  93, loss: 0.07628552615642548\n",
      "epoch:  94, loss: 0.07501830905675888\n",
      "epoch:  95, loss: 0.07642862945795059\n",
      "epoch:  96, loss: 0.07514633238315582\n",
      "epoch:  97, loss: 0.07404286414384842\n",
      "epoch:  98, loss: 0.07291258126497269\n",
      "epoch:  99, loss: 0.07180704176425934\n"
     ]
    }
   ],
   "source": [
    "model = Net(device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "opt = pytorch_soom.Newton(model.parameters(), lr=1, model=model)\n",
    "\n",
    "all_loss = {}\n",
    "for epoch in range(100):\n",
    "    print('epoch: ', epoch, end='')\n",
    "    all_loss[epoch+1] = 0\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x)\n",
    "\n",
    "        all_loss[epoch+1] += loss\n",
    "    all_loss[epoch+1] /= len(data_loader)\n",
    "    print(', loss: {}'.format(all_loss[epoch+1].detach().numpy().item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
