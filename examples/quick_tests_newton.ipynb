{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pytorch_soom\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import *\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.f1 = nn.Linear(input_size, 10, device=device)\n",
    "        self.f2 = nn.Linear(10, 20, device=device)\n",
    "        self.f3 = nn.Linear(20, 20, device=device)\n",
    "        self.f4 = nn.Linear(20, 10, device=device)\n",
    "        self.f5 = nn.Linear(10, 1, device=device)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        # self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.f1(x))\n",
    "        x = self.activation(self.f2(x))\n",
    "        x = self.activation(self.f3(x))\n",
    "        x = self.activation(self.f4(x))\n",
    "        x = self.f5(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_diabetes(return_X_y = True, scaled=False)\n",
    "# X, y = make_regression(n_samples=1000, n_features=100)\n",
    "# print(X.shape)\n",
    "\n",
    "X_scaler = MinMaxScaler()\n",
    "X = X_scaler.fit_transform(X)\n",
    "\n",
    "y_scaler = MinMaxScaler()\n",
    "y = y_scaler.fit_transform(y.reshape((-1, 1)))\n",
    "\n",
    "torch_data = TensorDataset(torch.Tensor(X).to(device), torch.Tensor(y).to(device))\n",
    "data_loader = DataLoader(torch_data, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.22615033388137817\n",
      "epoch:  1, loss: 0.059771452099084854\n",
      "epoch:  2, loss: 0.05810341611504555\n",
      "epoch:  3, loss: 0.050128042697906494\n",
      "epoch:  4, loss: 0.04632483050227165\n",
      "epoch:  5, loss: 0.04360844939947128\n",
      "epoch:  6, loss: 0.035846322774887085\n",
      "epoch:  7, loss: 0.03239009529352188\n",
      "epoch:  8, loss: 0.03190647438168526\n",
      "epoch:  9, loss: 0.027748161926865578\n",
      "epoch:  10, loss: 0.027609001845121384\n",
      "epoch:  11, loss: 0.025527413934469223\n",
      "epoch:  12, loss: 0.02480781264603138\n",
      "epoch:  13, loss: 0.024347418919205666\n",
      "epoch:  14, loss: 0.02412429265677929\n",
      "epoch:  15, loss: 0.023808855563402176\n",
      "epoch:  16, loss: 0.023439757525920868\n",
      "epoch:  17, loss: 0.02314920537173748\n",
      "epoch:  18, loss: 0.023126302286982536\n",
      "epoch:  19, loss: 0.022728100419044495\n",
      "epoch:  20, loss: 0.02230403944849968\n",
      "epoch:  21, loss: 0.021877767518162727\n",
      "epoch:  22, loss: 0.021844860166311264\n",
      "epoch:  23, loss: 0.021375112235546112\n",
      "epoch:  24, loss: 0.02125069685280323\n",
      "epoch:  25, loss: 0.02109985053539276\n",
      "epoch:  26, loss: 0.02027909830212593\n",
      "epoch:  27, loss: 0.01995161920785904\n",
      "epoch:  28, loss: 0.019901547580957413\n",
      "epoch:  29, loss: 0.019669221714138985\n",
      "epoch:  30, loss: 0.019518371671438217\n",
      "epoch:  31, loss: 0.01948116533458233\n",
      "epoch:  32, loss: 0.019412223249673843\n",
      "epoch:  33, loss: 0.01929561421275139\n",
      "epoch:  34, loss: 0.019291754812002182\n",
      "epoch:  35, loss: 0.019233286380767822\n",
      "epoch:  36, loss: 0.019143760204315186\n",
      "epoch:  37, loss: 0.019029265269637108\n",
      "epoch:  38, loss: 0.019009239971637726\n",
      "epoch:  39, loss: 0.01900673471391201\n",
      "epoch:  40, loss: 0.018968332558870316\n",
      "epoch:  41, loss: 0.018955212086439133\n",
      "epoch:  42, loss: 0.018949920311570168\n",
      "epoch:  43, loss: 0.01894274726510048\n",
      "epoch:  44, loss: 0.018942151218652725\n",
      "epoch:  45, loss: 0.018941638991236687\n",
      "epoch:  46, loss: 0.018939824774861336\n",
      "epoch:  47, loss: 0.018939798697829247\n",
      "epoch:  48, loss: 0.01893976517021656\n",
      "epoch:  49, loss: 0.01893937774002552\n",
      "epoch:  50, loss: 0.018938979133963585\n",
      "epoch:  51, loss: 0.018938705325126648\n",
      "epoch:  52, loss: 0.01893860660493374\n",
      "epoch:  53, loss: 0.018937736749649048\n",
      "epoch:  54, loss: 0.018937712535262108\n",
      "epoch:  55, loss: 0.018937712535262108\n",
      "epoch:  56, loss: 0.018937712535262108\n",
      "epoch:  57, loss: 0.018937712535262108\n",
      "epoch:  58, loss: 0.018937712535262108\n",
      "epoch:  59, loss: 0.018937712535262108\n",
      "epoch:  60, loss: 0.018937712535262108\n",
      "epoch:  61, loss: 0.018937712535262108\n",
      "epoch:  62, loss: 0.018937712535262108\n",
      "epoch:  63, loss: 0.018937712535262108\n",
      "epoch:  64, loss: 0.018937712535262108\n",
      "epoch:  65, loss: 0.018937712535262108\n",
      "epoch:  66, loss: 0.018937712535262108\n",
      "epoch:  67, loss: 0.018937712535262108\n",
      "epoch:  68, loss: 0.018937712535262108\n",
      "epoch:  69, loss: 0.018937712535262108\n",
      "epoch:  70, loss: 0.018937712535262108\n",
      "epoch:  71, loss: 0.018937712535262108\n",
      "epoch:  72, loss: 0.018937712535262108\n",
      "epoch:  73, loss: 0.018937712535262108\n",
      "epoch:  74, loss: 0.018937712535262108\n",
      "epoch:  75, loss: 0.018937712535262108\n",
      "epoch:  76, loss: 0.018937712535262108\n",
      "epoch:  77, loss: 0.018937712535262108\n",
      "epoch:  78, loss: 0.018937712535262108\n",
      "epoch:  79, loss: 0.018937712535262108\n",
      "epoch:  80, loss: 0.018937712535262108\n",
      "epoch:  81, loss: 0.018937712535262108\n",
      "epoch:  82, loss: 0.018937712535262108\n",
      "epoch:  83, loss: 0.018937712535262108\n",
      "epoch:  84, loss: 0.018937712535262108\n",
      "epoch:  85, loss: 0.018937712535262108\n",
      "epoch:  86, loss: 0.018937712535262108\n",
      "epoch:  87, loss: 0.018937712535262108\n",
      "epoch:  88, loss: 0.018937712535262108\n",
      "epoch:  89, loss: 0.018937712535262108\n",
      "epoch:  90, loss: 0.018937712535262108\n",
      "epoch:  91, loss: 0.018937712535262108\n",
      "epoch:  92, loss: 0.018937712535262108\n",
      "epoch:  93, loss: 0.018937712535262108\n",
      "epoch:  94, loss: 0.018937712535262108\n",
      "epoch:  95, loss: 0.018937712535262108\n",
      "epoch:  96, loss: 0.018937712535262108\n",
      "epoch:  97, loss: 0.018937712535262108\n",
      "epoch:  98, loss: 0.018937712535262108\n",
      "epoch:  99, loss: 0.018937712535262108\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size = X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "opt = pytorch_soom.Newton(model.parameters(), lr=1, model=model, hessian_approx=False, c1=1e-4, tau=0.5, line_search_method='backtrack')\n",
    "\n",
    "all_loss = {}\n",
    "for epoch in range(100):\n",
    "    print('epoch: ', epoch, end='')\n",
    "    all_loss[epoch+1] = 0\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y, loss_fn)\n",
    "\n",
    "        all_loss[epoch+1] += loss\n",
    "    all_loss[epoch+1] /= len(data_loader)\n",
    "    print(', loss: {}'.format(all_loss[epoch+1].detach().numpy().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.13586580753326416\n",
      "epoch:  1, loss: 0.05672239884734154\n",
      "epoch:  2, loss: 0.05422181636095047\n",
      "epoch:  3, loss: 0.0540899895131588\n",
      "epoch:  4, loss: 0.0533779039978981\n",
      "epoch:  5, loss: 0.05321604385972023\n",
      "epoch:  6, loss: 0.05292700603604317\n",
      "epoch:  7, loss: 0.0523856058716774\n",
      "epoch:  8, loss: 0.051473237574100494\n",
      "epoch:  9, loss: 0.051403313875198364\n",
      "epoch:  10, loss: 0.050941068679094315\n",
      "epoch:  11, loss: 0.05079410597681999\n",
      "epoch:  12, loss: 0.050296615809202194\n",
      "epoch:  13, loss: 0.050226327031850815\n",
      "epoch:  14, loss: 0.049812108278274536\n",
      "epoch:  15, loss: 0.049738992005586624\n",
      "epoch:  16, loss: 0.049449142068624496\n",
      "epoch:  17, loss: 0.049125298857688904\n",
      "epoch:  18, loss: 0.04873129725456238\n",
      "epoch:  19, loss: 0.048401445150375366\n",
      "epoch:  20, loss: 0.04737908020615578\n",
      "epoch:  21, loss: 0.04734838753938675\n",
      "epoch:  22, loss: 0.047150831669569016\n",
      "epoch:  23, loss: 0.046923331916332245\n",
      "epoch:  24, loss: 0.04684232175350189\n",
      "epoch:  25, loss: 0.04569000005722046\n",
      "epoch:  26, loss: 0.045032236725091934\n",
      "epoch:  27, loss: 0.04447919502854347\n",
      "epoch:  28, loss: 0.043040692806243896\n",
      "epoch:  29, loss: 0.042121704667806625\n",
      "epoch:  30, loss: 0.04122580587863922\n",
      "epoch:  31, loss: 0.040291573852300644\n",
      "epoch:  32, loss: 0.03927846625447273\n",
      "epoch:  33, loss: 0.03849495202302933\n",
      "epoch:  34, loss: 0.038059014827013016\n",
      "epoch:  35, loss: 0.03782996907830238\n",
      "epoch:  36, loss: 0.03767520189285278\n",
      "epoch:  37, loss: 0.03383037447929382\n",
      "epoch:  38, loss: 0.03375008702278137\n",
      "epoch:  39, loss: 0.033479243516922\n",
      "epoch:  40, loss: 0.0328340008854866\n",
      "epoch:  41, loss: 0.032458994537591934\n",
      "epoch:  42, loss: 0.03240188956260681\n",
      "epoch:  43, loss: 0.03222237899899483\n",
      "epoch:  44, loss: 0.03203468769788742\n",
      "epoch:  45, loss: 0.03188582882285118\n",
      "epoch:  46, loss: 0.031738657504320145\n",
      "epoch:  47, loss: 0.031734246760606766\n",
      "epoch:  48, loss: 0.03091314062476158\n",
      "epoch:  49, loss: 0.030649837106466293\n",
      "epoch:  50, loss: 0.03029404766857624\n",
      "epoch:  51, loss: 0.03022402711212635\n",
      "epoch:  52, loss: 0.030177049338817596\n",
      "epoch:  53, loss: 0.030151890590786934\n",
      "epoch:  54, loss: 0.030133413150906563\n",
      "epoch:  55, loss: 0.03002350963652134\n",
      "epoch:  56, loss: 0.03000549226999283\n",
      "epoch:  57, loss: 0.029959755018353462\n",
      "epoch:  58, loss: 0.0298968143761158\n",
      "epoch:  59, loss: 0.029888005927205086\n",
      "epoch:  60, loss: 0.02987966686487198\n",
      "epoch:  61, loss: 0.029870929196476936\n",
      "epoch:  62, loss: 0.02986205741763115\n",
      "epoch:  63, loss: 0.02985461615025997\n",
      "epoch:  64, loss: 0.029847148805856705\n",
      "epoch:  65, loss: 0.029841110110282898\n",
      "epoch:  66, loss: 0.029833681881427765\n",
      "epoch:  67, loss: 0.02983267605304718\n",
      "epoch:  68, loss: 0.029824571684002876\n",
      "epoch:  69, loss: 0.029816722497344017\n",
      "epoch:  70, loss: 0.02981037087738514\n",
      "epoch:  71, loss: 0.02980106696486473\n",
      "epoch:  72, loss: 0.02979176864027977\n",
      "epoch:  73, loss: 0.02979060262441635\n",
      "epoch:  74, loss: 0.029787223786115646\n",
      "epoch:  75, loss: 0.029784653335809708\n",
      "epoch:  76, loss: 0.029780296608805656\n",
      "epoch:  77, loss: 0.02977846749126911\n",
      "epoch:  78, loss: 0.029777228832244873\n",
      "epoch:  79, loss: 0.02977280505001545\n",
      "epoch:  80, loss: 0.029767107218503952\n",
      "epoch:  81, loss: 0.029765836894512177\n",
      "epoch:  82, loss: 0.02976240962743759\n",
      "epoch:  83, loss: 0.02974897064268589\n",
      "epoch:  84, loss: 0.029746122658252716\n",
      "epoch:  85, loss: 0.029738128185272217\n",
      "epoch:  86, loss: 0.029735660180449486\n",
      "epoch:  87, loss: 0.02973141521215439\n",
      "epoch:  88, loss: 0.029730426147580147\n",
      "epoch:  89, loss: 0.029725229367613792\n",
      "epoch:  90, loss: 0.029723042622208595\n",
      "epoch:  91, loss: 0.02971465513110161\n",
      "epoch:  92, loss: 0.02971344068646431\n",
      "epoch:  93, loss: 0.029710428789258003\n",
      "epoch:  94, loss: 0.029703697189688683\n",
      "epoch:  95, loss: 0.029700567945837975\n",
      "epoch:  96, loss: 0.029698647558689117\n",
      "epoch:  97, loss: 0.0296944472938776\n",
      "epoch:  98, loss: 0.029694058001041412\n",
      "epoch:  99, loss: 0.029687659814953804\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size = X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "opt = pytorch_soom.Newton(model.parameters(), lr=1, model=model, hessian_approx=True, c1=1e-4, tau=0.5, line_search_method='backtrack')\n",
    "\n",
    "all_loss = {}\n",
    "for epoch in range(100):\n",
    "    print('epoch: ', epoch, end='')\n",
    "    all_loss[epoch+1] = 0\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y, loss_fn)\n",
    "\n",
    "        all_loss[epoch+1] += loss\n",
    "    all_loss[epoch+1] /= len(data_loader)\n",
    "    print(', loss: {}'.format(all_loss[epoch+1].detach().numpy().item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
