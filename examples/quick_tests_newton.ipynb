{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pytorch_soom\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.f1 = nn.Linear(1, 10)\n",
    "        self.f2 = nn.Linear(10, 20)\n",
    "        self.f3 = nn.Linear(20, 20)\n",
    "        self.f4 = nn.Linear(20, 10)\n",
    "        self.f5 = nn.Linear(10, 1)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        # self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.f1(x))\n",
    "        x = self.activation(self.f2(x))\n",
    "        x = self.activation(self.f3(x))\n",
    "        x = self.activation(self.f4(x))\n",
    "        x = self.f5(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.uniform(0, 1, size=(300, 1))\n",
    "# y = X[:, 0] - X[:, 1]**2 + 2 * X[:, 2] * X[:, 3] + (1 / ((1 + X[:, 4]) ** 6))\n",
    "y = np.sinc(X).sum(axis=1, keepdims=True)\n",
    "\n",
    "torch_data = TensorDataset(torch.Tensor(X), torch.Tensor(y))\n",
    "data_loader = DataLoader(torch_data, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.6954212188720703\n",
      "epoch:  1, loss: 0.49472880363464355\n",
      "epoch:  2, loss: 0.3285824954509735\n",
      "epoch:  3, loss: 0.23577441275119781\n",
      "epoch:  4, loss: 0.18615181744098663\n",
      "epoch:  5, loss: 0.15461742877960205\n",
      "epoch:  6, loss: 0.13190722465515137\n",
      "epoch:  7, loss: 0.12261179834604263\n",
      "epoch:  8, loss: 0.11546588689088821\n",
      "epoch:  9, loss: 0.11171431094408035\n",
      "epoch:  10, loss: 0.10975382477045059\n",
      "epoch:  11, loss: 0.10867925733327866\n",
      "epoch:  12, loss: 0.1080649271607399\n",
      "epoch:  13, loss: 0.10769862681627274\n",
      "epoch:  14, loss: 0.10750968009233475\n",
      "epoch:  15, loss: 0.10738686472177505\n",
      "epoch:  16, loss: 0.10731904953718185\n",
      "epoch:  17, loss: 0.10726212710142136\n",
      "epoch:  18, loss: 0.10717464238405228\n",
      "epoch:  19, loss: 0.16936153173446655\n",
      "epoch:  20, loss: 0.2588174343109131\n",
      "epoch:  21, loss: 0.13776926696300507\n",
      "epoch:  22, loss: 0.12368468195199966\n",
      "epoch:  23, loss: 0.11617035418748856\n",
      "epoch:  24, loss: 0.11184003204107285\n",
      "epoch:  25, loss: 0.10974719375371933\n",
      "epoch:  26, loss: 0.10863876342773438\n",
      "epoch:  27, loss: 0.10805666446685791\n",
      "epoch:  28, loss: 0.10774946212768555\n",
      "epoch:  29, loss: 0.10756123811006546\n",
      "epoch:  30, loss: 0.10739034414291382\n",
      "epoch:  31, loss: 0.10728893429040909\n",
      "epoch:  32, loss: 0.10727348178625107\n",
      "epoch:  33, loss: 0.10723453015089035\n",
      "epoch:  34, loss: 0.10721755027770996\n",
      "epoch:  35, loss: 0.10720524936914444\n",
      "epoch:  36, loss: 0.10718362778425217\n",
      "epoch:  37, loss: 0.10718628019094467\n",
      "epoch:  38, loss: 0.1071842685341835\n",
      "epoch:  39, loss: 0.10718335956335068\n",
      "epoch:  40, loss: 0.10718941688537598\n",
      "epoch:  41, loss: 0.10717126727104187\n",
      "epoch:  42, loss: 0.10720684379339218\n",
      "epoch:  43, loss: 0.10717888921499252\n",
      "epoch:  44, loss: 0.10719134658575058\n",
      "epoch:  45, loss: 0.10718856006860733\n",
      "epoch:  46, loss: 0.1071772575378418\n",
      "epoch:  47, loss: 0.10718227177858353\n",
      "epoch:  48, loss: 0.10718121379613876\n",
      "epoch:  49, loss: 0.10717976093292236\n",
      "epoch:  50, loss: 0.10718917846679688\n",
      "epoch:  51, loss: 0.10717067867517471\n",
      "epoch:  52, loss: 0.10716336965560913\n",
      "epoch:  53, loss: 0.10718673467636108\n",
      "epoch:  54, loss: 0.10717520862817764\n",
      "epoch:  55, loss: 0.10718826204538345\n",
      "epoch:  56, loss: 0.10719293355941772\n",
      "epoch:  57, loss: 0.10719005018472672\n",
      "epoch:  58, loss: 0.10718046873807907\n",
      "epoch:  59, loss: 0.10719120502471924\n",
      "epoch:  60, loss: 0.10718295723199844\n",
      "epoch:  61, loss: 0.10718465596437454\n",
      "epoch:  62, loss: 0.10718081146478653\n",
      "epoch:  63, loss: 0.10717489570379257\n",
      "epoch:  64, loss: 0.10717904567718506\n",
      "epoch:  65, loss: 0.1071900799870491\n",
      "epoch:  66, loss: 0.10718044638633728\n",
      "epoch:  67, loss: 0.10717460513114929\n",
      "epoch:  68, loss: 0.10718762874603271\n",
      "epoch:  69, loss: 0.10703819990158081\n",
      "epoch:  70, loss: 0.10732000321149826\n",
      "epoch:  71, loss: 0.10725382715463638\n",
      "epoch:  72, loss: 0.10724234580993652\n",
      "epoch:  73, loss: 0.10721983760595322\n",
      "epoch:  74, loss: 0.10719341039657593\n",
      "epoch:  75, loss: 0.10721006244421005\n",
      "epoch:  76, loss: 0.10718876123428345\n",
      "epoch:  77, loss: 0.107196144759655\n",
      "epoch:  78, loss: 0.10719582438468933\n",
      "epoch:  79, loss: 0.10718559473752975\n",
      "epoch:  80, loss: 0.1071920096874237\n",
      "epoch:  81, loss: 0.10719431191682816\n",
      "epoch:  82, loss: 0.10718593746423721\n",
      "epoch:  83, loss: 0.10716000944375992\n",
      "epoch:  84, loss: 0.107185959815979\n",
      "epoch:  85, loss: 0.10719016939401627\n",
      "epoch:  86, loss: 0.1304607093334198\n",
      "epoch:  87, loss: 0.17008183896541595\n",
      "epoch:  88, loss: 0.14508093893527985\n",
      "epoch:  89, loss: 0.12220638990402222\n",
      "epoch:  90, loss: 0.11496448516845703\n",
      "epoch:  91, loss: 0.11192390322685242\n",
      "epoch:  92, loss: 0.11094266176223755\n",
      "epoch:  93, loss: 0.11052611470222473\n",
      "epoch:  94, loss: 0.10969456285238266\n",
      "epoch:  95, loss: 0.10856779664754868\n",
      "epoch:  96, loss: 0.10793773084878922\n",
      "epoch:  97, loss: 0.10766863077878952\n",
      "epoch:  98, loss: 0.10736557096242905\n",
      "epoch:  99, loss: 0.1073557510972023\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "loss_fn = nn.MSELoss()\n",
    "opt = pytorch_soom.Newton(model.parameters(), lr=1, model=model)\n",
    "\n",
    "all_loss = {}\n",
    "for epoch in range(100):\n",
    "    print('epoch: ', epoch, end='')\n",
    "    all_loss[epoch+1] = 0\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x)\n",
    "\n",
    "        all_loss[epoch+1] += loss\n",
    "    all_loss[epoch+1] /= len(data_loader)\n",
    "    print(', loss: {}'.format(all_loss[epoch+1].detach().numpy().item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
