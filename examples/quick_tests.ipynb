{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pytorch_soom\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.f1 = nn.Linear(1, 10)\n",
    "        self.f2 = nn.Linear(10, 20)\n",
    "        self.f3 = nn.Linear(20, 20)\n",
    "        self.f4 = nn.Linear(20, 10)\n",
    "        self.f5 = nn.Linear(10, 1)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        # self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.f1(x))\n",
    "        x = self.activation(self.f2(x))\n",
    "        x = self.activation(self.f3(x))\n",
    "        x = self.activation(self.f4(x))\n",
    "        x = self.f5(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.uniform(0, 1, size=(300, 1))\n",
    "# y = X[:, 0] - X[:, 1]**2 + 2 * X[:, 2] * X[:, 3] + (1 / ((1 + X[:, 4]) ** 6))\n",
    "y = np.sinc(X).sum(axis=1)\n",
    "\n",
    "torch_data = TensorDataset(torch.Tensor(X), torch.Tensor(y))\n",
    "data_loader = DataLoader(torch_data, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eugenio/.local/lib/python3.10/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([100])) that is different to the input size (torch.Size([100, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", loss: 0.15734048187732697\n",
      "epoch:  1, loss: 0.1430911272764206\n",
      "epoch:  2, loss: 0.1330896019935608\n",
      "epoch:  3, loss: 0.12587898969650269\n",
      "epoch:  4, loss: 0.12073305994272232\n",
      "epoch:  5, loss: 0.11710748821496964\n",
      "epoch:  6, loss: 0.11459001153707504\n",
      "epoch:  7, loss: 0.11285939812660217\n",
      "epoch:  8, loss: 0.11167964339256287\n",
      "epoch:  9, loss: 0.11088114231824875\n",
      "epoch:  10, loss: 0.1103433147072792\n",
      "epoch:  11, loss: 0.10998222976922989\n",
      "epoch:  12, loss: 0.1097400113940239\n",
      "epoch:  13, loss: 0.10957714170217514\n",
      "epoch:  14, loss: 0.10946730524301529\n",
      "epoch:  15, loss: 0.10939282178878784\n",
      "epoch:  16, loss: 0.10934203863143921\n",
      "epoch:  17, loss: 0.109307199716568\n",
      "epoch:  18, loss: 0.1092829704284668\n",
      "epoch:  19, loss: 0.1092660129070282\n",
      "epoch:  20, loss: 0.1092539057135582\n",
      "epoch:  21, loss: 0.10924515873193741\n",
      "epoch:  22, loss: 0.10923869162797928\n",
      "epoch:  23, loss: 0.10923391580581665\n",
      "epoch:  24, loss: 0.10923018306493759\n",
      "epoch:  25, loss: 0.10922729969024658\n",
      "epoch:  26, loss: 0.10922495275735855\n",
      "epoch:  27, loss: 0.10922303795814514\n",
      "epoch:  28, loss: 0.10922148078680038\n",
      "epoch:  29, loss: 0.10922018438577652\n",
      "epoch:  30, loss: 0.10921904444694519\n",
      "epoch:  31, loss: 0.10921809822320938\n",
      "epoch:  32, loss: 0.10921724885702133\n",
      "epoch:  33, loss: 0.10921648889780045\n",
      "epoch:  34, loss: 0.10921585559844971\n",
      "epoch:  35, loss: 0.10921525955200195\n",
      "epoch:  36, loss: 0.10921471565961838\n",
      "epoch:  37, loss: 0.10921420902013779\n",
      "epoch:  38, loss: 0.10921371728181839\n",
      "epoch:  39, loss: 0.10921325534582138\n",
      "epoch:  40, loss: 0.10921281576156616\n",
      "epoch:  41, loss: 0.10921237617731094\n",
      "epoch:  42, loss: 0.10921197384595871\n",
      "epoch:  43, loss: 0.10921153426170349\n",
      "epoch:  44, loss: 0.10921109467744827\n",
      "epoch:  45, loss: 0.10921064764261246\n",
      "epoch:  46, loss: 0.10921019315719604\n",
      "epoch:  47, loss: 0.10920974612236023\n",
      "epoch:  48, loss: 0.1092093363404274\n",
      "epoch:  49, loss: 0.10920893400907516\n",
      "epoch:  50, loss: 0.10920851677656174\n",
      "epoch:  51, loss: 0.1092081367969513\n",
      "epoch:  52, loss: 0.10920777171850204\n",
      "epoch:  53, loss: 0.10920742154121399\n",
      "epoch:  54, loss: 0.1092071533203125\n",
      "epoch:  55, loss: 0.10920683294534683\n",
      "epoch:  56, loss: 0.10920658707618713\n",
      "epoch:  57, loss: 0.10920631885528564\n",
      "epoch:  58, loss: 0.10920602083206177\n",
      "epoch:  59, loss: 0.10920584201812744\n",
      "epoch:  60, loss: 0.10920562595129013\n",
      "epoch:  61, loss: 0.10920542478561401\n",
      "epoch:  62, loss: 0.1092052087187767\n",
      "epoch:  63, loss: 0.1092049703001976\n",
      "epoch:  64, loss: 0.1092047467827797\n",
      "epoch:  65, loss: 0.10920453071594238\n",
      "epoch:  66, loss: 0.10920431464910507\n",
      "epoch:  67, loss: 0.10920413583517075\n",
      "epoch:  68, loss: 0.10920394212007523\n",
      "epoch:  69, loss: 0.1092037484049797\n",
      "epoch:  70, loss: 0.10920357704162598\n",
      "epoch:  71, loss: 0.10920340567827225\n",
      "epoch:  72, loss: 0.10920324921607971\n",
      "epoch:  73, loss: 0.10920309275388718\n",
      "epoch:  74, loss: 0.10920289903879166\n",
      "epoch:  75, loss: 0.10920272022485733\n",
      "epoch:  76, loss: 0.10920258611440659\n",
      "epoch:  77, loss: 0.10920243710279465\n",
      "epoch:  78, loss: 0.10920235514640808\n",
      "epoch:  79, loss: 0.10920228809118271\n",
      "epoch:  80, loss: 0.10920222848653793\n",
      "epoch:  81, loss: 0.10920218378305435\n",
      "epoch:  82, loss: 0.10920212417840958\n",
      "epoch:  83, loss: 0.1092020645737648\n",
      "epoch:  84, loss: 0.10920200496912003\n",
      "epoch:  85, loss: 0.10920194536447525\n",
      "epoch:  86, loss: 0.10920188575983047\n",
      "epoch:  87, loss: 0.1092018261551857\n",
      "epoch:  88, loss: 0.10920176655054092\n",
      "epoch:  89, loss: 0.10920173674821854\n",
      "epoch:  90, loss: 0.10920169204473495\n",
      "epoch:  91, loss: 0.10920164734125137\n",
      "epoch:  92, loss: 0.10920161753892899\n",
      "epoch:  93, loss: 0.10920160263776779\n",
      "epoch:  94, loss: 0.1092015877366066\n",
      "epoch:  95, loss: 0.1092015728354454\n",
      "epoch:  96, loss: 0.10920154303312302\n",
      "epoch:  97, loss: 0.10920152813196182\n",
      "epoch:  98, loss: 0.10920152068138123\n",
      "epoch:  99, loss: 0.10920149087905884\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "loss_fn = nn.MSELoss()\n",
    "opt = pytorch_soom.LM_cpd(model.parameters(), lr=1, model=model)\n",
    "\n",
    "all_loss = {}\n",
    "for epoch in range(100):\n",
    "    print('epoch: ', epoch, end='')\n",
    "    all_loss[epoch+1] = 0\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y)\n",
    "\n",
    "        all_loss[epoch+1] += loss\n",
    "    all_loss[epoch+1] /= len(data_loader)\n",
    "    print(', loss: {}'.format(all_loss[epoch+1].detach().numpy().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.2587035000324249\n",
      "epoch:  1, loss: 0.2397034913301468\n",
      "epoch:  2, loss: 0.22186093032360077\n",
      "epoch:  3, loss: 0.20486371219158173\n",
      "epoch:  4, loss: 0.18872815370559692\n",
      "epoch:  5, loss: 0.17377306520938873\n",
      "epoch:  6, loss: 0.16048216819763184\n",
      "epoch:  7, loss: 0.14862339198589325\n",
      "epoch:  8, loss: 0.13789282739162445\n",
      "epoch:  9, loss: 0.12829919159412384\n",
      "epoch:  10, loss: 0.11999199539422989\n",
      "epoch:  11, loss: 0.1131119504570961\n",
      "epoch:  12, loss: 0.1077706590294838\n",
      "epoch:  13, loss: 0.10401351004838943\n",
      "epoch:  14, loss: 0.1017664447426796\n",
      "epoch:  15, loss: 0.10079535096883774\n",
      "epoch:  16, loss: 0.10071191191673279\n",
      "epoch:  17, loss: 0.10105209797620773\n",
      "epoch:  18, loss: 0.101411372423172\n",
      "epoch:  19, loss: 0.10156051069498062\n",
      "epoch:  20, loss: 0.10146567970514297\n",
      "epoch:  21, loss: 0.10121766477823257\n",
      "epoch:  22, loss: 0.10093539953231812\n",
      "epoch:  23, loss: 0.10070154815912247\n",
      "epoch:  24, loss: 0.10054611414670944\n",
      "epoch:  25, loss: 0.10045836120843887\n",
      "epoch:  26, loss: 0.10041014105081558\n",
      "epoch:  27, loss: 0.1003735288977623\n",
      "epoch:  28, loss: 0.10033124685287476\n",
      "epoch:  29, loss: 0.10027652978897095\n",
      "epoch:  30, loss: 0.10021135210990906\n",
      "epoch:  31, loss: 0.10014278441667557\n",
      "epoch:  32, loss: 0.10008237510919571\n",
      "epoch:  33, loss: 0.10003989189863205\n",
      "epoch:  34, loss: 0.1000165268778801\n",
      "epoch:  35, loss: 0.09999989718198776\n",
      "epoch:  36, loss: 0.09997958689928055\n",
      "epoch:  37, loss: 0.09995520114898682\n",
      "epoch:  38, loss: 0.09992945194244385\n",
      "epoch:  39, loss: 0.09990579634904861\n",
      "epoch:  40, loss: 0.09988456964492798\n",
      "epoch:  41, loss: 0.09986528009176254\n",
      "epoch:  42, loss: 0.09984732419252396\n",
      "epoch:  43, loss: 0.0998300239443779\n",
      "epoch:  44, loss: 0.09981337934732437\n",
      "epoch:  45, loss: 0.09979762881994247\n",
      "epoch:  46, loss: 0.09978296607732773\n",
      "epoch:  47, loss: 0.09976961463689804\n",
      "epoch:  48, loss: 0.09975718706846237\n",
      "epoch:  49, loss: 0.09974543005228043\n",
      "epoch:  50, loss: 0.09973416477441788\n",
      "epoch:  51, loss: 0.09972333908081055\n",
      "epoch:  52, loss: 0.0997130274772644\n",
      "epoch:  53, loss: 0.09970322996377945\n",
      "epoch:  54, loss: 0.09969401359558105\n",
      "epoch:  55, loss: 0.09968537092208862\n",
      "epoch:  56, loss: 0.09967732429504395\n",
      "epoch:  57, loss: 0.09966981410980225\n",
      "epoch:  58, loss: 0.09966283291578293\n",
      "epoch:  59, loss: 0.09965644031763077\n",
      "epoch:  60, loss: 0.09965047240257263\n",
      "epoch:  61, loss: 0.09964493662118912\n",
      "epoch:  62, loss: 0.09963976591825485\n",
      "epoch:  63, loss: 0.09963490813970566\n",
      "epoch:  64, loss: 0.09963037818670273\n",
      "epoch:  65, loss: 0.09962615370750427\n",
      "epoch:  66, loss: 0.09962225705385208\n",
      "epoch:  67, loss: 0.09961865097284317\n",
      "epoch:  68, loss: 0.09961525350809097\n",
      "epoch:  69, loss: 0.09961210936307907\n",
      "epoch:  70, loss: 0.09960918873548508\n",
      "epoch:  71, loss: 0.09960649162530899\n",
      "epoch:  72, loss: 0.09960409253835678\n",
      "epoch:  73, loss: 0.09960183501243591\n",
      "epoch:  74, loss: 0.09959972649812698\n",
      "epoch:  75, loss: 0.0995977520942688\n",
      "epoch:  76, loss: 0.09959589689970016\n",
      "epoch:  77, loss: 0.09959415346384048\n",
      "epoch:  78, loss: 0.09959253668785095\n",
      "epoch:  79, loss: 0.09959101676940918\n",
      "epoch:  80, loss: 0.09958958625793457\n",
      "epoch:  81, loss: 0.09958825260400772\n",
      "epoch:  82, loss: 0.09958707541227341\n",
      "epoch:  83, loss: 0.09958597272634506\n",
      "epoch:  84, loss: 0.09958495944738388\n",
      "epoch:  85, loss: 0.09958399087190628\n",
      "epoch:  86, loss: 0.09958314895629883\n",
      "epoch:  87, loss: 0.09958235174417496\n",
      "epoch:  88, loss: 0.09958168119192123\n",
      "epoch:  89, loss: 0.0995810329914093\n",
      "epoch:  90, loss: 0.09958040714263916\n",
      "epoch:  91, loss: 0.0995798110961914\n",
      "epoch:  92, loss: 0.09957925230264664\n",
      "epoch:  93, loss: 0.09957877546548843\n",
      "epoch:  94, loss: 0.09957829862833023\n",
      "epoch:  95, loss: 0.09957783669233322\n",
      "epoch:  96, loss: 0.09957737475633621\n",
      "epoch:  97, loss: 0.09957697242498398\n",
      "epoch:  98, loss: 0.09957664459943771\n",
      "epoch:  99, loss: 0.09957632422447205\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "loss_fn = nn.MSELoss()\n",
    "opt = optim.Adam(model.parameters())\n",
    "\n",
    "all_loss = {}\n",
    "for epoch in range(100):\n",
    "    print('epoch: ', epoch, end = '')\n",
    "    all_loss[epoch+1] = 0\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step()\n",
    "\n",
    "        all_loss[epoch+1] += loss\n",
    "    all_loss[epoch+1] /= len(data_loader)\n",
    "    print(', loss: {}'.format(all_loss[epoch+1].detach().numpy().item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.5638055205345154\n",
      "epoch:  1, loss: 0.35222554206848145\n",
      "epoch:  2, loss: 0.2497107833623886\n",
      "epoch:  3, loss: 0.26668545603752136\n",
      "epoch:  4, loss: 0.24280951917171478\n",
      "epoch:  5, loss: 0.1740289330482483\n",
      "epoch:  6, loss: 0.1398189663887024\n",
      "epoch:  7, loss: 0.12207093834877014\n",
      "epoch:  8, loss: 0.11216113716363907\n",
      "epoch:  9, loss: 0.10635871440172195\n",
      "epoch:  10, loss: 0.10343404859304428\n",
      "epoch:  11, loss: 0.10179099440574646\n",
      "epoch:  12, loss: 0.10086837410926819\n",
      "epoch:  13, loss: 0.10033486038446426\n",
      "epoch:  14, loss: 0.10001718252897263\n",
      "epoch:  15, loss: 0.09983772039413452\n",
      "epoch:  16, loss: 0.09974392503499985\n",
      "epoch:  17, loss: 0.09969896078109741\n",
      "epoch:  18, loss: 0.09967195987701416\n",
      "epoch:  19, loss: 0.09965749830007553\n",
      "epoch:  20, loss: 0.10902784019708633\n",
      "epoch:  21, loss: 0.10495344549417496\n",
      "epoch:  22, loss: 0.10260497778654099\n",
      "epoch:  23, loss: 0.10129103064537048\n",
      "epoch:  24, loss: 0.10056231170892715\n",
      "epoch:  25, loss: 0.10014406591653824\n",
      "epoch:  26, loss: 0.09991274029016495\n",
      "epoch:  27, loss: 0.0997917577624321\n",
      "epoch:  28, loss: 0.09970695525407791\n",
      "epoch:  29, loss: 0.09967097640037537\n",
      "epoch:  30, loss: 0.0996457114815712\n",
      "epoch:  31, loss: 0.09963559359312057\n",
      "epoch:  32, loss: 0.09962224960327148\n",
      "epoch:  33, loss: 0.0996178612112999\n",
      "epoch:  34, loss: 0.09962306171655655\n",
      "epoch:  35, loss: 0.09961879253387451\n",
      "epoch:  36, loss: 0.09961351007223129\n",
      "epoch:  37, loss: 0.09961723536252975\n",
      "epoch:  38, loss: 0.09962150454521179\n",
      "epoch:  39, loss: 0.09962034970521927\n",
      "epoch:  40"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "loss_fn = nn.MSELoss()\n",
    "opt = pytorch_soom.Newton_cpd(model.parameters(), lr=1, model=model)\n",
    "\n",
    "all_loss = {}\n",
    "for epoch in range(100):\n",
    "    print('epoch: ', epoch, end='')\n",
    "    all_loss[epoch+1] = 0\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y)\n",
    "\n",
    "        all_loss[epoch+1] += loss\n",
    "    all_loss[epoch+1] /= len(data_loader)\n",
    "    print(', loss: {}'.format(all_loss[epoch+1].detach().numpy().item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
