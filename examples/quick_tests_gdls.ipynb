{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch_numopt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import *\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.f1 = nn.Linear(input_size, 10, device=device)\n",
    "        self.f2 = nn.Linear(10, 20, device=device)\n",
    "        self.f3 = nn.Linear(20, 20, device=device)\n",
    "        self.f4 = nn.Linear(20, 10, device=device)\n",
    "        self.f5 = nn.Linear(10, 1, device=device)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        # self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.f1(x))\n",
    "        x = self.activation(self.f2(x))\n",
    "        x = self.activation(self.f3(x))\n",
    "        x = self.activation(self.f4(x))\n",
    "        x = self.f5(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([800, 10]) torch.Size([800, 1])\n",
      "torch.Size([200, 10]) torch.Size([200, 1])\n"
     ]
    }
   ],
   "source": [
    "# X, y = load_diabetes(return_X_y=True, scaled=False)\n",
    "# X, y = make_regression(n_samples=1000, n_features=100)\n",
    "X, y = make_friedman1(n_samples=1000, noise=1e-2)\n",
    "\n",
    "X_scaler = MinMaxScaler()\n",
    "X = X_scaler.fit_transform(X)\n",
    "X = torch.Tensor(X).to(device)\n",
    "\n",
    "y_scaler = MinMaxScaler()\n",
    "y = y_scaler.fit_transform(y.reshape((-1, 1)))\n",
    "y = torch.Tensor(y).to(device)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "torch_data = TensorDataset(X_train, y_train)\n",
    "data_loader = DataLoader(torch_data, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.1857309341430664\n",
      "epoch:  1, loss: 0.11977101117372513\n",
      "epoch:  2, loss: 0.08207575976848602\n",
      "epoch:  3, loss: 0.06042532995343208\n",
      "epoch:  4, loss: 0.0479993000626564\n",
      "epoch:  5, loss: 0.040904659777879715\n",
      "epoch:  6, loss: 0.037045057862997055\n",
      "epoch:  7, loss: 0.03487472981214523\n",
      "epoch:  8, loss: 0.03357062488794327\n",
      "epoch:  9, loss: 0.03278426080942154\n",
      "epoch:  10, loss: 0.03230840712785721\n",
      "epoch:  11, loss: 0.03201870247721672\n",
      "epoch:  12, loss: 0.031841665506362915\n",
      "epoch:  13, loss: 0.03173256292939186\n",
      "epoch:  14, loss: 0.03172113001346588\n",
      "epoch:  15, loss: 0.03161948546767235\n",
      "epoch:  16, loss: 0.03156077861785889\n",
      "epoch:  17, loss: 0.03154498711228371\n",
      "epoch:  18, loss: 0.031459659337997437\n",
      "epoch:  19, loss: 0.0314033143222332\n",
      "epoch:  20, loss: 0.031397271901369095\n",
      "epoch:  21, loss: 0.031313639134168625\n",
      "epoch:  22, loss: 0.03126664087176323\n",
      "epoch:  23, loss: 0.031239226460456848\n",
      "epoch:  24, loss: 0.03120967000722885\n",
      "epoch:  25, loss: 0.031169770285487175\n",
      "epoch:  26, loss: 0.031167950481176376\n",
      "epoch:  27, loss: 0.031105762347579002\n",
      "epoch:  28, loss: 0.031070293858647346\n",
      "epoch:  29, loss: 0.03105374053120613\n",
      "epoch:  30, loss: 0.031000886112451553\n",
      "epoch:  31, loss: 0.030968965962529182\n",
      "epoch:  32, loss: 0.03094000741839409\n",
      "epoch:  33, loss: 0.030889835208654404\n",
      "epoch:  34, loss: 0.030859852209687233\n",
      "epoch:  35, loss: 0.03082074224948883\n",
      "epoch:  36, loss: 0.03077423758804798\n",
      "epoch:  37, loss: 0.03076806105673313\n",
      "epoch:  38, loss: 0.0306930523365736\n",
      "epoch:  39, loss: 0.0306495800614357\n",
      "epoch:  40, loss: 0.030628163367509842\n",
      "epoch:  41, loss: 0.03055742010474205\n",
      "epoch:  42, loss: 0.030515337362885475\n",
      "epoch:  43, loss: 0.03047756478190422\n",
      "epoch:  44, loss: 0.03040979616343975\n",
      "epoch:  45, loss: 0.030368652194738388\n",
      "epoch:  46, loss: 0.030315304175019264\n",
      "epoch:  47, loss: 0.03024778887629509\n",
      "epoch:  48, loss: 0.030246935784816742\n",
      "epoch:  49, loss: 0.030132446438074112\n",
      "epoch:  50, loss: 0.030066462233662605\n",
      "epoch:  51, loss: 0.030038900673389435\n",
      "epoch:  52, loss: 0.029925020411610603\n",
      "epoch:  53, loss: 0.02985914796590805\n",
      "epoch:  54, loss: 0.02980761043727398\n",
      "epoch:  55, loss: 0.029696768149733543\n",
      "epoch:  56, loss: 0.02963198721408844\n",
      "epoch:  57, loss: 0.0295637845993042\n",
      "epoch:  58, loss: 0.029450519010424614\n",
      "epoch:  59, loss: 0.029383879154920578\n",
      "epoch:  60, loss: 0.029304204508662224\n",
      "epoch:  61, loss: 0.029187100008130074\n",
      "epoch:  62, loss: 0.0291183739900589\n",
      "epoch:  63, loss: 0.029020080342888832\n",
      "epoch:  64, loss: 0.028899015858769417\n",
      "epoch:  65, loss: 0.02882867306470871\n",
      "epoch:  66, loss: 0.02871842123568058\n",
      "epoch:  67, loss: 0.028591422364115715\n",
      "epoch:  68, loss: 0.028517452999949455\n",
      "epoch:  69, loss: 0.028392449021339417\n",
      "epoch:  70, loss: 0.02825789712369442\n",
      "epoch:  71, loss: 0.028180720284581184\n",
      "epoch:  72, loss: 0.028045782819390297\n",
      "epoch:  73, loss: 0.027898376807570457\n",
      "epoch:  74, loss: 0.027815070003271103\n",
      "epoch:  75, loss: 0.027660192921757698\n",
      "epoch:  76, loss: 0.0274982713162899\n",
      "epoch:  77, loss: 0.02740708366036415\n",
      "epoch:  78, loss: 0.027256794273853302\n",
      "epoch:  79, loss: 0.027065599337220192\n",
      "epoch:  80, loss: 0.026964597404003143\n",
      "epoch:  81, loss: 0.0267885010689497\n",
      "epoch:  82, loss: 0.02658924087882042\n",
      "epoch:  83, loss: 0.026480451226234436\n",
      "epoch:  84, loss: 0.026329346001148224\n",
      "epoch:  85, loss: 0.026079807430505753\n",
      "epoch:  86, loss: 0.025955520570278168\n",
      "epoch:  87, loss: 0.025800414383411407\n",
      "epoch:  88, loss: 0.025523196905851364\n",
      "epoch:  89, loss: 0.025383640080690384\n",
      "epoch:  90, loss: 0.025301175191998482\n",
      "epoch:  91, loss: 0.02492484264075756\n",
      "epoch:  92, loss: 0.024761563166975975\n",
      "epoch:  93, loss: 0.02469409443438053\n",
      "epoch:  94, loss: 0.024279313161969185\n",
      "epoch:  95, loss: 0.024089768528938293\n",
      "epoch:  96, loss: 0.02398362196981907\n",
      "epoch:  97, loss: 0.023615743964910507\n",
      "epoch:  98, loss: 0.02337097004055977\n",
      "epoch:  99, loss: 0.023249642923474312\n",
      "epoch:  100, loss: 0.02287573367357254\n",
      "epoch:  101, loss: 0.022593175992369652\n",
      "epoch:  102, loss: 0.02245735190808773\n",
      "epoch:  103, loss: 0.022123808041214943\n",
      "epoch:  104, loss: 0.02177085354924202\n",
      "epoch:  105, loss: 0.02162334881722927\n",
      "epoch:  106, loss: 0.021280942484736443\n",
      "epoch:  107, loss: 0.020893264561891556\n",
      "epoch:  108, loss: 0.020733226090669632\n",
      "epoch:  109, loss: 0.020430931821465492\n",
      "epoch:  110, loss: 0.019972078502178192\n",
      "epoch:  111, loss: 0.01980871707201004\n",
      "epoch:  112, loss: 0.019462494179606438\n",
      "epoch:  113, loss: 0.01901054009795189\n",
      "epoch:  114, loss: 0.018848948180675507\n",
      "epoch:  115, loss: 0.018521370366215706\n",
      "epoch:  116, loss: 0.018031008541584015\n",
      "epoch:  117, loss: 0.017876891419291496\n",
      "epoch:  118, loss: 0.01741635799407959\n",
      "epoch:  119, loss: 0.017031574621796608\n",
      "epoch:  120, loss: 0.016898658126592636\n",
      "epoch:  121, loss: 0.016358841210603714\n",
      "epoch:  122, loss: 0.0160471610724926\n",
      "epoch:  123, loss: 0.015938900411128998\n",
      "epoch:  124, loss: 0.015285005792975426\n",
      "epoch:  125, loss: 0.015100550837814808\n",
      "epoch:  126, loss: 0.01485518179833889\n",
      "epoch:  127, loss: 0.014327972196042538\n",
      "epoch:  128, loss: 0.014219925738871098\n",
      "epoch:  129, loss: 0.01367500051856041\n",
      "epoch:  130, loss: 0.013481365516781807\n",
      "epoch:  131, loss: 0.013295149430632591\n",
      "epoch:  132, loss: 0.012821905314922333\n",
      "epoch:  133, loss: 0.01274433359503746\n",
      "epoch:  134, loss: 0.012240813113749027\n",
      "epoch:  135, loss: 0.012151693925261497\n",
      "epoch:  136, loss: 0.011771206744015217\n",
      "epoch:  137, loss: 0.011631994508206844\n",
      "epoch:  138, loss: 0.011332780122756958\n",
      "epoch:  139, loss: 0.01118139736354351\n",
      "epoch:  140, loss: 0.010993204079568386\n",
      "epoch:  141, loss: 0.010793949477374554\n",
      "epoch:  142, loss: 0.010589461773633957\n",
      "epoch:  143, loss: 0.01046324148774147\n",
      "epoch:  144, loss: 0.010258305817842484\n",
      "epoch:  145, loss: 0.01017863117158413\n",
      "epoch:  146, loss: 0.009968594647943974\n",
      "epoch:  147, loss: 0.009931661188602448\n",
      "epoch:  148, loss: 0.009738797321915627\n",
      "epoch:  149, loss: 0.009735830128192902\n",
      "epoch:  150, loss: 0.009548652917146683\n",
      "epoch:  151, loss: 0.009403485804796219\n",
      "epoch:  152, loss: 0.009384000673890114\n",
      "epoch:  153, loss: 0.009251066483557224\n",
      "epoch:  154, loss: 0.009143304079771042\n",
      "epoch:  155, loss: 0.009124457836151123\n",
      "epoch:  156, loss: 0.009020255878567696\n",
      "epoch:  157, loss: 0.008927690796554089\n",
      "epoch:  158, loss: 0.008853617124259472\n",
      "epoch:  159, loss: 0.008834763430058956\n",
      "epoch:  160, loss: 0.008760231547057629\n",
      "epoch:  161, loss: 0.008747320622205734\n",
      "epoch:  162, loss: 0.008680972270667553\n",
      "epoch:  163, loss: 0.008669855073094368\n",
      "epoch:  164, loss: 0.008610139600932598\n",
      "epoch:  165, loss: 0.008601084351539612\n",
      "epoch:  166, loss: 0.008546370081603527\n",
      "epoch:  167, loss: 0.008539347909390926\n",
      "epoch:  168, loss: 0.008488829247653484\n",
      "epoch:  169, loss: 0.008482912555336952\n",
      "epoch:  170, loss: 0.00843823328614235\n",
      "epoch:  171, loss: 0.008433143608272076\n",
      "epoch:  172, loss: 0.008393068797886372\n",
      "epoch:  173, loss: 0.008388116024434566\n",
      "epoch:  174, loss: 0.008350363001227379\n",
      "epoch:  175, loss: 0.008345626294612885\n",
      "epoch:  176, loss: 0.008311580866575241\n",
      "epoch:  177, loss: 0.008305897936224937\n",
      "epoch:  178, loss: 0.00827751960605383\n",
      "epoch:  179, loss: 0.008270451799035072\n",
      "epoch:  180, loss: 0.00824720598757267\n",
      "epoch:  181, loss: 0.008236824534833431\n",
      "epoch:  182, loss: 0.008223881013691425\n",
      "epoch:  183, loss: 0.008205469697713852\n",
      "epoch:  184, loss: 0.008202522993087769\n",
      "epoch:  185, loss: 0.008178692311048508\n",
      "epoch:  186, loss: 0.008175537921488285\n",
      "epoch:  187, loss: 0.00815590564161539\n",
      "epoch:  188, loss: 0.00815180316567421\n",
      "epoch:  189, loss: 0.008136902935802937\n",
      "epoch:  190, loss: 0.008130312897264957\n",
      "epoch:  191, loss: 0.008122357539832592\n",
      "epoch:  192, loss: 0.00811018981039524\n",
      "epoch:  193, loss: 0.008108198642730713\n",
      "epoch:  194, loss: 0.008091802708804607\n",
      "epoch:  195, loss: 0.008089546114206314\n",
      "epoch:  196, loss: 0.008074009791016579\n",
      "epoch:  197, loss: 0.00807100161910057\n",
      "epoch:  198, loss: 0.008057895116508007\n",
      "epoch:  199, loss: 0.008054046891629696\n",
      "epoch:  200, loss: 0.008046417497098446\n",
      "epoch:  201, loss: 0.008038935251533985\n",
      "epoch:  202, loss: 0.00803739670664072\n",
      "epoch:  203, loss: 0.008024204522371292\n",
      "epoch:  204, loss: 0.008022360503673553\n",
      "epoch:  205, loss: 0.008011090569198132\n",
      "epoch:  206, loss: 0.008008061908185482\n",
      "epoch:  207, loss: 0.008002626709640026\n",
      "epoch:  208, loss: 0.007994814775884151\n",
      "epoch:  209, loss: 0.007993537932634354\n",
      "epoch:  210, loss: 0.007983324117958546\n",
      "epoch:  211, loss: 0.007981836795806885\n",
      "epoch:  212, loss: 0.007972759194672108\n",
      "epoch:  213, loss: 0.00797040294855833\n",
      "epoch:  214, loss: 0.007964838296175003\n",
      "epoch:  215, loss: 0.007959701120853424\n",
      "epoch:  216, loss: 0.007958624511957169\n",
      "epoch:  217, loss: 0.007949593476951122\n",
      "epoch:  218, loss: 0.007948429323732853\n",
      "epoch:  219, loss: 0.007941232062876225\n",
      "epoch:  220, loss: 0.007939429953694344\n",
      "epoch:  221, loss: 0.007934694178402424\n",
      "epoch:  222, loss: 0.007930662482976913\n",
      "epoch:  223, loss: 0.00792975164949894\n",
      "epoch:  224, loss: 0.007922451943159103\n",
      "epoch:  225, loss: 0.007920600473880768\n",
      "epoch:  226, loss: 0.007916003465652466\n",
      "epoch:  227, loss: 0.007912246510386467\n",
      "epoch:  228, loss: 0.007911494933068752\n",
      "epoch:  229, loss: 0.007905324921011925\n",
      "epoch:  230, loss: 0.007904410362243652\n",
      "epoch:  231, loss: 0.007899329997599125\n",
      "epoch:  232, loss: 0.00789769645780325\n",
      "epoch:  233, loss: 0.007893775589764118\n",
      "epoch:  234, loss: 0.007890895940363407\n",
      "epoch:  235, loss: 0.007890192791819572\n",
      "epoch:  236, loss: 0.007884372025728226\n",
      "epoch:  237, loss: 0.007883532904088497\n",
      "epoch:  238, loss: 0.007878893055021763\n",
      "epoch:  239, loss: 0.007877234369516373\n",
      "epoch:  240, loss: 0.007875053212046623\n",
      "epoch:  241, loss: 0.007870923727750778\n",
      "epoch:  242, loss: 0.007870299741625786\n",
      "epoch:  243, loss: 0.00786510482430458\n",
      "epoch:  244, loss: 0.007864070124924183\n",
      "epoch:  245, loss: 0.007860628888010979\n",
      "epoch:  246, loss: 0.007857623510062695\n",
      "epoch:  247, loss: 0.007856948301196098\n",
      "epoch:  248, loss: 0.007851717062294483\n",
      "epoch:  249, loss: 0.007850741036236286\n",
      "epoch:  250, loss: 0.007846750319004059\n",
      "epoch:  251, loss: 0.00784552376717329\n",
      "epoch:  252, loss: 0.007844953797757626\n",
      "epoch:  253, loss: 0.007840649224817753\n",
      "epoch:  254, loss: 0.00784009788185358\n",
      "epoch:  255, loss: 0.007836195640265942\n",
      "epoch:  256, loss: 0.00783529318869114\n",
      "epoch:  257, loss: 0.007832995615899563\n",
      "epoch:  258, loss: 0.007830639369785786\n",
      "epoch:  259, loss: 0.007830160669982433\n",
      "epoch:  260, loss: 0.007826396264135838\n",
      "epoch:  261, loss: 0.007825714536011219\n",
      "epoch:  262, loss: 0.007822872139513493\n",
      "epoch:  263, loss: 0.007821150124073029\n",
      "epoch:  264, loss: 0.007820677943527699\n",
      "epoch:  265, loss: 0.007816818542778492\n",
      "epoch:  266, loss: 0.007816067896783352\n",
      "epoch:  267, loss: 0.007813884876668453\n",
      "epoch:  268, loss: 0.007812185678631067\n",
      "epoch:  269, loss: 0.0078118182718753815\n",
      "epoch:  270, loss: 0.007808977272361517\n",
      "epoch:  271, loss: 0.0078084878623485565\n",
      "epoch:  272, loss: 0.007806550711393356\n",
      "epoch:  273, loss: 0.007805291563272476\n",
      "epoch:  274, loss: 0.007804945111274719\n",
      "epoch:  275, loss: 0.007802378386259079\n",
      "epoch:  276, loss: 0.007801821921020746\n",
      "epoch:  277, loss: 0.007801071275025606\n",
      "epoch:  278, loss: 0.0077989776618778706\n",
      "epoch:  279, loss: 0.00779862143099308\n",
      "epoch:  280, loss: 0.0077973222360014915\n",
      "epoch:  281, loss: 0.007795876823365688\n",
      "epoch:  282, loss: 0.007795538753271103\n",
      "epoch:  283, loss: 0.007793463300913572\n",
      "epoch:  284, loss: 0.0077930330298841\n",
      "epoch:  285, loss: 0.0077924663200974464\n",
      "epoch:  286, loss: 0.007790680509060621\n",
      "epoch:  287, loss: 0.0077903736382722855\n",
      "epoch:  288, loss: 0.007788720540702343\n",
      "epoch:  289, loss: 0.007787908893078566\n",
      "epoch:  290, loss: 0.007787647657096386\n",
      "epoch:  291, loss: 0.00778568908572197\n",
      "epoch:  292, loss: 0.00778529467061162\n",
      "epoch:  293, loss: 0.0077837985008955\n",
      "epoch:  294, loss: 0.007783027365803719\n",
      "epoch:  295, loss: 0.007782766595482826\n",
      "epoch:  296, loss: 0.00778175937011838\n",
      "epoch:  297, loss: 0.007780575193464756\n",
      "epoch:  298, loss: 0.007780313491821289\n",
      "epoch:  299, loss: 0.007780089508742094\n",
      "epoch:  300, loss: 0.007778765168040991\n",
      "epoch:  301, loss: 0.007777871564030647\n",
      "epoch:  302, loss: 0.007777614519000053\n",
      "epoch:  303, loss: 0.0077773951925337315\n",
      "epoch:  304, loss: 0.007776087615638971\n",
      "epoch:  305, loss: 0.007775207981467247\n",
      "epoch:  306, loss: 0.00777495838701725\n",
      "epoch:  307, loss: 0.00777492206543684\n",
      "epoch:  308, loss: 0.007772846147418022\n",
      "epoch:  309, loss: 0.007772588171064854\n",
      "epoch:  310, loss: 0.007771321106702089\n",
      "epoch:  311, loss: 0.007770512253046036\n",
      "epoch:  312, loss: 0.007770265452563763\n",
      "epoch:  313, loss: 0.007770218420773745\n",
      "epoch:  314, loss: 0.007768407464027405\n",
      "epoch:  315, loss: 0.007768156938254833\n",
      "epoch:  316, loss: 0.00776702631264925\n",
      "epoch:  317, loss: 0.0077662537805736065\n",
      "epoch:  318, loss: 0.007766020949929953\n",
      "epoch:  319, loss: 0.0077645317651331425\n",
      "epoch:  320, loss: 0.007764147594571114\n",
      "epoch:  321, loss: 0.007763939443975687\n",
      "epoch:  322, loss: 0.007763636764138937\n",
      "epoch:  323, loss: 0.007762134540826082\n",
      "epoch:  324, loss: 0.007761869579553604\n",
      "epoch:  325, loss: 0.007761725690215826\n",
      "epoch:  326, loss: 0.007760033011436462\n",
      "epoch:  327, loss: 0.007759805303066969\n",
      "epoch:  328, loss: 0.007758425083011389\n",
      "epoch:  329, loss: 0.007757928222417831\n",
      "epoch:  330, loss: 0.0077577270567417145\n",
      "epoch:  331, loss: 0.007757123559713364\n",
      "epoch:  332, loss: 0.007755907252430916\n",
      "epoch:  333, loss: 0.007755700498819351\n",
      "epoch:  334, loss: 0.007754527498036623\n",
      "epoch:  335, loss: 0.007753818761557341\n",
      "epoch:  336, loss: 0.00775360269472003\n",
      "epoch:  337, loss: 0.00775317195802927\n",
      "epoch:  338, loss: 0.007751738652586937\n",
      "epoch:  339, loss: 0.007751524448394775\n",
      "epoch:  340, loss: 0.007750332821160555\n",
      "epoch:  341, loss: 0.007749640382826328\n",
      "epoch:  342, loss: 0.007749414537101984\n",
      "epoch:  343, loss: 0.007748052012175322\n",
      "epoch:  344, loss: 0.007747573312371969\n",
      "epoch:  345, loss: 0.0077473754063248634\n",
      "epoch:  346, loss: 0.007745884824544191\n",
      "epoch:  347, loss: 0.007745532784610987\n",
      "epoch:  348, loss: 0.007745353039354086\n",
      "epoch:  349, loss: 0.007744211703538895\n",
      "epoch:  350, loss: 0.007743518333882093\n",
      "epoch:  351, loss: 0.0077433232218027115\n",
      "epoch:  352, loss: 0.007741996087133884\n",
      "epoch:  353, loss: 0.007741489447653294\n",
      "epoch:  354, loss: 0.007741281762719154\n",
      "epoch:  355, loss: 0.007740078028291464\n",
      "epoch:  356, loss: 0.007739442400634289\n",
      "epoch:  357, loss: 0.0077392528764903545\n",
      "epoch:  358, loss: 0.007737766019999981\n",
      "epoch:  359, loss: 0.007737449835985899\n",
      "epoch:  360, loss: 0.007737256120890379\n",
      "epoch:  361, loss: 0.007735846098512411\n",
      "epoch:  362, loss: 0.007735447492450476\n",
      "epoch:  363, loss: 0.007735250983387232\n",
      "epoch:  364, loss: 0.007733827456831932\n",
      "epoch:  365, loss: 0.0077334214001894\n",
      "epoch:  366, loss: 0.007733121048659086\n",
      "epoch:  367, loss: 0.007731629069894552\n",
      "epoch:  368, loss: 0.00773139763623476\n",
      "epoch:  369, loss: 0.007730628363788128\n",
      "epoch:  370, loss: 0.007729571312665939\n",
      "epoch:  371, loss: 0.0077293491922318935\n",
      "epoch:  372, loss: 0.007727938704192638\n",
      "epoch:  373, loss: 0.007727494928985834\n",
      "epoch:  374, loss: 0.007727301679551601\n",
      "epoch:  375, loss: 0.007726007141172886\n",
      "epoch:  376, loss: 0.007725483272224665\n",
      "epoch:  377, loss: 0.007725275121629238\n",
      "epoch:  378, loss: 0.007723712828010321\n",
      "epoch:  379, loss: 0.007723364047706127\n",
      "epoch:  380, loss: 0.007723173126578331\n",
      "epoch:  381, loss: 0.007721493020653725\n",
      "epoch:  382, loss: 0.007721252273768187\n",
      "epoch:  383, loss: 0.0077204215340316296\n",
      "epoch:  384, loss: 0.007719386834651232\n",
      "epoch:  385, loss: 0.007719159591943026\n",
      "epoch:  386, loss: 0.007718412205576897\n",
      "epoch:  387, loss: 0.007717351894825697\n",
      "epoch:  388, loss: 0.0077171241864562035\n",
      "epoch:  389, loss: 0.007716083899140358\n",
      "epoch:  390, loss: 0.007715250831097364\n",
      "epoch:  391, loss: 0.0077150301076471806\n",
      "epoch:  392, loss: 0.007713541388511658\n",
      "epoch:  393, loss: 0.00771314837038517\n",
      "epoch:  394, loss: 0.007712833117693663\n",
      "epoch:  395, loss: 0.007711342070251703\n",
      "epoch:  396, loss: 0.007711086422204971\n",
      "epoch:  397, loss: 0.007710316218435764\n",
      "epoch:  398, loss: 0.007709232624620199\n",
      "epoch:  399, loss: 0.007709006778895855\n",
      "epoch:  400, loss: 0.007708038203418255\n",
      "epoch:  401, loss: 0.007707138080149889\n",
      "epoch:  402, loss: 0.007706895470619202\n",
      "epoch:  403, loss: 0.007705324795097113\n",
      "epoch:  404, loss: 0.007704822812229395\n",
      "epoch:  405, loss: 0.007704615127295256\n",
      "epoch:  406, loss: 0.007703106850385666\n",
      "epoch:  407, loss: 0.007702614646404982\n",
      "epoch:  408, loss: 0.007702395785599947\n",
      "epoch:  409, loss: 0.00770054804161191\n",
      "epoch:  410, loss: 0.007700243033468723\n",
      "epoch:  411, loss: 0.007700047455728054\n",
      "epoch:  412, loss: 0.0076981764286756516\n",
      "epoch:  413, loss: 0.007697884924709797\n",
      "epoch:  414, loss: 0.007696321699768305\n",
      "epoch:  415, loss: 0.007695628795772791\n",
      "epoch:  416, loss: 0.007695397827774286\n",
      "epoch:  417, loss: 0.007693603169173002\n",
      "epoch:  418, loss: 0.007693127263337374\n",
      "epoch:  419, loss: 0.0076929558999836445\n",
      "epoch:  420, loss: 0.007690844126045704\n",
      "epoch:  421, loss: 0.007690525148063898\n",
      "epoch:  422, loss: 0.007689415942877531\n",
      "epoch:  423, loss: 0.007688385900110006\n",
      "epoch:  424, loss: 0.007688118610531092\n",
      "epoch:  425, loss: 0.007686641067266464\n",
      "epoch:  426, loss: 0.007685934193432331\n",
      "epoch:  427, loss: 0.007685699034482241\n",
      "epoch:  428, loss: 0.007684237789362669\n",
      "epoch:  429, loss: 0.0076835500076413155\n",
      "epoch:  430, loss: 0.0076833246275782585\n",
      "epoch:  431, loss: 0.007681720890104771\n",
      "epoch:  432, loss: 0.007681186310946941\n",
      "epoch:  433, loss: 0.007680967915803194\n",
      "epoch:  434, loss: 0.007679425645619631\n",
      "epoch:  435, loss: 0.007678838912397623\n",
      "epoch:  436, loss: 0.007678616791963577\n",
      "epoch:  437, loss: 0.007677020039409399\n",
      "epoch:  438, loss: 0.007676531560719013\n",
      "epoch:  439, loss: 0.00767631595954299\n",
      "epoch:  440, loss: 0.007675015367567539\n",
      "epoch:  441, loss: 0.007674201857298613\n",
      "epoch:  442, loss: 0.007673964370042086\n",
      "epoch:  443, loss: 0.007672333158552647\n",
      "epoch:  444, loss: 0.007671820931136608\n",
      "epoch:  445, loss: 0.007671595085412264\n",
      "epoch:  446, loss: 0.007670113816857338\n",
      "epoch:  447, loss: 0.007669473998248577\n",
      "epoch:  448, loss: 0.007669252809137106\n",
      "epoch:  449, loss: 0.007667785510420799\n",
      "epoch:  450, loss: 0.007667133118957281\n",
      "epoch:  451, loss: 0.007666905410587788\n",
      "epoch:  452, loss: 0.00766531890258193\n",
      "epoch:  453, loss: 0.0076647670939564705\n",
      "epoch:  454, loss: 0.007664544507861137\n",
      "epoch:  455, loss: 0.007663535885512829\n",
      "epoch:  456, loss: 0.007662450894713402\n",
      "epoch:  457, loss: 0.007662203162908554\n",
      "epoch:  458, loss: 0.007661066018044949\n",
      "epoch:  459, loss: 0.007660155184566975\n",
      "epoch:  460, loss: 0.007659918628633022\n",
      "epoch:  461, loss: 0.007658639922738075\n",
      "epoch:  462, loss: 0.007657868321985006\n",
      "epoch:  463, loss: 0.0076576294377446175\n",
      "epoch:  464, loss: 0.007656427565962076\n",
      "epoch:  465, loss: 0.007655599620193243\n",
      "epoch:  466, loss: 0.007655363529920578\n",
      "epoch:  467, loss: 0.007653885055333376\n",
      "epoch:  468, loss: 0.007653276436030865\n",
      "epoch:  469, loss: 0.00765305757522583\n",
      "epoch:  470, loss: 0.007651399355381727\n",
      "epoch:  471, loss: 0.007650953717529774\n",
      "epoch:  472, loss: 0.007650743238627911\n",
      "epoch:  473, loss: 0.0076490603387355804\n",
      "epoch:  474, loss: 0.007648620754480362\n",
      "epoch:  475, loss: 0.007648401893675327\n",
      "epoch:  476, loss: 0.007646513171494007\n",
      "epoch:  477, loss: 0.0076462035067379475\n",
      "epoch:  478, loss: 0.007646086160093546\n",
      "epoch:  479, loss: 0.0076440973207354546\n",
      "epoch:  480, loss: 0.007643769960850477\n",
      "epoch:  481, loss: 0.007643148768693209\n",
      "epoch:  482, loss: 0.0076414537616074085\n",
      "epoch:  483, loss: 0.007641174364835024\n",
      "epoch:  484, loss: 0.007640055380761623\n",
      "epoch:  485, loss: 0.007638903334736824\n",
      "epoch:  486, loss: 0.007638619747012854\n",
      "epoch:  487, loss: 0.007637346163392067\n",
      "epoch:  488, loss: 0.007636359427124262\n",
      "epoch:  489, loss: 0.0076361061073839664\n",
      "epoch:  490, loss: 0.00763507466763258\n",
      "epoch:  491, loss: 0.007633873261511326\n",
      "epoch:  492, loss: 0.0076335882768034935\n",
      "epoch:  493, loss: 0.007632588967680931\n",
      "epoch:  494, loss: 0.00763130746781826\n",
      "epoch:  495, loss: 0.007631016429513693\n",
      "epoch:  496, loss: 0.007630903273820877\n",
      "epoch:  497, loss: 0.0076288157142698765\n",
      "epoch:  498, loss: 0.007628485094755888\n",
      "epoch:  499, loss: 0.007627225946635008\n",
      "epoch:  500, loss: 0.007626152131706476\n",
      "epoch:  501, loss: 0.007625867146998644\n",
      "epoch:  502, loss: 0.007624031510204077\n",
      "epoch:  503, loss: 0.007623470854014158\n",
      "epoch:  504, loss: 0.007623225916177034\n",
      "epoch:  505, loss: 0.0076216780580580235\n",
      "epoch:  506, loss: 0.0076208864338696\n",
      "epoch:  507, loss: 0.007620636373758316\n",
      "epoch:  508, loss: 0.007620325777679682\n",
      "epoch:  509, loss: 0.007618382107466459\n",
      "epoch:  510, loss: 0.007618056610226631\n",
      "epoch:  511, loss: 0.007616698741912842\n",
      "epoch:  512, loss: 0.007615613751113415\n",
      "epoch:  513, loss: 0.007615326438099146\n",
      "epoch:  514, loss: 0.007613804191350937\n",
      "epoch:  515, loss: 0.007612880319356918\n",
      "epoch:  516, loss: 0.0076125855557620525\n",
      "epoch:  517, loss: 0.007611330598592758\n",
      "epoch:  518, loss: 0.007610095664858818\n",
      "epoch:  519, loss: 0.007609781809151173\n",
      "epoch:  520, loss: 0.007609525695443153\n",
      "epoch:  521, loss: 0.0076072500087320805\n",
      "epoch:  522, loss: 0.007606895640492439\n",
      "epoch:  523, loss: 0.0076058125123381615\n",
      "epoch:  524, loss: 0.007604226469993591\n",
      "epoch:  525, loss: 0.007603881880640984\n",
      "epoch:  526, loss: 0.00760237080976367\n",
      "epoch:  527, loss: 0.007601307239383459\n",
      "epoch:  528, loss: 0.0076010036282241344\n",
      "epoch:  529, loss: 0.007599985226988792\n",
      "epoch:  530, loss: 0.007598559837788343\n",
      "epoch:  531, loss: 0.007598229218274355\n",
      "epoch:  532, loss: 0.0075974734500050545\n",
      "epoch:  533, loss: 0.007595801260322332\n",
      "epoch:  534, loss: 0.007595416158437729\n",
      "epoch:  535, loss: 0.007594773545861244\n",
      "epoch:  536, loss: 0.0075929234735667706\n",
      "epoch:  537, loss: 0.007592550944536924\n",
      "epoch:  538, loss: 0.007591500412672758\n",
      "epoch:  539, loss: 0.007590051740407944\n",
      "epoch:  540, loss: 0.007589720655232668\n",
      "epoch:  541, loss: 0.007588209118694067\n",
      "epoch:  542, loss: 0.00758718978613615\n",
      "epoch:  543, loss: 0.007586898747831583\n",
      "epoch:  544, loss: 0.00758611923083663\n",
      "epoch:  545, loss: 0.0075845010578632355\n",
      "epoch:  546, loss: 0.007584105245769024\n",
      "epoch:  547, loss: 0.007583847269415855\n",
      "epoch:  548, loss: 0.007581721059978008\n",
      "epoch:  549, loss: 0.007581316865980625\n",
      "epoch:  550, loss: 0.007581042125821114\n",
      "epoch:  551, loss: 0.007579312194138765\n",
      "epoch:  552, loss: 0.007578529883176088\n",
      "epoch:  553, loss: 0.007578232325613499\n",
      "epoch:  554, loss: 0.007577958516776562\n",
      "epoch:  555, loss: 0.007575830444693565\n",
      "epoch:  556, loss: 0.007575443014502525\n",
      "epoch:  557, loss: 0.0075751859694719315\n",
      "epoch:  558, loss: 0.007573103532195091\n",
      "epoch:  559, loss: 0.007572693284600973\n",
      "epoch:  560, loss: 0.007572449278086424\n",
      "epoch:  561, loss: 0.007570826914161444\n",
      "epoch:  562, loss: 0.007569947279989719\n",
      "epoch:  563, loss: 0.00756966695189476\n",
      "epoch:  564, loss: 0.007567580323666334\n",
      "epoch:  565, loss: 0.007567073218524456\n",
      "epoch:  566, loss: 0.007566786836832762\n",
      "epoch:  567, loss: 0.0075647952035069466\n",
      "epoch:  568, loss: 0.007564167957752943\n",
      "epoch:  569, loss: 0.007563886698335409\n",
      "epoch:  570, loss: 0.007561860140413046\n",
      "epoch:  571, loss: 0.007561258506029844\n",
      "epoch:  572, loss: 0.007560987025499344\n",
      "epoch:  573, loss: 0.007560115307569504\n",
      "epoch:  574, loss: 0.007558473385870457\n",
      "epoch:  575, loss: 0.007558120414614677\n",
      "epoch:  576, loss: 0.007556250784546137\n",
      "epoch:  577, loss: 0.00755546148866415\n",
      "epoch:  578, loss: 0.007555186748504639\n",
      "epoch:  579, loss: 0.007553169038146734\n",
      "epoch:  580, loss: 0.007552541326731443\n",
      "epoch:  581, loss: 0.007552273105829954\n",
      "epoch:  582, loss: 0.007550486363470554\n",
      "epoch:  583, loss: 0.007549641653895378\n",
      "epoch:  584, loss: 0.007549365982413292\n",
      "epoch:  585, loss: 0.007546997629106045\n",
      "epoch:  586, loss: 0.007546540349721909\n",
      "epoch:  587, loss: 0.007545834872871637\n",
      "epoch:  588, loss: 0.007543846964836121\n",
      "epoch:  589, loss: 0.007543470244854689\n",
      "epoch:  590, loss: 0.007542284205555916\n",
      "epoch:  591, loss: 0.007540789898484945\n",
      "epoch:  592, loss: 0.0075404406525194645\n",
      "epoch:  593, loss: 0.007539126090705395\n",
      "epoch:  594, loss: 0.007537698373198509\n",
      "epoch:  595, loss: 0.0075373463332653046\n",
      "epoch:  596, loss: 0.007536345161497593\n",
      "epoch:  597, loss: 0.007534612435847521\n",
      "epoch:  598, loss: 0.00753423199057579\n",
      "epoch:  599, loss: 0.007532639428973198\n",
      "epoch:  600, loss: 0.007531214505434036\n",
      "epoch:  601, loss: 0.0075308396480977535\n",
      "epoch:  602, loss: 0.007529607508331537\n",
      "epoch:  603, loss: 0.00752791715785861\n",
      "epoch:  604, loss: 0.007527538109570742\n",
      "epoch:  605, loss: 0.0075265211053192616\n",
      "epoch:  606, loss: 0.007524459157139063\n",
      "epoch:  607, loss: 0.00752404797822237\n",
      "epoch:  608, loss: 0.007523742504417896\n",
      "epoch:  609, loss: 0.007521762978285551\n",
      "epoch:  610, loss: 0.007520605809986591\n",
      "epoch:  611, loss: 0.007520237471908331\n",
      "epoch:  612, loss: 0.007519208360463381\n",
      "epoch:  613, loss: 0.0075171166099607944\n",
      "epoch:  614, loss: 0.0075166975148022175\n",
      "epoch:  615, loss: 0.007515377830713987\n",
      "epoch:  616, loss: 0.0075135864317417145\n",
      "epoch:  617, loss: 0.007513140328228474\n",
      "epoch:  618, loss: 0.007512103300541639\n",
      "epoch:  619, loss: 0.007510038558393717\n",
      "epoch:  620, loss: 0.0075096250511705875\n",
      "epoch:  621, loss: 0.007509023416787386\n",
      "epoch:  622, loss: 0.0075066606514155865\n",
      "epoch:  623, loss: 0.007506197318434715\n",
      "epoch:  624, loss: 0.007505891378968954\n",
      "epoch:  625, loss: 0.007503527216613293\n",
      "epoch:  626, loss: 0.007502825930714607\n",
      "epoch:  627, loss: 0.007502505090087652\n",
      "epoch:  628, loss: 0.007499909494072199\n",
      "epoch:  629, loss: 0.007499322295188904\n",
      "epoch:  630, loss: 0.007498986553400755\n",
      "epoch:  631, loss: 0.007496264763176441\n",
      "epoch:  632, loss: 0.007495665457099676\n",
      "epoch:  633, loss: 0.007495311088860035\n",
      "epoch:  634, loss: 0.007492891512811184\n",
      "epoch:  635, loss: 0.007491878233850002\n",
      "epoch:  636, loss: 0.00749150151386857\n",
      "epoch:  637, loss: 0.0074892230331897736\n",
      "epoch:  638, loss: 0.007488011848181486\n",
      "epoch:  639, loss: 0.0074876477010548115\n",
      "epoch:  640, loss: 0.0074850874952971935\n",
      "epoch:  641, loss: 0.007484251167625189\n",
      "epoch:  642, loss: 0.007483888883143663\n",
      "epoch:  643, loss: 0.007481958717107773\n",
      "epoch:  644, loss: 0.007480479311197996\n",
      "epoch:  645, loss: 0.007480103988200426\n",
      "epoch:  646, loss: 0.0074788290075957775\n",
      "epoch:  647, loss: 0.007476718630641699\n",
      "epoch:  648, loss: 0.007476297207176685\n",
      "epoch:  649, loss: 0.007473704870790243\n",
      "epoch:  650, loss: 0.007472698576748371\n",
      "epoch:  651, loss: 0.007472327910363674\n",
      "epoch:  652, loss: 0.007469436153769493\n",
      "epoch:  653, loss: 0.00746864452958107\n",
      "epoch:  654, loss: 0.007465891074389219\n",
      "epoch:  655, loss: 0.0074648139998316765\n",
      "epoch:  656, loss: 0.007464414928108454\n",
      "epoch:  657, loss: 0.007462719455361366\n",
      "epoch:  658, loss: 0.007460732012987137\n",
      "epoch:  659, loss: 0.007460250984877348\n",
      "epoch:  660, loss: 0.00745816994458437\n",
      "epoch:  661, loss: 0.007456599269062281\n",
      "epoch:  662, loss: 0.007456113584339619\n",
      "epoch:  663, loss: 0.007455158047378063\n",
      "epoch:  664, loss: 0.0074525028467178345\n",
      "epoch:  665, loss: 0.00745197432115674\n",
      "epoch:  666, loss: 0.0074516963213682175\n",
      "epoch:  667, loss: 0.007448345888406038\n",
      "epoch:  668, loss: 0.007447755895555019\n",
      "epoch:  669, loss: 0.007447379175573587\n",
      "epoch:  670, loss: 0.007444522809237242\n",
      "epoch:  671, loss: 0.007443672977387905\n",
      "epoch:  672, loss: 0.007443257607519627\n",
      "epoch:  673, loss: 0.007440275512635708\n",
      "epoch:  674, loss: 0.007439534645527601\n",
      "epoch:  675, loss: 0.007439143490046263\n",
      "epoch:  676, loss: 0.007437617983669043\n",
      "epoch:  677, loss: 0.007435440551489592\n",
      "epoch:  678, loss: 0.007434961851686239\n",
      "epoch:  679, loss: 0.007434322964400053\n",
      "epoch:  680, loss: 0.007431389298290014\n",
      "epoch:  681, loss: 0.007430813740938902\n",
      "epoch:  682, loss: 0.0074286689050495625\n",
      "epoch:  683, loss: 0.0074269138276577\n",
      "epoch:  684, loss: 0.007426406256854534\n",
      "epoch:  685, loss: 0.007423934526741505\n",
      "epoch:  686, loss: 0.007422533817589283\n",
      "epoch:  687, loss: 0.007422049529850483\n",
      "epoch:  688, loss: 0.007421701680868864\n",
      "epoch:  689, loss: 0.007418466731905937\n",
      "epoch:  690, loss: 0.0074178059585392475\n",
      "epoch:  691, loss: 0.007417389191687107\n",
      "epoch:  692, loss: 0.0074141742661595345\n",
      "epoch:  693, loss: 0.0074133300222456455\n",
      "epoch:  694, loss: 0.007412884850054979\n",
      "epoch:  695, loss: 0.007410351652652025\n",
      "epoch:  696, loss: 0.007408933248370886\n",
      "epoch:  697, loss: 0.007408476900309324\n",
      "epoch:  698, loss: 0.007405917625874281\n",
      "epoch:  699, loss: 0.0074045551009476185\n",
      "epoch:  700, loss: 0.0074040950275957584\n",
      "epoch:  701, loss: 0.007402402814477682\n",
      "epoch:  702, loss: 0.007400240283459425\n",
      "epoch:  703, loss: 0.007399764843285084\n",
      "epoch:  704, loss: 0.007398509886115789\n",
      "epoch:  705, loss: 0.007395935710519552\n",
      "epoch:  706, loss: 0.007395393215119839\n",
      "epoch:  707, loss: 0.007394525222480297\n",
      "epoch:  708, loss: 0.007391586899757385\n",
      "epoch:  709, loss: 0.007391041610389948\n",
      "epoch:  710, loss: 0.007389381993561983\n",
      "epoch:  711, loss: 0.007387204095721245\n",
      "epoch:  712, loss: 0.007386676035821438\n",
      "epoch:  713, loss: 0.0073854923248291016\n",
      "epoch:  714, loss: 0.007382845506072044\n",
      "epoch:  715, loss: 0.007382295560091734\n",
      "epoch:  716, loss: 0.00738073606044054\n",
      "epoch:  717, loss: 0.007378346752375364\n",
      "epoch:  718, loss: 0.00737780099734664\n",
      "epoch:  719, loss: 0.007376164197921753\n",
      "epoch:  720, loss: 0.007373808417469263\n",
      "epoch:  721, loss: 0.007373280357569456\n",
      "epoch:  722, loss: 0.007372877094894648\n",
      "epoch:  723, loss: 0.007369688246399164\n",
      "epoch:  724, loss: 0.007368783466517925\n",
      "epoch:  725, loss: 0.00736835366114974\n",
      "epoch:  726, loss: 0.007365201599895954\n",
      "epoch:  727, loss: 0.007364232558757067\n",
      "epoch:  728, loss: 0.007363785523921251\n",
      "epoch:  729, loss: 0.007361618336290121\n",
      "epoch:  730, loss: 0.007359721232205629\n",
      "epoch:  731, loss: 0.007359212264418602\n",
      "epoch:  732, loss: 0.007357509806752205\n",
      "epoch:  733, loss: 0.007355089299380779\n",
      "epoch:  734, loss: 0.007354489527642727\n",
      "epoch:  735, loss: 0.007353868335485458\n",
      "epoch:  736, loss: 0.007350515574216843\n",
      "epoch:  737, loss: 0.0073498268611729145\n",
      "epoch:  738, loss: 0.0073493956588208675\n",
      "epoch:  739, loss: 0.007345857098698616\n",
      "epoch:  740, loss: 0.007345172110944986\n",
      "epoch:  741, loss: 0.007344752084463835\n",
      "epoch:  742, loss: 0.007341428194195032\n",
      "epoch:  743, loss: 0.0073405299335718155\n",
      "epoch:  744, loss: 0.007340049836784601\n",
      "epoch:  745, loss: 0.007337676361203194\n",
      "epoch:  746, loss: 0.007335767149925232\n",
      "epoch:  747, loss: 0.00733520882204175\n",
      "epoch:  748, loss: 0.0073343501426279545\n",
      "epoch:  749, loss: 0.00733100064098835\n",
      "epoch:  750, loss: 0.0073303296230733395\n",
      "epoch:  751, loss: 0.007329897955060005\n",
      "epoch:  752, loss: 0.007326926104724407\n",
      "epoch:  753, loss: 0.007325579412281513\n",
      "epoch:  754, loss: 0.00732508534565568\n",
      "epoch:  755, loss: 0.007323211058974266\n",
      "epoch:  756, loss: 0.007320848759263754\n",
      "epoch:  757, loss: 0.0073202913627028465\n",
      "epoch:  758, loss: 0.00731861125677824\n",
      "epoch:  759, loss: 0.007316086441278458\n",
      "epoch:  760, loss: 0.007315495982766151\n",
      "epoch:  761, loss: 0.007314319722354412\n",
      "epoch:  762, loss: 0.007311316207051277\n",
      "epoch:  763, loss: 0.007310702931135893\n",
      "epoch:  764, loss: 0.007310275454074144\n",
      "epoch:  765, loss: 0.007306648418307304\n",
      "epoch:  766, loss: 0.007305887993425131\n",
      "epoch:  767, loss: 0.007305569015443325\n",
      "epoch:  768, loss: 0.007301707752048969\n",
      "epoch:  769, loss: 0.007301011122763157\n",
      "epoch:  770, loss: 0.007300561293959618\n",
      "epoch:  771, loss: 0.007296821102499962\n",
      "epoch:  772, loss: 0.0072960625402629375\n",
      "epoch:  773, loss: 0.007295623887330294\n",
      "epoch:  774, loss: 0.007292145863175392\n",
      "epoch:  775, loss: 0.007291183341294527\n",
      "epoch:  776, loss: 0.007290723733603954\n",
      "epoch:  777, loss: 0.00728754885494709\n",
      "epoch:  778, loss: 0.007286280393600464\n",
      "epoch:  779, loss: 0.007285765837877989\n",
      "epoch:  780, loss: 0.0072827525436878204\n",
      "epoch:  781, loss: 0.007281261030584574\n",
      "epoch:  782, loss: 0.007280708756297827\n",
      "epoch:  783, loss: 0.007277532014995813\n",
      "epoch:  784, loss: 0.007276124786585569\n",
      "epoch:  785, loss: 0.007275603245943785\n",
      "epoch:  786, loss: 0.007272270508110523\n",
      "epoch:  787, loss: 0.007271049078553915\n",
      "epoch:  788, loss: 0.007270516827702522\n",
      "epoch:  789, loss: 0.007268092595040798\n",
      "epoch:  790, loss: 0.00726604089140892\n",
      "epoch:  791, loss: 0.007265430875122547\n",
      "epoch:  792, loss: 0.007264935877174139\n",
      "epoch:  793, loss: 0.007261178456246853\n",
      "epoch:  794, loss: 0.0072603654116392136\n",
      "epoch:  795, loss: 0.0072598885744810104\n",
      "epoch:  796, loss: 0.007256402168422937\n",
      "epoch:  797, loss: 0.007255322299897671\n",
      "epoch:  798, loss: 0.007254787255078554\n",
      "epoch:  799, loss: 0.007252111565321684\n",
      "epoch:  800, loss: 0.007250191643834114\n",
      "epoch:  801, loss: 0.007249610498547554\n",
      "epoch:  802, loss: 0.007249133661389351\n",
      "epoch:  803, loss: 0.007245238870382309\n",
      "epoch:  804, loss: 0.007244392763823271\n",
      "epoch:  805, loss: 0.007243915926665068\n",
      "epoch:  806, loss: 0.007240267936140299\n",
      "epoch:  807, loss: 0.007239243946969509\n",
      "epoch:  808, loss: 0.00723872147500515\n",
      "epoch:  809, loss: 0.007236735429614782\n",
      "epoch:  810, loss: 0.0072341118939220905\n",
      "epoch:  811, loss: 0.00723345298320055\n",
      "epoch:  812, loss: 0.0072317468002438545\n",
      "epoch:  813, loss: 0.0072289262898266315\n",
      "epoch:  814, loss: 0.0072281090542674065\n",
      "epoch:  815, loss: 0.007227614987641573\n",
      "epoch:  816, loss: 0.0072235544212162495\n",
      "epoch:  817, loss: 0.007222714368253946\n",
      "epoch:  818, loss: 0.007222175132483244\n",
      "epoch:  819, loss: 0.0072207218036055565\n",
      "epoch:  820, loss: 0.007217441685497761\n",
      "epoch:  821, loss: 0.007216724567115307\n",
      "epoch:  822, loss: 0.007216222118586302\n",
      "epoch:  823, loss: 0.007211985997855663\n",
      "epoch:  824, loss: 0.007211163640022278\n",
      "epoch:  825, loss: 0.007210665848106146\n",
      "epoch:  826, loss: 0.0072076767683029175\n",
      "epoch:  827, loss: 0.007205843925476074\n",
      "epoch:  828, loss: 0.0072051649913191795\n",
      "epoch:  829, loss: 0.007204574532806873\n",
      "epoch:  830, loss: 0.007200354244560003\n",
      "epoch:  831, loss: 0.007199535146355629\n",
      "epoch:  832, loss: 0.007199020590633154\n",
      "epoch:  833, loss: 0.007195152807980776\n",
      "epoch:  834, loss: 0.0071938978508114815\n",
      "epoch:  835, loss: 0.007193326950073242\n",
      "epoch:  836, loss: 0.007189531344920397\n",
      "epoch:  837, loss: 0.007188100833445787\n",
      "epoch:  838, loss: 0.007187479175627232\n",
      "epoch:  839, loss: 0.0071838367730379105\n",
      "epoch:  840, loss: 0.007182106841355562\n",
      "epoch:  841, loss: 0.007181457709521055\n",
      "epoch:  842, loss: 0.00717796990647912\n",
      "epoch:  843, loss: 0.007176067214459181\n",
      "epoch:  844, loss: 0.007175344042479992\n",
      "epoch:  845, loss: 0.007173127960413694\n",
      "epoch:  846, loss: 0.007169967517256737\n",
      "epoch:  847, loss: 0.007169229444116354\n",
      "epoch:  848, loss: 0.0071686748415231705\n",
      "epoch:  849, loss: 0.007165021728724241\n",
      "epoch:  850, loss: 0.007163274101912975\n",
      "epoch:  851, loss: 0.00716265756636858\n",
      "epoch:  852, loss: 0.007161541376262903\n",
      "epoch:  853, loss: 0.007157533895224333\n",
      "epoch:  854, loss: 0.007156713400036097\n",
      "epoch:  855, loss: 0.0071561383083462715\n",
      "epoch:  856, loss: 0.007153834216296673\n",
      "epoch:  857, loss: 0.007150922901928425\n",
      "epoch:  858, loss: 0.007150107063353062\n",
      "epoch:  859, loss: 0.007149568758904934\n",
      "epoch:  860, loss: 0.007146115880459547\n",
      "epoch:  861, loss: 0.007144163828343153\n",
      "epoch:  862, loss: 0.007143446244299412\n",
      "epoch:  863, loss: 0.007142293266952038\n",
      "epoch:  864, loss: 0.007138177752494812\n",
      "epoch:  865, loss: 0.007137298583984375\n",
      "epoch:  866, loss: 0.0071367197670042515\n",
      "epoch:  867, loss: 0.007133200764656067\n",
      "epoch:  868, loss: 0.007131204474717379\n",
      "epoch:  869, loss: 0.007130537182092667\n",
      "epoch:  870, loss: 0.007129977457225323\n",
      "epoch:  871, loss: 0.00712576974183321\n",
      "epoch:  872, loss: 0.0071243224665522575\n",
      "epoch:  873, loss: 0.007123682647943497\n",
      "epoch:  874, loss: 0.007123391143977642\n",
      "epoch:  875, loss: 0.007118387147784233\n",
      "epoch:  876, loss: 0.007117399945855141\n",
      "epoch:  877, loss: 0.007116826716810465\n",
      "epoch:  878, loss: 0.007112118415534496\n",
      "epoch:  879, loss: 0.007111136335879564\n",
      "epoch:  880, loss: 0.0071105132810771465\n",
      "epoch:  881, loss: 0.007106551434844732\n",
      "epoch:  882, loss: 0.007104862481355667\n",
      "epoch:  883, loss: 0.007104158867150545\n",
      "epoch:  884, loss: 0.00710079213604331\n",
      "epoch:  885, loss: 0.007098485250025988\n",
      "epoch:  886, loss: 0.007097707595676184\n",
      "epoch:  887, loss: 0.007096945773810148\n",
      "epoch:  888, loss: 0.007092318031936884\n",
      "epoch:  889, loss: 0.007091232109814882\n",
      "epoch:  890, loss: 0.0070906346663832664\n",
      "epoch:  891, loss: 0.007086091674864292\n",
      "epoch:  892, loss: 0.007084772456437349\n",
      "epoch:  893, loss: 0.007084141951054335\n",
      "epoch:  894, loss: 0.00708030117675662\n",
      "epoch:  895, loss: 0.00707817729562521\n",
      "epoch:  896, loss: 0.007077472750097513\n",
      "epoch:  897, loss: 0.00707364222034812\n",
      "epoch:  898, loss: 0.007071248255670071\n",
      "epoch:  899, loss: 0.00707040773704648\n",
      "epoch:  900, loss: 0.00706989923492074\n",
      "epoch:  901, loss: 0.007064340636134148\n",
      "epoch:  902, loss: 0.007063199765980244\n",
      "epoch:  903, loss: 0.007062484044581652\n",
      "epoch:  904, loss: 0.0070579154416918755\n",
      "epoch:  905, loss: 0.007055944297462702\n",
      "epoch:  906, loss: 0.0070551540702581406\n",
      "epoch:  907, loss: 0.007054534740746021\n",
      "epoch:  908, loss: 0.007051642518490553\n",
      "epoch:  909, loss: 0.007048308849334717\n",
      "epoch:  910, loss: 0.0070474459789693356\n",
      "epoch:  911, loss: 0.007045552600175142\n",
      "epoch:  912, loss: 0.007041423115879297\n",
      "epoch:  913, loss: 0.007040334865450859\n",
      "epoch:  914, loss: 0.0070397029630839825\n",
      "epoch:  915, loss: 0.00703473761677742\n",
      "epoch:  916, loss: 0.007033435627818108\n",
      "epoch:  917, loss: 0.00703272083774209\n",
      "epoch:  918, loss: 0.007031697779893875\n",
      "epoch:  919, loss: 0.007026721723377705\n",
      "epoch:  920, loss: 0.007025720551609993\n",
      "epoch:  921, loss: 0.0070250569842755795\n",
      "epoch:  922, loss: 0.007019958458840847\n",
      "epoch:  923, loss: 0.0070185791701078415\n",
      "epoch:  924, loss: 0.007017850410193205\n",
      "epoch:  925, loss: 0.007015801500529051\n",
      "epoch:  926, loss: 0.007011520676314831\n",
      "epoch:  927, loss: 0.0070104897022247314\n",
      "epoch:  928, loss: 0.0070098089054226875\n",
      "epoch:  929, loss: 0.0070041571743786335\n",
      "epoch:  930, loss: 0.007002666126936674\n",
      "epoch:  931, loss: 0.0070017678663134575\n",
      "epoch:  932, loss: 0.007001016288995743\n",
      "epoch:  933, loss: 0.006997263059020042\n",
      "epoch:  934, loss: 0.006993027403950691\n",
      "epoch:  935, loss: 0.006991895847022533\n",
      "epoch:  936, loss: 0.0069918460212647915\n",
      "epoch:  937, loss: 0.0069846599362790585\n",
      "epoch:  938, loss: 0.006983054336160421\n",
      "epoch:  939, loss: 0.006982256192713976\n",
      "epoch:  940, loss: 0.00697892252355814\n",
      "epoch:  941, loss: 0.006974716205149889\n",
      "epoch:  942, loss: 0.006973554380238056\n",
      "epoch:  943, loss: 0.006972753908485174\n",
      "epoch:  944, loss: 0.006966001354157925\n",
      "epoch:  945, loss: 0.006964663974940777\n",
      "epoch:  946, loss: 0.006963825318962336\n",
      "epoch:  947, loss: 0.006957280449569225\n",
      "epoch:  948, loss: 0.006955495569854975\n",
      "epoch:  949, loss: 0.00695462292060256\n",
      "epoch:  950, loss: 0.006949219852685928\n",
      "epoch:  951, loss: 0.006946590729057789\n",
      "epoch:  952, loss: 0.006945600733160973\n",
      "epoch:  953, loss: 0.006943006068468094\n",
      "epoch:  954, loss: 0.006937806494534016\n",
      "epoch:  955, loss: 0.006936565041542053\n",
      "epoch:  956, loss: 0.0069357603788375854\n",
      "epoch:  957, loss: 0.006930549629032612\n",
      "epoch:  958, loss: 0.006927786394953728\n",
      "epoch:  959, loss: 0.0069267903454601765\n",
      "epoch:  960, loss: 0.00692312978208065\n",
      "epoch:  961, loss: 0.00691875210031867\n",
      "epoch:  962, loss: 0.006917485501617193\n",
      "epoch:  963, loss: 0.006916657090187073\n",
      "epoch:  964, loss: 0.006910120137035847\n",
      "epoch:  965, loss: 0.0069083403795957565\n",
      "epoch:  966, loss: 0.006907449569553137\n",
      "epoch:  967, loss: 0.006903407163918018\n",
      "epoch:  968, loss: 0.006899641826748848\n",
      "epoch:  969, loss: 0.006898561026901007\n",
      "epoch:  970, loss: 0.006898084189742804\n",
      "epoch:  971, loss: 0.0068910871632397175\n",
      "epoch:  972, loss: 0.006889563985168934\n",
      "epoch:  973, loss: 0.006888668518513441\n",
      "epoch:  974, loss: 0.0068825287744402885\n",
      "epoch:  975, loss: 0.006880009081214666\n",
      "epoch:  976, loss: 0.006878996267914772\n",
      "epoch:  977, loss: 0.00687304325401783\n",
      "epoch:  978, loss: 0.00687011843547225\n",
      "epoch:  979, loss: 0.006869101896882057\n",
      "epoch:  980, loss: 0.0068646930158138275\n",
      "epoch:  981, loss: 0.006860469002276659\n",
      "epoch:  982, loss: 0.006859241519123316\n",
      "epoch:  983, loss: 0.006857774220407009\n",
      "epoch:  984, loss: 0.006850730627775192\n",
      "epoch:  985, loss: 0.0068491376005113125\n",
      "epoch:  986, loss: 0.006848211865872145\n",
      "epoch:  987, loss: 0.006840612273663282\n",
      "epoch:  988, loss: 0.006838733330368996\n",
      "epoch:  989, loss: 0.006837707944214344\n",
      "epoch:  990, loss: 0.006830432452261448\n",
      "epoch:  991, loss: 0.006828008219599724\n",
      "epoch:  992, loss: 0.006826920434832573\n",
      "epoch:  993, loss: 0.006825053598731756\n",
      "epoch:  994, loss: 0.006818013731390238\n",
      "epoch:  995, loss: 0.006816393695771694\n",
      "epoch:  996, loss: 0.006815394852310419\n",
      "epoch:  997, loss: 0.006808820180594921\n",
      "epoch:  998, loss: 0.006806103978306055\n",
      "epoch:  999, loss: 0.006805009674280882\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size=X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "opt = torch_numopt.GradientDescentLS(model.parameters(), lr=1, model=model, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"armijo\")\n",
    "# opt = torch_numopt.GradientDescentLS(model.parameters(), lr=1, model=model, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"wolfe\")\n",
    "# opt = torch_numopt.GradientDescentLS(model.parameters(), lr=1, model=model, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"strong-wolfe\")\n",
    "# opt = torch_numopt.GradientDescentLS(model.parameters(), lr=1, model=model, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"goldstein\")\n",
    "\n",
    "\n",
    "all_loss = {}\n",
    "for epoch in range(1000):\n",
    "    print(\"epoch: \", epoch, end=\"\")\n",
    "    all_loss[epoch + 1] = 0\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y, loss_fn)\n",
    "\n",
    "        all_loss[epoch + 1] += loss\n",
    "    all_loss[epoch + 1] /= len(data_loader)\n",
    "    print(\", loss: {}\".format(all_loss[epoch + 1].detach().numpy().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics: R2 = 0.7243558132376975\n",
      "Test metrics:  R2 = 0.7701979767426024\n"
     ]
    }
   ],
   "source": [
    "pred_train = model.forward(X_train).detach()\n",
    "pred_test = model.forward(X_test).detach()\n",
    "print(f\"Train metrics: R2 = {r2_score(pred_train, y_train)}\")\n",
    "print(f\"Test metrics:  R2 = {r2_score(pred_test, y_test)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
