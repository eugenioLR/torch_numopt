{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch_numopt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import *\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.f1 = nn.Linear(input_size, 10, device=device)\n",
    "        self.f2 = nn.Linear(10, 20, device=device)\n",
    "        self.f3 = nn.Linear(20, 20, device=device)\n",
    "        self.f4 = nn.Linear(20, 10, device=device)\n",
    "        self.f5 = nn.Linear(10, 1, device=device)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        # self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.f1(x))\n",
    "        x = self.activation(self.f2(x))\n",
    "        x = self.activation(self.f3(x))\n",
    "        x = self.activation(self.f4(x))\n",
    "        x = self.f5(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([800, 10]) torch.Size([800, 1])\n",
      "torch.Size([200, 10]) torch.Size([200, 1])\n"
     ]
    }
   ],
   "source": [
    "# X, y = load_diabetes(return_X_y=True, scaled=False)\n",
    "# X, y = make_regression(n_samples=1000, n_features=100)\n",
    "X, y = make_friedman1(n_samples=1000, noise=1e-2)\n",
    "\n",
    "X_scaler = MinMaxScaler()\n",
    "X = X_scaler.fit_transform(X)\n",
    "X = torch.Tensor(X).to(device)\n",
    "\n",
    "y_scaler = MinMaxScaler()\n",
    "y = y_scaler.fit_transform(y.reshape((-1, 1)))\n",
    "y = torch.Tensor(y).to(device)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "torch_data = TensorDataset(X_train, y_train)\n",
    "data_loader = DataLoader(torch_data, batch_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No Line search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.7050851583480835\n",
      "epoch:  1, loss: 0.6846209168434143\n",
      "epoch:  2, loss: 0.6648592948913574\n",
      "epoch:  3, loss: 0.6457696557044983\n",
      "epoch:  4, loss: 0.6273196339607239\n",
      "epoch:  5, loss: 0.6094874143600464\n",
      "epoch:  6, loss: 0.5922489762306213\n",
      "epoch:  7, loss: 0.5755816698074341\n",
      "epoch:  8, loss: 0.5594626069068909\n",
      "epoch:  9, loss: 0.5438734292984009\n",
      "epoch:  10, loss: 0.5287901163101196\n",
      "epoch:  11, loss: 0.5141943097114563\n",
      "epoch:  12, loss: 0.5000653862953186\n",
      "epoch:  13, loss: 0.4863907992839813\n",
      "epoch:  14, loss: 0.47315213084220886\n",
      "epoch:  15, loss: 0.4603339433670044\n",
      "epoch:  16, loss: 0.4479227066040039\n",
      "epoch:  17, loss: 0.4359026849269867\n",
      "epoch:  18, loss: 0.42426154017448425\n",
      "epoch:  19, loss: 0.41298922896385193\n",
      "epoch:  20, loss: 0.40206992626190186\n",
      "epoch:  21, loss: 0.3914913833141327\n",
      "epoch:  22, loss: 0.38124287128448486\n",
      "epoch:  23, loss: 0.3713095486164093\n",
      "epoch:  24, loss: 0.36169540882110596\n",
      "epoch:  25, loss: 0.352390855550766\n",
      "epoch:  26, loss: 0.3433999717235565\n",
      "epoch:  27, loss: 0.33472946286201477\n",
      "epoch:  28, loss: 0.3263673782348633\n",
      "epoch:  29, loss: 0.3183106482028961\n",
      "epoch:  30, loss: 0.3105577528476715\n",
      "epoch:  31, loss: 0.30310630798339844\n",
      "epoch:  32, loss: 0.2959485948085785\n",
      "epoch:  33, loss: 0.2890545427799225\n",
      "epoch:  34, loss: 0.2824210226535797\n",
      "epoch:  35, loss: 0.2760316729545593\n",
      "epoch:  36, loss: 0.269863486289978\n",
      "epoch:  37, loss: 0.2639104127883911\n",
      "epoch:  38, loss: 0.25815680623054504\n",
      "epoch:  39, loss: 0.2525794208049774\n",
      "epoch:  40, loss: 0.2471846342086792\n",
      "epoch:  41, loss: 0.24195373058319092\n",
      "epoch:  42, loss: 0.23688288033008575\n",
      "epoch:  43, loss: 0.23195886611938477\n",
      "epoch:  44, loss: 0.22717119753360748\n",
      "epoch:  45, loss: 0.22251775860786438\n",
      "epoch:  46, loss: 0.21799694001674652\n",
      "epoch:  47, loss: 0.2136010229587555\n",
      "epoch:  48, loss: 0.20932544767856598\n",
      "epoch:  49, loss: 0.20516259968280792\n",
      "epoch:  50, loss: 0.20110580325126648\n",
      "epoch:  51, loss: 0.19715452194213867\n",
      "epoch:  52, loss: 0.19330482184886932\n",
      "epoch:  53, loss: 0.18955449759960175\n",
      "epoch:  54, loss: 0.18590103089809418\n",
      "epoch:  55, loss: 0.1823396235704422\n",
      "epoch:  56, loss: 0.17886686325073242\n",
      "epoch:  57, loss: 0.1754806935787201\n",
      "epoch:  58, loss: 0.17217840254306793\n",
      "epoch:  59, loss: 0.1689591258764267\n",
      "epoch:  60, loss: 0.16581976413726807\n",
      "epoch:  61, loss: 0.16275648772716522\n",
      "epoch:  62, loss: 0.15976791083812714\n",
      "epoch:  63, loss: 0.15685103833675385\n",
      "epoch:  64, loss: 0.15400458872318268\n",
      "epoch:  65, loss: 0.15122701227664948\n",
      "epoch:  66, loss: 0.14851631224155426\n",
      "epoch:  67, loss: 0.14587007462978363\n",
      "epoch:  68, loss: 0.1432875245809555\n",
      "epoch:  69, loss: 0.1407671868801117\n",
      "epoch:  70, loss: 0.13830748200416565\n",
      "epoch:  71, loss: 0.13590623438358307\n",
      "epoch:  72, loss: 0.13356249034404755\n",
      "epoch:  73, loss: 0.13127490878105164\n",
      "epoch:  74, loss: 0.1290423572063446\n",
      "epoch:  75, loss: 0.12686297297477722\n",
      "epoch:  76, loss: 0.12473561614751816\n",
      "epoch:  77, loss: 0.12265880405902863\n",
      "epoch:  78, loss: 0.12063132226467133\n",
      "epoch:  79, loss: 0.11865217238664627\n",
      "epoch:  80, loss: 0.11672037094831467\n",
      "epoch:  81, loss: 0.11483453959226608\n",
      "epoch:  82, loss: 0.11299353837966919\n",
      "epoch:  83, loss: 0.1111963763833046\n",
      "epoch:  84, loss: 0.10944215953350067\n",
      "epoch:  85, loss: 0.10773022472858429\n",
      "epoch:  86, loss: 0.10605902969837189\n",
      "epoch:  87, loss: 0.10442761331796646\n",
      "epoch:  88, loss: 0.1028350442647934\n",
      "epoch:  89, loss: 0.10128013789653778\n",
      "epoch:  90, loss: 0.09976205229759216\n",
      "epoch:  91, loss: 0.09828007966279984\n",
      "epoch:  92, loss: 0.09683333337306976\n",
      "epoch:  93, loss: 0.09542104601860046\n",
      "epoch:  94, loss: 0.09404224157333374\n",
      "epoch:  95, loss: 0.09269621968269348\n",
      "epoch:  96, loss: 0.09138230234384537\n",
      "epoch:  97, loss: 0.09009990096092224\n",
      "epoch:  98, loss: 0.08884813636541367\n",
      "epoch:  99, loss: 0.08762629330158234\n",
      "epoch:  100, loss: 0.0864335149526596\n",
      "epoch:  101, loss: 0.08526908606290817\n",
      "epoch:  102, loss: 0.08413233608007431\n",
      "epoch:  103, loss: 0.0830225870013237\n",
      "epoch:  104, loss: 0.08193941414356232\n",
      "epoch:  105, loss: 0.08088213205337524\n",
      "epoch:  106, loss: 0.0798502191901207\n",
      "epoch:  107, loss: 0.07884295284748077\n",
      "epoch:  108, loss: 0.07785972952842712\n",
      "epoch:  109, loss: 0.0768999457359314\n",
      "epoch:  110, loss: 0.07596312463283539\n",
      "epoch:  111, loss: 0.07504887878894806\n",
      "epoch:  112, loss: 0.07415668666362762\n",
      "epoch:  113, loss: 0.07328582555055618\n",
      "epoch:  114, loss: 0.07243576645851135\n",
      "epoch:  115, loss: 0.0716061145067215\n",
      "epoch:  116, loss: 0.07079629600048065\n",
      "epoch:  117, loss: 0.07000584900379181\n",
      "epoch:  118, loss: 0.06923431158065796\n",
      "epoch:  119, loss: 0.06848131120204926\n",
      "epoch:  120, loss: 0.0677463561296463\n",
      "epoch:  121, loss: 0.06702900677919388\n",
      "epoch:  122, loss: 0.06632894277572632\n",
      "epoch:  123, loss: 0.06564568728208542\n",
      "epoch:  124, loss: 0.06497885286808014\n",
      "epoch:  125, loss: 0.06432817876338959\n",
      "epoch:  126, loss: 0.06369314342737198\n",
      "epoch:  127, loss: 0.06307335942983627\n",
      "epoch:  128, loss: 0.0624685175716877\n",
      "epoch:  129, loss: 0.06187831982970238\n",
      "epoch:  130, loss: 0.061302341520786285\n",
      "epoch:  131, loss: 0.06074028089642525\n",
      "epoch:  132, loss: 0.06019178405404091\n",
      "epoch:  133, loss: 0.0596565343439579\n",
      "epoch:  134, loss: 0.05913420766592026\n",
      "epoch:  135, loss: 0.058624517172575\n",
      "epoch:  136, loss: 0.05812718719244003\n",
      "epoch:  137, loss: 0.05764187499880791\n",
      "epoch:  138, loss: 0.057168275117874146\n",
      "epoch:  139, loss: 0.05670613422989845\n",
      "epoch:  140, loss: 0.056255172938108444\n",
      "epoch:  141, loss: 0.05581510439515114\n",
      "epoch:  142, loss: 0.05538570508360863\n",
      "epoch:  143, loss: 0.054966699331998825\n",
      "epoch:  144, loss: 0.05455785244703293\n",
      "epoch:  145, loss: 0.054158903658390045\n",
      "epoch:  146, loss: 0.05376961827278137\n",
      "epoch:  147, loss: 0.053389787673950195\n",
      "epoch:  148, loss: 0.05301922932267189\n",
      "epoch:  149, loss: 0.052657708525657654\n",
      "epoch:  150, loss: 0.05230496823787689\n",
      "epoch:  151, loss: 0.05196081101894379\n",
      "epoch:  152, loss: 0.05162506550550461\n",
      "epoch:  153, loss: 0.05129753053188324\n",
      "epoch:  154, loss: 0.05097797513008118\n",
      "epoch:  155, loss: 0.05066617950797081\n",
      "epoch:  156, loss: 0.05036197230219841\n",
      "epoch:  157, loss: 0.05006518214941025\n",
      "epoch:  158, loss: 0.049775630235672\n",
      "epoch:  159, loss: 0.04949314147233963\n",
      "epoch:  160, loss: 0.049217578023672104\n",
      "epoch:  161, loss: 0.0489487461745739\n",
      "epoch:  162, loss: 0.04868648573756218\n",
      "epoch:  163, loss: 0.048430636525154114\n",
      "epoch:  164, loss: 0.048181068152189255\n",
      "epoch:  165, loss: 0.04793757572770119\n",
      "epoch:  166, loss: 0.047700051218271255\n",
      "epoch:  167, loss: 0.04746832698583603\n",
      "epoch:  168, loss: 0.047242261469364166\n",
      "epoch:  169, loss: 0.047021716833114624\n",
      "epoch:  170, loss: 0.04680658504366875\n",
      "epoch:  171, loss: 0.0465967170894146\n",
      "epoch:  172, loss: 0.04639197885990143\n",
      "epoch:  173, loss: 0.04619227722287178\n",
      "epoch:  174, loss: 0.045997489243745804\n",
      "epoch:  175, loss: 0.04580751806497574\n",
      "epoch:  176, loss: 0.045622218400239944\n",
      "epoch:  177, loss: 0.04544147104024887\n",
      "epoch:  178, loss: 0.04526513069868088\n",
      "epoch:  179, loss: 0.0450931042432785\n",
      "epoch:  180, loss: 0.04492530971765518\n",
      "epoch:  181, loss: 0.04476165398955345\n",
      "epoch:  182, loss: 0.04460202157497406\n",
      "epoch:  183, loss: 0.044446300715208054\n",
      "epoch:  184, loss: 0.04429442435503006\n",
      "epoch:  185, loss: 0.044146277010440826\n",
      "epoch:  186, loss: 0.04400176927447319\n",
      "epoch:  187, loss: 0.04386081174015999\n",
      "epoch:  188, loss: 0.043723322451114655\n",
      "epoch:  189, loss: 0.043589212000370026\n",
      "epoch:  190, loss: 0.04345839470624924\n",
      "epoch:  191, loss: 0.04333079233765602\n",
      "epoch:  192, loss: 0.04320633411407471\n",
      "epoch:  193, loss: 0.04308493435382843\n",
      "epoch:  194, loss: 0.04296652972698212\n",
      "epoch:  195, loss: 0.0428510420024395\n",
      "epoch:  196, loss: 0.042738400399684906\n",
      "epoch:  197, loss: 0.04262853041291237\n",
      "epoch:  198, loss: 0.04252137988805771\n",
      "epoch:  199, loss: 0.04241686314344406\n",
      "epoch:  200, loss: 0.04231492057442665\n",
      "epoch:  201, loss: 0.0422155000269413\n",
      "epoch:  202, loss: 0.04211852699518204\n",
      "epoch:  203, loss: 0.04202396050095558\n",
      "epoch:  204, loss: 0.041931718587875366\n",
      "epoch:  205, loss: 0.04184175655245781\n",
      "epoch:  206, loss: 0.04175402224063873\n",
      "epoch:  207, loss: 0.041668448597192764\n",
      "epoch:  208, loss: 0.04158499836921692\n",
      "epoch:  209, loss: 0.041503600776195526\n",
      "epoch:  210, loss: 0.041424211114645004\n",
      "epoch:  211, loss: 0.04134678840637207\n",
      "epoch:  212, loss: 0.04127127677202225\n",
      "epoch:  213, loss: 0.04119763523340225\n",
      "epoch:  214, loss: 0.04112580791115761\n",
      "epoch:  215, loss: 0.04105575382709503\n",
      "epoch:  216, loss: 0.04098743945360184\n",
      "epoch:  217, loss: 0.04092080891132355\n",
      "epoch:  218, loss: 0.040855832397937775\n",
      "epoch:  219, loss: 0.04079245403409004\n",
      "epoch:  220, loss: 0.04073064401745796\n",
      "epoch:  221, loss: 0.04067035764455795\n",
      "epoch:  222, loss: 0.04061155393719673\n",
      "epoch:  223, loss: 0.040554214268922806\n",
      "epoch:  224, loss: 0.040498290210962296\n",
      "epoch:  225, loss: 0.040443748235702515\n",
      "epoch:  226, loss: 0.04039054736495018\n",
      "epoch:  227, loss: 0.0403386689722538\n",
      "epoch:  228, loss: 0.04028807580471039\n",
      "epoch:  229, loss: 0.04023874178528786\n",
      "epoch:  230, loss: 0.04019062966108322\n",
      "epoch:  231, loss: 0.040143705904483795\n",
      "epoch:  232, loss: 0.04009794071316719\n",
      "epoch:  233, loss: 0.04005331173539162\n",
      "epoch:  234, loss: 0.04000978171825409\n",
      "epoch:  235, loss: 0.039967332035303116\n",
      "epoch:  236, loss: 0.03992592170834541\n",
      "epoch:  237, loss: 0.039885539561510086\n",
      "epoch:  238, loss: 0.039846159517765045\n",
      "epoch:  239, loss: 0.0398077517747879\n",
      "epoch:  240, loss: 0.039770279079675674\n",
      "epoch:  241, loss: 0.03973373398184776\n",
      "epoch:  242, loss: 0.039698097854852676\n",
      "epoch:  243, loss: 0.03966333717107773\n",
      "epoch:  244, loss: 0.039629437029361725\n",
      "epoch:  245, loss: 0.03959637135267258\n",
      "epoch:  246, loss: 0.03956412896513939\n",
      "epoch:  247, loss: 0.039532676339149475\n",
      "epoch:  248, loss: 0.03950200229883194\n",
      "epoch:  249, loss: 0.0394720658659935\n",
      "epoch:  250, loss: 0.039442867040634155\n",
      "epoch:  251, loss: 0.039414383471012115\n",
      "epoch:  252, loss: 0.03938660770654678\n",
      "epoch:  253, loss: 0.03935950994491577\n",
      "epoch:  254, loss: 0.039333075284957886\n",
      "epoch:  255, loss: 0.03930728882551193\n",
      "epoch:  256, loss: 0.03928213194012642\n",
      "epoch:  257, loss: 0.03925759345293045\n",
      "epoch:  258, loss: 0.03923365846276283\n",
      "epoch:  259, loss: 0.03921030834317207\n",
      "epoch:  260, loss: 0.03918753191828728\n",
      "epoch:  261, loss: 0.03916531801223755\n",
      "epoch:  262, loss: 0.0391436442732811\n",
      "epoch:  263, loss: 0.03912249580025673\n",
      "epoch:  264, loss: 0.039101868867874146\n",
      "epoch:  265, loss: 0.03908174857497215\n",
      "epoch:  266, loss: 0.03906212002038956\n",
      "epoch:  267, loss: 0.039042968302965164\n",
      "epoch:  268, loss: 0.039024289697408676\n",
      "epoch:  269, loss: 0.0390060693025589\n",
      "epoch:  270, loss: 0.03898829594254494\n",
      "epoch:  271, loss: 0.038970958441495895\n",
      "epoch:  272, loss: 0.03895403817296028\n",
      "epoch:  273, loss: 0.038937538862228394\n",
      "epoch:  274, loss: 0.03892144560813904\n",
      "epoch:  275, loss: 0.038905736058950424\n",
      "epoch:  276, loss: 0.03889041021466255\n",
      "epoch:  277, loss: 0.03887544944882393\n",
      "epoch:  278, loss: 0.038860857486724854\n",
      "epoch:  279, loss: 0.03884660825133324\n",
      "epoch:  280, loss: 0.038832712918519974\n",
      "epoch:  281, loss: 0.03881916031241417\n",
      "epoch:  282, loss: 0.038805946707725525\n",
      "epoch:  283, loss: 0.03879304230213165\n",
      "epoch:  284, loss: 0.03878045454621315\n",
      "epoch:  285, loss: 0.03876815736293793\n",
      "epoch:  286, loss: 0.03875615447759628\n",
      "epoch:  287, loss: 0.03874443843960762\n",
      "epoch:  288, loss: 0.03873300179839134\n",
      "epoch:  289, loss: 0.03872183710336685\n",
      "epoch:  290, loss: 0.03871094062924385\n",
      "epoch:  291, loss: 0.038700301200151443\n",
      "epoch:  292, loss: 0.03868992254137993\n",
      "epoch:  293, loss: 0.038679782301187515\n",
      "epoch:  294, loss: 0.038669899106025696\n",
      "epoch:  295, loss: 0.03866024687886238\n",
      "epoch:  296, loss: 0.03865082561969757\n",
      "epoch:  297, loss: 0.03864162787795067\n",
      "epoch:  298, loss: 0.03863264620304108\n",
      "epoch:  299, loss: 0.0386238768696785\n",
      "epoch:  300, loss: 0.03861531242728233\n",
      "epoch:  301, loss: 0.038606952875852585\n",
      "epoch:  302, loss: 0.038598790764808655\n",
      "epoch:  303, loss: 0.03859080746769905\n",
      "epoch:  304, loss: 0.038582999259233475\n",
      "epoch:  305, loss: 0.03857537358999252\n",
      "epoch:  306, loss: 0.0385679267346859\n",
      "epoch:  307, loss: 0.038560643792152405\n",
      "epoch:  308, loss: 0.03855353966355324\n",
      "epoch:  309, loss: 0.03854662552475929\n",
      "epoch:  310, loss: 0.03853986784815788\n",
      "epoch:  311, loss: 0.03853326663374901\n",
      "epoch:  312, loss: 0.03852681815624237\n",
      "epoch:  313, loss: 0.03852052241563797\n",
      "epoch:  314, loss: 0.03851436823606491\n",
      "epoch:  315, loss: 0.038508348166942596\n",
      "epoch:  316, loss: 0.03850247338414192\n",
      "epoch:  317, loss: 0.0384967215359211\n",
      "epoch:  318, loss: 0.038491107523441315\n",
      "epoch:  319, loss: 0.03848561272025108\n",
      "epoch:  320, loss: 0.038480244576931\n",
      "epoch:  321, loss: 0.03847500681877136\n",
      "epoch:  322, loss: 0.03846988081932068\n",
      "epoch:  323, loss: 0.038464874029159546\n",
      "epoch:  324, loss: 0.03845997154712677\n",
      "epoch:  325, loss: 0.03845517337322235\n",
      "epoch:  326, loss: 0.03845047950744629\n",
      "epoch:  327, loss: 0.038445886224508286\n",
      "epoch:  328, loss: 0.038441404700279236\n",
      "epoch:  329, loss: 0.038437023758888245\n",
      "epoch:  330, loss: 0.03843274712562561\n",
      "epoch:  331, loss: 0.03842856362462044\n",
      "epoch:  332, loss: 0.03842448070645332\n",
      "epoch:  333, loss: 0.038420479744672775\n",
      "epoch:  334, loss: 0.03841657191514969\n",
      "epoch:  335, loss: 0.03841274604201317\n",
      "epoch:  336, loss: 0.03840899094939232\n",
      "epoch:  337, loss: 0.03840532526373863\n",
      "epoch:  338, loss: 0.038401734083890915\n",
      "epoch:  339, loss: 0.038398224860429764\n",
      "epoch:  340, loss: 0.03839477524161339\n",
      "epoch:  341, loss: 0.03839139640331268\n",
      "epoch:  342, loss: 0.038388077169656754\n",
      "epoch:  343, loss: 0.0383848212659359\n",
      "epoch:  344, loss: 0.038381632417440414\n",
      "epoch:  345, loss: 0.0383785143494606\n",
      "epoch:  346, loss: 0.038375455886125565\n",
      "epoch:  347, loss: 0.0383724607527256\n",
      "epoch:  348, loss: 0.03836953267455101\n",
      "epoch:  349, loss: 0.038366664201021194\n",
      "epoch:  350, loss: 0.03836384415626526\n",
      "epoch:  351, loss: 0.0383610799908638\n",
      "epoch:  352, loss: 0.03835836797952652\n",
      "epoch:  353, loss: 0.038355711847543716\n",
      "epoch:  354, loss: 0.03835310414433479\n",
      "epoch:  355, loss: 0.03835054486989975\n",
      "epoch:  356, loss: 0.038348015397787094\n",
      "epoch:  357, loss: 0.03834555298089981\n",
      "epoch:  358, loss: 0.03834313154220581\n",
      "epoch:  359, loss: 0.03834075853228569\n",
      "epoch:  360, loss: 0.038338422775268555\n",
      "epoch:  361, loss: 0.0383361279964447\n",
      "epoch:  362, loss: 0.038333866745233536\n",
      "epoch:  363, loss: 0.03833165019750595\n",
      "epoch:  364, loss: 0.03832947835326195\n",
      "epoch:  365, loss: 0.03832731395959854\n",
      "epoch:  366, loss: 0.038325171917676926\n",
      "epoch:  367, loss: 0.03832307457923889\n",
      "epoch:  368, loss: 0.03832100331783295\n",
      "epoch:  369, loss: 0.03831896930932999\n",
      "epoch:  370, loss: 0.03831698000431061\n",
      "epoch:  371, loss: 0.03831503540277481\n",
      "epoch:  372, loss: 0.038313139230012894\n",
      "epoch:  373, loss: 0.03831128031015396\n",
      "epoch:  374, loss: 0.03830944001674652\n",
      "epoch:  375, loss: 0.038307614624500275\n",
      "epoch:  376, loss: 0.038305822759866714\n",
      "epoch:  377, loss: 0.03830406814813614\n",
      "epoch:  378, loss: 0.038302332162857056\n",
      "epoch:  379, loss: 0.03830062970519066\n",
      "epoch:  380, loss: 0.03829897567629814\n",
      "epoch:  381, loss: 0.03829734027385712\n",
      "epoch:  382, loss: 0.03829573094844818\n",
      "epoch:  383, loss: 0.03829415142536163\n",
      "epoch:  384, loss: 0.03829260170459747\n",
      "epoch:  385, loss: 0.0382910817861557\n",
      "epoch:  386, loss: 0.038289595395326614\n",
      "epoch:  387, loss: 0.03828814625740051\n",
      "epoch:  388, loss: 0.03828670457005501\n",
      "epoch:  389, loss: 0.03828529268503189\n",
      "epoch:  390, loss: 0.03828389570116997\n",
      "epoch:  391, loss: 0.038282524794340134\n",
      "epoch:  392, loss: 0.038281168788671494\n",
      "epoch:  393, loss: 0.038279835134744644\n",
      "epoch:  394, loss: 0.03827850893139839\n",
      "epoch:  395, loss: 0.03827720507979393\n",
      "epoch:  396, loss: 0.03827591985464096\n",
      "epoch:  397, loss: 0.038274649530649185\n",
      "epoch:  398, loss: 0.03827338293194771\n",
      "epoch:  399, loss: 0.03827214241027832\n",
      "epoch:  400, loss: 0.03827091306447983\n",
      "epoch:  401, loss: 0.03826970234513283\n",
      "epoch:  402, loss: 0.03826850280165672\n",
      "epoch:  403, loss: 0.03826731815934181\n",
      "epoch:  404, loss: 0.0382661409676075\n",
      "epoch:  405, loss: 0.03826497867703438\n",
      "epoch:  406, loss: 0.03826381266117096\n",
      "epoch:  407, loss: 0.03826265037059784\n",
      "epoch:  408, loss: 0.03826150298118591\n",
      "epoch:  409, loss: 0.03826037049293518\n",
      "epoch:  410, loss: 0.038259245455265045\n",
      "epoch:  411, loss: 0.0382581390440464\n",
      "epoch:  412, loss: 0.03825705498456955\n",
      "epoch:  413, loss: 0.03825598210096359\n",
      "epoch:  414, loss: 0.03825492039322853\n",
      "epoch:  415, loss: 0.03825382515788078\n",
      "epoch:  416, loss: 0.03825272247195244\n",
      "epoch:  417, loss: 0.03825162351131439\n",
      "epoch:  418, loss: 0.03825054317712784\n",
      "epoch:  419, loss: 0.038249481469392776\n",
      "epoch:  420, loss: 0.03824843838810921\n",
      "epoch:  421, loss: 0.038247425109148026\n",
      "epoch:  422, loss: 0.03824642673134804\n",
      "epoch:  423, loss: 0.03824542835354805\n",
      "epoch:  424, loss: 0.038244396448135376\n",
      "epoch:  425, loss: 0.038243368268013\n",
      "epoch:  426, loss: 0.03824232518672943\n",
      "epoch:  427, loss: 0.03824128210544586\n",
      "epoch:  428, loss: 0.03824023902416229\n",
      "epoch:  429, loss: 0.03823920711874962\n",
      "epoch:  430, loss: 0.03823819011449814\n",
      "epoch:  431, loss: 0.038237184286117554\n",
      "epoch:  432, loss: 0.03823617100715637\n",
      "epoch:  433, loss: 0.03823515772819519\n",
      "epoch:  434, loss: 0.0382341593503952\n",
      "epoch:  435, loss: 0.03823316469788551\n",
      "epoch:  436, loss: 0.03823216259479523\n",
      "epoch:  437, loss: 0.03823116794228554\n",
      "epoch:  438, loss: 0.03823015093803406\n",
      "epoch:  439, loss: 0.038229137659072876\n",
      "epoch:  440, loss: 0.038228124380111694\n",
      "epoch:  441, loss: 0.03822712227702141\n",
      "epoch:  442, loss: 0.03822613134980202\n",
      "epoch:  443, loss: 0.03822516277432442\n",
      "epoch:  444, loss: 0.03822420910000801\n",
      "epoch:  445, loss: 0.0382232628762722\n",
      "epoch:  446, loss: 0.03822227939963341\n",
      "epoch:  447, loss: 0.038221314549446106\n",
      "epoch:  448, loss: 0.038220345973968506\n",
      "epoch:  449, loss: 0.038219381123781204\n",
      "epoch:  450, loss: 0.03821840509772301\n",
      "epoch:  451, loss: 0.038217440247535706\n",
      "epoch:  452, loss: 0.03821646794676781\n",
      "epoch:  453, loss: 0.03821549564599991\n",
      "epoch:  454, loss: 0.03821453079581261\n",
      "epoch:  455, loss: 0.03821356222033501\n",
      "epoch:  456, loss: 0.038212597370147705\n",
      "epoch:  457, loss: 0.0382116362452507\n",
      "epoch:  458, loss: 0.0382106751203537\n",
      "epoch:  459, loss: 0.03820972144603729\n",
      "epoch:  460, loss: 0.038208767771720886\n",
      "epoch:  461, loss: 0.03820781782269478\n",
      "epoch:  462, loss: 0.03820686414837837\n",
      "epoch:  463, loss: 0.038205910474061966\n",
      "epoch:  464, loss: 0.03820495307445526\n",
      "epoch:  465, loss: 0.038203999400138855\n",
      "epoch:  466, loss: 0.03820304572582245\n",
      "epoch:  467, loss: 0.038202084600925446\n",
      "epoch:  468, loss: 0.03820112720131874\n",
      "epoch:  469, loss: 0.038200173527002335\n",
      "epoch:  470, loss: 0.03819921612739563\n",
      "epoch:  471, loss: 0.03819827735424042\n",
      "epoch:  472, loss: 0.0381973497569561\n",
      "epoch:  473, loss: 0.03819642961025238\n",
      "epoch:  474, loss: 0.03819551691412926\n",
      "epoch:  475, loss: 0.03819459676742554\n",
      "epoch:  476, loss: 0.03819366544485092\n",
      "epoch:  477, loss: 0.0381927415728569\n",
      "epoch:  478, loss: 0.03819181025028229\n",
      "epoch:  479, loss: 0.03819088265299797\n",
      "epoch:  480, loss: 0.038189955055713654\n",
      "epoch:  481, loss: 0.038189031183719635\n",
      "epoch:  482, loss: 0.038188111037015915\n",
      "epoch:  483, loss: 0.038187190890312195\n",
      "epoch:  484, loss: 0.038186267018318176\n",
      "epoch:  485, loss: 0.03818533569574356\n",
      "epoch:  486, loss: 0.03818440064787865\n",
      "epoch:  487, loss: 0.03818349167704582\n",
      "epoch:  488, loss: 0.038182586431503296\n",
      "epoch:  489, loss: 0.03818165883421898\n",
      "epoch:  490, loss: 0.03818073496222496\n",
      "epoch:  491, loss: 0.03817983716726303\n",
      "epoch:  492, loss: 0.03817891329526901\n",
      "epoch:  493, loss: 0.03817800432443619\n",
      "epoch:  494, loss: 0.03817711025476456\n",
      "epoch:  495, loss: 0.03817621245980263\n",
      "epoch:  496, loss: 0.038175322115421295\n",
      "epoch:  497, loss: 0.03817443922162056\n",
      "epoch:  498, loss: 0.038173556327819824\n",
      "epoch:  499, loss: 0.038172658532857895\n",
      "epoch:  500, loss: 0.03817174583673477\n",
      "epoch:  501, loss: 0.03817082941532135\n",
      "epoch:  502, loss: 0.038169898092746735\n",
      "epoch:  503, loss: 0.038168955594301224\n",
      "epoch:  504, loss: 0.038168009370565414\n",
      "epoch:  505, loss: 0.03816705569624901\n",
      "epoch:  506, loss: 0.0381661131978035\n",
      "epoch:  507, loss: 0.03816519305109978\n",
      "epoch:  508, loss: 0.03816427290439606\n",
      "epoch:  509, loss: 0.03816335275769234\n",
      "epoch:  510, loss: 0.038162440061569214\n",
      "epoch:  511, loss: 0.03816152736544609\n",
      "epoch:  512, loss: 0.03816059231758118\n",
      "epoch:  513, loss: 0.038159653544425964\n",
      "epoch:  514, loss: 0.03815870359539986\n",
      "epoch:  515, loss: 0.03815774247050285\n",
      "epoch:  516, loss: 0.03815677762031555\n",
      "epoch:  517, loss: 0.03815584257245064\n",
      "epoch:  518, loss: 0.038154907524585724\n",
      "epoch:  519, loss: 0.038153961300849915\n",
      "epoch:  520, loss: 0.0381530225276947\n",
      "epoch:  521, loss: 0.03815208375453949\n",
      "epoch:  522, loss: 0.038151130080223083\n",
      "epoch:  523, loss: 0.03815016895532608\n",
      "epoch:  524, loss: 0.03814921900629997\n",
      "epoch:  525, loss: 0.03814828768372536\n",
      "epoch:  526, loss: 0.038147345185279846\n",
      "epoch:  527, loss: 0.03814639896154404\n",
      "epoch:  528, loss: 0.038145456463098526\n",
      "epoch:  529, loss: 0.038144517689943314\n",
      "epoch:  530, loss: 0.038143597543239594\n",
      "epoch:  531, loss: 0.03814268484711647\n",
      "epoch:  532, loss: 0.038141779601573944\n",
      "epoch:  533, loss: 0.03814087063074112\n",
      "epoch:  534, loss: 0.0381399542093277\n",
      "epoch:  535, loss: 0.03813904896378517\n",
      "epoch:  536, loss: 0.03813815489411354\n",
      "epoch:  537, loss: 0.03813726454973221\n",
      "epoch:  538, loss: 0.038136377930641174\n",
      "epoch:  539, loss: 0.03813549503684044\n",
      "epoch:  540, loss: 0.03813461586833\n",
      "epoch:  541, loss: 0.03813372924923897\n",
      "epoch:  542, loss: 0.03813286870718002\n",
      "epoch:  543, loss: 0.03813200443983078\n",
      "epoch:  544, loss: 0.038131147623062134\n",
      "epoch:  545, loss: 0.03813028335571289\n",
      "epoch:  546, loss: 0.03812941536307335\n",
      "epoch:  547, loss: 0.038128554821014404\n",
      "epoch:  548, loss: 0.03812767565250397\n",
      "epoch:  549, loss: 0.03812680393457413\n",
      "epoch:  550, loss: 0.038125939667224884\n",
      "epoch:  551, loss: 0.03812507912516594\n",
      "epoch:  552, loss: 0.038124218583106995\n",
      "epoch:  553, loss: 0.03812335804104805\n",
      "epoch:  554, loss: 0.038122497498989105\n",
      "epoch:  555, loss: 0.03812164068222046\n",
      "epoch:  556, loss: 0.03812078386545181\n",
      "epoch:  557, loss: 0.03811991959810257\n",
      "epoch:  558, loss: 0.03811904788017273\n",
      "epoch:  559, loss: 0.038118164986371994\n",
      "epoch:  560, loss: 0.03811728581786156\n",
      "epoch:  561, loss: 0.038116421550512314\n",
      "epoch:  562, loss: 0.03811555355787277\n",
      "epoch:  563, loss: 0.03811468929052353\n",
      "epoch:  564, loss: 0.03811382129788399\n",
      "epoch:  565, loss: 0.03811294212937355\n",
      "epoch:  566, loss: 0.03811206296086311\n",
      "epoch:  567, loss: 0.038111183792352676\n",
      "epoch:  568, loss: 0.03811030462384224\n",
      "epoch:  569, loss: 0.038109418004751205\n",
      "epoch:  570, loss: 0.03810853883624077\n",
      "epoch:  571, loss: 0.038107652217149734\n",
      "epoch:  572, loss: 0.038106776773929596\n",
      "epoch:  573, loss: 0.03810590133070946\n",
      "epoch:  574, loss: 0.03810502961277962\n",
      "epoch:  575, loss: 0.038104161620140076\n",
      "epoch:  576, loss: 0.03810330480337143\n",
      "epoch:  577, loss: 0.03810244798660278\n",
      "epoch:  578, loss: 0.03810158371925354\n",
      "epoch:  579, loss: 0.038100715726614\n",
      "epoch:  580, loss: 0.038099851459264755\n",
      "epoch:  581, loss: 0.03809899464249611\n",
      "epoch:  582, loss: 0.03809814155101776\n",
      "epoch:  583, loss: 0.038097284734249115\n",
      "epoch:  584, loss: 0.03809642791748047\n",
      "epoch:  585, loss: 0.03809557482600212\n",
      "epoch:  586, loss: 0.038094714283943176\n",
      "epoch:  587, loss: 0.03809385374188423\n",
      "epoch:  588, loss: 0.03809299319982529\n",
      "epoch:  589, loss: 0.03809214383363724\n",
      "epoch:  590, loss: 0.038091324269771576\n",
      "epoch:  591, loss: 0.03809049725532532\n",
      "epoch:  592, loss: 0.038089677691459656\n",
      "epoch:  593, loss: 0.0380888506770134\n",
      "epoch:  594, loss: 0.03808802738785744\n",
      "epoch:  595, loss: 0.038087211549282074\n",
      "epoch:  596, loss: 0.03808639943599701\n",
      "epoch:  597, loss: 0.03808559104800224\n",
      "epoch:  598, loss: 0.03808479756116867\n",
      "epoch:  599, loss: 0.0380840003490448\n",
      "epoch:  600, loss: 0.03808319941163063\n",
      "epoch:  601, loss: 0.03808239847421646\n",
      "epoch:  602, loss: 0.03808159753680229\n",
      "epoch:  603, loss: 0.03808080404996872\n",
      "epoch:  604, loss: 0.038080014288425446\n",
      "epoch:  605, loss: 0.03807922452688217\n",
      "epoch:  606, loss: 0.0380784347653389\n",
      "epoch:  607, loss: 0.03807765617966652\n",
      "epoch:  608, loss: 0.03807689994573593\n",
      "epoch:  609, loss: 0.03807614743709564\n",
      "epoch:  610, loss: 0.03807539865374565\n",
      "epoch:  611, loss: 0.03807465359568596\n",
      "epoch:  612, loss: 0.038073912262916565\n",
      "epoch:  613, loss: 0.038073163479566574\n",
      "epoch:  614, loss: 0.03807242214679718\n",
      "epoch:  615, loss: 0.03807167708873749\n",
      "epoch:  616, loss: 0.03807094320654869\n",
      "epoch:  617, loss: 0.038070205599069595\n",
      "epoch:  618, loss: 0.0380694754421711\n",
      "epoch:  619, loss: 0.038068752735853195\n",
      "epoch:  620, loss: 0.03806803748011589\n",
      "epoch:  621, loss: 0.03806732967495918\n",
      "epoch:  622, loss: 0.038066621869802475\n",
      "epoch:  623, loss: 0.03806591406464577\n",
      "epoch:  624, loss: 0.03806522861123085\n",
      "epoch:  625, loss: 0.03806452453136444\n",
      "epoch:  626, loss: 0.03806382045149803\n",
      "epoch:  627, loss: 0.03806311637163162\n",
      "epoch:  628, loss: 0.03806241229176521\n",
      "epoch:  629, loss: 0.0380617193877697\n",
      "epoch:  630, loss: 0.038061026483774185\n",
      "epoch:  631, loss: 0.03806033730506897\n",
      "epoch:  632, loss: 0.038059648126363754\n",
      "epoch:  633, loss: 0.038058966398239136\n",
      "epoch:  634, loss: 0.03805828467011452\n",
      "epoch:  635, loss: 0.0380575954914093\n",
      "epoch:  636, loss: 0.03805691376328468\n",
      "epoch:  637, loss: 0.03805621713399887\n",
      "epoch:  638, loss: 0.03805554285645485\n",
      "epoch:  639, loss: 0.038054853677749634\n",
      "epoch:  640, loss: 0.03805417940020561\n",
      "epoch:  641, loss: 0.03805350512266159\n",
      "epoch:  642, loss: 0.03805283457040787\n",
      "epoch:  643, loss: 0.03805215656757355\n",
      "epoch:  644, loss: 0.03805149719119072\n",
      "epoch:  645, loss: 0.038050830364227295\n",
      "epoch:  646, loss: 0.03805017098784447\n",
      "epoch:  647, loss: 0.03804951533675194\n",
      "epoch:  648, loss: 0.03804885968565941\n",
      "epoch:  649, loss: 0.038048211485147476\n",
      "epoch:  650, loss: 0.038047559559345245\n",
      "epoch:  651, loss: 0.03804690018296242\n",
      "epoch:  652, loss: 0.038046255707740784\n",
      "epoch:  653, loss: 0.03804561495780945\n",
      "epoch:  654, loss: 0.03804497420787811\n",
      "epoch:  655, loss: 0.038044337183237076\n",
      "epoch:  656, loss: 0.038043707609176636\n",
      "epoch:  657, loss: 0.03804309666156769\n",
      "epoch:  658, loss: 0.038042496889829636\n",
      "epoch:  659, loss: 0.03804190084338188\n",
      "epoch:  660, loss: 0.03804130479693413\n",
      "epoch:  661, loss: 0.038040705025196075\n",
      "epoch:  662, loss: 0.03804010525345802\n",
      "epoch:  663, loss: 0.03803950920701027\n",
      "epoch:  664, loss: 0.038038913160562515\n",
      "epoch:  665, loss: 0.03803831711411476\n",
      "epoch:  666, loss: 0.038037728518247604\n",
      "epoch:  667, loss: 0.03803713247179985\n",
      "epoch:  668, loss: 0.038036543875932693\n",
      "epoch:  669, loss: 0.03803595155477524\n",
      "epoch:  670, loss: 0.038035351783037186\n",
      "epoch:  671, loss: 0.038034748286008835\n",
      "epoch:  672, loss: 0.03803415596485138\n",
      "epoch:  673, loss: 0.038033563643693924\n",
      "epoch:  674, loss: 0.03803296387195587\n",
      "epoch:  675, loss: 0.038032375276088715\n",
      "epoch:  676, loss: 0.03803178668022156\n",
      "epoch:  677, loss: 0.0380311943590641\n",
      "epoch:  678, loss: 0.03803060203790665\n",
      "epoch:  679, loss: 0.03803000599145889\n",
      "epoch:  680, loss: 0.03802940994501114\n",
      "epoch:  681, loss: 0.038028813898563385\n",
      "epoch:  682, loss: 0.03802822157740593\n",
      "epoch:  683, loss: 0.038027625530958176\n",
      "epoch:  684, loss: 0.03802702948451042\n",
      "epoch:  685, loss: 0.038026440888643265\n",
      "epoch:  686, loss: 0.03802585229277611\n",
      "epoch:  687, loss: 0.03802526742219925\n",
      "epoch:  688, loss: 0.03802467882633209\n",
      "epoch:  689, loss: 0.038024090230464935\n",
      "epoch:  690, loss: 0.038023512810468674\n",
      "epoch:  691, loss: 0.038022927939891815\n",
      "epoch:  692, loss: 0.03802235424518585\n",
      "epoch:  693, loss: 0.03802177309989929\n",
      "epoch:  694, loss: 0.03802119567990303\n",
      "epoch:  695, loss: 0.03802061453461647\n",
      "epoch:  696, loss: 0.03802003338932991\n",
      "epoch:  697, loss: 0.03801945969462395\n",
      "epoch:  698, loss: 0.03801887854933739\n",
      "epoch:  699, loss: 0.03801829740405083\n",
      "epoch:  700, loss: 0.038017719984054565\n",
      "epoch:  701, loss: 0.038017142564058304\n",
      "epoch:  702, loss: 0.03801657259464264\n",
      "epoch:  703, loss: 0.03801601007580757\n",
      "epoch:  704, loss: 0.0380154624581337\n",
      "epoch:  705, loss: 0.03801490366458893\n",
      "epoch:  706, loss: 0.038014356046915054\n",
      "epoch:  707, loss: 0.038013800978660583\n",
      "epoch:  708, loss: 0.03801325336098671\n",
      "epoch:  709, loss: 0.038012705743312836\n",
      "epoch:  710, loss: 0.03801215812563896\n",
      "epoch:  711, loss: 0.03801162168383598\n",
      "epoch:  712, loss: 0.03801107779145241\n",
      "epoch:  713, loss: 0.03801053762435913\n",
      "epoch:  714, loss: 0.038009997457265854\n",
      "epoch:  715, loss: 0.038009461015462875\n",
      "epoch:  716, loss: 0.0380089245736599\n",
      "epoch:  717, loss: 0.03800838440656662\n",
      "epoch:  718, loss: 0.03800784796476364\n",
      "epoch:  719, loss: 0.03800731152296066\n",
      "epoch:  720, loss: 0.03800678253173828\n",
      "epoch:  721, loss: 0.0380062498152256\n",
      "epoch:  722, loss: 0.038005728274583817\n",
      "epoch:  723, loss: 0.03800520673394203\n",
      "epoch:  724, loss: 0.03800468519330025\n",
      "epoch:  725, loss: 0.03800417110323906\n",
      "epoch:  726, loss: 0.03800365328788757\n",
      "epoch:  727, loss: 0.03800313547253609\n",
      "epoch:  728, loss: 0.0380026176571846\n",
      "epoch:  729, loss: 0.038002099841833115\n",
      "epoch:  730, loss: 0.03800157830119133\n",
      "epoch:  731, loss: 0.038001056760549545\n",
      "epoch:  732, loss: 0.03800053521990776\n",
      "epoch:  733, loss: 0.038000013679265976\n",
      "epoch:  734, loss: 0.03799949958920479\n",
      "epoch:  735, loss: 0.0379989817738533\n",
      "epoch:  736, loss: 0.037998467683792114\n",
      "epoch:  737, loss: 0.037997957319021225\n",
      "epoch:  738, loss: 0.03799743950366974\n",
      "epoch:  739, loss: 0.03799692913889885\n",
      "epoch:  740, loss: 0.03799641877412796\n",
      "epoch:  741, loss: 0.03799591585993767\n",
      "epoch:  742, loss: 0.03799540549516678\n",
      "epoch:  743, loss: 0.03799489885568619\n",
      "epoch:  744, loss: 0.037994395941495895\n",
      "epoch:  745, loss: 0.037993889302015305\n",
      "epoch:  746, loss: 0.037993382662534714\n",
      "epoch:  747, loss: 0.03799287974834442\n",
      "epoch:  748, loss: 0.03799236938357353\n",
      "epoch:  749, loss: 0.03799186274409294\n",
      "epoch:  750, loss: 0.03799135610461235\n",
      "epoch:  751, loss: 0.03799085319042206\n",
      "epoch:  752, loss: 0.03799034655094147\n",
      "epoch:  753, loss: 0.037989843636751175\n",
      "epoch:  754, loss: 0.03798934072256088\n",
      "epoch:  755, loss: 0.03798883780837059\n",
      "epoch:  756, loss: 0.0379883348941803\n",
      "epoch:  757, loss: 0.037987831979990005\n",
      "epoch:  758, loss: 0.03798732906579971\n",
      "epoch:  759, loss: 0.03798682615160942\n",
      "epoch:  760, loss: 0.03798632323741913\n",
      "epoch:  761, loss: 0.037985820323228836\n",
      "epoch:  762, loss: 0.037985313683748245\n",
      "epoch:  763, loss: 0.03798481076955795\n",
      "epoch:  764, loss: 0.03798430785536766\n",
      "epoch:  765, loss: 0.03798380121588707\n",
      "epoch:  766, loss: 0.03798329830169678\n",
      "epoch:  767, loss: 0.037982791662216187\n",
      "epoch:  768, loss: 0.037982288748025894\n",
      "epoch:  769, loss: 0.0379817858338356\n",
      "epoch:  770, loss: 0.03798128291964531\n",
      "epoch:  771, loss: 0.03798078000545502\n",
      "epoch:  772, loss: 0.037980273365974426\n",
      "epoch:  773, loss: 0.037979766726493835\n",
      "epoch:  774, loss: 0.03797926381230354\n",
      "epoch:  775, loss: 0.03797875717282295\n",
      "epoch:  776, loss: 0.03797825425863266\n",
      "epoch:  777, loss: 0.03797775134444237\n",
      "epoch:  778, loss: 0.03797724470496178\n",
      "epoch:  779, loss: 0.03797674551606178\n",
      "epoch:  780, loss: 0.03797624260187149\n",
      "epoch:  781, loss: 0.0379757396876812\n",
      "epoch:  782, loss: 0.037975236773490906\n",
      "epoch:  783, loss: 0.037974726408720016\n",
      "epoch:  784, loss: 0.037974223494529724\n",
      "epoch:  785, loss: 0.03797372430562973\n",
      "epoch:  786, loss: 0.03797322139143944\n",
      "epoch:  787, loss: 0.037972718477249146\n",
      "epoch:  788, loss: 0.037972211837768555\n",
      "epoch:  789, loss: 0.03797170892357826\n",
      "epoch:  790, loss: 0.03797120973467827\n",
      "epoch:  791, loss: 0.037970710545778275\n",
      "epoch:  792, loss: 0.03797021135687828\n",
      "epoch:  793, loss: 0.037969715893268585\n",
      "epoch:  794, loss: 0.03796922042965889\n",
      "epoch:  795, loss: 0.03796872869133949\n",
      "epoch:  796, loss: 0.0379682332277298\n",
      "epoch:  797, loss: 0.0379677452147007\n",
      "epoch:  798, loss: 0.037967249751091\n",
      "epoch:  799, loss: 0.037966758012771606\n",
      "epoch:  800, loss: 0.03796626627445221\n",
      "epoch:  801, loss: 0.037965770810842514\n",
      "epoch:  802, loss: 0.037965282797813416\n",
      "epoch:  803, loss: 0.03796478733420372\n",
      "epoch:  804, loss: 0.03796429559588432\n",
      "epoch:  805, loss: 0.037963803857564926\n",
      "epoch:  806, loss: 0.03796330839395523\n",
      "epoch:  807, loss: 0.037962816655635834\n",
      "epoch:  808, loss: 0.03796231746673584\n",
      "epoch:  809, loss: 0.037961818277835846\n",
      "epoch:  810, loss: 0.03796132653951645\n",
      "epoch:  811, loss: 0.037960831075906754\n",
      "epoch:  812, loss: 0.03796033561229706\n",
      "epoch:  813, loss: 0.03795984387397766\n",
      "epoch:  814, loss: 0.037959352135658264\n",
      "epoch:  815, loss: 0.037958864122629166\n",
      "epoch:  816, loss: 0.03795837238430977\n",
      "epoch:  817, loss: 0.03795788437128067\n",
      "epoch:  818, loss: 0.03795739263296127\n",
      "epoch:  819, loss: 0.037956904619932175\n",
      "epoch:  820, loss: 0.03795641288161278\n",
      "epoch:  821, loss: 0.03795592486858368\n",
      "epoch:  822, loss: 0.03795543685555458\n",
      "epoch:  823, loss: 0.037954945117235184\n",
      "epoch:  824, loss: 0.03795445337891579\n",
      "epoch:  825, loss: 0.03795396536588669\n",
      "epoch:  826, loss: 0.03795347362756729\n",
      "epoch:  827, loss: 0.037952981889247894\n",
      "epoch:  828, loss: 0.0379524901509285\n",
      "epoch:  829, loss: 0.0379520021378994\n",
      "epoch:  830, loss: 0.03795151039958\n",
      "epoch:  831, loss: 0.0379510223865509\n",
      "epoch:  832, loss: 0.03795052692294121\n",
      "epoch:  833, loss: 0.03795003518462181\n",
      "epoch:  834, loss: 0.037949539721012115\n",
      "epoch:  835, loss: 0.03794905170798302\n",
      "epoch:  836, loss: 0.03794855996966362\n",
      "epoch:  837, loss: 0.03794806823134422\n",
      "epoch:  838, loss: 0.037947580218315125\n",
      "epoch:  839, loss: 0.03794708847999573\n",
      "epoch:  840, loss: 0.03794659674167633\n",
      "epoch:  841, loss: 0.037946101278066635\n",
      "epoch:  842, loss: 0.03794561326503754\n",
      "epoch:  843, loss: 0.03794511780142784\n",
      "epoch:  844, loss: 0.037944626063108444\n",
      "epoch:  845, loss: 0.037944141775369644\n",
      "epoch:  846, loss: 0.03794364631175995\n",
      "epoch:  847, loss: 0.03794315457344055\n",
      "epoch:  848, loss: 0.037942662835121155\n",
      "epoch:  849, loss: 0.037942174822092056\n",
      "epoch:  850, loss: 0.03794167935848236\n",
      "epoch:  851, loss: 0.037941187620162964\n",
      "epoch:  852, loss: 0.037940699607133865\n",
      "epoch:  853, loss: 0.03794020414352417\n",
      "epoch:  854, loss: 0.03793971240520477\n",
      "epoch:  855, loss: 0.037939220666885376\n",
      "epoch:  856, loss: 0.03793872892856598\n",
      "epoch:  857, loss: 0.03793824091553688\n",
      "epoch:  858, loss: 0.037937749177217484\n",
      "epoch:  859, loss: 0.037937261164188385\n",
      "epoch:  860, loss: 0.03793676942586899\n",
      "epoch:  861, loss: 0.03793628141283989\n",
      "epoch:  862, loss: 0.03793578967452049\n",
      "epoch:  863, loss: 0.037935297936201096\n",
      "epoch:  864, loss: 0.0379348061978817\n",
      "epoch:  865, loss: 0.0379343144595623\n",
      "epoch:  866, loss: 0.0379338264465332\n",
      "epoch:  867, loss: 0.037933334708213806\n",
      "epoch:  868, loss: 0.03793284669518471\n",
      "epoch:  869, loss: 0.03793235495686531\n",
      "epoch:  870, loss: 0.03793187066912651\n",
      "epoch:  871, loss: 0.03793139383196831\n",
      "epoch:  872, loss: 0.03793090954422951\n",
      "epoch:  873, loss: 0.037930428981781006\n",
      "epoch:  874, loss: 0.037929948419332504\n",
      "epoch:  875, loss: 0.0379294715821743\n",
      "epoch:  876, loss: 0.0379289872944355\n",
      "epoch:  877, loss: 0.037928506731987\n",
      "epoch:  878, loss: 0.0379280224442482\n",
      "epoch:  879, loss: 0.0379275381565094\n",
      "epoch:  880, loss: 0.0379270575940609\n",
      "epoch:  881, loss: 0.0379265733063221\n",
      "epoch:  882, loss: 0.037926096469163895\n",
      "epoch:  883, loss: 0.03792561590671539\n",
      "epoch:  884, loss: 0.03792513534426689\n",
      "epoch:  885, loss: 0.03792465850710869\n",
      "epoch:  886, loss: 0.03792417794466019\n",
      "epoch:  887, loss: 0.037923697382211685\n",
      "epoch:  888, loss: 0.03792322054505348\n",
      "epoch:  889, loss: 0.037922751158475876\n",
      "epoch:  890, loss: 0.03792227432131767\n",
      "epoch:  891, loss: 0.03792179748415947\n",
      "epoch:  892, loss: 0.037921320647001266\n",
      "epoch:  893, loss: 0.03792084753513336\n",
      "epoch:  894, loss: 0.03792037069797516\n",
      "epoch:  895, loss: 0.037919893860816956\n",
      "epoch:  896, loss: 0.03791942074894905\n",
      "epoch:  897, loss: 0.03791894018650055\n",
      "epoch:  898, loss: 0.037918463349342346\n",
      "epoch:  899, loss: 0.03791798651218414\n",
      "epoch:  900, loss: 0.03791750967502594\n",
      "epoch:  901, loss: 0.037917036563158035\n",
      "epoch:  902, loss: 0.03791655972599983\n",
      "epoch:  903, loss: 0.03791608661413193\n",
      "epoch:  904, loss: 0.03791561350226402\n",
      "epoch:  905, loss: 0.03791514039039612\n",
      "epoch:  906, loss: 0.037914663553237915\n",
      "epoch:  907, loss: 0.03791418671607971\n",
      "epoch:  908, loss: 0.03791371360421181\n",
      "epoch:  909, loss: 0.037913236767053604\n",
      "epoch:  910, loss: 0.0379127599298954\n",
      "epoch:  911, loss: 0.037912290543317795\n",
      "epoch:  912, loss: 0.03791181370615959\n",
      "epoch:  913, loss: 0.03791134059429169\n",
      "epoch:  914, loss: 0.037910860031843185\n",
      "epoch:  915, loss: 0.03791038691997528\n",
      "epoch:  916, loss: 0.03790991008281708\n",
      "epoch:  917, loss: 0.037909433245658875\n",
      "epoch:  918, loss: 0.03790895640850067\n",
      "epoch:  919, loss: 0.03790847957134247\n",
      "epoch:  920, loss: 0.037908006459474564\n",
      "epoch:  921, loss: 0.03790752962231636\n",
      "epoch:  922, loss: 0.03790705278515816\n",
      "epoch:  923, loss: 0.03790657967329025\n",
      "epoch:  924, loss: 0.03790610656142235\n",
      "epoch:  925, loss: 0.03790564090013504\n",
      "epoch:  926, loss: 0.037905171513557434\n",
      "epoch:  927, loss: 0.03790470212697983\n",
      "epoch:  928, loss: 0.03790423274040222\n",
      "epoch:  929, loss: 0.037903763353824615\n",
      "epoch:  930, loss: 0.03790329769253731\n",
      "epoch:  931, loss: 0.0379028357565403\n",
      "epoch:  932, loss: 0.03790237382054329\n",
      "epoch:  933, loss: 0.03790190815925598\n",
      "epoch:  934, loss: 0.03790144994854927\n",
      "epoch:  935, loss: 0.037900980561971664\n",
      "epoch:  936, loss: 0.037900518625974655\n",
      "epoch:  937, loss: 0.037900060415267944\n",
      "epoch:  938, loss: 0.03789959102869034\n",
      "epoch:  939, loss: 0.03789913281798363\n",
      "epoch:  940, loss: 0.03789866715669632\n",
      "epoch:  941, loss: 0.03789820149540901\n",
      "epoch:  942, loss: 0.037897739559412\n",
      "epoch:  943, loss: 0.03789728134870529\n",
      "epoch:  944, loss: 0.03789682313799858\n",
      "epoch:  945, loss: 0.03789636492729187\n",
      "epoch:  946, loss: 0.03789590671658516\n",
      "epoch:  947, loss: 0.03789545223116875\n",
      "epoch:  948, loss: 0.037894994020462036\n",
      "epoch:  949, loss: 0.03789453208446503\n",
      "epoch:  950, loss: 0.037894077599048615\n",
      "epoch:  951, loss: 0.0378936231136322\n",
      "epoch:  952, loss: 0.03789316490292549\n",
      "epoch:  953, loss: 0.03789271041750908\n",
      "epoch:  954, loss: 0.03789224848151207\n",
      "epoch:  955, loss: 0.03789179027080536\n",
      "epoch:  956, loss: 0.03789133206009865\n",
      "epoch:  957, loss: 0.03789087384939194\n",
      "epoch:  958, loss: 0.037890415638685226\n",
      "epoch:  959, loss: 0.03788995370268822\n",
      "epoch:  960, loss: 0.037889495491981506\n",
      "epoch:  961, loss: 0.0378890335559845\n",
      "epoch:  962, loss: 0.037888575345277786\n",
      "epoch:  963, loss: 0.03788811340928078\n",
      "epoch:  964, loss: 0.03788765147328377\n",
      "epoch:  965, loss: 0.03788718581199646\n",
      "epoch:  966, loss: 0.03788672387599945\n",
      "epoch:  967, loss: 0.03788625821471214\n",
      "epoch:  968, loss: 0.037885796278715134\n",
      "epoch:  969, loss: 0.037885334342718124\n",
      "epoch:  970, loss: 0.03788486868143082\n",
      "epoch:  971, loss: 0.03788440674543381\n",
      "epoch:  972, loss: 0.0378839410841465\n",
      "epoch:  973, loss: 0.03788347914814949\n",
      "epoch:  974, loss: 0.03788301348686218\n",
      "epoch:  975, loss: 0.03788255155086517\n",
      "epoch:  976, loss: 0.037882085889577866\n",
      "epoch:  977, loss: 0.03788161650300026\n",
      "epoch:  978, loss: 0.03788115829229355\n",
      "epoch:  979, loss: 0.03788068890571594\n",
      "epoch:  980, loss: 0.03788022696971893\n",
      "epoch:  981, loss: 0.037879761308431625\n",
      "epoch:  982, loss: 0.037879299372434616\n",
      "epoch:  983, loss: 0.03787883371114731\n",
      "epoch:  984, loss: 0.0378783717751503\n",
      "epoch:  985, loss: 0.03787790983915329\n",
      "epoch:  986, loss: 0.03787745535373688\n",
      "epoch:  987, loss: 0.037877000868320465\n",
      "epoch:  988, loss: 0.037876542657613754\n",
      "epoch:  989, loss: 0.03787608444690704\n",
      "epoch:  990, loss: 0.03787563741207123\n",
      "epoch:  991, loss: 0.037875182926654816\n",
      "epoch:  992, loss: 0.037874735891819\n",
      "epoch:  993, loss: 0.03787428140640259\n",
      "epoch:  994, loss: 0.03787383437156677\n",
      "epoch:  995, loss: 0.03787338361144066\n",
      "epoch:  996, loss: 0.03787293657660484\n",
      "epoch:  997, loss: 0.03787248581647873\n",
      "epoch:  998, loss: 0.037872038781642914\n",
      "epoch:  999, loss: 0.0378715880215168\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size=X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "opt = torch_numopt.GradientDescentLS(model=model, lr=0.005, line_search_method=\"const\")\n",
    "\n",
    "all_loss = {}\n",
    "for epoch in range(1000):\n",
    "    print(\"epoch: \", epoch, end=\"\")\n",
    "    all_loss[epoch + 1] = 0\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y, loss_fn)\n",
    "\n",
    "        all_loss[epoch + 1] += loss\n",
    "    all_loss[epoch + 1] /= len(data_loader)\n",
    "    print(\", loss: {}\".format(all_loss[epoch + 1].detach().numpy().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics: R2 = -12325.920207410292\n",
      "Test metrics:  R2 = -10845.116244829333\n"
     ]
    }
   ],
   "source": [
    "pred_train = model.forward(X_train).detach()\n",
    "pred_test = model.forward(X_test).detach()\n",
    "print(f\"Train metrics: R2 = {r2_score(pred_train, y_train)}\")\n",
    "print(f\"Test metrics:  R2 = {r2_score(pred_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backtrack line search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.11204637587070465\n",
      "epoch:  1, loss: 0.08480273932218552\n",
      "epoch:  2, loss: 0.06693968176841736\n",
      "epoch:  3, loss: 0.05539634823799133\n",
      "epoch:  4, loss: 0.04845454543828964\n",
      "epoch:  5, loss: 0.04429106414318085\n",
      "epoch:  6, loss: 0.04179423674941063\n",
      "epoch:  7, loss: 0.04029738903045654\n",
      "epoch:  8, loss: 0.03940034285187721\n",
      "epoch:  9, loss: 0.03886290639638901\n",
      "epoch:  10, loss: 0.03854095935821533\n",
      "epoch:  11, loss: 0.03834807127714157\n",
      "epoch:  12, loss: 0.03823243826627731\n",
      "epoch:  13, loss: 0.03816304728388786\n",
      "epoch:  14, loss: 0.03812132403254509\n",
      "epoch:  15, loss: 0.038096148520708084\n",
      "epoch:  16, loss: 0.03808087483048439\n",
      "epoch:  17, loss: 0.03807152062654495\n",
      "epoch:  18, loss: 0.03806570917367935\n",
      "epoch:  19, loss: 0.038064900785684586\n",
      "epoch:  20, loss: 0.03805931285023689\n",
      "epoch:  21, loss: 0.038058456033468246\n",
      "epoch:  22, loss: 0.038053084164857864\n",
      "epoch:  23, loss: 0.03805212676525116\n",
      "epoch:  24, loss: 0.03804696723818779\n",
      "epoch:  25, loss: 0.03804595395922661\n",
      "epoch:  26, loss: 0.03804100677371025\n",
      "epoch:  27, loss: 0.03803987801074982\n",
      "epoch:  28, loss: 0.03803512454032898\n",
      "epoch:  29, loss: 0.03803382068872452\n",
      "epoch:  30, loss: 0.0380292572081089\n",
      "epoch:  31, loss: 0.038027822971343994\n",
      "epoch:  32, loss: 0.038023460656404495\n",
      "epoch:  33, loss: 0.03802191838622093\n",
      "epoch:  34, loss: 0.03801773488521576\n",
      "epoch:  35, loss: 0.03801603242754936\n",
      "epoch:  36, loss: 0.03801203519105911\n",
      "epoch:  37, loss: 0.03801026567816734\n",
      "epoch:  38, loss: 0.03800645470619202\n",
      "epoch:  39, loss: 0.03800465911626816\n",
      "epoch:  40, loss: 0.03800465166568756\n",
      "epoch:  41, loss: 0.037999168038368225\n",
      "epoch:  42, loss: 0.03799887001514435\n",
      "epoch:  43, loss: 0.0379936583340168\n",
      "epoch:  44, loss: 0.03799330070614815\n",
      "epoch:  45, loss: 0.03798834607005119\n",
      "epoch:  46, loss: 0.03798782452940941\n",
      "epoch:  47, loss: 0.03798311576247215\n",
      "epoch:  48, loss: 0.03798248618841171\n",
      "epoch:  49, loss: 0.037978023290634155\n",
      "epoch:  50, loss: 0.037977319210767746\n",
      "epoch:  51, loss: 0.0379730761051178\n",
      "epoch:  52, loss: 0.03797218203544617\n",
      "epoch:  53, loss: 0.03796813637018204\n",
      "epoch:  54, loss: 0.03796697407960892\n",
      "epoch:  55, loss: 0.0379631333053112\n",
      "epoch:  56, loss: 0.03796187788248062\n",
      "epoch:  57, loss: 0.03795823082327843\n",
      "epoch:  58, loss: 0.037956856191158295\n",
      "epoch:  59, loss: 0.03795339912176132\n",
      "epoch:  60, loss: 0.03795195743441582\n",
      "epoch:  61, loss: 0.03795195743441582\n",
      "epoch:  62, loss: 0.03794702887535095\n",
      "epoch:  63, loss: 0.037946876138448715\n",
      "epoch:  64, loss: 0.03794219344854355\n",
      "epoch:  65, loss: 0.037941742688417435\n",
      "epoch:  66, loss: 0.03793730214238167\n",
      "epoch:  67, loss: 0.03793654218316078\n",
      "epoch:  68, loss: 0.03793232887983322\n",
      "epoch:  69, loss: 0.03793139383196831\n",
      "epoch:  70, loss: 0.03792739287018776\n",
      "epoch:  71, loss: 0.037926264107227325\n",
      "epoch:  72, loss: 0.03792246803641319\n",
      "epoch:  73, loss: 0.03792119398713112\n",
      "epoch:  74, loss: 0.0379176028072834\n",
      "epoch:  75, loss: 0.03791625797748566\n",
      "epoch:  76, loss: 0.037912845611572266\n",
      "epoch:  77, loss: 0.03791135177016258\n",
      "epoch:  78, loss: 0.037908121943473816\n",
      "epoch:  79, loss: 0.03790651634335518\n",
      "epoch:  80, loss: 0.037906207144260406\n",
      "epoch:  81, loss: 0.03790160268545151\n",
      "epoch:  82, loss: 0.03790122643113136\n",
      "epoch:  83, loss: 0.03789686784148216\n",
      "epoch:  84, loss: 0.03789624571800232\n",
      "epoch:  85, loss: 0.03789210319519043\n",
      "epoch:  86, loss: 0.037891268730163574\n",
      "epoch:  87, loss: 0.03788731247186661\n",
      "epoch:  88, loss: 0.0378861129283905\n",
      "epoch:  89, loss: 0.03788234293460846\n",
      "epoch:  90, loss: 0.03788090869784355\n",
      "epoch:  91, loss: 0.03787732124328613\n",
      "epoch:  92, loss: 0.0378757081925869\n",
      "epoch:  93, loss: 0.03787566348910332\n",
      "epoch:  94, loss: 0.03787054494023323\n",
      "epoch:  95, loss: 0.03787026181817055\n",
      "epoch:  96, loss: 0.03786538168787956\n",
      "epoch:  97, loss: 0.03786474093794823\n",
      "epoch:  98, loss: 0.037860095500946045\n",
      "epoch:  99, loss: 0.037859246134757996\n",
      "epoch:  100, loss: 0.037854816764593124\n",
      "epoch:  101, loss: 0.03785376995801926\n",
      "epoch:  102, loss: 0.037849556654691696\n",
      "epoch:  103, loss: 0.0378483347594738\n",
      "epoch:  104, loss: 0.037844303995370865\n",
      "epoch:  105, loss: 0.037842798978090286\n",
      "epoch:  106, loss: 0.037838950753211975\n",
      "epoch:  107, loss: 0.03783721476793289\n",
      "epoch:  108, loss: 0.03783707320690155\n",
      "epoch:  109, loss: 0.037831488996744156\n",
      "epoch:  110, loss: 0.03783098980784416\n",
      "epoch:  111, loss: 0.03782564401626587\n",
      "epoch:  112, loss: 0.037824880331754684\n",
      "epoch:  113, loss: 0.03781977295875549\n",
      "epoch:  114, loss: 0.03781881928443909\n",
      "epoch:  115, loss: 0.03781391680240631\n",
      "epoch:  116, loss: 0.03781269118189812\n",
      "epoch:  117, loss: 0.03780796006321907\n",
      "epoch:  118, loss: 0.03780630975961685\n",
      "epoch:  119, loss: 0.03780175372958183\n",
      "epoch:  120, loss: 0.03779982030391693\n",
      "epoch:  121, loss: 0.03779544308781624\n",
      "epoch:  122, loss: 0.037793323397636414\n",
      "epoch:  123, loss: 0.03779314458370209\n",
      "epoch:  124, loss: 0.03778670355677605\n",
      "epoch:  125, loss: 0.03778626024723053\n",
      "epoch:  126, loss: 0.037780046463012695\n",
      "epoch:  127, loss: 0.03777925670146942\n",
      "epoch:  128, loss: 0.0377732515335083\n",
      "epoch:  129, loss: 0.03777220845222473\n",
      "epoch:  130, loss: 0.037766389548778534\n",
      "epoch:  131, loss: 0.03776508569717407\n",
      "epoch:  132, loss: 0.0377594530582428\n",
      "epoch:  133, loss: 0.037757810205221176\n",
      "epoch:  134, loss: 0.03775232657790184\n",
      "epoch:  135, loss: 0.03775029256939888\n",
      "epoch:  136, loss: 0.03774494677782059\n",
      "epoch:  137, loss: 0.03774242103099823\n",
      "epoch:  138, loss: 0.03774234652519226\n",
      "epoch:  139, loss: 0.03773423284292221\n",
      "epoch:  140, loss: 0.03773367032408714\n",
      "epoch:  141, loss: 0.03772575780749321\n",
      "epoch:  142, loss: 0.03772478178143501\n",
      "epoch:  143, loss: 0.037717048078775406\n",
      "epoch:  144, loss: 0.03771562501788139\n",
      "epoch:  145, loss: 0.037708014249801636\n",
      "epoch:  146, loss: 0.037705957889556885\n",
      "epoch:  147, loss: 0.03769845888018608\n",
      "epoch:  148, loss: 0.03769585117697716\n",
      "epoch:  149, loss: 0.037688445299863815\n",
      "epoch:  150, loss: 0.03768541291356087\n",
      "epoch:  151, loss: 0.03767811506986618\n",
      "epoch:  152, loss: 0.03767446428537369\n",
      "epoch:  153, loss: 0.0376741848886013\n",
      "epoch:  154, loss: 0.03766279295086861\n",
      "epoch:  155, loss: 0.03766193240880966\n",
      "epoch:  156, loss: 0.037650614976882935\n",
      "epoch:  157, loss: 0.03764912486076355\n",
      "epoch:  158, loss: 0.03763787820935249\n",
      "epoch:  159, loss: 0.037635982036590576\n",
      "epoch:  160, loss: 0.037624746561050415\n",
      "epoch:  161, loss: 0.03762204200029373\n",
      "epoch:  162, loss: 0.037610724568367004\n",
      "epoch:  163, loss: 0.037606850266456604\n",
      "epoch:  164, loss: 0.03759545087814331\n",
      "epoch:  165, loss: 0.03759096935391426\n",
      "epoch:  166, loss: 0.03757954016327858\n",
      "epoch:  167, loss: 0.037574734538793564\n",
      "epoch:  168, loss: 0.03756316006183624\n",
      "epoch:  169, loss: 0.03755778446793556\n",
      "epoch:  170, loss: 0.03754603490233421\n",
      "epoch:  171, loss: 0.03753945976495743\n",
      "epoch:  172, loss: 0.03753885626792908\n",
      "epoch:  173, loss: 0.03751927241683006\n",
      "epoch:  174, loss: 0.037518128752708435\n",
      "epoch:  175, loss: 0.03749801218509674\n",
      "epoch:  176, loss: 0.03749573603272438\n",
      "epoch:  177, loss: 0.03747514635324478\n",
      "epoch:  178, loss: 0.03747297450900078\n",
      "epoch:  179, loss: 0.03745168447494507\n",
      "epoch:  180, loss: 0.03744908794760704\n",
      "epoch:  181, loss: 0.03742699325084686\n",
      "epoch:  182, loss: 0.037424102425575256\n",
      "epoch:  183, loss: 0.03740106523036957\n",
      "epoch:  184, loss: 0.037397608160972595\n",
      "epoch:  185, loss: 0.03737351670861244\n",
      "epoch:  186, loss: 0.037369996309280396\n",
      "epoch:  187, loss: 0.03734458610415459\n",
      "epoch:  188, loss: 0.0373406708240509\n",
      "epoch:  189, loss: 0.03731392323970795\n",
      "epoch:  190, loss: 0.03731103986501694\n",
      "epoch:  191, loss: 0.03728250414133072\n",
      "epoch:  192, loss: 0.037280868738889694\n",
      "epoch:  193, loss: 0.0372503362596035\n",
      "epoch:  194, loss: 0.03724971041083336\n",
      "epoch:  195, loss: 0.0372166745364666\n",
      "epoch:  196, loss: 0.03719596937298775\n",
      "epoch:  197, loss: 0.03718199580907822\n",
      "epoch:  198, loss: 0.03715958446264267\n",
      "epoch:  199, loss: 0.0371461883187294\n",
      "epoch:  200, loss: 0.03712156414985657\n",
      "epoch:  201, loss: 0.03710893541574478\n",
      "epoch:  202, loss: 0.03708181530237198\n",
      "epoch:  203, loss: 0.03707103431224823\n",
      "epoch:  204, loss: 0.03704078122973442\n",
      "epoch:  205, loss: 0.03703178092837334\n",
      "epoch:  206, loss: 0.03699791803956032\n",
      "epoch:  207, loss: 0.03699232265353203\n",
      "epoch:  208, loss: 0.036953769624233246\n",
      "epoch:  209, loss: 0.03695228323340416\n",
      "epoch:  210, loss: 0.03690813481807709\n",
      "epoch:  211, loss: 0.036880623549222946\n",
      "epoch:  212, loss: 0.03686197102069855\n",
      "epoch:  213, loss: 0.03683027625083923\n",
      "epoch:  214, loss: 0.03681464120745659\n",
      "epoch:  215, loss: 0.036777887493371964\n",
      "epoch:  216, loss: 0.03676610812544823\n",
      "epoch:  217, loss: 0.03672268986701965\n",
      "epoch:  218, loss: 0.03671730309724808\n",
      "epoch:  219, loss: 0.03666559234261513\n",
      "epoch:  220, loss: 0.03663337603211403\n",
      "epoch:  221, loss: 0.03660877048969269\n",
      "epoch:  222, loss: 0.03657006099820137\n",
      "epoch:  223, loss: 0.03655216842889786\n",
      "epoch:  224, loss: 0.036505237221717834\n",
      "epoch:  225, loss: 0.036495745182037354\n",
      "epoch:  226, loss: 0.03643796592950821\n",
      "epoch:  227, loss: 0.03640222176909447\n",
      "epoch:  228, loss: 0.036369647830724716\n",
      "epoch:  229, loss: 0.03632558137178421\n",
      "epoch:  230, loss: 0.03630368784070015\n",
      "epoch:  231, loss: 0.0362481027841568\n",
      "epoch:  232, loss: 0.03624057397246361\n",
      "epoch:  233, loss: 0.036169372498989105\n",
      "epoch:  234, loss: 0.03612564504146576\n",
      "epoch:  235, loss: 0.03609262779355049\n",
      "epoch:  236, loss: 0.036035820841789246\n",
      "epoch:  237, loss: 0.036019857972860336\n",
      "epoch:  238, loss: 0.03594476357102394\n",
      "epoch:  239, loss: 0.03589862957596779\n",
      "epoch:  240, loss: 0.03585594519972801\n",
      "epoch:  241, loss: 0.0357944555580616\n",
      "epoch:  242, loss: 0.03577375039458275\n",
      "epoch:  243, loss: 0.035689860582351685\n",
      "epoch:  244, loss: 0.03563844785094261\n",
      "epoch:  245, loss: 0.03559034690260887\n",
      "epoch:  246, loss: 0.03551926463842392\n",
      "epoch:  247, loss: 0.03550272062420845\n",
      "epoch:  248, loss: 0.03540116548538208\n",
      "epoch:  249, loss: 0.035339780151844025\n",
      "epoch:  250, loss: 0.03529378026723862\n",
      "epoch:  251, loss: 0.03520532697439194\n",
      "epoch:  252, loss: 0.03515094891190529\n",
      "epoch:  253, loss: 0.035076674073934555\n",
      "epoch:  254, loss: 0.03499828279018402\n",
      "epoch:  255, loss: 0.03496858850121498\n",
      "epoch:  256, loss: 0.03484899550676346\n",
      "epoch:  257, loss: 0.0347779244184494\n",
      "epoch:  258, loss: 0.03471609950065613\n",
      "epoch:  259, loss: 0.034607406705617905\n",
      "epoch:  260, loss: 0.034541577100753784\n",
      "epoch:  261, loss: 0.0344531424343586\n",
      "epoch:  262, loss: 0.03435031324625015\n",
      "epoch:  263, loss: 0.03428694233298302\n",
      "epoch:  264, loss: 0.03417619690299034\n",
      "epoch:  265, loss: 0.03407445549964905\n",
      "epoch:  266, loss: 0.03404867649078369\n",
      "epoch:  267, loss: 0.033876944333314896\n",
      "epoch:  268, loss: 0.03377896547317505\n",
      "epoch:  269, loss: 0.03373464569449425\n",
      "epoch:  270, loss: 0.03355957195162773\n",
      "epoch:  271, loss: 0.033462632447481155\n",
      "epoch:  272, loss: 0.033394698053598404\n",
      "epoch:  273, loss: 0.03322281315922737\n",
      "epoch:  274, loss: 0.033124327659606934\n",
      "epoch:  275, loss: 0.03306281939148903\n",
      "epoch:  276, loss: 0.03286796063184738\n",
      "epoch:  277, loss: 0.032761186361312866\n",
      "epoch:  278, loss: 0.03271273523569107\n",
      "epoch:  279, loss: 0.0324910506606102\n",
      "epoch:  280, loss: 0.03237073868513107\n",
      "epoch:  281, loss: 0.03231959789991379\n",
      "epoch:  282, loss: 0.032072763890028\n",
      "epoch:  283, loss: 0.03194265440106392\n",
      "epoch:  284, loss: 0.03189381584525108\n",
      "epoch:  285, loss: 0.031616657972335815\n",
      "epoch:  286, loss: 0.0314759686589241\n",
      "epoch:  287, loss: 0.0314582958817482\n",
      "epoch:  288, loss: 0.031138351187109947\n",
      "epoch:  289, loss: 0.030978478491306305\n",
      "epoch:  290, loss: 0.030885936692357063\n",
      "epoch:  291, loss: 0.03064761310815811\n",
      "epoch:  292, loss: 0.030450306832790375\n",
      "epoch:  293, loss: 0.03034391440451145\n",
      "epoch:  294, loss: 0.030106697231531143\n",
      "epoch:  295, loss: 0.029881028458476067\n",
      "epoch:  296, loss: 0.029760299250483513\n",
      "epoch:  297, loss: 0.029543494805693626\n",
      "epoch:  298, loss: 0.029270416125655174\n",
      "epoch:  299, loss: 0.029135270044207573\n",
      "epoch:  300, loss: 0.02892884984612465\n",
      "epoch:  301, loss: 0.02861384116113186\n",
      "epoch:  302, loss: 0.028467584401369095\n",
      "epoch:  303, loss: 0.02833537571132183\n",
      "epoch:  304, loss: 0.027940087020397186\n",
      "epoch:  305, loss: 0.027767835184931755\n",
      "epoch:  306, loss: 0.0277014821767807\n",
      "epoch:  307, loss: 0.02721620537340641\n",
      "epoch:  308, loss: 0.02701280079782009\n",
      "epoch:  309, loss: 0.026907214894890785\n",
      "epoch:  310, loss: 0.02644057758152485\n",
      "epoch:  311, loss: 0.026223473250865936\n",
      "epoch:  312, loss: 0.026111925020813942\n",
      "epoch:  313, loss: 0.025672689080238342\n",
      "epoch:  314, loss: 0.025407716631889343\n",
      "epoch:  315, loss: 0.02528315596282482\n",
      "epoch:  316, loss: 0.02486841380596161\n",
      "epoch:  317, loss: 0.024561915546655655\n",
      "epoch:  318, loss: 0.024430623278021812\n",
      "epoch:  319, loss: 0.02396494895219803\n",
      "epoch:  320, loss: 0.023676717653870583\n",
      "epoch:  321, loss: 0.023553399369120598\n",
      "epoch:  322, loss: 0.023065738379955292\n",
      "epoch:  323, loss: 0.022780131548643112\n",
      "epoch:  324, loss: 0.02266455814242363\n",
      "epoch:  325, loss: 0.02213769406080246\n",
      "epoch:  326, loss: 0.02187885344028473\n",
      "epoch:  327, loss: 0.021768413484096527\n",
      "epoch:  328, loss: 0.02121747098863125\n",
      "epoch:  329, loss: 0.02097947522997856\n",
      "epoch:  330, loss: 0.020878450945019722\n",
      "epoch:  331, loss: 0.020268788561224937\n",
      "epoch:  332, loss: 0.02008633315563202\n",
      "epoch:  333, loss: 0.01986297406256199\n",
      "epoch:  334, loss: 0.019347352907061577\n",
      "epoch:  335, loss: 0.019223062321543694\n",
      "epoch:  336, loss: 0.018721112981438637\n",
      "epoch:  337, loss: 0.01847720704972744\n",
      "epoch:  338, loss: 0.018390608951449394\n",
      "epoch:  339, loss: 0.017809422686696053\n",
      "epoch:  340, loss: 0.01766645349562168\n",
      "epoch:  341, loss: 0.017292680218815804\n",
      "epoch:  342, loss: 0.016984065994620323\n",
      "epoch:  343, loss: 0.016905153170228004\n",
      "epoch:  344, loss: 0.016376091167330742\n",
      "epoch:  345, loss: 0.016256580129265785\n",
      "epoch:  346, loss: 0.015874290838837624\n",
      "epoch:  347, loss: 0.015650015324354172\n",
      "epoch:  348, loss: 0.015533684752881527\n",
      "epoch:  349, loss: 0.01508636400103569\n",
      "epoch:  350, loss: 0.015022818930447102\n",
      "epoch:  351, loss: 0.014569551683962345\n",
      "epoch:  352, loss: 0.014496146701276302\n",
      "epoch:  353, loss: 0.014102485030889511\n",
      "epoch:  354, loss: 0.0140146529302001\n",
      "epoch:  355, loss: 0.013668575324118137\n",
      "epoch:  356, loss: 0.013574584387242794\n",
      "epoch:  357, loss: 0.013257930055260658\n",
      "epoch:  358, loss: 0.013174226507544518\n",
      "epoch:  359, loss: 0.012869766913354397\n",
      "epoch:  360, loss: 0.01280934363603592\n",
      "epoch:  361, loss: 0.012516076676547527\n",
      "epoch:  362, loss: 0.012476254254579544\n",
      "epoch:  363, loss: 0.012206723913550377\n",
      "epoch:  364, loss: 0.012173536233603954\n",
      "epoch:  365, loss: 0.011924849823117256\n",
      "epoch:  366, loss: 0.011698582209646702\n",
      "epoch:  367, loss: 0.011550630442798138\n",
      "epoch:  368, loss: 0.011471616104245186\n",
      "epoch:  369, loss: 0.011285840533673763\n",
      "epoch:  370, loss: 0.011117734014987946\n",
      "epoch:  371, loss: 0.010965399444103241\n",
      "epoch:  372, loss: 0.010829169303178787\n",
      "epoch:  373, loss: 0.010814529843628407\n",
      "epoch:  374, loss: 0.01068919152021408\n",
      "epoch:  375, loss: 0.010619381442666054\n",
      "epoch:  376, loss: 0.010562608018517494\n",
      "epoch:  377, loss: 0.010473975911736488\n",
      "epoch:  378, loss: 0.010447239503264427\n",
      "epoch:  379, loss: 0.010360319167375565\n",
      "epoch:  380, loss: 0.010342486202716827\n",
      "epoch:  381, loss: 0.010266927070915699\n",
      "epoch:  382, loss: 0.010246923193335533\n",
      "epoch:  383, loss: 0.010177685879170895\n",
      "epoch:  384, loss: 0.010161166079342365\n",
      "epoch:  385, loss: 0.010098341852426529\n",
      "epoch:  386, loss: 0.01008264534175396\n",
      "epoch:  387, loss: 0.010027272626757622\n",
      "epoch:  388, loss: 0.01001154538244009\n",
      "epoch:  389, loss: 0.00996134988963604\n",
      "epoch:  390, loss: 0.009946302510797977\n",
      "epoch:  391, loss: 0.009899543598294258\n",
      "epoch:  392, loss: 0.009886671788990498\n",
      "epoch:  393, loss: 0.00984759908169508\n",
      "epoch:  394, loss: 0.009832624346017838\n",
      "epoch:  395, loss: 0.009806930087506771\n",
      "epoch:  396, loss: 0.009784705936908722\n",
      "epoch:  397, loss: 0.009774592705070972\n",
      "epoch:  398, loss: 0.009741906076669693\n",
      "epoch:  399, loss: 0.009737815707921982\n",
      "epoch:  400, loss: 0.009703174233436584\n",
      "epoch:  401, loss: 0.009699029847979546\n",
      "epoch:  402, loss: 0.009667038917541504\n",
      "epoch:  403, loss: 0.009663400240242481\n",
      "epoch:  404, loss: 0.00963603239506483\n",
      "epoch:  405, loss: 0.009631626307964325\n",
      "epoch:  406, loss: 0.009608830325305462\n",
      "epoch:  407, loss: 0.009602385573089123\n",
      "epoch:  408, loss: 0.009589679539203644\n",
      "epoch:  409, loss: 0.00957641564309597\n",
      "epoch:  410, loss: 0.009573899209499359\n",
      "epoch:  411, loss: 0.009553709998726845\n",
      "epoch:  412, loss: 0.009550592862069607\n",
      "epoch:  413, loss: 0.009533802047371864\n",
      "epoch:  414, loss: 0.0095291743054986\n",
      "epoch:  415, loss: 0.009528535418212414\n",
      "epoch:  416, loss: 0.00951059814542532\n",
      "epoch:  417, loss: 0.00950853805989027\n",
      "epoch:  418, loss: 0.009494692087173462\n",
      "epoch:  419, loss: 0.009491615928709507\n",
      "epoch:  420, loss: 0.009482864290475845\n",
      "epoch:  421, loss: 0.009475767612457275\n",
      "epoch:  422, loss: 0.009474179707467556\n",
      "epoch:  423, loss: 0.009461244568228722\n",
      "epoch:  424, loss: 0.00945932138711214\n",
      "epoch:  425, loss: 0.00944956112653017\n",
      "epoch:  426, loss: 0.00944537203758955\n",
      "epoch:  427, loss: 0.009444065392017365\n",
      "epoch:  428, loss: 0.00943277683109045\n",
      "epoch:  429, loss: 0.009430899284780025\n",
      "epoch:  430, loss: 0.009421659633517265\n",
      "epoch:  431, loss: 0.009418127126991749\n",
      "epoch:  432, loss: 0.00941690243780613\n",
      "epoch:  433, loss: 0.00940658524632454\n",
      "epoch:  434, loss: 0.009404834359884262\n",
      "epoch:  435, loss: 0.009398889727890491\n",
      "epoch:  436, loss: 0.009393243119120598\n",
      "epoch:  437, loss: 0.009392009116709232\n",
      "epoch:  438, loss: 0.00938402395695448\n",
      "epoch:  439, loss: 0.009381097741425037\n",
      "epoch:  440, loss: 0.009380016475915909\n",
      "epoch:  441, loss: 0.00937180407345295\n",
      "epoch:  442, loss: 0.009369426406919956\n",
      "epoch:  443, loss: 0.009368419647216797\n",
      "epoch:  444, loss: 0.009360245428979397\n",
      "epoch:  445, loss: 0.009358343668282032\n",
      "epoch:  446, loss: 0.009355306625366211\n",
      "epoch:  447, loss: 0.009348553605377674\n",
      "epoch:  448, loss: 0.009347306564450264\n",
      "epoch:  449, loss: 0.009340984746813774\n",
      "epoch:  450, loss: 0.009337383322417736\n",
      "epoch:  451, loss: 0.009336372837424278\n",
      "epoch:  452, loss: 0.009328143671154976\n",
      "epoch:  453, loss: 0.009326734580099583\n",
      "epoch:  454, loss: 0.009322957135736942\n",
      "epoch:  455, loss: 0.009318163618445396\n",
      "epoch:  456, loss: 0.009317082352936268\n",
      "epoch:  457, loss: 0.009310920722782612\n",
      "epoch:  458, loss: 0.009308625012636185\n",
      "epoch:  459, loss: 0.009307739324867725\n",
      "epoch:  460, loss: 0.009301725775003433\n",
      "epoch:  461, loss: 0.009299544617533684\n",
      "epoch:  462, loss: 0.00929872877895832\n",
      "epoch:  463, loss: 0.009293373674154282\n",
      "epoch:  464, loss: 0.009291118942201138\n",
      "epoch:  465, loss: 0.009290345944464207\n",
      "epoch:  466, loss: 0.00928391795605421\n",
      "epoch:  467, loss: 0.009282751940190792\n",
      "epoch:  468, loss: 0.009280872531235218\n",
      "epoch:  469, loss: 0.009275415912270546\n",
      "epoch:  470, loss: 0.009274427779018879\n",
      "epoch:  471, loss: 0.009270713664591312\n",
      "epoch:  472, loss: 0.009266871958971024\n",
      "epoch:  473, loss: 0.009266004897654057\n",
      "epoch:  474, loss: 0.009259955026209354\n",
      "epoch:  475, loss: 0.009258306585252285\n",
      "epoch:  476, loss: 0.009257559664547443\n",
      "epoch:  477, loss: 0.00925096683204174\n",
      "epoch:  478, loss: 0.009250029921531677\n",
      "epoch:  479, loss: 0.009245287626981735\n",
      "epoch:  480, loss: 0.009242464788258076\n",
      "epoch:  481, loss: 0.009241636842489243\n",
      "epoch:  482, loss: 0.009236075915396214\n",
      "epoch:  483, loss: 0.009234246797859669\n",
      "epoch:  484, loss: 0.009233521297574043\n",
      "epoch:  485, loss: 0.009227347560226917\n",
      "epoch:  486, loss: 0.009226263500750065\n",
      "epoch:  487, loss: 0.009224185720086098\n",
      "epoch:  488, loss: 0.009218988940119743\n",
      "epoch:  489, loss: 0.009218048304319382\n",
      "epoch:  490, loss: 0.009213809855282307\n",
      "epoch:  491, loss: 0.009210874326527119\n",
      "epoch:  492, loss: 0.009210127405822277\n",
      "epoch:  493, loss: 0.009205683134496212\n",
      "epoch:  494, loss: 0.009203561581671238\n",
      "epoch:  495, loss: 0.00920285377651453\n",
      "epoch:  496, loss: 0.009197482839226723\n",
      "epoch:  497, loss: 0.00919634010642767\n",
      "epoch:  498, loss: 0.009195235557854176\n",
      "epoch:  499, loss: 0.00919022411108017\n",
      "epoch:  500, loss: 0.00918933842331171\n",
      "epoch:  501, loss: 0.00918553676456213\n",
      "epoch:  502, loss: 0.009183081798255444\n",
      "epoch:  503, loss: 0.009182418696582317\n",
      "epoch:  504, loss: 0.009177688509225845\n",
      "epoch:  505, loss: 0.009176227264106274\n",
      "epoch:  506, loss: 0.009175601415336132\n",
      "epoch:  507, loss: 0.00917042326182127\n",
      "epoch:  508, loss: 0.009169313125312328\n",
      "epoch:  509, loss: 0.009167292155325413\n",
      "epoch:  510, loss: 0.009163051843643188\n",
      "epoch:  511, loss: 0.009162215515971184\n",
      "epoch:  512, loss: 0.009158245287835598\n",
      "epoch:  513, loss: 0.009155445732176304\n",
      "epoch:  514, loss: 0.00915464386343956\n",
      "epoch:  515, loss: 0.009148676879703999\n",
      "epoch:  516, loss: 0.009147066622972488\n",
      "epoch:  517, loss: 0.009145461022853851\n",
      "epoch:  518, loss: 0.009139113128185272\n",
      "epoch:  519, loss: 0.009138013236224651\n",
      "epoch:  520, loss: 0.009133859537541866\n",
      "epoch:  521, loss: 0.009129480458796024\n",
      "epoch:  522, loss: 0.009128355421125889\n",
      "epoch:  523, loss: 0.009123130701482296\n",
      "epoch:  524, loss: 0.00911928340792656\n",
      "epoch:  525, loss: 0.009118285961449146\n",
      "epoch:  526, loss: 0.009111463092267513\n",
      "epoch:  527, loss: 0.009109841659665108\n",
      "epoch:  528, loss: 0.009109062142670155\n",
      "epoch:  529, loss: 0.009103154763579369\n",
      "epoch:  530, loss: 0.009101626463234425\n",
      "epoch:  531, loss: 0.009100835770368576\n",
      "epoch:  532, loss: 0.009094718843698502\n",
      "epoch:  533, loss: 0.009093781001865864\n",
      "epoch:  534, loss: 0.009090684354305267\n",
      "epoch:  535, loss: 0.009086916223168373\n",
      "epoch:  536, loss: 0.009086033329367638\n",
      "epoch:  537, loss: 0.00908096693456173\n",
      "epoch:  538, loss: 0.009078999049961567\n",
      "epoch:  539, loss: 0.009078305214643478\n",
      "epoch:  540, loss: 0.00907418504357338\n",
      "epoch:  541, loss: 0.009071851149201393\n",
      "epoch:  542, loss: 0.009071140550076962\n",
      "epoch:  543, loss: 0.009066244587302208\n",
      "epoch:  544, loss: 0.009064430370926857\n",
      "epoch:  545, loss: 0.00906371045857668\n",
      "epoch:  546, loss: 0.009058787487447262\n",
      "epoch:  547, loss: 0.009057056158781052\n",
      "epoch:  548, loss: 0.009056339040398598\n",
      "epoch:  549, loss: 0.009051145054399967\n",
      "epoch:  550, loss: 0.009049734100699425\n",
      "epoch:  551, loss: 0.00904907938092947\n",
      "epoch:  552, loss: 0.009043903090059757\n",
      "epoch:  553, loss: 0.009042629972100258\n",
      "epoch:  554, loss: 0.00904197245836258\n",
      "epoch:  555, loss: 0.00903693214058876\n",
      "epoch:  556, loss: 0.009035528637468815\n",
      "epoch:  557, loss: 0.009034869261085987\n",
      "epoch:  558, loss: 0.009029687382280827\n",
      "epoch:  559, loss: 0.00902819074690342\n",
      "epoch:  560, loss: 0.009027469903230667\n",
      "epoch:  561, loss: 0.009022499434649944\n",
      "epoch:  562, loss: 0.009020574390888214\n",
      "epoch:  563, loss: 0.009019856341183186\n",
      "epoch:  564, loss: 0.009014968760311604\n",
      "epoch:  565, loss: 0.009013140574097633\n",
      "epoch:  566, loss: 0.00901239924132824\n",
      "epoch:  567, loss: 0.009006683714687824\n",
      "epoch:  568, loss: 0.009005403146147728\n",
      "epoch:  569, loss: 0.00900472141802311\n",
      "epoch:  570, loss: 0.008999709039926529\n",
      "epoch:  571, loss: 0.008997918106615543\n",
      "epoch:  572, loss: 0.008997190743684769\n",
      "epoch:  573, loss: 0.008991694077849388\n",
      "epoch:  574, loss: 0.008990111760795116\n",
      "epoch:  575, loss: 0.008989360183477402\n",
      "epoch:  576, loss: 0.008984135463833809\n",
      "epoch:  577, loss: 0.008981960825622082\n",
      "epoch:  578, loss: 0.008981172926723957\n",
      "epoch:  579, loss: 0.00897721666842699\n",
      "epoch:  580, loss: 0.008974091149866581\n",
      "epoch:  581, loss: 0.008973195217549801\n",
      "epoch:  582, loss: 0.008968427777290344\n",
      "epoch:  583, loss: 0.008965923450887203\n",
      "epoch:  584, loss: 0.008965115994215012\n",
      "epoch:  585, loss: 0.008960572071373463\n",
      "epoch:  586, loss: 0.008957868441939354\n",
      "epoch:  587, loss: 0.008956985548138618\n",
      "epoch:  588, loss: 0.00895347073674202\n",
      "epoch:  589, loss: 0.008949615061283112\n",
      "epoch:  590, loss: 0.008948691189289093\n",
      "epoch:  591, loss: 0.008946606889367104\n",
      "epoch:  592, loss: 0.008941764011979103\n",
      "epoch:  593, loss: 0.00894076470285654\n",
      "epoch:  594, loss: 0.008938013575971127\n",
      "epoch:  595, loss: 0.008933886885643005\n",
      "epoch:  596, loss: 0.00893300399184227\n",
      "epoch:  597, loss: 0.008930712006986141\n",
      "epoch:  598, loss: 0.008926426991820335\n",
      "epoch:  599, loss: 0.008925439789891243\n",
      "epoch:  600, loss: 0.008924753405153751\n",
      "epoch:  601, loss: 0.008919764310121536\n",
      "epoch:  602, loss: 0.008917686529457569\n",
      "epoch:  603, loss: 0.008916905149817467\n",
      "epoch:  604, loss: 0.008914231322705746\n",
      "epoch:  605, loss: 0.008910032920539379\n",
      "epoch:  606, loss: 0.008909095078706741\n",
      "epoch:  607, loss: 0.008906028233468533\n",
      "epoch:  608, loss: 0.008902222849428654\n",
      "epoch:  609, loss: 0.008901272900402546\n",
      "epoch:  610, loss: 0.008897917345166206\n",
      "epoch:  611, loss: 0.00889426376670599\n",
      "epoch:  612, loss: 0.008893363177776337\n",
      "epoch:  613, loss: 0.008891192264854908\n",
      "epoch:  614, loss: 0.008886657655239105\n",
      "epoch:  615, loss: 0.008885697461664677\n",
      "epoch:  616, loss: 0.008884141221642494\n",
      "epoch:  617, loss: 0.008878981694579124\n",
      "epoch:  618, loss: 0.008877924643456936\n",
      "epoch:  619, loss: 0.008877232670783997\n",
      "epoch:  620, loss: 0.00887157954275608\n",
      "epoch:  621, loss: 0.00887015089392662\n",
      "epoch:  622, loss: 0.008869422599673271\n",
      "epoch:  623, loss: 0.008863691240549088\n",
      "epoch:  624, loss: 0.008862349204719067\n",
      "epoch:  625, loss: 0.008861630223691463\n",
      "epoch:  626, loss: 0.008856652304530144\n",
      "epoch:  627, loss: 0.008854703046381474\n",
      "epoch:  628, loss: 0.008853946812450886\n",
      "epoch:  629, loss: 0.00884982105344534\n",
      "epoch:  630, loss: 0.008847084827721119\n",
      "epoch:  631, loss: 0.008846256881952286\n",
      "epoch:  632, loss: 0.008842099457979202\n",
      "epoch:  633, loss: 0.00883952435106039\n",
      "epoch:  634, loss: 0.008838687092065811\n",
      "epoch:  635, loss: 0.00883487332612276\n",
      "epoch:  636, loss: 0.00883158203214407\n",
      "epoch:  637, loss: 0.008830682374536991\n",
      "epoch:  638, loss: 0.008829089812934399\n",
      "epoch:  639, loss: 0.008823737502098083\n",
      "epoch:  640, loss: 0.008822650648653507\n",
      "epoch:  641, loss: 0.008820837363600731\n",
      "epoch:  642, loss: 0.00881548784673214\n",
      "epoch:  643, loss: 0.008814378641545773\n",
      "epoch:  644, loss: 0.008813061751425266\n",
      "epoch:  645, loss: 0.0088071683421731\n",
      "epoch:  646, loss: 0.008806068450212479\n",
      "epoch:  647, loss: 0.008805355057120323\n",
      "epoch:  648, loss: 0.008798932656645775\n",
      "epoch:  649, loss: 0.008797701448202133\n",
      "epoch:  650, loss: 0.008796244859695435\n",
      "epoch:  651, loss: 0.008790502324700356\n",
      "epoch:  652, loss: 0.008789394982159138\n",
      "epoch:  653, loss: 0.008789075538516045\n",
      "epoch:  654, loss: 0.00878195557743311\n",
      "epoch:  655, loss: 0.00878063589334488\n",
      "epoch:  656, loss: 0.008779816329479218\n",
      "epoch:  657, loss: 0.008772553876042366\n",
      "epoch:  658, loss: 0.008771238848567009\n",
      "epoch:  659, loss: 0.008770816028118134\n",
      "epoch:  660, loss: 0.00876259058713913\n",
      "epoch:  661, loss: 0.00876107718795538\n",
      "epoch:  662, loss: 0.008760151453316212\n",
      "epoch:  663, loss: 0.00875446479767561\n",
      "epoch:  664, loss: 0.00875130109488964\n",
      "epoch:  665, loss: 0.008750229142606258\n",
      "epoch:  666, loss: 0.0087500661611557\n",
      "epoch:  667, loss: 0.008741761557757854\n",
      "epoch:  668, loss: 0.008740306831896305\n",
      "epoch:  669, loss: 0.008739407174289227\n",
      "epoch:  670, loss: 0.008732068352401257\n",
      "epoch:  671, loss: 0.008730482310056686\n",
      "epoch:  672, loss: 0.008729568682610989\n",
      "epoch:  673, loss: 0.008722485043108463\n",
      "epoch:  674, loss: 0.008720681071281433\n",
      "epoch:  675, loss: 0.008719764649868011\n",
      "epoch:  676, loss: 0.008713587187230587\n",
      "epoch:  677, loss: 0.008711034432053566\n",
      "epoch:  678, loss: 0.008710072375833988\n",
      "epoch:  679, loss: 0.008705070242285728\n",
      "epoch:  680, loss: 0.008701578713953495\n",
      "epoch:  681, loss: 0.008700440637767315\n",
      "epoch:  682, loss: 0.00869869813323021\n",
      "epoch:  683, loss: 0.008691915310919285\n",
      "epoch:  684, loss: 0.008690569549798965\n",
      "epoch:  685, loss: 0.008688600733876228\n",
      "epoch:  686, loss: 0.008682085201144218\n",
      "epoch:  687, loss: 0.008680700324475765\n",
      "epoch:  688, loss: 0.008679823018610477\n",
      "epoch:  689, loss: 0.008672824129462242\n",
      "epoch:  690, loss: 0.008670996874570847\n",
      "epoch:  691, loss: 0.008670084178447723\n",
      "epoch:  692, loss: 0.008662920445203781\n",
      "epoch:  693, loss: 0.008661177940666676\n",
      "epoch:  694, loss: 0.008660267107188702\n",
      "epoch:  695, loss: 0.008652950637042522\n",
      "epoch:  696, loss: 0.008651291951537132\n",
      "epoch:  697, loss: 0.008650393225252628\n",
      "epoch:  698, loss: 0.008643078617751598\n",
      "epoch:  699, loss: 0.008641461841762066\n",
      "epoch:  700, loss: 0.00864056684076786\n",
      "epoch:  701, loss: 0.008633176796138287\n",
      "epoch:  702, loss: 0.008631547912955284\n",
      "epoch:  703, loss: 0.008630632422864437\n",
      "epoch:  704, loss: 0.008623162284493446\n",
      "epoch:  705, loss: 0.008621377870440483\n",
      "epoch:  706, loss: 0.008620448410511017\n",
      "epoch:  707, loss: 0.008613221347332\n",
      "epoch:  708, loss: 0.008611330762505531\n",
      "epoch:  709, loss: 0.00861040037125349\n",
      "epoch:  710, loss: 0.008604293689131737\n",
      "epoch:  711, loss: 0.00860153790563345\n",
      "epoch:  712, loss: 0.008600528351962566\n",
      "epoch:  713, loss: 0.008595647290349007\n",
      "epoch:  714, loss: 0.00859181396663189\n",
      "epoch:  715, loss: 0.00859056506305933\n",
      "epoch:  716, loss: 0.008587710559368134\n",
      "epoch:  717, loss: 0.008581968955695629\n",
      "epoch:  718, loss: 0.008580717258155346\n",
      "epoch:  719, loss: 0.00857948511838913\n",
      "epoch:  720, loss: 0.008572402410209179\n",
      "epoch:  721, loss: 0.008570804260671139\n",
      "epoch:  722, loss: 0.008569875732064247\n",
      "epoch:  723, loss: 0.00856210757046938\n",
      "epoch:  724, loss: 0.008560467511415482\n",
      "epoch:  725, loss: 0.0085595129057765\n",
      "epoch:  726, loss: 0.00855169352144003\n",
      "epoch:  727, loss: 0.008550050668418407\n",
      "epoch:  728, loss: 0.008549108169972897\n",
      "epoch:  729, loss: 0.008544622920453548\n",
      "epoch:  730, loss: 0.008540389128029346\n",
      "epoch:  731, loss: 0.008539075031876564\n",
      "epoch:  732, loss: 0.008538187481462955\n",
      "epoch:  733, loss: 0.00853100698441267\n",
      "epoch:  734, loss: 0.008529195562005043\n",
      "epoch:  735, loss: 0.008528265170753002\n",
      "epoch:  736, loss: 0.008521943353116512\n",
      "epoch:  737, loss: 0.008519344963133335\n",
      "epoch:  738, loss: 0.00851832702755928\n",
      "epoch:  739, loss: 0.00851326435804367\n",
      "epoch:  740, loss: 0.008509546518325806\n",
      "epoch:  741, loss: 0.008508419618010521\n",
      "epoch:  742, loss: 0.008505077101290226\n",
      "epoch:  743, loss: 0.00849981140345335\n",
      "epoch:  744, loss: 0.008498537354171276\n",
      "epoch:  745, loss: 0.008496126160025597\n",
      "epoch:  746, loss: 0.008489873260259628\n",
      "epoch:  747, loss: 0.008488484658300877\n",
      "epoch:  748, loss: 0.008487487211823463\n",
      "epoch:  749, loss: 0.008479633368551731\n",
      "epoch:  750, loss: 0.008478048257529736\n",
      "epoch:  751, loss: 0.008477064780890942\n",
      "epoch:  752, loss: 0.00846913643181324\n",
      "epoch:  753, loss: 0.00846728216856718\n",
      "epoch:  754, loss: 0.008466288447380066\n",
      "epoch:  755, loss: 0.008459437638521194\n",
      "epoch:  756, loss: 0.008456523530185223\n",
      "epoch:  757, loss: 0.008455383591353893\n",
      "epoch:  758, loss: 0.008449588902294636\n",
      "epoch:  759, loss: 0.0084455581381917\n",
      "epoch:  760, loss: 0.008444301784038544\n",
      "epoch:  761, loss: 0.008442121557891369\n",
      "epoch:  762, loss: 0.008434942923486233\n",
      "epoch:  763, loss: 0.008433209732174873\n",
      "epoch:  764, loss: 0.008432161062955856\n",
      "epoch:  765, loss: 0.008424966596066952\n",
      "epoch:  766, loss: 0.008421997539699078\n",
      "epoch:  767, loss: 0.008420843631029129\n",
      "epoch:  768, loss: 0.008416613563895226\n",
      "epoch:  769, loss: 0.008410908281803131\n",
      "epoch:  770, loss: 0.008409392088651657\n",
      "epoch:  771, loss: 0.008408332243561745\n",
      "epoch:  772, loss: 0.008401048369705677\n",
      "epoch:  773, loss: 0.008397947996854782\n",
      "epoch:  774, loss: 0.008396657183766365\n",
      "epoch:  775, loss: 0.008395764976739883\n",
      "epoch:  776, loss: 0.008387093432247639\n",
      "epoch:  777, loss: 0.008385322988033295\n",
      "epoch:  778, loss: 0.008384262211620808\n",
      "epoch:  779, loss: 0.008378244936466217\n",
      "epoch:  780, loss: 0.00837420392781496\n",
      "epoch:  781, loss: 0.008372915908694267\n",
      "epoch:  782, loss: 0.008371885865926743\n",
      "epoch:  783, loss: 0.008363528177142143\n",
      "epoch:  784, loss: 0.008361497893929482\n",
      "epoch:  785, loss: 0.008360340259969234\n",
      "epoch:  786, loss: 0.00835482683032751\n",
      "epoch:  787, loss: 0.008350200951099396\n",
      "epoch:  788, loss: 0.008348751813173294\n",
      "epoch:  789, loss: 0.008348489180207253\n",
      "epoch:  790, loss: 0.008338837884366512\n",
      "epoch:  791, loss: 0.008336909115314484\n",
      "epoch:  792, loss: 0.008335801772773266\n",
      "epoch:  793, loss: 0.008328484371304512\n",
      "epoch:  794, loss: 0.008325181901454926\n",
      "epoch:  795, loss: 0.00832392182201147\n",
      "epoch:  796, loss: 0.008319931104779243\n",
      "epoch:  797, loss: 0.008313805796205997\n",
      "epoch:  798, loss: 0.008312198333442211\n",
      "epoch:  799, loss: 0.008311117999255657\n",
      "epoch:  800, loss: 0.008303283713757992\n",
      "epoch:  801, loss: 0.008300554007291794\n",
      "epoch:  802, loss: 0.008299393579363823\n",
      "epoch:  803, loss: 0.008294071070849895\n",
      "epoch:  804, loss: 0.00828914437443018\n",
      "epoch:  805, loss: 0.00828771386295557\n",
      "epoch:  806, loss: 0.008286681957542896\n",
      "epoch:  807, loss: 0.008279181085526943\n",
      "epoch:  808, loss: 0.008276185020804405\n",
      "epoch:  809, loss: 0.008274960331618786\n",
      "epoch:  810, loss: 0.008270497433841228\n",
      "epoch:  811, loss: 0.008264507167041302\n",
      "epoch:  812, loss: 0.008262952789664268\n",
      "epoch:  813, loss: 0.008261877112090588\n",
      "epoch:  814, loss: 0.008253014646470547\n",
      "epoch:  815, loss: 0.008250965736806393\n",
      "epoch:  816, loss: 0.008249830454587936\n",
      "epoch:  817, loss: 0.008242141455411911\n",
      "epoch:  818, loss: 0.00823896937072277\n",
      "epoch:  819, loss: 0.008237667381763458\n",
      "epoch:  820, loss: 0.008232353255152702\n",
      "epoch:  821, loss: 0.008226909674704075\n",
      "epoch:  822, loss: 0.00822538509964943\n",
      "epoch:  823, loss: 0.008224285207688808\n",
      "epoch:  824, loss: 0.008216322399675846\n",
      "epoch:  825, loss: 0.008213335648179054\n",
      "epoch:  826, loss: 0.00821208767592907\n",
      "epoch:  827, loss: 0.008206818252801895\n",
      "epoch:  828, loss: 0.008201518096029758\n",
      "epoch:  829, loss: 0.008199954405426979\n",
      "epoch:  830, loss: 0.008198871277272701\n",
      "epoch:  831, loss: 0.008190013468265533\n",
      "epoch:  832, loss: 0.008187872357666492\n",
      "epoch:  833, loss: 0.008186702616512775\n",
      "epoch:  834, loss: 0.008179701864719391\n",
      "epoch:  835, loss: 0.008176039904356003\n",
      "epoch:  836, loss: 0.008174717426300049\n",
      "epoch:  837, loss: 0.008173910900950432\n",
      "epoch:  838, loss: 0.008164956234395504\n",
      "epoch:  839, loss: 0.008162859827280045\n",
      "epoch:  840, loss: 0.008161727339029312\n",
      "epoch:  841, loss: 0.00815504789352417\n",
      "epoch:  842, loss: 0.00815118569880724\n",
      "epoch:  843, loss: 0.008149786852300167\n",
      "epoch:  844, loss: 0.008148721419274807\n",
      "epoch:  845, loss: 0.008140256628394127\n",
      "epoch:  846, loss: 0.00813799537718296\n",
      "epoch:  847, loss: 0.008136793039739132\n",
      "epoch:  848, loss: 0.00813137087970972\n",
      "epoch:  849, loss: 0.008126466535031796\n",
      "epoch:  850, loss: 0.008124972693622112\n",
      "epoch:  851, loss: 0.008123885840177536\n",
      "epoch:  852, loss: 0.008116034790873528\n",
      "epoch:  853, loss: 0.00811326876282692\n",
      "epoch:  854, loss: 0.008112010546028614\n",
      "epoch:  855, loss: 0.008110202848911285\n",
      "epoch:  856, loss: 0.008102454245090485\n",
      "epoch:  857, loss: 0.008100318722426891\n",
      "epoch:  858, loss: 0.008099151775240898\n",
      "epoch:  859, loss: 0.008094968274235725\n",
      "epoch:  860, loss: 0.008089251816272736\n",
      "epoch:  861, loss: 0.008087602443993092\n",
      "epoch:  862, loss: 0.008086505346000195\n",
      "epoch:  863, loss: 0.008081763051450253\n",
      "epoch:  864, loss: 0.008076678961515427\n",
      "epoch:  865, loss: 0.008074972778558731\n",
      "epoch:  866, loss: 0.008073893375694752\n",
      "epoch:  867, loss: 0.008067385293543339\n",
      "epoch:  868, loss: 0.008063689805567265\n",
      "epoch:  869, loss: 0.00806230679154396\n",
      "epoch:  870, loss: 0.008061261847615242\n",
      "epoch:  871, loss: 0.008053246885538101\n",
      "epoch:  872, loss: 0.008050844073295593\n",
      "epoch:  873, loss: 0.008049584925174713\n",
      "epoch:  874, loss: 0.008048907853662968\n",
      "epoch:  875, loss: 0.008040253072977066\n",
      "epoch:  876, loss: 0.0080381128937006\n",
      "epoch:  877, loss: 0.008036976680159569\n",
      "epoch:  878, loss: 0.008031207136809826\n",
      "epoch:  879, loss: 0.0080269118770957\n",
      "epoch:  880, loss: 0.008025400340557098\n",
      "epoch:  881, loss: 0.008024346083402634\n",
      "epoch:  882, loss: 0.008017072454094887\n",
      "epoch:  883, loss: 0.008014105260372162\n",
      "epoch:  884, loss: 0.008012794889509678\n",
      "epoch:  885, loss: 0.008011767640709877\n",
      "epoch:  886, loss: 0.008004608564078808\n",
      "epoch:  887, loss: 0.008001653477549553\n",
      "epoch:  888, loss: 0.008000360801815987\n",
      "epoch:  889, loss: 0.007999353110790253\n",
      "epoch:  890, loss: 0.007991242222487926\n",
      "epoch:  891, loss: 0.007989175617694855\n",
      "epoch:  892, loss: 0.007988056167960167\n",
      "epoch:  893, loss: 0.00798441469669342\n",
      "epoch:  894, loss: 0.007978525012731552\n",
      "epoch:  895, loss: 0.007976869121193886\n",
      "epoch:  896, loss: 0.007975807413458824\n",
      "epoch:  897, loss: 0.007969127967953682\n",
      "epoch:  898, loss: 0.007965912111103535\n",
      "epoch:  899, loss: 0.007964622229337692\n",
      "epoch:  900, loss: 0.007963641546666622\n",
      "epoch:  901, loss: 0.007956157438457012\n",
      "epoch:  902, loss: 0.007953732274472713\n",
      "epoch:  903, loss: 0.007952570915222168\n",
      "epoch:  904, loss: 0.007948461920022964\n",
      "epoch:  905, loss: 0.00794319435954094\n",
      "epoch:  906, loss: 0.00794155802577734\n",
      "epoch:  907, loss: 0.007940507493913174\n",
      "epoch:  908, loss: 0.007933726534247398\n",
      "epoch:  909, loss: 0.007930661551654339\n",
      "epoch:  910, loss: 0.007929395884275436\n",
      "epoch:  911, loss: 0.007928257808089256\n",
      "epoch:  912, loss: 0.007920312695205212\n",
      "epoch:  913, loss: 0.007918331772089005\n",
      "epoch:  914, loss: 0.007917228154838085\n",
      "epoch:  915, loss: 0.007912903092801571\n",
      "epoch:  916, loss: 0.0079078059643507\n",
      "epoch:  917, loss: 0.007906254380941391\n",
      "epoch:  918, loss: 0.007905223406851292\n",
      "epoch:  919, loss: 0.007898780517280102\n",
      "epoch:  920, loss: 0.007895147427916527\n",
      "epoch:  921, loss: 0.007893755100667477\n",
      "epoch:  922, loss: 0.007892714813351631\n",
      "epoch:  923, loss: 0.007888109423220158\n",
      "epoch:  924, loss: 0.007883035577833652\n",
      "epoch:  925, loss: 0.007881495170295238\n",
      "epoch:  926, loss: 0.007880393415689468\n",
      "epoch:  927, loss: 0.007873733527958393\n",
      "epoch:  928, loss: 0.007870466448366642\n",
      "epoch:  929, loss: 0.007869021035730839\n",
      "epoch:  930, loss: 0.007867957465350628\n",
      "epoch:  931, loss: 0.007862801663577557\n",
      "epoch:  932, loss: 0.007858146913349628\n",
      "epoch:  933, loss: 0.007856611162424088\n",
      "epoch:  934, loss: 0.007855537347495556\n",
      "epoch:  935, loss: 0.00785036850720644\n",
      "epoch:  936, loss: 0.007845588028430939\n",
      "epoch:  937, loss: 0.007844011299312115\n",
      "epoch:  938, loss: 0.007842912338674068\n",
      "epoch:  939, loss: 0.007840578444302082\n",
      "epoch:  940, loss: 0.007833545096218586\n",
      "epoch:  941, loss: 0.00783165916800499\n",
      "epoch:  942, loss: 0.007830467075109482\n",
      "epoch:  943, loss: 0.007826519198715687\n",
      "epoch:  944, loss: 0.00782076921314001\n",
      "epoch:  945, loss: 0.007818974554538727\n",
      "epoch:  946, loss: 0.007817910984158516\n",
      "epoch:  947, loss: 0.007814704440534115\n",
      "epoch:  948, loss: 0.007808291353285313\n",
      "epoch:  949, loss: 0.007806381210684776\n",
      "epoch:  950, loss: 0.007805251982063055\n",
      "epoch:  951, loss: 0.00779600627720356\n",
      "epoch:  952, loss: 0.007794029079377651\n",
      "epoch:  953, loss: 0.007792788092046976\n",
      "epoch:  954, loss: 0.007790412753820419\n",
      "epoch:  955, loss: 0.007782931439578533\n",
      "epoch:  956, loss: 0.007780752144753933\n",
      "epoch:  957, loss: 0.0077794636599719524\n",
      "epoch:  958, loss: 0.00777839682996273\n",
      "epoch:  959, loss: 0.007771893870085478\n",
      "epoch:  960, loss: 0.007767965085804462\n",
      "epoch:  961, loss: 0.007766372989863157\n",
      "epoch:  962, loss: 0.007765256334096193\n",
      "epoch:  963, loss: 0.00775905791670084\n",
      "epoch:  964, loss: 0.00775493960827589\n",
      "epoch:  965, loss: 0.007753391750156879\n",
      "epoch:  966, loss: 0.007752280682325363\n",
      "epoch:  967, loss: 0.007746586110442877\n",
      "epoch:  968, loss: 0.007742171175777912\n",
      "epoch:  969, loss: 0.0077405706979334354\n",
      "epoch:  970, loss: 0.007739427033811808\n",
      "epoch:  971, loss: 0.00773550383746624\n",
      "epoch:  972, loss: 0.007729727774858475\n",
      "epoch:  973, loss: 0.007727805525064468\n",
      "epoch:  974, loss: 0.007726613432168961\n",
      "epoch:  975, loss: 0.007725575007498264\n",
      "epoch:  976, loss: 0.00771990604698658\n",
      "epoch:  977, loss: 0.007715761195868254\n",
      "epoch:  978, loss: 0.007714033126831055\n",
      "epoch:  979, loss: 0.007712883874773979\n",
      "epoch:  980, loss: 0.007711885962635279\n",
      "epoch:  981, loss: 0.007705667056143284\n",
      "epoch:  982, loss: 0.0077020335011184216\n",
      "epoch:  983, loss: 0.007700534071773291\n",
      "epoch:  984, loss: 0.007699447683990002\n",
      "epoch:  985, loss: 0.007695781998336315\n",
      "epoch:  986, loss: 0.007690110243856907\n",
      "epoch:  987, loss: 0.0076882545836269855\n",
      "epoch:  988, loss: 0.007687096018344164\n",
      "epoch:  989, loss: 0.007686092983931303\n",
      "epoch:  990, loss: 0.0076796007342636585\n",
      "epoch:  991, loss: 0.007676446344703436\n",
      "epoch:  992, loss: 0.007674938999116421\n",
      "epoch:  993, loss: 0.007673845160752535\n",
      "epoch:  994, loss: 0.007672864943742752\n",
      "epoch:  995, loss: 0.007665966171771288\n",
      "epoch:  996, loss: 0.007663089781999588\n",
      "epoch:  997, loss: 0.007661762181669474\n",
      "epoch:  998, loss: 0.0076607149094343185\n",
      "epoch:  999, loss: 0.007656832225620747\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size=X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "opt = torch_numopt.GradientDescentLS(model=model, lr=1, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"armijo\")\n",
    "# opt = torch_numopt.GradientDescentLS(model=model, lr=1, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"wolfe\")\n",
    "# opt = torch_numopt.GradientDescentLS(model=model, lr=1, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"strong-wolfe\")\n",
    "# opt = torch_numopt.GradientDescentLS(model=model, lr=1, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"goldstein\")\n",
    "\n",
    "all_loss = {}\n",
    "for epoch in range(1000):\n",
    "    print(\"epoch: \", epoch, end=\"\")\n",
    "    all_loss[epoch + 1] = 0\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y, loss_fn)\n",
    "\n",
    "        all_loss[epoch + 1] += loss\n",
    "    all_loss[epoch + 1] /= len(data_loader)\n",
    "    print(\", loss: {}\".format(all_loss[epoch + 1].detach().numpy().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics: R2 = 0.7932532270998592\n",
      "Test metrics:  R2 = 0.7384289812147724\n"
     ]
    }
   ],
   "source": [
    "pred_train = model.forward(X_train).detach()\n",
    "pred_test = model.forward(X_test).detach()\n",
    "print(f\"Train metrics: R2 = {r2_score(pred_train, y_train)}\")\n",
    "print(f\"Test metrics:  R2 = {r2_score(pred_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bisection line search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.16624541580677032\n",
      "epoch:  1, loss: 0.04118894040584564\n",
      "epoch:  2, loss: 0.0379139669239521\n",
      "epoch:  3, loss: 0.03787641227245331\n",
      "epoch:  4, loss: 0.03783929720520973\n",
      "epoch:  5, loss: 0.037801891565322876\n",
      "epoch:  6, loss: 0.03776330128312111\n",
      "epoch:  7, loss: 0.03772393614053726\n",
      "epoch:  8, loss: 0.0376836434006691\n",
      "epoch:  9, loss: 0.037623774260282516\n",
      "epoch:  10, loss: 0.03755364567041397\n",
      "epoch:  11, loss: 0.03734752535820007\n",
      "epoch:  12, loss: 0.03718986362218857\n",
      "epoch:  13, loss: 0.0371243841946125\n",
      "epoch:  14, loss: 0.03708893433213234\n",
      "epoch:  15, loss: 0.037007030099630356\n",
      "epoch:  16, loss: 0.03692341968417168\n",
      "epoch:  17, loss: 0.03684909641742706\n",
      "epoch:  18, loss: 0.03681043162941933\n",
      "epoch:  19, loss: 0.036716751754283905\n",
      "epoch:  20, loss: 0.036620527505874634\n",
      "epoch:  21, loss: 0.0365365706384182\n",
      "epoch:  22, loss: 0.03648823872208595\n",
      "epoch:  23, loss: 0.036377716809511185\n",
      "epoch:  24, loss: 0.03626425564289093\n",
      "epoch:  25, loss: 0.036181915551424026\n",
      "epoch:  26, loss: 0.03610225021839142\n",
      "epoch:  27, loss: 0.035967905074357986\n",
      "epoch:  28, loss: 0.03584003075957298\n",
      "epoch:  29, loss: 0.035782426595687866\n",
      "epoch:  30, loss: 0.03562444821000099\n",
      "epoch:  31, loss: 0.0354674756526947\n",
      "epoch:  32, loss: 0.03538309410214424\n",
      "epoch:  33, loss: 0.03531506657600403\n",
      "epoch:  34, loss: 0.035124827176332474\n",
      "epoch:  35, loss: 0.03494451567530632\n",
      "epoch:  36, loss: 0.03486032038927078\n",
      "epoch:  37, loss: 0.03463668376207352\n",
      "epoch:  38, loss: 0.034440670162439346\n",
      "epoch:  39, loss: 0.03432285785675049\n",
      "epoch:  40, loss: 0.034065116196870804\n",
      "epoch:  41, loss: 0.033930279314517975\n",
      "epoch:  42, loss: 0.03379836305975914\n",
      "epoch:  43, loss: 0.03367926552891731\n",
      "epoch:  44, loss: 0.033374056220054626\n",
      "epoch:  45, loss: 0.03321728855371475\n",
      "epoch:  46, loss: 0.033095959573984146\n",
      "epoch:  47, loss: 0.03274146467447281\n",
      "epoch:  48, loss: 0.03256147727370262\n",
      "epoch:  49, loss: 0.03242608532309532\n",
      "epoch:  50, loss: 0.032028552144765854\n",
      "epoch:  51, loss: 0.03183767944574356\n",
      "epoch:  52, loss: 0.031659841537475586\n",
      "epoch:  53, loss: 0.031237764284014702\n",
      "epoch:  54, loss: 0.031045209616422653\n",
      "epoch:  55, loss: 0.030604399740695953\n",
      "epoch:  56, loss: 0.030372262001037598\n",
      "epoch:  57, loss: 0.02998216450214386\n",
      "epoch:  58, loss: 0.02971465140581131\n",
      "epoch:  59, loss: 0.029445581138134003\n",
      "epoch:  60, loss: 0.029222941026091576\n",
      "epoch:  61, loss: 0.028780503198504448\n",
      "epoch:  62, loss: 0.02846764400601387\n",
      "epoch:  63, loss: 0.028154511004686356\n",
      "epoch:  64, loss: 0.02791454643011093\n",
      "epoch:  65, loss: 0.027572641149163246\n",
      "epoch:  66, loss: 0.027336446568369865\n",
      "epoch:  67, loss: 0.026824677363038063\n",
      "epoch:  68, loss: 0.02643056958913803\n",
      "epoch:  69, loss: 0.026066355407238007\n",
      "epoch:  70, loss: 0.025794703513383865\n",
      "epoch:  71, loss: 0.02545066364109516\n",
      "epoch:  72, loss: 0.02521696500480175\n",
      "epoch:  73, loss: 0.02442912571132183\n",
      "epoch:  74, loss: 0.02417541667819023\n",
      "epoch:  75, loss: 0.023347828537225723\n",
      "epoch:  76, loss: 0.02308580093085766\n",
      "epoch:  77, loss: 0.022160258144140244\n",
      "epoch:  78, loss: 0.021881232038140297\n",
      "epoch:  79, loss: 0.021610992029309273\n",
      "epoch:  80, loss: 0.021345393732190132\n",
      "epoch:  81, loss: 0.02108309045433998\n",
      "epoch:  82, loss: 0.02072202041745186\n",
      "epoch:  83, loss: 0.02042028121650219\n",
      "epoch:  84, loss: 0.019400442019104958\n",
      "epoch:  85, loss: 0.01867096871137619\n",
      "epoch:  86, loss: 0.018266692757606506\n",
      "epoch:  87, loss: 0.017361223697662354\n",
      "epoch:  88, loss: 0.017175650224089622\n",
      "epoch:  89, loss: 0.01629776321351528\n",
      "epoch:  90, loss: 0.016145091503858566\n",
      "epoch:  91, loss: 0.015375485643744469\n",
      "epoch:  92, loss: 0.015196489170193672\n",
      "epoch:  93, loss: 0.014730505645275116\n",
      "epoch:  94, loss: 0.014420983381569386\n",
      "epoch:  95, loss: 0.014294593594968319\n",
      "epoch:  96, loss: 0.013882155530154705\n",
      "epoch:  97, loss: 0.013607816770672798\n",
      "epoch:  98, loss: 0.013502913527190685\n",
      "epoch:  99, loss: 0.013255830854177475\n",
      "epoch:  100, loss: 0.01313877385109663\n",
      "epoch:  101, loss: 0.013011008501052856\n",
      "epoch:  102, loss: 0.012929991818964481\n",
      "epoch:  103, loss: 0.012609506025910378\n",
      "epoch:  104, loss: 0.01240924559533596\n",
      "epoch:  105, loss: 0.012338194064795971\n",
      "epoch:  106, loss: 0.01217862032353878\n",
      "epoch:  107, loss: 0.01209847442805767\n",
      "epoch:  108, loss: 0.0120096979662776\n",
      "epoch:  109, loss: 0.011946877464652061\n",
      "epoch:  110, loss: 0.011818592436611652\n",
      "epoch:  111, loss: 0.01173928752541542\n",
      "epoch:  112, loss: 0.011665464378893375\n",
      "epoch:  113, loss: 0.011609315872192383\n",
      "epoch:  114, loss: 0.011537258513271809\n",
      "epoch:  115, loss: 0.011487378738820553\n",
      "epoch:  116, loss: 0.011417742818593979\n",
      "epoch:  117, loss: 0.011372820474207401\n",
      "epoch:  118, loss: 0.011280996724963188\n",
      "epoch:  119, loss: 0.011214111931622028\n",
      "epoch:  120, loss: 0.011158610694110394\n",
      "epoch:  121, loss: 0.011116092093288898\n",
      "epoch:  122, loss: 0.011063183657824993\n",
      "epoch:  123, loss: 0.011023479513823986\n",
      "epoch:  124, loss: 0.010974224656820297\n",
      "epoch:  125, loss: 0.01093650795519352\n",
      "epoch:  126, loss: 0.010890811681747437\n",
      "epoch:  127, loss: 0.010854922235012054\n",
      "epoch:  128, loss: 0.010814341716468334\n",
      "epoch:  129, loss: 0.010779215954244137\n",
      "epoch:  130, loss: 0.01075219176709652\n",
      "epoch:  131, loss: 0.010688479989767075\n",
      "epoch:  132, loss: 0.010648911818861961\n",
      "epoch:  133, loss: 0.01061171293258667\n",
      "epoch:  134, loss: 0.010586190968751907\n",
      "epoch:  135, loss: 0.010552781634032726\n",
      "epoch:  136, loss: 0.010527096688747406\n",
      "epoch:  137, loss: 0.010498110204935074\n",
      "epoch:  138, loss: 0.01047790888696909\n",
      "epoch:  139, loss: 0.010421372950077057\n",
      "epoch:  140, loss: 0.01039815228432417\n",
      "epoch:  141, loss: 0.010351271368563175\n",
      "epoch:  142, loss: 0.010325654409825802\n",
      "epoch:  143, loss: 0.010289319790899754\n",
      "epoch:  144, loss: 0.01025957427918911\n",
      "epoch:  145, loss: 0.010235891677439213\n",
      "epoch:  146, loss: 0.010219664312899113\n",
      "epoch:  147, loss: 0.010200309567153454\n",
      "epoch:  148, loss: 0.01018636953085661\n",
      "epoch:  149, loss: 0.010146604850888252\n",
      "epoch:  150, loss: 0.010131831280887127\n",
      "epoch:  151, loss: 0.01009758748114109\n",
      "epoch:  152, loss: 0.01008201390504837\n",
      "epoch:  153, loss: 0.0100543312728405\n",
      "epoch:  154, loss: 0.01003686711192131\n",
      "epoch:  155, loss: 0.010013955645263195\n",
      "epoch:  156, loss: 0.009995359927415848\n",
      "epoch:  157, loss: 0.009980373084545135\n",
      "epoch:  158, loss: 0.009970348328351974\n",
      "epoch:  159, loss: 0.009961496107280254\n",
      "epoch:  160, loss: 0.009947801940143108\n",
      "epoch:  161, loss: 0.009938517585396767\n",
      "epoch:  162, loss: 0.009930243715643883\n",
      "epoch:  163, loss: 0.009917309507727623\n",
      "epoch:  164, loss: 0.009908649139106274\n",
      "epoch:  165, loss: 0.009898391552269459\n",
      "epoch:  166, loss: 0.009890283457934856\n",
      "epoch:  167, loss: 0.009868323802947998\n",
      "epoch:  168, loss: 0.009860320948064327\n",
      "epoch:  169, loss: 0.009840265847742558\n",
      "epoch:  170, loss: 0.00983238685876131\n",
      "epoch:  171, loss: 0.00981359276920557\n",
      "epoch:  172, loss: 0.009806365706026554\n",
      "epoch:  173, loss: 0.009788435883820057\n",
      "epoch:  174, loss: 0.009781445376574993\n",
      "epoch:  175, loss: 0.009764255955815315\n",
      "epoch:  176, loss: 0.00975746102631092\n",
      "epoch:  177, loss: 0.009741338901221752\n",
      "epoch:  178, loss: 0.009734862484037876\n",
      "epoch:  179, loss: 0.009720382280647755\n",
      "epoch:  180, loss: 0.009714266285300255\n",
      "epoch:  181, loss: 0.009701037779450417\n",
      "epoch:  182, loss: 0.009695146232843399\n",
      "epoch:  183, loss: 0.009682775475084782\n",
      "epoch:  184, loss: 0.009677434340119362\n",
      "epoch:  185, loss: 0.00966557115316391\n",
      "epoch:  186, loss: 0.00966087356209755\n",
      "epoch:  187, loss: 0.009645680896937847\n",
      "epoch:  188, loss: 0.00963145587593317\n",
      "epoch:  189, loss: 0.009626604616641998\n",
      "epoch:  190, loss: 0.009623123332858086\n",
      "epoch:  191, loss: 0.00960805919021368\n",
      "epoch:  192, loss: 0.009598061442375183\n",
      "epoch:  193, loss: 0.00959375873208046\n",
      "epoch:  194, loss: 0.009591088630259037\n",
      "epoch:  195, loss: 0.009577694348990917\n",
      "epoch:  196, loss: 0.00956968404352665\n",
      "epoch:  197, loss: 0.009565701708197594\n",
      "epoch:  198, loss: 0.009563074447214603\n",
      "epoch:  199, loss: 0.00954984501004219\n",
      "epoch:  200, loss: 0.009543241001665592\n",
      "epoch:  201, loss: 0.009539710357785225\n",
      "epoch:  202, loss: 0.009537169709801674\n",
      "epoch:  203, loss: 0.00953487679362297\n",
      "epoch:  204, loss: 0.009531551972031593\n",
      "epoch:  205, loss: 0.00952956173568964\n",
      "epoch:  206, loss: 0.009518145583570004\n",
      "epoch:  207, loss: 0.009512887336313725\n",
      "epoch:  208, loss: 0.009509905241429806\n",
      "epoch:  209, loss: 0.009507857263088226\n",
      "epoch:  210, loss: 0.009506016038358212\n",
      "epoch:  211, loss: 0.009503291919827461\n",
      "epoch:  212, loss: 0.009501748718321323\n",
      "epoch:  213, loss: 0.0094918766990304\n",
      "epoch:  214, loss: 0.009488439187407494\n",
      "epoch:  215, loss: 0.009485098533332348\n",
      "epoch:  216, loss: 0.009482060559093952\n",
      "epoch:  217, loss: 0.009478280320763588\n",
      "epoch:  218, loss: 0.00947587937116623\n",
      "epoch:  219, loss: 0.009472198784351349\n",
      "epoch:  220, loss: 0.009470043703913689\n",
      "epoch:  221, loss: 0.009466378949582577\n",
      "epoch:  222, loss: 0.00946441013365984\n",
      "epoch:  223, loss: 0.009460617788136005\n",
      "epoch:  224, loss: 0.009459024295210838\n",
      "epoch:  225, loss: 0.009453658945858479\n",
      "epoch:  226, loss: 0.009449459612369537\n",
      "epoch:  227, loss: 0.00944772083312273\n",
      "epoch:  228, loss: 0.009446407668292522\n",
      "epoch:  229, loss: 0.009445209056138992\n",
      "epoch:  230, loss: 0.009443595074117184\n",
      "epoch:  231, loss: 0.009442462585866451\n",
      "epoch:  232, loss: 0.009436700493097305\n",
      "epoch:  233, loss: 0.009433915838599205\n",
      "epoch:  234, loss: 0.00943189486861229\n",
      "epoch:  235, loss: 0.009429611265659332\n",
      "epoch:  236, loss: 0.009427089244127274\n",
      "epoch:  237, loss: 0.009425249882042408\n",
      "epoch:  238, loss: 0.009422456845641136\n",
      "epoch:  239, loss: 0.00942092202603817\n",
      "epoch:  240, loss: 0.009417137131094933\n",
      "epoch:  241, loss: 0.009412862360477448\n",
      "epoch:  242, loss: 0.009411382488906384\n",
      "epoch:  243, loss: 0.009410345926880836\n",
      "epoch:  244, loss: 0.009404562413692474\n",
      "epoch:  245, loss: 0.009402388706803322\n",
      "epoch:  246, loss: 0.009400094859302044\n",
      "epoch:  247, loss: 0.009398175403475761\n",
      "epoch:  248, loss: 0.009395209141075611\n",
      "epoch:  249, loss: 0.009393906220793724\n",
      "epoch:  250, loss: 0.009389321319758892\n",
      "epoch:  251, loss: 0.00938573107123375\n",
      "epoch:  252, loss: 0.00938416551798582\n",
      "epoch:  253, loss: 0.009383021853864193\n",
      "epoch:  254, loss: 0.00938198622316122\n",
      "epoch:  255, loss: 0.009380605071783066\n",
      "epoch:  256, loss: 0.009379653260111809\n",
      "epoch:  257, loss: 0.009374183602631092\n",
      "epoch:  258, loss: 0.009372161701321602\n",
      "epoch:  259, loss: 0.009369696490466595\n",
      "epoch:  260, loss: 0.009368188679218292\n",
      "epoch:  261, loss: 0.009364758618175983\n",
      "epoch:  262, loss: 0.009361125528812408\n",
      "epoch:  263, loss: 0.009359857067465782\n",
      "epoch:  264, loss: 0.0093590272590518\n",
      "epoch:  265, loss: 0.00935397669672966\n",
      "epoch:  266, loss: 0.009352178312838078\n",
      "epoch:  267, loss: 0.009349954314529896\n",
      "epoch:  268, loss: 0.00934866163879633\n",
      "epoch:  269, loss: 0.00934643018990755\n",
      "epoch:  270, loss: 0.009345311671495438\n",
      "epoch:  271, loss: 0.009342159144580364\n",
      "epoch:  272, loss: 0.009339171461760998\n",
      "epoch:  273, loss: 0.009337961673736572\n",
      "epoch:  274, loss: 0.009337091818451881\n",
      "epoch:  275, loss: 0.009336311370134354\n",
      "epoch:  276, loss: 0.009335342794656754\n",
      "epoch:  277, loss: 0.009334397502243519\n",
      "epoch:  278, loss: 0.009330066852271557\n",
      "epoch:  279, loss: 0.009328080341219902\n",
      "epoch:  280, loss: 0.009326402097940445\n",
      "epoch:  281, loss: 0.00932476855814457\n",
      "epoch:  282, loss: 0.009323131293058395\n",
      "epoch:  283, loss: 0.009321603924036026\n",
      "epoch:  284, loss: 0.009319867938756943\n",
      "epoch:  285, loss: 0.009318679571151733\n",
      "epoch:  286, loss: 0.00931683462113142\n",
      "epoch:  287, loss: 0.009315830655395985\n",
      "epoch:  288, loss: 0.009313071146607399\n",
      "epoch:  289, loss: 0.00931051280349493\n",
      "epoch:  290, loss: 0.009309536777436733\n",
      "epoch:  291, loss: 0.009308797307312489\n",
      "epoch:  292, loss: 0.009308126755058765\n",
      "epoch:  293, loss: 0.00930727832019329\n",
      "epoch:  294, loss: 0.009306569583714008\n",
      "epoch:  295, loss: 0.009303227998316288\n",
      "epoch:  296, loss: 0.009301516227424145\n",
      "epoch:  297, loss: 0.009300079196691513\n",
      "epoch:  298, loss: 0.009298770688474178\n",
      "epoch:  299, loss: 0.009297150187194347\n",
      "epoch:  300, loss: 0.009296041913330555\n",
      "epoch:  301, loss: 0.009294262155890465\n",
      "epoch:  302, loss: 0.009293345734477043\n",
      "epoch:  303, loss: 0.009290730580687523\n",
      "epoch:  304, loss: 0.009288224391639233\n",
      "epoch:  305, loss: 0.009287213906645775\n",
      "epoch:  306, loss: 0.009286441840231419\n",
      "epoch:  307, loss: 0.009285759180784225\n",
      "epoch:  308, loss: 0.009284889325499535\n",
      "epoch:  309, loss: 0.00928414799273014\n",
      "epoch:  310, loss: 0.00928080640733242\n",
      "epoch:  311, loss: 0.009278960525989532\n",
      "epoch:  312, loss: 0.009277542121708393\n",
      "epoch:  313, loss: 0.00927619356662035\n",
      "epoch:  314, loss: 0.009274451993405819\n",
      "epoch:  315, loss: 0.009273398667573929\n",
      "epoch:  316, loss: 0.009271056391298771\n",
      "epoch:  317, loss: 0.00926828384399414\n",
      "epoch:  318, loss: 0.009267417713999748\n",
      "epoch:  319, loss: 0.009266750887036324\n",
      "epoch:  320, loss: 0.009263387881219387\n",
      "epoch:  321, loss: 0.009261741302907467\n",
      "epoch:  322, loss: 0.009260202758014202\n",
      "epoch:  323, loss: 0.009258981794118881\n",
      "epoch:  324, loss: 0.00925685279071331\n",
      "epoch:  325, loss: 0.00925398524850607\n",
      "epoch:  326, loss: 0.009253013879060745\n",
      "epoch:  327, loss: 0.009252416901290417\n",
      "epoch:  328, loss: 0.009248523972928524\n",
      "epoch:  329, loss: 0.009247378446161747\n",
      "epoch:  330, loss: 0.009245616383850574\n",
      "epoch:  331, loss: 0.00924470741301775\n",
      "epoch:  332, loss: 0.009242025204002857\n",
      "epoch:  333, loss: 0.009239813312888145\n",
      "epoch:  334, loss: 0.00923878513276577\n",
      "epoch:  335, loss: 0.009238049387931824\n",
      "epoch:  336, loss: 0.00923738069832325\n",
      "epoch:  337, loss: 0.009236454963684082\n",
      "epoch:  338, loss: 0.009235815145075321\n",
      "epoch:  339, loss: 0.00923212245106697\n",
      "epoch:  340, loss: 0.009230748750269413\n",
      "epoch:  341, loss: 0.009229186922311783\n",
      "epoch:  342, loss: 0.009228158742189407\n",
      "epoch:  343, loss: 0.009225992485880852\n",
      "epoch:  344, loss: 0.009223297238349915\n",
      "epoch:  345, loss: 0.009222337044775486\n",
      "epoch:  346, loss: 0.009221610613167286\n",
      "epoch:  347, loss: 0.009220944717526436\n",
      "epoch:  348, loss: 0.009220033884048462\n",
      "epoch:  349, loss: 0.009219367057085037\n",
      "epoch:  350, loss: 0.009216150268912315\n",
      "epoch:  351, loss: 0.009214158169925213\n",
      "epoch:  352, loss: 0.00921257957816124\n",
      "epoch:  353, loss: 0.009211513213813305\n",
      "epoch:  354, loss: 0.009208635427057743\n",
      "epoch:  355, loss: 0.009206736460328102\n",
      "epoch:  356, loss: 0.009205484762787819\n",
      "epoch:  357, loss: 0.009204179048538208\n",
      "epoch:  358, loss: 0.009201845154166222\n",
      "epoch:  359, loss: 0.009199499152600765\n",
      "epoch:  360, loss: 0.009198540821671486\n",
      "epoch:  361, loss: 0.009197862818837166\n",
      "epoch:  362, loss: 0.009197249077260494\n",
      "epoch:  363, loss: 0.009196644648909569\n",
      "epoch:  364, loss: 0.009195796214044094\n",
      "epoch:  365, loss: 0.009195182472467422\n",
      "epoch:  366, loss: 0.009191684424877167\n",
      "epoch:  367, loss: 0.009190392680466175\n",
      "epoch:  368, loss: 0.00918878335505724\n",
      "epoch:  369, loss: 0.009187828749418259\n",
      "epoch:  370, loss: 0.00918545201420784\n",
      "epoch:  371, loss: 0.009182961657643318\n",
      "epoch:  372, loss: 0.009182045236229897\n",
      "epoch:  373, loss: 0.00918148085474968\n",
      "epoch:  374, loss: 0.009177221916615963\n",
      "epoch:  375, loss: 0.009176505729556084\n",
      "epoch:  376, loss: 0.00917264074087143\n",
      "epoch:  377, loss: 0.009171424433588982\n",
      "epoch:  378, loss: 0.009169630706310272\n",
      "epoch:  379, loss: 0.009168761782348156\n",
      "epoch:  380, loss: 0.009165666997432709\n",
      "epoch:  381, loss: 0.009163710288703442\n",
      "epoch:  382, loss: 0.009162205271422863\n",
      "epoch:  383, loss: 0.009161013178527355\n",
      "epoch:  384, loss: 0.009159214794635773\n",
      "epoch:  385, loss: 0.009158259257674217\n",
      "epoch:  386, loss: 0.009154761210083961\n",
      "epoch:  387, loss: 0.00915300752967596\n",
      "epoch:  388, loss: 0.009151298552751541\n",
      "epoch:  389, loss: 0.009150146506726742\n",
      "epoch:  390, loss: 0.00914821308106184\n",
      "epoch:  391, loss: 0.00914726685732603\n",
      "epoch:  392, loss: 0.009144078008830547\n",
      "epoch:  393, loss: 0.00914178416132927\n",
      "epoch:  394, loss: 0.009140235371887684\n",
      "epoch:  395, loss: 0.009138815104961395\n",
      "epoch:  396, loss: 0.009135951288044453\n",
      "epoch:  397, loss: 0.009133449755609035\n",
      "epoch:  398, loss: 0.009132367558777332\n",
      "epoch:  399, loss: 0.00913161225616932\n",
      "epoch:  400, loss: 0.009130888618528843\n",
      "epoch:  401, loss: 0.009129814803600311\n",
      "epoch:  402, loss: 0.009129149839282036\n",
      "epoch:  403, loss: 0.009124506264925003\n",
      "epoch:  404, loss: 0.009123183786869049\n",
      "epoch:  405, loss: 0.009120064787566662\n",
      "epoch:  406, loss: 0.009116871282458305\n",
      "epoch:  407, loss: 0.009115414693951607\n",
      "epoch:  408, loss: 0.009113602340221405\n",
      "epoch:  409, loss: 0.009111610241234303\n",
      "epoch:  410, loss: 0.009110101498663425\n",
      "epoch:  411, loss: 0.009107129648327827\n",
      "epoch:  412, loss: 0.009103585034608841\n",
      "epoch:  413, loss: 0.00910238828510046\n",
      "epoch:  414, loss: 0.00910167209804058\n",
      "epoch:  415, loss: 0.009097103960812092\n",
      "epoch:  416, loss: 0.009095439687371254\n",
      "epoch:  417, loss: 0.00909314677119255\n",
      "epoch:  418, loss: 0.009091983549296856\n",
      "epoch:  419, loss: 0.009088097140192986\n",
      "epoch:  420, loss: 0.00908573903143406\n",
      "epoch:  421, loss: 0.009084510616958141\n",
      "epoch:  422, loss: 0.009083699434995651\n",
      "epoch:  423, loss: 0.009082920849323273\n",
      "epoch:  424, loss: 0.009081903845071793\n",
      "epoch:  425, loss: 0.009081047959625721\n",
      "epoch:  426, loss: 0.009076195769011974\n",
      "epoch:  427, loss: 0.009074652567505836\n",
      "epoch:  428, loss: 0.009072558023035526\n",
      "epoch:  429, loss: 0.009071415290236473\n",
      "epoch:  430, loss: 0.009068678133189678\n",
      "epoch:  431, loss: 0.00906552653759718\n",
      "epoch:  432, loss: 0.00906434841454029\n",
      "epoch:  433, loss: 0.009063483215868473\n",
      "epoch:  434, loss: 0.009062706492841244\n",
      "epoch:  435, loss: 0.009061703458428383\n",
      "epoch:  436, loss: 0.009060800075531006\n",
      "epoch:  437, loss: 0.00905600469559431\n",
      "epoch:  438, loss: 0.00905457790941\n",
      "epoch:  439, loss: 0.009051363915205002\n",
      "epoch:  440, loss: 0.009048575535416603\n",
      "epoch:  441, loss: 0.009047404862940311\n",
      "epoch:  442, loss: 0.009046565741300583\n",
      "epoch:  443, loss: 0.00904580857604742\n",
      "epoch:  444, loss: 0.009044828824698925\n",
      "epoch:  445, loss: 0.009043995290994644\n",
      "epoch:  446, loss: 0.009039662778377533\n",
      "epoch:  447, loss: 0.009038027375936508\n",
      "epoch:  448, loss: 0.009035847149789333\n",
      "epoch:  449, loss: 0.009034779854118824\n",
      "epoch:  450, loss: 0.009031003341078758\n",
      "epoch:  451, loss: 0.009028267115354538\n",
      "epoch:  452, loss: 0.009026983752846718\n",
      "epoch:  453, loss: 0.009026049636304379\n",
      "epoch:  454, loss: 0.00902517419308424\n",
      "epoch:  455, loss: 0.00902408454567194\n",
      "epoch:  456, loss: 0.009023084305226803\n",
      "epoch:  457, loss: 0.009018085896968842\n",
      "epoch:  458, loss: 0.009016196243464947\n",
      "epoch:  459, loss: 0.009012877941131592\n",
      "epoch:  460, loss: 0.009008743800222874\n",
      "epoch:  461, loss: 0.00900715310126543\n",
      "epoch:  462, loss: 0.009006264619529247\n",
      "epoch:  463, loss: 0.008999571204185486\n",
      "epoch:  464, loss: 0.008998305536806583\n",
      "epoch:  465, loss: 0.008994185365736485\n",
      "epoch:  466, loss: 0.008990696631371975\n",
      "epoch:  467, loss: 0.008989166468381882\n",
      "epoch:  468, loss: 0.00898811500519514\n",
      "epoch:  469, loss: 0.008987108245491982\n",
      "epoch:  470, loss: 0.008985856547951698\n",
      "epoch:  471, loss: 0.008984696120023727\n",
      "epoch:  472, loss: 0.008978926576673985\n",
      "epoch:  473, loss: 0.008976784534752369\n",
      "epoch:  474, loss: 0.008974147960543633\n",
      "epoch:  475, loss: 0.008972818031907082\n",
      "epoch:  476, loss: 0.00896886270493269\n",
      "epoch:  477, loss: 0.008965537883341312\n",
      "epoch:  478, loss: 0.008964164182543755\n",
      "epoch:  479, loss: 0.00896324310451746\n",
      "epoch:  480, loss: 0.008962349034845829\n",
      "epoch:  481, loss: 0.008961312472820282\n",
      "epoch:  482, loss: 0.008960279636085033\n",
      "epoch:  483, loss: 0.008955752477049828\n",
      "epoch:  484, loss: 0.008953521028161049\n",
      "epoch:  485, loss: 0.00895177572965622\n",
      "epoch:  486, loss: 0.008949984796345234\n",
      "epoch:  487, loss: 0.00894783716648817\n",
      "epoch:  488, loss: 0.008946488611400127\n",
      "epoch:  489, loss: 0.00894413236528635\n",
      "epoch:  490, loss: 0.008943022228777409\n",
      "epoch:  491, loss: 0.00893902312964201\n",
      "epoch:  492, loss: 0.008936496451497078\n",
      "epoch:  493, loss: 0.008934678509831429\n",
      "epoch:  494, loss: 0.008932987228035927\n",
      "epoch:  495, loss: 0.008930671960115433\n",
      "epoch:  496, loss: 0.008929399773478508\n",
      "epoch:  497, loss: 0.008925647474825382\n",
      "epoch:  498, loss: 0.008922737091779709\n",
      "epoch:  499, loss: 0.008921420201659203\n",
      "epoch:  500, loss: 0.008920492604374886\n",
      "epoch:  501, loss: 0.008919637650251389\n",
      "epoch:  502, loss: 0.008918571285903454\n",
      "epoch:  503, loss: 0.008917585015296936\n",
      "epoch:  504, loss: 0.008912879973649979\n",
      "epoch:  505, loss: 0.008911021053791046\n",
      "epoch:  506, loss: 0.008908897638320923\n",
      "epoch:  507, loss: 0.008907615207135677\n",
      "epoch:  508, loss: 0.008904630318284035\n",
      "epoch:  509, loss: 0.00890131015330553\n",
      "epoch:  510, loss: 0.008900013752281666\n",
      "epoch:  511, loss: 0.008899119682610035\n",
      "epoch:  512, loss: 0.008898262865841389\n",
      "epoch:  513, loss: 0.008897477760910988\n",
      "epoch:  514, loss: 0.008896421641111374\n",
      "epoch:  515, loss: 0.00889547262340784\n",
      "epoch:  516, loss: 0.008890335448086262\n",
      "epoch:  517, loss: 0.008889025077223778\n",
      "epoch:  518, loss: 0.008884351700544357\n",
      "epoch:  519, loss: 0.008882641792297363\n",
      "epoch:  520, loss: 0.008879384025931358\n",
      "epoch:  521, loss: 0.008876290172338486\n",
      "epoch:  522, loss: 0.008874534629285336\n",
      "epoch:  523, loss: 0.008872833102941513\n",
      "epoch:  524, loss: 0.008868785575032234\n",
      "epoch:  525, loss: 0.008866453543305397\n",
      "epoch:  526, loss: 0.008864514529705048\n",
      "epoch:  527, loss: 0.008863012306392193\n",
      "epoch:  528, loss: 0.008860141970217228\n",
      "epoch:  529, loss: 0.008856705389916897\n",
      "epoch:  530, loss: 0.00885537639260292\n",
      "epoch:  531, loss: 0.008854446932673454\n",
      "epoch:  532, loss: 0.008853605017066002\n",
      "epoch:  533, loss: 0.008852795697748661\n",
      "epoch:  534, loss: 0.008851993829011917\n",
      "epoch:  535, loss: 0.008850968442857265\n",
      "epoch:  536, loss: 0.008850005455315113\n",
      "epoch:  537, loss: 0.008845051750540733\n",
      "epoch:  538, loss: 0.008843549527227879\n",
      "epoch:  539, loss: 0.008839747868478298\n",
      "epoch:  540, loss: 0.008837001398205757\n",
      "epoch:  541, loss: 0.00883517600595951\n",
      "epoch:  542, loss: 0.008833501487970352\n",
      "epoch:  543, loss: 0.008830627426505089\n",
      "epoch:  544, loss: 0.00882691703736782\n",
      "epoch:  545, loss: 0.00882558710873127\n",
      "epoch:  546, loss: 0.008824684657156467\n",
      "epoch:  547, loss: 0.008823818527162075\n",
      "epoch:  548, loss: 0.008822981268167496\n",
      "epoch:  549, loss: 0.008821722120046616\n",
      "epoch:  550, loss: 0.00882093608379364\n",
      "epoch:  551, loss: 0.008814934641122818\n",
      "epoch:  552, loss: 0.008814023807644844\n",
      "epoch:  553, loss: 0.008813164196908474\n",
      "epoch:  554, loss: 0.00881232786923647\n",
      "epoch:  555, loss: 0.008811223320662975\n",
      "epoch:  556, loss: 0.00881031434983015\n",
      "epoch:  557, loss: 0.008805150166153908\n",
      "epoch:  558, loss: 0.008803678676486015\n",
      "epoch:  559, loss: 0.008799065835773945\n",
      "epoch:  560, loss: 0.008796967566013336\n",
      "epoch:  561, loss: 0.008793067187070847\n",
      "epoch:  562, loss: 0.008790293708443642\n",
      "epoch:  563, loss: 0.00878895539790392\n",
      "epoch:  564, loss: 0.008788050152361393\n",
      "epoch:  565, loss: 0.008787194266915321\n",
      "epoch:  566, loss: 0.008786369115114212\n",
      "epoch:  567, loss: 0.00878551322966814\n",
      "epoch:  568, loss: 0.008784396573901176\n",
      "epoch:  569, loss: 0.008783437311649323\n",
      "epoch:  570, loss: 0.008778238669037819\n",
      "epoch:  571, loss: 0.00877656601369381\n",
      "epoch:  572, loss: 0.008772053755819798\n",
      "epoch:  573, loss: 0.008769643492996693\n",
      "epoch:  574, loss: 0.008767290972173214\n",
      "epoch:  575, loss: 0.00876588374376297\n",
      "epoch:  576, loss: 0.00876044761389494\n",
      "epoch:  577, loss: 0.00875900313258171\n",
      "epoch:  578, loss: 0.008754128590226173\n",
      "epoch:  579, loss: 0.008751947432756424\n",
      "epoch:  580, loss: 0.00874701514840126\n",
      "epoch:  581, loss: 0.008744892664253712\n",
      "epoch:  582, loss: 0.00874149426817894\n",
      "epoch:  583, loss: 0.008737784810364246\n",
      "epoch:  584, loss: 0.008735481649637222\n",
      "epoch:  585, loss: 0.008733999915421009\n",
      "epoch:  586, loss: 0.008728868328034878\n",
      "epoch:  587, loss: 0.008726891130208969\n",
      "epoch:  588, loss: 0.008722719736397266\n",
      "epoch:  589, loss: 0.00871972180902958\n",
      "epoch:  590, loss: 0.008717301301658154\n",
      "epoch:  591, loss: 0.008715772069990635\n",
      "epoch:  592, loss: 0.008711136877536774\n",
      "epoch:  593, loss: 0.008708426728844643\n",
      "epoch:  594, loss: 0.008706007152795792\n",
      "epoch:  595, loss: 0.008704427629709244\n",
      "epoch:  596, loss: 0.00869875866919756\n",
      "epoch:  597, loss: 0.008696939796209335\n",
      "epoch:  598, loss: 0.008691899478435516\n",
      "epoch:  599, loss: 0.00868937000632286\n",
      "epoch:  600, loss: 0.008685745298862457\n",
      "epoch:  601, loss: 0.008681679144501686\n",
      "epoch:  602, loss: 0.008678917773067951\n",
      "epoch:  603, loss: 0.00867739412933588\n",
      "epoch:  604, loss: 0.008672022260725498\n",
      "epoch:  605, loss: 0.008669380098581314\n",
      "epoch:  606, loss: 0.00866432674229145\n",
      "epoch:  607, loss: 0.008661294355988503\n",
      "epoch:  608, loss: 0.008658623322844505\n",
      "epoch:  609, loss: 0.008656981401145458\n",
      "epoch:  610, loss: 0.00865231640636921\n",
      "epoch:  611, loss: 0.00864886399358511\n",
      "epoch:  612, loss: 0.008646005764603615\n",
      "epoch:  613, loss: 0.00864438060671091\n",
      "epoch:  614, loss: 0.008638165891170502\n",
      "epoch:  615, loss: 0.008635994978249073\n",
      "epoch:  616, loss: 0.008631998673081398\n",
      "epoch:  617, loss: 0.00862751342356205\n",
      "epoch:  618, loss: 0.008624793961644173\n",
      "epoch:  619, loss: 0.008622951805591583\n",
      "epoch:  620, loss: 0.008617005310952663\n",
      "epoch:  621, loss: 0.008614663034677505\n",
      "epoch:  622, loss: 0.00861049722880125\n",
      "epoch:  623, loss: 0.008606445975601673\n",
      "epoch:  624, loss: 0.008603619411587715\n",
      "epoch:  625, loss: 0.008602134883403778\n",
      "epoch:  626, loss: 0.00859618466347456\n",
      "epoch:  627, loss: 0.008594119921326637\n",
      "epoch:  628, loss: 0.008589626289904118\n",
      "epoch:  629, loss: 0.00858604721724987\n",
      "epoch:  630, loss: 0.008583658374845982\n",
      "epoch:  631, loss: 0.0085817351937294\n",
      "epoch:  632, loss: 0.008575979620218277\n",
      "epoch:  633, loss: 0.008573714643716812\n",
      "epoch:  634, loss: 0.008570806123316288\n",
      "epoch:  635, loss: 0.008569461293518543\n",
      "epoch:  636, loss: 0.008562848903238773\n",
      "epoch:  637, loss: 0.008561440743505955\n",
      "epoch:  638, loss: 0.00855599157512188\n",
      "epoch:  639, loss: 0.008553358726203442\n",
      "epoch:  640, loss: 0.00855071097612381\n",
      "epoch:  641, loss: 0.008548993617296219\n",
      "epoch:  642, loss: 0.008542883209884167\n",
      "epoch:  643, loss: 0.00854089017957449\n",
      "epoch:  644, loss: 0.008535846136510372\n",
      "epoch:  645, loss: 0.008532721549272537\n",
      "epoch:  646, loss: 0.008529732003808022\n",
      "epoch:  647, loss: 0.008528347127139568\n",
      "epoch:  648, loss: 0.008522924035787582\n",
      "epoch:  649, loss: 0.008520153351128101\n",
      "epoch:  650, loss: 0.00851611327379942\n",
      "epoch:  651, loss: 0.008511845022439957\n",
      "epoch:  652, loss: 0.008509736508131027\n",
      "epoch:  653, loss: 0.008507337421178818\n",
      "epoch:  654, loss: 0.008501543663442135\n",
      "epoch:  655, loss: 0.00849887728691101\n",
      "epoch:  656, loss: 0.008495914749801159\n",
      "epoch:  657, loss: 0.008494250476360321\n",
      "epoch:  658, loss: 0.008487374521791935\n",
      "epoch:  659, loss: 0.008485601283609867\n",
      "epoch:  660, loss: 0.008480374701321125\n",
      "epoch:  661, loss: 0.008476916700601578\n",
      "epoch:  662, loss: 0.008472233079373837\n",
      "epoch:  663, loss: 0.008468362502753735\n",
      "epoch:  664, loss: 0.008465397171676159\n",
      "epoch:  665, loss: 0.008463753387331963\n",
      "epoch:  666, loss: 0.008457396179437637\n",
      "epoch:  667, loss: 0.00845517497509718\n",
      "epoch:  668, loss: 0.008449763990938663\n",
      "epoch:  669, loss: 0.008446603082120419\n",
      "epoch:  670, loss: 0.008442375808954239\n",
      "epoch:  671, loss: 0.008438057266175747\n",
      "epoch:  672, loss: 0.008435479365289211\n",
      "epoch:  673, loss: 0.008433422073721886\n",
      "epoch:  674, loss: 0.008426744490861893\n",
      "epoch:  675, loss: 0.008424762636423111\n",
      "epoch:  676, loss: 0.008419353514909744\n",
      "epoch:  677, loss: 0.00841605756431818\n",
      "epoch:  678, loss: 0.008411308750510216\n",
      "epoch:  679, loss: 0.00840718112885952\n",
      "epoch:  680, loss: 0.00840405561029911\n",
      "epoch:  681, loss: 0.008402430452406406\n",
      "epoch:  682, loss: 0.008394796401262283\n",
      "epoch:  683, loss: 0.008393550291657448\n",
      "epoch:  684, loss: 0.008387570269405842\n",
      "epoch:  685, loss: 0.008384649641811848\n",
      "epoch:  686, loss: 0.008378380909562111\n",
      "epoch:  687, loss: 0.008375724777579308\n",
      "epoch:  688, loss: 0.008369390852749348\n",
      "epoch:  689, loss: 0.008366398513317108\n",
      "epoch:  690, loss: 0.008362974040210247\n",
      "epoch:  691, loss: 0.00836134608834982\n",
      "epoch:  692, loss: 0.008354020304977894\n",
      "epoch:  693, loss: 0.008351973257958889\n",
      "epoch:  694, loss: 0.008346140384674072\n",
      "epoch:  695, loss: 0.008342485874891281\n",
      "epoch:  696, loss: 0.008336069993674755\n",
      "epoch:  697, loss: 0.008333118632435799\n",
      "epoch:  698, loss: 0.008327119052410126\n",
      "epoch:  699, loss: 0.008323580026626587\n",
      "epoch:  700, loss: 0.008317957632243633\n",
      "epoch:  701, loss: 0.008314072154462337\n",
      "epoch:  702, loss: 0.008308324962854385\n",
      "epoch:  703, loss: 0.00830451212823391\n",
      "epoch:  704, loss: 0.00829983502626419\n",
      "epoch:  705, loss: 0.008295025676488876\n",
      "epoch:  706, loss: 0.008290383033454418\n",
      "epoch:  707, loss: 0.008285409770905972\n",
      "epoch:  708, loss: 0.008279521018266678\n",
      "epoch:  709, loss: 0.008275921456515789\n",
      "epoch:  710, loss: 0.00826902873814106\n",
      "epoch:  711, loss: 0.00826637540012598\n",
      "epoch:  712, loss: 0.008259893395006657\n",
      "epoch:  713, loss: 0.008256899192929268\n",
      "epoch:  714, loss: 0.0082499198615551\n",
      "epoch:  715, loss: 0.008247447200119495\n",
      "epoch:  716, loss: 0.008243891410529613\n",
      "epoch:  717, loss: 0.008242146112024784\n",
      "epoch:  718, loss: 0.008234764449298382\n",
      "epoch:  719, loss: 0.00823242962360382\n",
      "epoch:  720, loss: 0.00822416041046381\n",
      "epoch:  721, loss: 0.008222517557442188\n",
      "epoch:  722, loss: 0.008214520290493965\n",
      "epoch:  723, loss: 0.008212444372475147\n",
      "epoch:  724, loss: 0.008204525336623192\n",
      "epoch:  725, loss: 0.008202511817216873\n",
      "epoch:  726, loss: 0.008195554837584496\n",
      "epoch:  727, loss: 0.008192778564989567\n",
      "epoch:  728, loss: 0.008186886087059975\n",
      "epoch:  729, loss: 0.008182980120182037\n",
      "epoch:  730, loss: 0.008177677169442177\n",
      "epoch:  731, loss: 0.008172942325472832\n",
      "epoch:  732, loss: 0.0081693259999156\n",
      "epoch:  733, loss: 0.008167481049895287\n",
      "epoch:  734, loss: 0.00815863162279129\n",
      "epoch:  735, loss: 0.008157202042639256\n",
      "epoch:  736, loss: 0.008147677406668663\n",
      "epoch:  737, loss: 0.008146422915160656\n",
      "epoch:  738, loss: 0.008144816383719444\n",
      "epoch:  739, loss: 0.008143272250890732\n",
      "epoch:  740, loss: 0.008134379051625729\n",
      "epoch:  741, loss: 0.00813314225524664\n",
      "epoch:  742, loss: 0.008124086074531078\n",
      "epoch:  743, loss: 0.008122813887894154\n",
      "epoch:  744, loss: 0.008121561259031296\n",
      "epoch:  745, loss: 0.008120345883071423\n",
      "epoch:  746, loss: 0.008119132369756699\n",
      "epoch:  747, loss: 0.00811793003231287\n",
      "epoch:  748, loss: 0.008116381242871284\n",
      "epoch:  749, loss: 0.008114833384752274\n",
      "epoch:  750, loss: 0.008106358349323273\n",
      "epoch:  751, loss: 0.008104696869850159\n",
      "epoch:  752, loss: 0.008096153847873211\n",
      "epoch:  753, loss: 0.008094335906207561\n",
      "epoch:  754, loss: 0.008084731176495552\n",
      "epoch:  755, loss: 0.008083587512373924\n",
      "epoch:  756, loss: 0.00807341281324625\n",
      "epoch:  757, loss: 0.00807206891477108\n",
      "epoch:  758, loss: 0.00807069893926382\n",
      "epoch:  759, loss: 0.00806896947324276\n",
      "epoch:  760, loss: 0.008067205548286438\n",
      "epoch:  761, loss: 0.008057339116930962\n",
      "epoch:  762, loss: 0.008056027814745903\n",
      "epoch:  763, loss: 0.008046235889196396\n",
      "epoch:  764, loss: 0.008044798858463764\n",
      "epoch:  765, loss: 0.00803436990827322\n",
      "epoch:  766, loss: 0.008032676763832569\n",
      "epoch:  767, loss: 0.008030958473682404\n",
      "epoch:  768, loss: 0.008020914159715176\n",
      "epoch:  769, loss: 0.008019776083528996\n",
      "epoch:  770, loss: 0.008009294047951698\n",
      "epoch:  771, loss: 0.008007893338799477\n",
      "epoch:  772, loss: 0.008006498217582703\n",
      "epoch:  773, loss: 0.008005130104720592\n",
      "epoch:  774, loss: 0.008003740571439266\n",
      "epoch:  775, loss: 0.00800235290080309\n",
      "epoch:  776, loss: 0.008000981993973255\n",
      "epoch:  777, loss: 0.00799915473908186\n",
      "epoch:  778, loss: 0.00799746997654438\n",
      "epoch:  779, loss: 0.007987001910805702\n",
      "epoch:  780, loss: 0.007985620759427547\n",
      "epoch:  781, loss: 0.007984194904565811\n",
      "epoch:  782, loss: 0.007982790470123291\n",
      "epoch:  783, loss: 0.007981404662132263\n",
      "epoch:  784, loss: 0.007979613728821278\n",
      "epoch:  785, loss: 0.007977820932865143\n",
      "epoch:  786, loss: 0.007967100478708744\n",
      "epoch:  787, loss: 0.007965661585330963\n",
      "epoch:  788, loss: 0.007953792810440063\n",
      "epoch:  789, loss: 0.007951748557388783\n",
      "epoch:  790, loss: 0.00794983096420765\n",
      "epoch:  791, loss: 0.007937861606478691\n",
      "epoch:  792, loss: 0.007936298847198486\n",
      "epoch:  793, loss: 0.007934723049402237\n",
      "epoch:  794, loss: 0.007933142594993114\n",
      "epoch:  795, loss: 0.007931108586490154\n",
      "epoch:  796, loss: 0.00792909786105156\n",
      "epoch:  797, loss: 0.0079168900847435\n",
      "epoch:  798, loss: 0.007915527559816837\n",
      "epoch:  799, loss: 0.007903593592345715\n",
      "epoch:  800, loss: 0.007902024313807487\n",
      "epoch:  801, loss: 0.007899877615272999\n",
      "epoch:  802, loss: 0.007898092269897461\n",
      "epoch:  803, loss: 0.007886344566941261\n",
      "epoch:  804, loss: 0.007884630002081394\n",
      "epoch:  805, loss: 0.007871517911553383\n",
      "epoch:  806, loss: 0.007869702763855457\n",
      "epoch:  807, loss: 0.007867954671382904\n",
      "epoch:  808, loss: 0.007866239175200462\n",
      "epoch:  809, loss: 0.007864550687372684\n",
      "epoch:  810, loss: 0.007862885482609272\n",
      "epoch:  811, loss: 0.00786117184907198\n",
      "epoch:  812, loss: 0.007858742028474808\n",
      "epoch:  813, loss: 0.0078566400334239\n",
      "epoch:  814, loss: 0.00784329418092966\n",
      "epoch:  815, loss: 0.007841548882424831\n",
      "epoch:  816, loss: 0.007839808240532875\n",
      "epoch:  817, loss: 0.007838115096092224\n",
      "epoch:  818, loss: 0.007836436852812767\n",
      "epoch:  819, loss: 0.007834300398826599\n",
      "epoch:  820, loss: 0.00783220212906599\n",
      "epoch:  821, loss: 0.007819892838597298\n",
      "epoch:  822, loss: 0.00781833752989769\n",
      "epoch:  823, loss: 0.007805579341948032\n",
      "epoch:  824, loss: 0.007804171647876501\n",
      "epoch:  825, loss: 0.007791342679411173\n",
      "epoch:  826, loss: 0.007790016010403633\n",
      "epoch:  827, loss: 0.007777841296046972\n",
      "epoch:  828, loss: 0.007776100654155016\n",
      "epoch:  829, loss: 0.007774395402520895\n",
      "epoch:  830, loss: 0.007772682700306177\n",
      "epoch:  831, loss: 0.0077710337936878204\n",
      "epoch:  832, loss: 0.007769360672682524\n",
      "epoch:  833, loss: 0.007767736911773682\n",
      "epoch:  834, loss: 0.007766075897961855\n",
      "epoch:  835, loss: 0.007764441892504692\n",
      "epoch:  836, loss: 0.007762804627418518\n",
      "epoch:  837, loss: 0.007761182729154825\n",
      "epoch:  838, loss: 0.007759545464068651\n",
      "epoch:  839, loss: 0.007757895160466433\n",
      "epoch:  840, loss: 0.0077562700025737286\n",
      "epoch:  841, loss: 0.007754581980407238\n",
      "epoch:  842, loss: 0.007752886973321438\n",
      "epoch:  843, loss: 0.007751080207526684\n",
      "epoch:  844, loss: 0.007749323733150959\n",
      "epoch:  845, loss: 0.007747509051114321\n",
      "epoch:  846, loss: 0.007745745591819286\n",
      "epoch:  847, loss: 0.007743895519524813\n",
      "epoch:  848, loss: 0.007742169313132763\n",
      "epoch:  849, loss: 0.007740401197224855\n",
      "epoch:  850, loss: 0.007738709449768066\n",
      "epoch:  851, loss: 0.007736926898360252\n",
      "epoch:  852, loss: 0.007735205814242363\n",
      "epoch:  853, loss: 0.007733376696705818\n",
      "epoch:  854, loss: 0.0077315689995884895\n",
      "epoch:  855, loss: 0.007729737553745508\n",
      "epoch:  856, loss: 0.007727961987257004\n",
      "epoch:  857, loss: 0.007726054172962904\n",
      "epoch:  858, loss: 0.0077241831459105015\n",
      "epoch:  859, loss: 0.00772225484251976\n",
      "epoch:  860, loss: 0.0077203987166285515\n",
      "epoch:  861, loss: 0.007718546781688929\n",
      "epoch:  862, loss: 0.00771672185510397\n",
      "epoch:  863, loss: 0.0077148862183094025\n",
      "epoch:  864, loss: 0.007712529972195625\n",
      "epoch:  865, loss: 0.0077101970091462135\n",
      "epoch:  866, loss: 0.007696193642914295\n",
      "epoch:  867, loss: 0.007694346364587545\n",
      "epoch:  868, loss: 0.007692495360970497\n",
      "epoch:  869, loss: 0.007690624799579382\n",
      "epoch:  870, loss: 0.007688684388995171\n",
      "epoch:  871, loss: 0.007686779368668795\n",
      "epoch:  872, loss: 0.007684909272938967\n",
      "epoch:  873, loss: 0.007683051750063896\n",
      "epoch:  874, loss: 0.0076812077313661575\n",
      "epoch:  875, loss: 0.007679379545152187\n",
      "epoch:  876, loss: 0.007676960900425911\n",
      "epoch:  877, loss: 0.00767465541139245\n",
      "epoch:  878, loss: 0.007660434115678072\n",
      "epoch:  879, loss: 0.007657957263290882\n",
      "epoch:  880, loss: 0.007655723486095667\n",
      "epoch:  881, loss: 0.007641396019607782\n",
      "epoch:  882, loss: 0.007639506831765175\n",
      "epoch:  883, loss: 0.00763764139264822\n",
      "epoch:  884, loss: 0.007635754533112049\n",
      "epoch:  885, loss: 0.007633890490978956\n",
      "epoch:  886, loss: 0.007632011082023382\n",
      "epoch:  887, loss: 0.007629575207829475\n",
      "epoch:  888, loss: 0.007627278100699186\n",
      "epoch:  889, loss: 0.007613071706146002\n",
      "epoch:  890, loss: 0.007611213717609644\n",
      "epoch:  891, loss: 0.007609361317008734\n",
      "epoch:  892, loss: 0.007607541047036648\n",
      "epoch:  893, loss: 0.007605722174048424\n",
      "epoch:  894, loss: 0.007603921927511692\n",
      "epoch:  895, loss: 0.007602089550346136\n",
      "epoch:  896, loss: 0.007600266952067614\n",
      "epoch:  897, loss: 0.00759845320135355\n",
      "epoch:  898, loss: 0.007596688345074654\n",
      "epoch:  899, loss: 0.007594911381602287\n",
      "epoch:  900, loss: 0.0075932033360004425\n",
      "epoch:  901, loss: 0.0075914799235761166\n",
      "epoch:  902, loss: 0.007589802611619234\n",
      "epoch:  903, loss: 0.007588091306388378\n",
      "epoch:  904, loss: 0.007586468942463398\n",
      "epoch:  905, loss: 0.007584767881780863\n",
      "epoch:  906, loss: 0.007583188824355602\n",
      "epoch:  907, loss: 0.007581495214253664\n",
      "epoch:  908, loss: 0.007579897530376911\n",
      "epoch:  909, loss: 0.0075782062485814095\n",
      "epoch:  910, loss: 0.007576610893011093\n",
      "epoch:  911, loss: 0.007574906572699547\n",
      "epoch:  912, loss: 0.007573230192065239\n",
      "epoch:  913, loss: 0.007571469992399216\n",
      "epoch:  914, loss: 0.0075697582215070724\n",
      "epoch:  915, loss: 0.007568000350147486\n",
      "epoch:  916, loss: 0.007566290907561779\n",
      "epoch:  917, loss: 0.00756455073133111\n",
      "epoch:  918, loss: 0.007562873885035515\n",
      "epoch:  919, loss: 0.00756110530346632\n",
      "epoch:  920, loss: 0.0075594158843159676\n",
      "epoch:  921, loss: 0.007557650096714497\n",
      "epoch:  922, loss: 0.007555982563644648\n",
      "epoch:  923, loss: 0.007554210256785154\n",
      "epoch:  924, loss: 0.007552454248070717\n",
      "epoch:  925, loss: 0.007550710346549749\n",
      "epoch:  926, loss: 0.007549062371253967\n",
      "epoch:  927, loss: 0.007547301705926657\n",
      "epoch:  928, loss: 0.00754567189142108\n",
      "epoch:  929, loss: 0.007543919607996941\n",
      "epoch:  930, loss: 0.00754222646355629\n",
      "epoch:  931, loss: 0.007540485821664333\n",
      "epoch:  932, loss: 0.007538855541497469\n",
      "epoch:  933, loss: 0.007537137251347303\n",
      "epoch:  934, loss: 0.0075354548171162605\n",
      "epoch:  935, loss: 0.00753374770283699\n",
      "epoch:  936, loss: 0.007532103452831507\n",
      "epoch:  937, loss: 0.007530379109084606\n",
      "epoch:  938, loss: 0.007528678048402071\n",
      "epoch:  939, loss: 0.007526986766606569\n",
      "epoch:  940, loss: 0.007525317836552858\n",
      "epoch:  941, loss: 0.007523639127612114\n",
      "epoch:  942, loss: 0.00752198277041316\n",
      "epoch:  943, loss: 0.007520309183746576\n",
      "epoch:  944, loss: 0.007518583443015814\n",
      "epoch:  945, loss: 0.00751687865704298\n",
      "epoch:  946, loss: 0.0075155929662287235\n",
      "epoch:  947, loss: 0.007502014748752117\n",
      "epoch:  948, loss: 0.007499088533222675\n",
      "epoch:  949, loss: 0.007497596088796854\n",
      "epoch:  950, loss: 0.007483999710530043\n",
      "epoch:  951, loss: 0.007481634616851807\n",
      "epoch:  952, loss: 0.007479460444301367\n",
      "epoch:  953, loss: 0.00746579933911562\n",
      "epoch:  954, loss: 0.0074640605598688126\n",
      "epoch:  955, loss: 0.007462387904524803\n",
      "epoch:  956, loss: 0.007460704538971186\n",
      "epoch:  957, loss: 0.0074590276926755905\n",
      "epoch:  958, loss: 0.00745735689997673\n",
      "epoch:  959, loss: 0.007456015795469284\n",
      "epoch:  960, loss: 0.007442358881235123\n",
      "epoch:  961, loss: 0.007440584246069193\n",
      "epoch:  962, loss: 0.007438848726451397\n",
      "epoch:  963, loss: 0.007437118794769049\n",
      "epoch:  964, loss: 0.007435398641973734\n",
      "epoch:  965, loss: 0.007433691993355751\n",
      "epoch:  966, loss: 0.007431969512254\n",
      "epoch:  967, loss: 0.0074302940629422665\n",
      "epoch:  968, loss: 0.007428567390888929\n",
      "epoch:  969, loss: 0.007427250035107136\n",
      "epoch:  970, loss: 0.007413314655423164\n",
      "epoch:  971, loss: 0.0074106547981500626\n",
      "epoch:  972, loss: 0.007408836390823126\n",
      "epoch:  973, loss: 0.007395012304186821\n",
      "epoch:  974, loss: 0.00739324651658535\n",
      "epoch:  975, loss: 0.0073915328830480576\n",
      "epoch:  976, loss: 0.0073897759430110455\n",
      "epoch:  977, loss: 0.007388112600892782\n",
      "epoch:  978, loss: 0.007386746350675821\n",
      "epoch:  979, loss: 0.0073724836111068726\n",
      "epoch:  980, loss: 0.0073610516265034676\n",
      "epoch:  981, loss: 0.007357078138738871\n",
      "epoch:  982, loss: 0.007342723198235035\n",
      "epoch:  983, loss: 0.007339958101511002\n",
      "epoch:  984, loss: 0.007338074967265129\n",
      "epoch:  985, loss: 0.0073236823081970215\n",
      "epoch:  986, loss: 0.007312323898077011\n",
      "epoch:  987, loss: 0.007308499421924353\n",
      "epoch:  988, loss: 0.007294199429452419\n",
      "epoch:  989, loss: 0.00729009136557579\n",
      "epoch:  990, loss: 0.007286014501005411\n",
      "epoch:  991, loss: 0.007271696347743273\n",
      "epoch:  992, loss: 0.007268824614584446\n",
      "epoch:  993, loss: 0.0072669400833547115\n",
      "epoch:  994, loss: 0.0072524817660450935\n",
      "epoch:  995, loss: 0.007249969057738781\n",
      "epoch:  996, loss: 0.007247747387737036\n",
      "epoch:  997, loss: 0.0072336806915700436\n",
      "epoch:  998, loss: 0.007232238072901964\n",
      "epoch:  999, loss: 0.007217708043754101\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size=X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "opt = torch_numopt.GradientDescentLS(model=model, lr=1, line_search_method=\"bisect\")\n",
    "\n",
    "all_loss = {}\n",
    "for epoch in range(1000):\n",
    "    print(\"epoch: \", epoch, end=\"\")\n",
    "    all_loss[epoch + 1] = 0\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y, loss_fn)\n",
    "\n",
    "        all_loss[epoch + 1] += loss\n",
    "    all_loss[epoch + 1] /= len(data_loader)\n",
    "    print(\", loss: {}\".format(all_loss[epoch + 1].detach().numpy().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics: R2 = 0.7597976717455035\n",
      "Test metrics:  R2 = 0.7406935411879965\n"
     ]
    }
   ],
   "source": [
    "pred_train = model.forward(X_train).detach()\n",
    "pred_test = model.forward(X_test).detach()\n",
    "print(f\"Train metrics: R2 = {r2_score(pred_train, y_train)}\")\n",
    "print(f\"Test metrics:  R2 = {r2_score(pred_test, y_test)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_numopt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
