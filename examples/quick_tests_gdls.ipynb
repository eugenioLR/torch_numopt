{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch_numopt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import *\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.f1 = nn.Linear(input_size, 10, device=device)\n",
    "        self.f2 = nn.Linear(10, 20, device=device)\n",
    "        self.f3 = nn.Linear(20, 20, device=device)\n",
    "        self.f4 = nn.Linear(20, 10, device=device)\n",
    "        self.f5 = nn.Linear(10, 1, device=device)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        # self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.f1(x))\n",
    "        x = self.activation(self.f2(x))\n",
    "        x = self.activation(self.f3(x))\n",
    "        x = self.activation(self.f4(x))\n",
    "        x = self.f5(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([800, 10]) torch.Size([800, 1])\n",
      "torch.Size([200, 10]) torch.Size([200, 1])\n"
     ]
    }
   ],
   "source": [
    "# X, y = load_diabetes(return_X_y=True, scaled=False)\n",
    "# X, y = make_regression(n_samples=1000, n_features=100)\n",
    "X, y = make_friedman1(n_samples=1000, noise=1e-2)\n",
    "\n",
    "X_scaler = MinMaxScaler()\n",
    "X = X_scaler.fit_transform(X)\n",
    "X = torch.Tensor(X).to(device)\n",
    "\n",
    "y_scaler = MinMaxScaler()\n",
    "y = y_scaler.fit_transform(y.reshape((-1, 1)))\n",
    "y = torch.Tensor(y).to(device)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "torch_data = TensorDataset(X_train, y_train)\n",
    "data_loader = DataLoader(torch_data, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.17843085527420044\n",
      "epoch:  1, loss: 0.10298769176006317\n",
      "epoch:  2, loss: 0.06401793658733368\n",
      "epoch:  3, loss: 0.045652423053979874\n",
      "epoch:  4, loss: 0.03745931759476662\n",
      "epoch:  5, loss: 0.033963643014431\n",
      "epoch:  6, loss: 0.032518401741981506\n",
      "epoch:  7, loss: 0.03193080797791481\n",
      "epoch:  8, loss: 0.031691618263721466\n",
      "epoch:  9, loss: 0.03159115090966225\n",
      "epoch:  10, loss: 0.03154517337679863\n",
      "epoch:  11, loss: 0.0315202958881855\n",
      "epoch:  12, loss: 0.0314621701836586\n",
      "epoch:  13, loss: 0.03140959516167641\n",
      "epoch:  14, loss: 0.031381942331790924\n",
      "epoch:  15, loss: 0.03132307529449463\n",
      "epoch:  16, loss: 0.031261738389730453\n",
      "epoch:  17, loss: 0.03122875466942787\n",
      "epoch:  18, loss: 0.031132852658629417\n",
      "epoch:  19, loss: 0.031062008813023567\n",
      "epoch:  20, loss: 0.031025825068354607\n",
      "epoch:  21, loss: 0.030982768163084984\n",
      "epoch:  22, loss: 0.030896609649062157\n",
      "epoch:  23, loss: 0.030857443809509277\n",
      "epoch:  24, loss: 0.03084859810769558\n",
      "epoch:  25, loss: 0.03074750490486622\n",
      "epoch:  26, loss: 0.030704088509082794\n",
      "epoch:  27, loss: 0.03068017028272152\n",
      "epoch:  28, loss: 0.030591104179620743\n",
      "epoch:  29, loss: 0.030543679371476173\n",
      "epoch:  30, loss: 0.030517810955643654\n",
      "epoch:  31, loss: 0.03042733669281006\n",
      "epoch:  32, loss: 0.030373958870768547\n",
      "epoch:  33, loss: 0.030345821753144264\n",
      "epoch:  34, loss: 0.03025713376700878\n",
      "epoch:  35, loss: 0.03019803576171398\n",
      "epoch:  36, loss: 0.030167045071721077\n",
      "epoch:  37, loss: 0.03008456900715828\n",
      "epoch:  38, loss: 0.03001355566084385\n",
      "epoch:  39, loss: 0.02997857891023159\n",
      "epoch:  40, loss: 0.02989516779780388\n",
      "epoch:  41, loss: 0.02981739491224289\n",
      "epoch:  42, loss: 0.029778938740491867\n",
      "epoch:  43, loss: 0.029701190069317818\n",
      "epoch:  44, loss: 0.029609408229589462\n",
      "epoch:  45, loss: 0.029566414654254913\n",
      "epoch:  46, loss: 0.029482755810022354\n",
      "epoch:  47, loss: 0.029383080080151558\n",
      "epoch:  48, loss: 0.029335658997297287\n",
      "epoch:  49, loss: 0.029261846095323563\n",
      "epoch:  50, loss: 0.0291424747556448\n",
      "epoch:  51, loss: 0.0290889460593462\n",
      "epoch:  52, loss: 0.029016347602009773\n",
      "epoch:  53, loss: 0.028882401064038277\n",
      "epoch:  54, loss: 0.0288216732442379\n",
      "epoch:  55, loss: 0.028762223199009895\n",
      "epoch:  56, loss: 0.028600890189409256\n",
      "epoch:  57, loss: 0.028532113879919052\n",
      "epoch:  58, loss: 0.028476888313889503\n",
      "epoch:  59, loss: 0.02829558216035366\n",
      "epoch:  60, loss: 0.028217777609825134\n",
      "epoch:  61, loss: 0.02818598784506321\n",
      "epoch:  62, loss: 0.027966441586613655\n",
      "epoch:  63, loss: 0.02787846513092518\n",
      "epoch:  64, loss: 0.027848921716213226\n",
      "epoch:  65, loss: 0.02760528028011322\n",
      "epoch:  66, loss: 0.02750590816140175\n",
      "epoch:  67, loss: 0.0274520106613636\n",
      "epoch:  68, loss: 0.027217306196689606\n",
      "epoch:  69, loss: 0.027101030573248863\n",
      "epoch:  70, loss: 0.027041153982281685\n",
      "epoch:  71, loss: 0.02678488753736019\n",
      "epoch:  72, loss: 0.026656625792384148\n",
      "epoch:  73, loss: 0.026590775698423386\n",
      "epoch:  74, loss: 0.026326216757297516\n",
      "epoch:  75, loss: 0.026177244260907173\n",
      "epoch:  76, loss: 0.02610529400408268\n",
      "epoch:  77, loss: 0.025816289708018303\n",
      "epoch:  78, loss: 0.025655627250671387\n",
      "epoch:  79, loss: 0.025578105822205544\n",
      "epoch:  80, loss: 0.025275053456425667\n",
      "epoch:  81, loss: 0.025094782933592796\n",
      "epoch:  82, loss: 0.025011323392391205\n",
      "epoch:  83, loss: 0.02467002347111702\n",
      "epoch:  84, loss: 0.02448091097176075\n",
      "epoch:  85, loss: 0.024392783641815186\n",
      "epoch:  86, loss: 0.024029411375522614\n",
      "epoch:  87, loss: 0.023823659867048264\n",
      "epoch:  88, loss: 0.023731540888547897\n",
      "epoch:  89, loss: 0.023313332349061966\n",
      "epoch:  90, loss: 0.02311072126030922\n",
      "epoch:  91, loss: 0.023016702383756638\n",
      "epoch:  92, loss: 0.022561611607670784\n",
      "epoch:  93, loss: 0.02235325612127781\n",
      "epoch:  94, loss: 0.022259041666984558\n",
      "epoch:  95, loss: 0.02173547074198723\n",
      "epoch:  96, loss: 0.02154667116701603\n",
      "epoch:  97, loss: 0.02149082161486149\n",
      "epoch:  98, loss: 0.02086736634373665\n",
      "epoch:  99, loss: 0.020702412351965904\n",
      "epoch:  100, loss: 0.020419511944055557\n",
      "epoch:  101, loss: 0.01996384561061859\n",
      "epoch:  102, loss: 0.019824381917715073\n",
      "epoch:  103, loss: 0.019416777417063713\n",
      "epoch:  104, loss: 0.019046975299715996\n",
      "epoch:  105, loss: 0.0189320407807827\n",
      "epoch:  106, loss: 0.018363455310463905\n",
      "epoch:  107, loss: 0.018130028620362282\n",
      "epoch:  108, loss: 0.018120942637324333\n",
      "epoch:  109, loss: 0.01737983711063862\n",
      "epoch:  110, loss: 0.017239777371287346\n",
      "epoch:  111, loss: 0.016783516854047775\n",
      "epoch:  112, loss: 0.016469277441501617\n",
      "epoch:  113, loss: 0.01637493073940277\n",
      "epoch:  114, loss: 0.015787072479724884\n",
      "epoch:  115, loss: 0.015629548579454422\n",
      "epoch:  116, loss: 0.015265819616615772\n",
      "epoch:  117, loss: 0.01493445597589016\n",
      "epoch:  118, loss: 0.014853965491056442\n",
      "epoch:  119, loss: 0.01431773230433464\n",
      "epoch:  120, loss: 0.014203879050910473\n",
      "epoch:  121, loss: 0.013795901089906693\n",
      "epoch:  122, loss: 0.01361433882266283\n",
      "epoch:  123, loss: 0.013432217761874199\n",
      "epoch:  124, loss: 0.01308208703994751\n",
      "epoch:  125, loss: 0.013027659617364407\n",
      "epoch:  126, loss: 0.012606802396476269\n",
      "epoch:  127, loss: 0.012550527229905128\n",
      "epoch:  128, loss: 0.01218256913125515\n",
      "epoch:  129, loss: 0.012119639664888382\n",
      "epoch:  130, loss: 0.011794354766607285\n",
      "epoch:  131, loss: 0.011729943566024303\n",
      "epoch:  132, loss: 0.011439945548772812\n",
      "epoch:  133, loss: 0.011377730406820774\n",
      "epoch:  134, loss: 0.011109480634331703\n",
      "epoch:  135, loss: 0.011062698438763618\n",
      "epoch:  136, loss: 0.010816710069775581\n",
      "epoch:  137, loss: 0.010779707692563534\n",
      "epoch:  138, loss: 0.010552135296165943\n",
      "epoch:  139, loss: 0.010525827296078205\n",
      "epoch:  140, loss: 0.01031651720404625\n",
      "epoch:  141, loss: 0.010191667824983597\n",
      "epoch:  142, loss: 0.010104910470545292\n",
      "epoch:  143, loss: 0.00994477141648531\n",
      "epoch:  144, loss: 0.009911227971315384\n",
      "epoch:  145, loss: 0.00975209940224886\n",
      "epoch:  146, loss: 0.00973465945571661\n",
      "epoch:  147, loss: 0.00958908349275589\n",
      "epoch:  148, loss: 0.009476900100708008\n",
      "epoch:  149, loss: 0.009443171322345734\n",
      "epoch:  150, loss: 0.009324387647211552\n",
      "epoch:  151, loss: 0.009311322122812271\n",
      "epoch:  152, loss: 0.009202438406646252\n",
      "epoch:  153, loss: 0.009110946208238602\n",
      "epoch:  154, loss: 0.009093375876545906\n",
      "epoch:  155, loss: 0.00900289136916399\n",
      "epoch:  156, loss: 0.008919593878090382\n",
      "epoch:  157, loss: 0.008842897601425648\n",
      "epoch:  158, loss: 0.008772645145654678\n",
      "epoch:  159, loss: 0.008714908733963966\n",
      "epoch:  160, loss: 0.008701512590050697\n",
      "epoch:  161, loss: 0.008642399683594704\n",
      "epoch:  162, loss: 0.008588794618844986\n",
      "epoch:  163, loss: 0.008582458831369877\n",
      "epoch:  164, loss: 0.008532289415597916\n",
      "epoch:  165, loss: 0.008486307226121426\n",
      "epoch:  166, loss: 0.008444367907941341\n",
      "epoch:  167, loss: 0.008435762487351894\n",
      "epoch:  168, loss: 0.008401164785027504\n",
      "epoch:  169, loss: 0.008365386165678501\n",
      "epoch:  170, loss: 0.008361529558897018\n",
      "epoch:  171, loss: 0.008328520692884922\n",
      "epoch:  172, loss: 0.008298921398818493\n",
      "epoch:  173, loss: 0.008295509032905102\n",
      "epoch:  174, loss: 0.00826769508421421\n",
      "epoch:  175, loss: 0.00824225228279829\n",
      "epoch:  176, loss: 0.008224839344620705\n",
      "epoch:  177, loss: 0.0082164341583848\n",
      "epoch:  178, loss: 0.008194767870008945\n",
      "epoch:  179, loss: 0.00818822905421257\n",
      "epoch:  180, loss: 0.00817281473428011\n",
      "epoch:  181, loss: 0.008155194111168385\n",
      "epoch:  182, loss: 0.008152707479894161\n",
      "epoch:  183, loss: 0.008135599084198475\n",
      "epoch:  184, loss: 0.008119949139654636\n",
      "epoch:  185, loss: 0.008118179626762867\n",
      "epoch:  186, loss: 0.008103441447019577\n",
      "epoch:  187, loss: 0.008091190829873085\n",
      "epoch:  188, loss: 0.00808884296566248\n",
      "epoch:  189, loss: 0.008076666854321957\n",
      "epoch:  190, loss: 0.008075809106230736\n",
      "epoch:  191, loss: 0.008064210414886475\n",
      "epoch:  192, loss: 0.008054674603044987\n",
      "epoch:  193, loss: 0.008052809163928032\n",
      "epoch:  194, loss: 0.008043094538152218\n",
      "epoch:  195, loss: 0.008037731982767582\n",
      "epoch:  196, loss: 0.008033058606088161\n",
      "epoch:  197, loss: 0.00802455935627222\n",
      "epoch:  198, loss: 0.008023526519536972\n",
      "epoch:  199, loss: 0.008015395142138004\n",
      "epoch:  200, loss: 0.008007991127669811\n",
      "epoch:  201, loss: 0.008006962947547436\n",
      "epoch:  202, loss: 0.007999652065336704\n",
      "epoch:  203, loss: 0.007994298823177814\n",
      "epoch:  204, loss: 0.007992129772901535\n",
      "epoch:  205, loss: 0.007985907606780529\n",
      "epoch:  206, loss: 0.007985137403011322\n",
      "epoch:  207, loss: 0.007979308255016804\n",
      "epoch:  208, loss: 0.007978660054504871\n",
      "epoch:  209, loss: 0.007973033003509045\n",
      "epoch:  210, loss: 0.007968873716890812\n",
      "epoch:  211, loss: 0.007967134937644005\n",
      "epoch:  212, loss: 0.007962136529386044\n",
      "epoch:  213, loss: 0.007961517199873924\n",
      "epoch:  214, loss: 0.007956626825034618\n",
      "epoch:  215, loss: 0.007952846586704254\n",
      "epoch:  216, loss: 0.007951515726745129\n",
      "epoch:  217, loss: 0.007947105914354324\n",
      "epoch:  218, loss: 0.007946545258164406\n",
      "epoch:  219, loss: 0.007942382246255875\n",
      "epoch:  220, loss: 0.00794049259275198\n",
      "epoch:  221, loss: 0.007937991991639137\n",
      "epoch:  222, loss: 0.007934484630823135\n",
      "epoch:  223, loss: 0.007933779619634151\n",
      "epoch:  224, loss: 0.007930160500109196\n",
      "epoch:  225, loss: 0.007929747924208641\n",
      "epoch:  226, loss: 0.00792634766548872\n",
      "epoch:  227, loss: 0.007925947196781635\n",
      "epoch:  228, loss: 0.007922609336674213\n",
      "epoch:  229, loss: 0.00791983399540186\n",
      "epoch:  230, loss: 0.007919028401374817\n",
      "epoch:  231, loss: 0.007916070520877838\n",
      "epoch:  232, loss: 0.007915555499494076\n",
      "epoch:  233, loss: 0.00791250728070736\n",
      "epoch:  234, loss: 0.007910485379397869\n",
      "epoch:  235, loss: 0.007909207604825497\n",
      "epoch:  236, loss: 0.007906349375844002\n",
      "epoch:  237, loss: 0.007905958220362663\n",
      "epoch:  238, loss: 0.007903176359832287\n",
      "epoch:  239, loss: 0.007902780547738075\n",
      "epoch:  240, loss: 0.007899894379079342\n",
      "epoch:  241, loss: 0.007898570038378239\n",
      "epoch:  242, loss: 0.007896766066551208\n",
      "epoch:  243, loss: 0.007894234731793404\n",
      "epoch:  244, loss: 0.007893725298345089\n",
      "epoch:  245, loss: 0.00789102353155613\n",
      "epoch:  246, loss: 0.007890711538493633\n",
      "epoch:  247, loss: 0.007888025604188442\n",
      "epoch:  248, loss: 0.007885895669460297\n",
      "epoch:  249, loss: 0.007885115221142769\n",
      "epoch:  250, loss: 0.007882657460868359\n",
      "epoch:  251, loss: 0.007882248610258102\n",
      "epoch:  252, loss: 0.007879800163209438\n",
      "epoch:  253, loss: 0.007879400625824928\n",
      "epoch:  254, loss: 0.00787684228271246\n",
      "epoch:  255, loss: 0.007876516319811344\n",
      "epoch:  256, loss: 0.007873957976698875\n",
      "epoch:  257, loss: 0.007871896959841251\n",
      "epoch:  258, loss: 0.007871227338910103\n",
      "epoch:  259, loss: 0.007868918590247631\n",
      "epoch:  260, loss: 0.007868554443120956\n",
      "epoch:  261, loss: 0.007866146974265575\n",
      "epoch:  262, loss: 0.007864696905016899\n",
      "epoch:  263, loss: 0.007863514125347137\n",
      "epoch:  264, loss: 0.007861625403165817\n",
      "epoch:  265, loss: 0.007860895246267319\n",
      "epoch:  266, loss: 0.007858646102249622\n",
      "epoch:  267, loss: 0.007858321070671082\n",
      "epoch:  268, loss: 0.007855967618525028\n",
      "epoch:  269, loss: 0.007855704985558987\n",
      "epoch:  270, loss: 0.007853385992348194\n",
      "epoch:  271, loss: 0.007851461879909039\n",
      "epoch:  272, loss: 0.0078508909791708\n",
      "epoch:  273, loss: 0.007848836481571198\n",
      "epoch:  274, loss: 0.007848418317735195\n",
      "epoch:  275, loss: 0.007846224121749401\n",
      "epoch:  276, loss: 0.007845939137041569\n",
      "epoch:  277, loss: 0.007843713276088238\n",
      "epoch:  278, loss: 0.007842008024454117\n",
      "epoch:  279, loss: 0.00784126203507185\n",
      "epoch:  280, loss: 0.007839216850697994\n",
      "epoch:  281, loss: 0.007838817313313484\n",
      "epoch:  282, loss: 0.007836665958166122\n",
      "epoch:  283, loss: 0.007836389355361462\n",
      "epoch:  284, loss: 0.0078342380002141\n",
      "epoch:  285, loss: 0.007833975367248058\n",
      "epoch:  286, loss: 0.007831793278455734\n",
      "epoch:  287, loss: 0.007830101065337658\n",
      "epoch:  288, loss: 0.007829373702406883\n",
      "epoch:  289, loss: 0.007827326655387878\n",
      "epoch:  290, loss: 0.007826965302228928\n",
      "epoch:  291, loss: 0.007824835367500782\n",
      "epoch:  292, loss: 0.00782456062734127\n",
      "epoch:  293, loss: 0.007822387851774693\n",
      "epoch:  294, loss: 0.007821345701813698\n",
      "epoch:  295, loss: 0.007819976657629013\n",
      "epoch:  296, loss: 0.007818046025931835\n",
      "epoch:  297, loss: 0.0078175850212574\n",
      "epoch:  298, loss: 0.007815510034561157\n",
      "epoch:  299, loss: 0.00781518965959549\n",
      "epoch:  300, loss: 0.007813082076609135\n",
      "epoch:  301, loss: 0.007812809199094772\n",
      "epoch:  302, loss: 0.007810698822140694\n",
      "epoch:  303, loss: 0.007810438983142376\n",
      "epoch:  304, loss: 0.007808268070220947\n",
      "epoch:  305, loss: 0.007807910442352295\n",
      "epoch:  306, loss: 0.0078057944774627686\n",
      "epoch:  307, loss: 0.007804136723279953\n",
      "epoch:  308, loss: 0.007803312968462706\n",
      "epoch:  309, loss: 0.007801309693604708\n",
      "epoch:  310, loss: 0.007800824008882046\n",
      "epoch:  311, loss: 0.007798686157912016\n",
      "epoch:  312, loss: 0.007798356935381889\n",
      "epoch:  313, loss: 0.007796183694154024\n",
      "epoch:  314, loss: 0.0077959271147847176\n",
      "epoch:  315, loss: 0.007793733384460211\n",
      "epoch:  316, loss: 0.007793177850544453\n",
      "epoch:  317, loss: 0.007791253738105297\n",
      "epoch:  318, loss: 0.007789496332406998\n",
      "epoch:  319, loss: 0.007788806688040495\n",
      "epoch:  320, loss: 0.007786675821989775\n",
      "epoch:  321, loss: 0.007786340545862913\n",
      "epoch:  322, loss: 0.007784152403473854\n",
      "epoch:  323, loss: 0.0077838902361691\n",
      "epoch:  324, loss: 0.007781627122312784\n",
      "epoch:  325, loss: 0.007779336534440517\n",
      "epoch:  326, loss: 0.007779070641845465\n",
      "epoch:  327, loss: 0.007776751648634672\n",
      "epoch:  328, loss: 0.00777474045753479\n",
      "epoch:  329, loss: 0.007774156518280506\n",
      "epoch:  330, loss: 0.007771804928779602\n",
      "epoch:  331, loss: 0.007771485019475222\n",
      "epoch:  332, loss: 0.007769075687974691\n",
      "epoch:  333, loss: 0.007767774630337954\n",
      "epoch:  334, loss: 0.007766475901007652\n",
      "epoch:  335, loss: 0.00776438694447279\n",
      "epoch:  336, loss: 0.007763841655105352\n",
      "epoch:  337, loss: 0.007761689834296703\n",
      "epoch:  338, loss: 0.007761245593428612\n",
      "epoch:  339, loss: 0.00775916175916791\n",
      "epoch:  340, loss: 0.007758614607155323\n",
      "epoch:  341, loss: 0.007756354752928019\n",
      "epoch:  342, loss: 0.0077560353092849255\n",
      "epoch:  343, loss: 0.0077537610195577145\n",
      "epoch:  344, loss: 0.0077524627558887005\n",
      "epoch:  345, loss: 0.007751243654638529\n",
      "epoch:  346, loss: 0.007749894168227911\n",
      "epoch:  347, loss: 0.007748754695057869\n",
      "epoch:  348, loss: 0.007746824529021978\n",
      "epoch:  349, loss: 0.007746235001832247\n",
      "epoch:  350, loss: 0.007744230795651674\n",
      "epoch:  351, loss: 0.007743746507912874\n",
      "epoch:  352, loss: 0.007741796318441629\n",
      "epoch:  353, loss: 0.007741293869912624\n",
      "epoch:  354, loss: 0.007739134598523378\n",
      "epoch:  355, loss: 0.007738820277154446\n",
      "epoch:  356, loss: 0.0077366516925394535\n",
      "epoch:  357, loss: 0.007736355531960726\n",
      "epoch:  358, loss: 0.007734185550361872\n",
      "epoch:  359, loss: 0.007733920589089394\n",
      "epoch:  360, loss: 0.007731742691248655\n",
      "epoch:  361, loss: 0.007731463760137558\n",
      "epoch:  362, loss: 0.007729247212409973\n",
      "epoch:  363, loss: 0.007728752680122852\n",
      "epoch:  364, loss: 0.007726758252829313\n",
      "epoch:  365, loss: 0.0077253105118870735\n",
      "epoch:  366, loss: 0.007724301423877478\n",
      "epoch:  367, loss: 0.007721977774053812\n",
      "epoch:  368, loss: 0.007721694651991129\n",
      "epoch:  369, loss: 0.007719431072473526\n",
      "epoch:  370, loss: 0.007719141431152821\n",
      "epoch:  371, loss: 0.007716808933764696\n",
      "epoch:  372, loss: 0.007715684827417135\n",
      "epoch:  373, loss: 0.007714227307587862\n",
      "epoch:  374, loss: 0.007712667807936668\n",
      "epoch:  375, loss: 0.007711645215749741\n",
      "epoch:  376, loss: 0.007709897123277187\n",
      "epoch:  377, loss: 0.007709075231105089\n",
      "epoch:  378, loss: 0.007707507815212011\n",
      "epoch:  379, loss: 0.0077065094374120235\n",
      "epoch:  380, loss: 0.0077050006948411465\n",
      "epoch:  381, loss: 0.007703968323767185\n",
      "epoch:  382, loss: 0.0077024488709867\n",
      "epoch:  383, loss: 0.007701405789703131\n",
      "epoch:  384, loss: 0.007699802052229643\n",
      "epoch:  385, loss: 0.007698840461671352\n",
      "epoch:  386, loss: 0.007697422988712788\n",
      "epoch:  387, loss: 0.007696254178881645\n",
      "epoch:  388, loss: 0.007694517262279987\n",
      "epoch:  389, loss: 0.007693668827414513\n",
      "epoch:  390, loss: 0.0076919179409742355\n",
      "epoch:  391, loss: 0.00769105926156044\n",
      "epoch:  392, loss: 0.007689397782087326\n",
      "epoch:  393, loss: 0.007688456680625677\n",
      "epoch:  394, loss: 0.007686760276556015\n",
      "epoch:  395, loss: 0.0076858606189489365\n",
      "epoch:  396, loss: 0.007684130221605301\n",
      "epoch:  397, loss: 0.007683256641030312\n",
      "epoch:  398, loss: 0.0076817721128463745\n",
      "epoch:  399, loss: 0.007680675480514765\n",
      "epoch:  400, loss: 0.00767955556511879\n",
      "epoch:  401, loss: 0.007678091060370207\n",
      "epoch:  402, loss: 0.007676007226109505\n",
      "epoch:  403, loss: 0.007675474975258112\n",
      "epoch:  404, loss: 0.007673108484596014\n",
      "epoch:  405, loss: 0.007672752719372511\n",
      "epoch:  406, loss: 0.0076704854145646095\n",
      "epoch:  407, loss: 0.007670032791793346\n",
      "epoch:  408, loss: 0.0076679205521941185\n",
      "epoch:  409, loss: 0.00766734080389142\n",
      "epoch:  410, loss: 0.007665303535759449\n",
      "epoch:  411, loss: 0.007664630189538002\n",
      "epoch:  412, loss: 0.007663080468773842\n",
      "epoch:  413, loss: 0.007662008982151747\n",
      "epoch:  414, loss: 0.007659744005650282\n",
      "epoch:  415, loss: 0.007659243419766426\n",
      "epoch:  416, loss: 0.007656741887331009\n",
      "epoch:  417, loss: 0.007656396832317114\n",
      "epoch:  418, loss: 0.007654623594135046\n",
      "epoch:  419, loss: 0.00765370624139905\n",
      "epoch:  420, loss: 0.0076514072716236115\n",
      "epoch:  421, loss: 0.007650899700820446\n",
      "epoch:  422, loss: 0.007648342754691839\n",
      "epoch:  423, loss: 0.007647993043065071\n",
      "epoch:  424, loss: 0.007645709440112114\n",
      "epoch:  425, loss: 0.007645170204341412\n",
      "epoch:  426, loss: 0.0076434677466750145\n",
      "epoch:  427, loss: 0.0076423813588917255\n",
      "epoch:  428, loss: 0.0076402323320508\n",
      "epoch:  429, loss: 0.007639484480023384\n",
      "epoch:  430, loss: 0.007637206930667162\n",
      "epoch:  431, loss: 0.007636526133865118\n",
      "epoch:  432, loss: 0.007636045105755329\n",
      "epoch:  433, loss: 0.007633758708834648\n",
      "epoch:  434, loss: 0.007633007597178221\n",
      "epoch:  435, loss: 0.007630963809788227\n",
      "epoch:  436, loss: 0.0076294634491205215\n",
      "epoch:  437, loss: 0.0076280576176941395\n",
      "epoch:  438, loss: 0.007627767510712147\n",
      "epoch:  439, loss: 0.007625363301485777\n",
      "epoch:  440, loss: 0.007624968886375427\n",
      "epoch:  441, loss: 0.007622509729117155\n",
      "epoch:  442, loss: 0.007622188422828913\n",
      "epoch:  443, loss: 0.0076197427697479725\n",
      "epoch:  444, loss: 0.007619349751621485\n",
      "epoch:  445, loss: 0.007616761140525341\n",
      "epoch:  446, loss: 0.007616663351655006\n",
      "epoch:  447, loss: 0.007613899651914835\n",
      "epoch:  448, loss: 0.007613583002239466\n",
      "epoch:  449, loss: 0.007611003704369068\n",
      "epoch:  450, loss: 0.007610240485519171\n",
      "epoch:  451, loss: 0.007608120329678059\n",
      "epoch:  452, loss: 0.007607823237776756\n",
      "epoch:  453, loss: 0.007605315651744604\n",
      "epoch:  454, loss: 0.007604961283504963\n",
      "epoch:  455, loss: 0.007602707017213106\n",
      "epoch:  456, loss: 0.007602111902087927\n",
      "epoch:  457, loss: 0.007600446231663227\n",
      "epoch:  458, loss: 0.007599279750138521\n",
      "epoch:  459, loss: 0.007597443647682667\n",
      "epoch:  460, loss: 0.007596304640173912\n",
      "epoch:  461, loss: 0.007594329304993153\n",
      "epoch:  462, loss: 0.007593381218612194\n",
      "epoch:  463, loss: 0.007591600995510817\n",
      "epoch:  464, loss: 0.007590464781969786\n",
      "epoch:  465, loss: 0.007588378619402647\n",
      "epoch:  466, loss: 0.007587572559714317\n",
      "epoch:  467, loss: 0.007585566490888596\n",
      "epoch:  468, loss: 0.007584599312394857\n",
      "epoch:  469, loss: 0.007582319434732199\n",
      "epoch:  470, loss: 0.007581590209156275\n",
      "epoch:  471, loss: 0.0075797452591359615\n",
      "epoch:  472, loss: 0.007578575052320957\n",
      "epoch:  473, loss: 0.007576622534543276\n",
      "epoch:  474, loss: 0.007575548719614744\n",
      "epoch:  475, loss: 0.007574079558253288\n",
      "epoch:  476, loss: 0.007572537288069725\n",
      "epoch:  477, loss: 0.00757171493023634\n",
      "epoch:  478, loss: 0.007569530047476292\n",
      "epoch:  479, loss: 0.0075689381919801235\n",
      "epoch:  480, loss: 0.007566550746560097\n",
      "epoch:  481, loss: 0.007565662264823914\n",
      "epoch:  482, loss: 0.007563544902950525\n",
      "epoch:  483, loss: 0.007563192863017321\n",
      "epoch:  484, loss: 0.007560518570244312\n",
      "epoch:  485, loss: 0.007560200523585081\n",
      "epoch:  486, loss: 0.007557491771876812\n",
      "epoch:  487, loss: 0.007557121571153402\n",
      "epoch:  488, loss: 0.007554322946816683\n",
      "epoch:  489, loss: 0.007554222829639912\n",
      "epoch:  490, loss: 0.007551233284175396\n",
      "epoch:  491, loss: 0.007550447713583708\n",
      "epoch:  492, loss: 0.0075481547974050045\n",
      "epoch:  493, loss: 0.00754763837903738\n",
      "epoch:  494, loss: 0.007545029744505882\n",
      "epoch:  495, loss: 0.007543699815869331\n",
      "epoch:  496, loss: 0.007541893050074577\n",
      "epoch:  497, loss: 0.007541255559772253\n",
      "epoch:  498, loss: 0.00753870839253068\n",
      "epoch:  499, loss: 0.0075372252613306046\n",
      "epoch:  500, loss: 0.007535425014793873\n",
      "epoch:  501, loss: 0.00753435492515564\n",
      "epoch:  502, loss: 0.007532135117799044\n",
      "epoch:  503, loss: 0.007530882488936186\n",
      "epoch:  504, loss: 0.007528857793658972\n",
      "epoch:  505, loss: 0.007527451496571302\n",
      "epoch:  506, loss: 0.007525535766035318\n",
      "epoch:  507, loss: 0.007523954380303621\n",
      "epoch:  508, loss: 0.007522194180637598\n",
      "epoch:  509, loss: 0.0075209480710327625\n",
      "epoch:  510, loss: 0.007518845610320568\n",
      "epoch:  511, loss: 0.007516875863075256\n",
      "epoch:  512, loss: 0.007515443488955498\n",
      "epoch:  513, loss: 0.007513567339628935\n",
      "epoch:  514, loss: 0.007511964533478022\n",
      "epoch:  515, loss: 0.007509647402912378\n",
      "epoch:  516, loss: 0.007508447393774986\n",
      "epoch:  517, loss: 0.00750608928501606\n",
      "epoch:  518, loss: 0.0075048524886369705\n",
      "epoch:  519, loss: 0.007502876687794924\n",
      "epoch:  520, loss: 0.007501272484660149\n",
      "epoch:  521, loss: 0.0074986363761126995\n",
      "epoch:  522, loss: 0.007497585378587246\n",
      "epoch:  523, loss: 0.007494835648685694\n",
      "epoch:  524, loss: 0.0074935960583388805\n",
      "epoch:  525, loss: 0.0074905650690197945\n",
      "epoch:  526, loss: 0.007489578332751989\n",
      "epoch:  527, loss: 0.007486179005354643\n",
      "epoch:  528, loss: 0.00748541671782732\n",
      "epoch:  529, loss: 0.007481718435883522\n",
      "epoch:  530, loss: 0.007481132168322802\n",
      "epoch:  531, loss: 0.007477432489395142\n",
      "epoch:  532, loss: 0.007476904429495335\n",
      "epoch:  533, loss: 0.007473477628082037\n",
      "epoch:  534, loss: 0.0074727158062160015\n",
      "epoch:  535, loss: 0.007468980737030506\n",
      "epoch:  536, loss: 0.007468411698937416\n",
      "epoch:  537, loss: 0.007464522495865822\n",
      "epoch:  538, loss: 0.007463972549885511\n",
      "epoch:  539, loss: 0.0074602230452001095\n",
      "epoch:  540, loss: 0.0074595920741558075\n",
      "epoch:  541, loss: 0.007455844897776842\n",
      "epoch:  542, loss: 0.00745509285479784\n",
      "epoch:  543, loss: 0.007451759651303291\n",
      "epoch:  544, loss: 0.007450611796230078\n",
      "epoch:  545, loss: 0.007448180112987757\n",
      "epoch:  546, loss: 0.007446174044162035\n",
      "epoch:  547, loss: 0.0074447151273489\n",
      "epoch:  548, loss: 0.007441690191626549\n",
      "epoch:  549, loss: 0.007440150249749422\n",
      "epoch:  550, loss: 0.007437177933752537\n",
      "epoch:  551, loss: 0.007435262203216553\n",
      "epoch:  552, loss: 0.00743267172947526\n",
      "epoch:  553, loss: 0.007431081496179104\n",
      "epoch:  554, loss: 0.007428115699440241\n",
      "epoch:  555, loss: 0.007425742223858833\n",
      "epoch:  556, loss: 0.007423508912324905\n",
      "epoch:  557, loss: 0.007421792019158602\n",
      "epoch:  558, loss: 0.007418954744935036\n",
      "epoch:  559, loss: 0.007417012937366962\n",
      "epoch:  560, loss: 0.007414319086819887\n",
      "epoch:  561, loss: 0.007412247825413942\n",
      "epoch:  562, loss: 0.007409747689962387\n",
      "epoch:  563, loss: 0.007407086435705423\n",
      "epoch:  564, loss: 0.0074050589464604855\n",
      "epoch:  565, loss: 0.007403151597827673\n",
      "epoch:  566, loss: 0.007400340400636196\n",
      "epoch:  567, loss: 0.007398839108645916\n",
      "epoch:  568, loss: 0.007395628839731216\n",
      "epoch:  569, loss: 0.007393781095743179\n",
      "epoch:  570, loss: 0.007390843238681555\n",
      "epoch:  571, loss: 0.007388682570308447\n",
      "epoch:  572, loss: 0.007385921664535999\n",
      "epoch:  573, loss: 0.007383739575743675\n",
      "epoch:  574, loss: 0.007381013128906488\n",
      "epoch:  575, loss: 0.007379317656159401\n",
      "epoch:  576, loss: 0.007376100867986679\n",
      "epoch:  577, loss: 0.007375063840299845\n",
      "epoch:  578, loss: 0.007371159736067057\n",
      "epoch:  579, loss: 0.0073706465773284435\n",
      "epoch:  580, loss: 0.007366314996033907\n",
      "epoch:  581, loss: 0.007365589030086994\n",
      "epoch:  582, loss: 0.007361529860645533\n",
      "epoch:  583, loss: 0.007360419724136591\n",
      "epoch:  584, loss: 0.007356658112257719\n",
      "epoch:  585, loss: 0.007355359848588705\n",
      "epoch:  586, loss: 0.0073524294421076775\n",
      "epoch:  587, loss: 0.007350304629653692\n",
      "epoch:  588, loss: 0.007348950952291489\n",
      "epoch:  589, loss: 0.007345248479396105\n",
      "epoch:  590, loss: 0.007344967219978571\n",
      "epoch:  591, loss: 0.00734018674120307\n",
      "epoch:  592, loss: 0.007340054493397474\n",
      "epoch:  593, loss: 0.007334945257753134\n",
      "epoch:  594, loss: 0.00733438553288579\n",
      "epoch:  595, loss: 0.0073294686153531075\n",
      "epoch:  596, loss: 0.007327883038669825\n",
      "epoch:  597, loss: 0.007323878817260265\n",
      "epoch:  598, loss: 0.00732249952852726\n",
      "epoch:  599, loss: 0.007318217772990465\n",
      "epoch:  600, loss: 0.007317658513784409\n",
      "epoch:  601, loss: 0.007312485482543707\n",
      "epoch:  602, loss: 0.0073116677813231945\n",
      "epoch:  603, loss: 0.007306684739887714\n",
      "epoch:  604, loss: 0.007306089624762535\n",
      "epoch:  605, loss: 0.007300775032490492\n",
      "epoch:  606, loss: 0.007300170604139566\n",
      "epoch:  607, loss: 0.007294852286577225\n",
      "epoch:  608, loss: 0.007294244132936001\n",
      "epoch:  609, loss: 0.007288986351341009\n",
      "epoch:  610, loss: 0.007288358174264431\n",
      "epoch:  611, loss: 0.00728306220844388\n",
      "epoch:  612, loss: 0.007282437290996313\n",
      "epoch:  613, loss: 0.00727723678573966\n",
      "epoch:  614, loss: 0.007276514079421759\n",
      "epoch:  615, loss: 0.007271324284374714\n",
      "epoch:  616, loss: 0.007270576898008585\n",
      "epoch:  617, loss: 0.007265334017574787\n",
      "epoch:  618, loss: 0.007264544256031513\n",
      "epoch:  619, loss: 0.007259519305080175\n",
      "epoch:  620, loss: 0.0072583043947815895\n",
      "epoch:  621, loss: 0.007253649644553661\n",
      "epoch:  622, loss: 0.007251891307532787\n",
      "epoch:  623, loss: 0.007247282657772303\n",
      "epoch:  624, loss: 0.007245349697768688\n",
      "epoch:  625, loss: 0.007241068407893181\n",
      "epoch:  626, loss: 0.00723881833255291\n",
      "epoch:  627, loss: 0.0072347200475633144\n",
      "epoch:  628, loss: 0.007232180330902338\n",
      "epoch:  629, loss: 0.0072284722700715065\n",
      "epoch:  630, loss: 0.00722554512321949\n",
      "epoch:  631, loss: 0.007221643812954426\n",
      "epoch:  632, loss: 0.007218888029456139\n",
      "epoch:  633, loss: 0.007214848417788744\n",
      "epoch:  634, loss: 0.007212204392999411\n",
      "epoch:  635, loss: 0.007208430208265781\n",
      "epoch:  636, loss: 0.0072054313495755196\n",
      "epoch:  637, loss: 0.007203417830169201\n",
      "epoch:  638, loss: 0.0071986098773777485\n",
      "epoch:  639, loss: 0.007197907660156488\n",
      "epoch:  640, loss: 0.007191734854131937\n",
      "epoch:  641, loss: 0.007191009353846312\n",
      "epoch:  642, loss: 0.007184803951531649\n",
      "epoch:  643, loss: 0.007184032816439867\n",
      "epoch:  644, loss: 0.007177815306931734\n",
      "epoch:  645, loss: 0.00717691658064723\n",
      "epoch:  646, loss: 0.007170868571847677\n",
      "epoch:  647, loss: 0.0071698506362736225\n",
      "epoch:  648, loss: 0.00716432323679328\n",
      "epoch:  649, loss: 0.007162751164287329\n",
      "epoch:  650, loss: 0.007158145774155855\n",
      "epoch:  651, loss: 0.007155583705753088\n",
      "epoch:  652, loss: 0.007153026293963194\n",
      "epoch:  653, loss: 0.0071484302170574665\n",
      "epoch:  654, loss: 0.007147697731852531\n",
      "epoch:  655, loss: 0.007141366135329008\n",
      "epoch:  656, loss: 0.007140478119254112\n",
      "epoch:  657, loss: 0.007134707178920507\n",
      "epoch:  658, loss: 0.007133237551897764\n",
      "epoch:  659, loss: 0.007128596771508455\n",
      "epoch:  660, loss: 0.007125968113541603\n",
      "epoch:  661, loss: 0.007123109884560108\n",
      "epoch:  662, loss: 0.007118553388863802\n",
      "epoch:  663, loss: 0.007117797154933214\n",
      "epoch:  664, loss: 0.007111087907105684\n",
      "epoch:  665, loss: 0.007110288366675377\n",
      "epoch:  666, loss: 0.007103664334863424\n",
      "epoch:  667, loss: 0.007102785166352987\n",
      "epoch:  668, loss: 0.00709633668884635\n",
      "epoch:  669, loss: 0.007095300126820803\n",
      "epoch:  670, loss: 0.007089301478117704\n",
      "epoch:  671, loss: 0.007087650243192911\n",
      "epoch:  672, loss: 0.007083281874656677\n",
      "epoch:  673, loss: 0.007080028764903545\n",
      "epoch:  674, loss: 0.007079277187585831\n",
      "epoch:  675, loss: 0.0070727430284023285\n",
      "epoch:  676, loss: 0.007071573752909899\n",
      "epoch:  677, loss: 0.007065602578222752\n",
      "epoch:  678, loss: 0.007063805591315031\n",
      "epoch:  679, loss: 0.007058471906930208\n",
      "epoch:  680, loss: 0.007055874913930893\n",
      "epoch:  681, loss: 0.007053447421640158\n",
      "epoch:  682, loss: 0.007047784980386496\n",
      "epoch:  683, loss: 0.007046943996101618\n",
      "epoch:  684, loss: 0.007039627525955439\n",
      "epoch:  685, loss: 0.007038698997348547\n",
      "epoch:  686, loss: 0.007031698245555162\n",
      "epoch:  687, loss: 0.007030388340353966\n",
      "epoch:  688, loss: 0.007024351507425308\n",
      "epoch:  689, loss: 0.0070220851339399815\n",
      "epoch:  690, loss: 0.0070180995389819145\n",
      "epoch:  691, loss: 0.007013800088316202\n",
      "epoch:  692, loss: 0.007012950722128153\n",
      "epoch:  693, loss: 0.007005465216934681\n",
      "epoch:  694, loss: 0.0070045338943600655\n",
      "epoch:  695, loss: 0.006997299380600452\n",
      "epoch:  696, loss: 0.006996188312768936\n",
      "epoch:  697, loss: 0.006989751942455769\n",
      "epoch:  698, loss: 0.006987801752984524\n",
      "epoch:  699, loss: 0.006983072031289339\n",
      "epoch:  700, loss: 0.006979295983910561\n",
      "epoch:  701, loss: 0.0069785271771252155\n",
      "epoch:  702, loss: 0.006970771588385105\n",
      "epoch:  703, loss: 0.006969810929149389\n",
      "epoch:  704, loss: 0.006962388288229704\n",
      "epoch:  705, loss: 0.006961076986044645\n",
      "epoch:  706, loss: 0.006954450625926256\n",
      "epoch:  707, loss: 0.006952302064746618\n",
      "epoch:  708, loss: 0.006947671063244343\n",
      "epoch:  709, loss: 0.006943590007722378\n",
      "epoch:  710, loss: 0.006942702457308769\n",
      "epoch:  711, loss: 0.006935024168342352\n",
      "epoch:  712, loss: 0.006933813914656639\n",
      "epoch:  713, loss: 0.006926632020622492\n",
      "epoch:  714, loss: 0.006924840155988932\n",
      "epoch:  715, loss: 0.006919149309396744\n",
      "epoch:  716, loss: 0.006915799342095852\n",
      "epoch:  717, loss: 0.006914897356182337\n",
      "epoch:  718, loss: 0.006906932685524225\n",
      "epoch:  719, loss: 0.006905749905854464\n",
      "epoch:  720, loss: 0.0068982564844191074\n",
      "epoch:  721, loss: 0.006896516308188438\n",
      "epoch:  722, loss: 0.006891678087413311\n",
      "epoch:  723, loss: 0.006887268275022507\n",
      "epoch:  724, loss: 0.006886310409754515\n",
      "epoch:  725, loss: 0.0068780817091465\n",
      "epoch:  726, loss: 0.006876773666590452\n",
      "epoch:  727, loss: 0.0068692839704453945\n",
      "epoch:  728, loss: 0.006867195479571819\n",
      "epoch:  729, loss: 0.006863204296678305\n",
      "epoch:  730, loss: 0.0068575553596019745\n",
      "epoch:  731, loss: 0.006856579799205065\n",
      "epoch:  732, loss: 0.006848067045211792\n",
      "epoch:  733, loss: 0.006846862845122814\n",
      "epoch:  734, loss: 0.006838659755885601\n",
      "epoch:  735, loss: 0.006836944725364447\n",
      "epoch:  736, loss: 0.006830340716987848\n",
      "epoch:  737, loss: 0.006826965603977442\n",
      "epoch:  738, loss: 0.0068259648978710175\n",
      "epoch:  739, loss: 0.006817066576331854\n",
      "epoch:  740, loss: 0.006815882865339518\n",
      "epoch:  741, loss: 0.006807683035731316\n",
      "epoch:  742, loss: 0.006805784069001675\n",
      "epoch:  743, loss: 0.0068008084781467915\n",
      "epoch:  744, loss: 0.006795702967792749\n",
      "epoch:  745, loss: 0.006794600281864405\n",
      "epoch:  746, loss: 0.006785843521356583\n",
      "epoch:  747, loss: 0.006784338504076004\n",
      "epoch:  748, loss: 0.006777300965040922\n",
      "epoch:  749, loss: 0.006774058565497398\n",
      "epoch:  750, loss: 0.006773035507649183\n",
      "epoch:  751, loss: 0.00676420284435153\n",
      "epoch:  752, loss: 0.006762698758393526\n",
      "epoch:  753, loss: 0.0067567541263997555\n",
      "epoch:  754, loss: 0.006752451881766319\n",
      "epoch:  755, loss: 0.006751363165676594\n",
      "epoch:  756, loss: 0.006743023172020912\n",
      "epoch:  757, loss: 0.0067407917231321335\n",
      "epoch:  758, loss: 0.006738211959600449\n",
      "epoch:  759, loss: 0.006729984190315008\n",
      "epoch:  760, loss: 0.0067286710254848\n",
      "epoch:  761, loss: 0.006719878874719143\n",
      "epoch:  762, loss: 0.006717535201460123\n",
      "epoch:  763, loss: 0.006714131683111191\n",
      "epoch:  764, loss: 0.006706499494612217\n",
      "epoch:  765, loss: 0.006705209147185087\n",
      "epoch:  766, loss: 0.006696774158626795\n",
      "epoch:  767, loss: 0.006693938747048378\n",
      "epoch:  768, loss: 0.006693328730762005\n",
      "epoch:  769, loss: 0.006682842504233122\n",
      "epoch:  770, loss: 0.006681413855403662\n",
      "epoch:  771, loss: 0.0066727204248309135\n",
      "epoch:  772, loss: 0.006669866852462292\n",
      "epoch:  773, loss: 0.006667352747172117\n",
      "epoch:  774, loss: 0.006658296100795269\n",
      "epoch:  775, loss: 0.006656848359853029\n",
      "epoch:  776, loss: 0.0066473400220274925\n",
      "epoch:  777, loss: 0.006644852925091982\n",
      "epoch:  778, loss: 0.006642056163400412\n",
      "epoch:  779, loss: 0.0066328467801213264\n",
      "epoch:  780, loss: 0.006631369702517986\n",
      "epoch:  781, loss: 0.006622280925512314\n",
      "epoch:  782, loss: 0.00661891046911478\n",
      "epoch:  783, loss: 0.006617669016122818\n",
      "epoch:  784, loss: 0.006607317831367254\n",
      "epoch:  785, loss: 0.006605209782719612\n",
      "epoch:  786, loss: 0.0065993573516607285\n",
      "epoch:  787, loss: 0.006592842284590006\n",
      "epoch:  788, loss: 0.006591447629034519\n",
      "epoch:  789, loss: 0.0065811024978756905\n",
      "epoch:  790, loss: 0.006578817497938871\n",
      "epoch:  791, loss: 0.006573979277163744\n",
      "epoch:  792, loss: 0.006566199474036694\n",
      "epoch:  793, loss: 0.006564733572304249\n",
      "epoch:  794, loss: 0.006554285064339638\n",
      "epoch:  795, loss: 0.006551593542098999\n",
      "epoch:  796, loss: 0.006545821204781532\n",
      "epoch:  797, loss: 0.006538413930684328\n",
      "epoch:  798, loss: 0.006536884233355522\n",
      "epoch:  799, loss: 0.006526892073452473\n",
      "epoch:  800, loss: 0.006523770745843649\n",
      "epoch:  801, loss: 0.006522427313029766\n",
      "epoch:  802, loss: 0.006511164829134941\n",
      "epoch:  803, loss: 0.006509206723421812\n",
      "epoch:  804, loss: 0.006503211334347725\n",
      "epoch:  805, loss: 0.006495886016637087\n",
      "epoch:  806, loss: 0.006494355853646994\n",
      "epoch:  807, loss: 0.006484378129243851\n",
      "epoch:  808, loss: 0.006481016054749489\n",
      "epoch:  809, loss: 0.006479690782725811\n",
      "epoch:  810, loss: 0.006468051578849554\n",
      "epoch:  811, loss: 0.006466233171522617\n",
      "epoch:  812, loss: 0.006458107382059097\n",
      "epoch:  813, loss: 0.006452913861721754\n",
      "epoch:  814, loss: 0.006451502908021212\n",
      "epoch:  815, loss: 0.006440345197916031\n",
      "epoch:  816, loss: 0.006438113283365965\n",
      "epoch:  817, loss: 0.00643390417098999\n",
      "epoch:  818, loss: 0.006425115279853344\n",
      "epoch:  819, loss: 0.006423546001315117\n",
      "epoch:  820, loss: 0.0064141047187149525\n",
      "epoch:  821, loss: 0.006410202011466026\n",
      "epoch:  822, loss: 0.006408850662410259\n",
      "epoch:  823, loss: 0.006397327873855829\n",
      "epoch:  824, loss: 0.006395363714545965\n",
      "epoch:  825, loss: 0.006390771828591824\n",
      "epoch:  826, loss: 0.006382356863468885\n",
      "epoch:  827, loss: 0.006380771286785603\n",
      "epoch:  828, loss: 0.006372467149049044\n",
      "epoch:  829, loss: 0.006367694586515427\n",
      "epoch:  830, loss: 0.006366293411701918\n",
      "epoch:  831, loss: 0.006356032099574804\n",
      "epoch:  832, loss: 0.006352961529046297\n",
      "epoch:  833, loss: 0.0063516064547002316\n",
      "epoch:  834, loss: 0.006339792162179947\n",
      "epoch:  835, loss: 0.00633787689730525\n",
      "epoch:  836, loss: 0.006331196054816246\n",
      "epoch:  837, loss: 0.006324315443634987\n",
      "epoch:  838, loss: 0.00632270285859704\n",
      "epoch:  839, loss: 0.006313132122159004\n",
      "epoch:  840, loss: 0.006308896467089653\n",
      "epoch:  841, loss: 0.006307457573711872\n",
      "epoch:  842, loss: 0.006297043524682522\n",
      "epoch:  843, loss: 0.006293600890785456\n",
      "epoch:  844, loss: 0.006292182020843029\n",
      "epoch:  845, loss: 0.006280888803303242\n",
      "epoch:  846, loss: 0.006278194021433592\n",
      "epoch:  847, loss: 0.00627682963386178\n",
      "epoch:  848, loss: 0.006265329662710428\n",
      "epoch:  849, loss: 0.006262805312871933\n",
      "epoch:  850, loss: 0.006261576432734728\n",
      "epoch:  851, loss: 0.00624927319586277\n",
      "epoch:  852, loss: 0.006247343961149454\n",
      "epoch:  853, loss: 0.006239759735763073\n",
      "epoch:  854, loss: 0.006233178544789553\n",
      "epoch:  855, loss: 0.006231554783880711\n",
      "epoch:  856, loss: 0.006222075782716274\n",
      "epoch:  857, loss: 0.006217217538505793\n",
      "epoch:  858, loss: 0.006215642672032118\n",
      "epoch:  859, loss: 0.006205492187291384\n",
      "epoch:  860, loss: 0.006200995296239853\n",
      "epoch:  861, loss: 0.0061994874849915504\n",
      "epoch:  862, loss: 0.006187139544636011\n",
      "epoch:  863, loss: 0.006184512283653021\n",
      "epoch:  864, loss: 0.006178421899676323\n",
      "epoch:  865, loss: 0.006169530563056469\n",
      "epoch:  866, loss: 0.006167677231132984\n",
      "epoch:  867, loss: 0.006158456671983004\n",
      "epoch:  868, loss: 0.006152534391731024\n",
      "epoch:  869, loss: 0.006150821223855019\n",
      "epoch:  870, loss: 0.006140798330307007\n",
      "epoch:  871, loss: 0.0061355940997600555\n",
      "epoch:  872, loss: 0.006133909337222576\n",
      "epoch:  873, loss: 0.006122691556811333\n",
      "epoch:  874, loss: 0.006118508987128735\n",
      "epoch:  875, loss: 0.006116892211139202\n",
      "epoch:  876, loss: 0.00610335823148489\n",
      "epoch:  877, loss: 0.006101167295128107\n",
      "epoch:  878, loss: 0.006095133256167173\n",
      "epoch:  879, loss: 0.0060858214274048805\n",
      "epoch:  880, loss: 0.006083666812628508\n",
      "epoch:  881, loss: 0.006075473967939615\n",
      "epoch:  882, loss: 0.00606755493208766\n",
      "epoch:  883, loss: 0.0060657174326479435\n",
      "epoch:  884, loss: 0.006052732467651367\n",
      "epoch:  885, loss: 0.006048941984772682\n",
      "epoch:  886, loss: 0.006047222763299942\n",
      "epoch:  887, loss: 0.006035196594893932\n",
      "epoch:  888, loss: 0.006030506920069456\n",
      "epoch:  889, loss: 0.006028797011822462\n",
      "epoch:  890, loss: 0.006015630904585123\n",
      "epoch:  891, loss: 0.006012223195284605\n",
      "epoch:  892, loss: 0.006010556593537331\n",
      "epoch:  893, loss: 0.0059974584728479385\n",
      "epoch:  894, loss: 0.0059938435442745686\n",
      "epoch:  895, loss: 0.0059921895153820515\n",
      "epoch:  896, loss: 0.005978885106742382\n",
      "epoch:  897, loss: 0.00597560266032815\n",
      "epoch:  898, loss: 0.005973911844193935\n",
      "epoch:  899, loss: 0.005959865637123585\n",
      "epoch:  900, loss: 0.005956901237368584\n",
      "epoch:  901, loss: 0.005955204833298922\n",
      "epoch:  902, loss: 0.005940628238022327\n",
      "epoch:  903, loss: 0.005938020534813404\n",
      "epoch:  904, loss: 0.005936339031904936\n",
      "epoch:  905, loss: 0.005922211799770594\n",
      "epoch:  906, loss: 0.00591919245198369\n",
      "epoch:  907, loss: 0.005917502101510763\n",
      "epoch:  908, loss: 0.005903380457311869\n",
      "epoch:  909, loss: 0.005900463089346886\n",
      "epoch:  910, loss: 0.005898745264858007\n",
      "epoch:  911, loss: 0.005885790102183819\n",
      "epoch:  912, loss: 0.005881879478693008\n",
      "epoch:  913, loss: 0.005880154203623533\n",
      "epoch:  914, loss: 0.005867890082299709\n",
      "epoch:  915, loss: 0.005863548722118139\n",
      "epoch:  916, loss: 0.005861744284629822\n",
      "epoch:  917, loss: 0.005850088782608509\n",
      "epoch:  918, loss: 0.005845263134688139\n",
      "epoch:  919, loss: 0.005843445658683777\n",
      "epoch:  920, loss: 0.00583275593817234\n",
      "epoch:  921, loss: 0.005827100481837988\n",
      "epoch:  922, loss: 0.005825183819979429\n",
      "epoch:  923, loss: 0.005815821699798107\n",
      "epoch:  924, loss: 0.005809098016470671\n",
      "epoch:  925, loss: 0.005807050503790379\n",
      "epoch:  926, loss: 0.005799970123916864\n",
      "epoch:  927, loss: 0.005791285075247288\n",
      "epoch:  928, loss: 0.00578901544213295\n",
      "epoch:  929, loss: 0.005786775145679712\n",
      "epoch:  930, loss: 0.005773745942860842\n",
      "epoch:  931, loss: 0.005771119147539139\n",
      "epoch:  932, loss: 0.005769515410065651\n",
      "epoch:  933, loss: 0.005756525322794914\n",
      "epoch:  934, loss: 0.005753174889832735\n",
      "epoch:  935, loss: 0.005751477554440498\n",
      "epoch:  936, loss: 0.005739591084420681\n",
      "epoch:  937, loss: 0.005735214799642563\n",
      "epoch:  938, loss: 0.005733366124331951\n",
      "epoch:  939, loss: 0.005721799097955227\n",
      "epoch:  940, loss: 0.005717025604099035\n",
      "epoch:  941, loss: 0.005715156905353069\n",
      "epoch:  942, loss: 0.005705786403268576\n",
      "epoch:  943, loss: 0.005698968656361103\n",
      "epoch:  944, loss: 0.00569683313369751\n",
      "epoch:  945, loss: 0.005692329257726669\n",
      "epoch:  946, loss: 0.005681005772203207\n",
      "epoch:  947, loss: 0.005678529851138592\n",
      "epoch:  948, loss: 0.0056768846698105335\n",
      "epoch:  949, loss: 0.005663442425429821\n",
      "epoch:  950, loss: 0.005660135764628649\n",
      "epoch:  951, loss: 0.005658367183059454\n",
      "epoch:  952, loss: 0.005644972436130047\n",
      "epoch:  953, loss: 0.005641393829137087\n",
      "epoch:  954, loss: 0.005639570765197277\n",
      "epoch:  955, loss: 0.005629498511552811\n",
      "epoch:  956, loss: 0.005622983444482088\n",
      "epoch:  957, loss: 0.0056208400055766106\n",
      "epoch:  958, loss: 0.005618332419544458\n",
      "epoch:  959, loss: 0.0056052440777421\n",
      "epoch:  960, loss: 0.005602217745035887\n",
      "epoch:  961, loss: 0.005600488279014826\n",
      "epoch:  962, loss: 0.005589934531599283\n",
      "epoch:  963, loss: 0.0055839987471699715\n",
      "epoch:  964, loss: 0.005581865087151527\n",
      "epoch:  965, loss: 0.005579081829637289\n",
      "epoch:  966, loss: 0.005566326901316643\n",
      "epoch:  967, loss: 0.00556338531896472\n",
      "epoch:  968, loss: 0.005561633035540581\n",
      "epoch:  969, loss: 0.005550669506192207\n",
      "epoch:  970, loss: 0.005545241758227348\n",
      "epoch:  971, loss: 0.005543180275708437\n",
      "epoch:  972, loss: 0.005541556049138308\n",
      "epoch:  973, loss: 0.005528520327061415\n",
      "epoch:  974, loss: 0.00552487513050437\n",
      "epoch:  975, loss: 0.0055230604484677315\n",
      "epoch:  976, loss: 0.005517421290278435\n",
      "epoch:  977, loss: 0.005507372785359621\n",
      "epoch:  978, loss: 0.005504659377038479\n",
      "epoch:  979, loss: 0.0055029368959367275\n",
      "epoch:  980, loss: 0.005492363125085831\n",
      "epoch:  981, loss: 0.00548664340749383\n",
      "epoch:  982, loss: 0.005484465975314379\n",
      "epoch:  983, loss: 0.005482822190970182\n",
      "epoch:  984, loss: 0.005470488220453262\n",
      "epoch:  985, loss: 0.005466420669108629\n",
      "epoch:  986, loss: 0.005464511923491955\n",
      "epoch:  987, loss: 0.005456853657960892\n",
      "epoch:  988, loss: 0.005448495969176292\n",
      "epoch:  989, loss: 0.005446212366223335\n",
      "epoch:  990, loss: 0.005444538779556751\n",
      "epoch:  991, loss: 0.0054322537034749985\n",
      "epoch:  992, loss: 0.005427851807326078\n",
      "epoch:  993, loss: 0.005425936076790094\n",
      "epoch:  994, loss: 0.005414715502411127\n",
      "epoch:  995, loss: 0.005408946890383959\n",
      "epoch:  996, loss: 0.005406851880252361\n",
      "epoch:  997, loss: 0.0054046595469117165\n",
      "epoch:  998, loss: 0.0053911712020635605\n",
      "epoch:  999, loss: 0.005388015881180763\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size=X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "opt = torch_numopt.GradientDescentLS(params=model.parameters(), model=model, lr=1, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"armijo\")\n",
    "# opt = torch_numopt.GradientDescentLS(params=model.parameters(), model=model, lr=1, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"wolfe\")\n",
    "# opt = torch_numopt.GradientDescentLS(params=model.parameters(), model=model, lr=1, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"strong-wolfe\")\n",
    "# opt = torch_numopt.GradientDescentLS(params=model.parameters(), model=model, lr=1, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"goldstein\")\n",
    "\n",
    "all_loss = {}\n",
    "for epoch in range(1000):\n",
    "    print(\"epoch: \", epoch, end=\"\")\n",
    "    all_loss[epoch + 1] = 0\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y, loss_fn)\n",
    "\n",
    "        all_loss[epoch + 1] += loss\n",
    "    all_loss[epoch + 1] /= len(data_loader)\n",
    "    print(\", loss: {}\".format(all_loss[epoch + 1].detach().numpy().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics: R2 = 0.7847538171480244\n",
      "Test metrics:  R2 = 0.7983723281341311\n"
     ]
    }
   ],
   "source": [
    "pred_train = model.forward(X_train).detach()\n",
    "pred_test = model.forward(X_test).detach()\n",
    "print(f\"Train metrics: R2 = {r2_score(pred_train, y_train)}\")\n",
    "print(f\"Test metrics:  R2 = {r2_score(pred_test, y_test)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
