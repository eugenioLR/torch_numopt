{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch_numopt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import *\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.f1 = nn.Linear(input_size, 10, device=device)\n",
    "        self.f2 = nn.Linear(10, 20, device=device)\n",
    "        self.f3 = nn.Linear(20, 20, device=device)\n",
    "        self.f4 = nn.Linear(20, 10, device=device)\n",
    "        self.f5 = nn.Linear(10, 1, device=device)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        # self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.f1(x))\n",
    "        x = self.activation(self.f2(x))\n",
    "        x = self.activation(self.f3(x))\n",
    "        x = self.activation(self.f4(x))\n",
    "        x = self.f5(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([800, 10]) torch.Size([800, 1])\n",
      "torch.Size([200, 10]) torch.Size([200, 1])\n"
     ]
    }
   ],
   "source": [
    "# X, y = load_diabetes(return_X_y=True, scaled=False)\n",
    "# X, y = make_regression(n_samples=1000, n_features=100)\n",
    "X, y = make_friedman1(n_samples=1000, noise=1e-2)\n",
    "\n",
    "X_scaler = MinMaxScaler()\n",
    "X = X_scaler.fit_transform(X)\n",
    "X = torch.Tensor(X).to(device)\n",
    "\n",
    "y_scaler = MinMaxScaler()\n",
    "y = y_scaler.fit_transform(y.reshape((-1, 1)))\n",
    "y = torch.Tensor(y).to(device)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "torch_data = TensorDataset(X_train, y_train)\n",
    "data_loader = DataLoader(torch_data, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.552892804145813\n",
      "epoch:  1, loss: 0.2987568974494934\n",
      "epoch:  2, loss: 0.16441497206687927\n",
      "epoch:  3, loss: 0.09599409252405167\n",
      "epoch:  4, loss: 0.06289730966091156\n",
      "epoch:  5, loss: 0.04767049849033356\n",
      "epoch:  6, loss: 0.040951646864414215\n",
      "epoch:  7, loss: 0.038077838718891144\n",
      "epoch:  8, loss: 0.0368734747171402\n",
      "epoch:  9, loss: 0.036373261362314224\n",
      "epoch:  10, loss: 0.03616387024521828\n",
      "epoch:  11, loss: 0.036073021590709686\n",
      "epoch:  12, loss: 0.03602994233369827\n",
      "epoch:  13, loss: 0.03600570559501648\n",
      "epoch:  14, loss: 0.03593648225069046\n",
      "epoch:  15, loss: 0.03588350862264633\n",
      "epoch:  16, loss: 0.03587307780981064\n",
      "epoch:  17, loss: 0.035746656358242035\n",
      "epoch:  18, loss: 0.035684891045093536\n",
      "epoch:  19, loss: 0.03567744418978691\n",
      "epoch:  20, loss: 0.03553331643342972\n",
      "epoch:  21, loss: 0.035466086119413376\n",
      "epoch:  22, loss: 0.035430099815130234\n",
      "epoch:  23, loss: 0.035364601761102676\n",
      "epoch:  24, loss: 0.03529410809278488\n",
      "epoch:  25, loss: 0.03525877371430397\n",
      "epoch:  26, loss: 0.03523438796401024\n",
      "epoch:  27, loss: 0.0351538211107254\n",
      "epoch:  28, loss: 0.03511515632271767\n",
      "epoch:  29, loss: 0.035106219351291656\n",
      "epoch:  30, loss: 0.03501543030142784\n",
      "epoch:  31, loss: 0.03497228026390076\n",
      "epoch:  32, loss: 0.03494787961244583\n",
      "epoch:  33, loss: 0.034874673932790756\n",
      "epoch:  34, loss: 0.03482584282755852\n",
      "epoch:  35, loss: 0.034799132496118546\n",
      "epoch:  36, loss: 0.03472784534096718\n",
      "epoch:  37, loss: 0.03467279672622681\n",
      "epoch:  38, loss: 0.03464303910732269\n",
      "epoch:  39, loss: 0.03457336500287056\n",
      "epoch:  40, loss: 0.034509751945734024\n",
      "epoch:  41, loss: 0.03447655215859413\n",
      "epoch:  42, loss: 0.03440885990858078\n",
      "epoch:  43, loss: 0.03433699160814285\n",
      "epoch:  44, loss: 0.03429976478219032\n",
      "epoch:  45, loss: 0.03423653915524483\n",
      "epoch:  46, loss: 0.034152135252952576\n",
      "epoch:  47, loss: 0.03410997614264488\n",
      "epoch:  48, loss: 0.03404944762587547\n",
      "epoch:  49, loss: 0.03395320475101471\n",
      "epoch:  50, loss: 0.033905528485774994\n",
      "epoch:  51, loss: 0.033851828426122665\n",
      "epoch:  52, loss: 0.03373832628130913\n",
      "epoch:  53, loss: 0.03368396311998367\n",
      "epoch:  54, loss: 0.033633507788181305\n",
      "epoch:  55, loss: 0.03350437432527542\n",
      "epoch:  56, loss: 0.033442843705415726\n",
      "epoch:  57, loss: 0.0334048792719841\n",
      "epoch:  58, loss: 0.0332520566880703\n",
      "epoch:  59, loss: 0.033182017505168915\n",
      "epoch:  60, loss: 0.03315258026123047\n",
      "epoch:  61, loss: 0.032977212220430374\n",
      "epoch:  62, loss: 0.03289727121591568\n",
      "epoch:  63, loss: 0.03289071097970009\n",
      "epoch:  64, loss: 0.03267867863178253\n",
      "epoch:  65, loss: 0.03258626535534859\n",
      "epoch:  66, loss: 0.032535359263420105\n",
      "epoch:  67, loss: 0.03234979137778282\n",
      "epoch:  68, loss: 0.03224393352866173\n",
      "epoch:  69, loss: 0.032186321914196014\n",
      "epoch:  70, loss: 0.03199503943324089\n",
      "epoch:  71, loss: 0.031868938356637955\n",
      "epoch:  72, loss: 0.031803544610738754\n",
      "epoch:  73, loss: 0.03159981220960617\n",
      "epoch:  74, loss: 0.03145653009414673\n",
      "epoch:  75, loss: 0.03138274699449539\n",
      "epoch:  76, loss: 0.0311770960688591\n",
      "epoch:  77, loss: 0.031004497781395912\n",
      "epoch:  78, loss: 0.030920658260583878\n",
      "epoch:  79, loss: 0.0307058896869421\n",
      "epoch:  80, loss: 0.030507748946547508\n",
      "epoch:  81, loss: 0.030412515625357628\n",
      "epoch:  82, loss: 0.03020664118230343\n",
      "epoch:  83, loss: 0.02996690757572651\n",
      "epoch:  84, loss: 0.0298586655408144\n",
      "epoch:  85, loss: 0.029644673690199852\n",
      "epoch:  86, loss: 0.029374178498983383\n",
      "epoch:  87, loss: 0.0292540043592453\n",
      "epoch:  88, loss: 0.02905449829995632\n",
      "epoch:  89, loss: 0.028730785474181175\n",
      "epoch:  90, loss: 0.028596609830856323\n",
      "epoch:  91, loss: 0.028394430875778198\n",
      "epoch:  92, loss: 0.0280287005007267\n",
      "epoch:  93, loss: 0.027880661189556122\n",
      "epoch:  94, loss: 0.027687301859259605\n",
      "epoch:  95, loss: 0.02726413495838642\n",
      "epoch:  96, loss: 0.027103513479232788\n",
      "epoch:  97, loss: 0.026890894398093224\n",
      "epoch:  98, loss: 0.026429632678627968\n",
      "epoch:  99, loss: 0.026258759200572968\n",
      "epoch:  100, loss: 0.026028364896774292\n",
      "epoch:  101, loss: 0.025524841621518135\n",
      "epoch:  102, loss: 0.02534736506640911\n",
      "epoch:  103, loss: 0.025056468322873116\n",
      "epoch:  104, loss: 0.024545693770051003\n",
      "epoch:  105, loss: 0.02436835691332817\n",
      "epoch:  106, loss: 0.02399405464529991\n",
      "epoch:  107, loss: 0.023492204025387764\n",
      "epoch:  108, loss: 0.023321837186813354\n",
      "epoch:  109, loss: 0.022828249260783195\n",
      "epoch:  110, loss: 0.022377200424671173\n",
      "epoch:  111, loss: 0.022218842059373856\n",
      "epoch:  112, loss: 0.021598001942038536\n",
      "epoch:  113, loss: 0.021213170140981674\n",
      "epoch:  114, loss: 0.02107273042201996\n",
      "epoch:  115, loss: 0.020297255367040634\n",
      "epoch:  116, loss: 0.02002318762242794\n",
      "epoch:  117, loss: 0.019879382103681564\n",
      "epoch:  118, loss: 0.019012799486517906\n",
      "epoch:  119, loss: 0.018831491470336914\n",
      "epoch:  120, loss: 0.018196934834122658\n",
      "epoch:  121, loss: 0.01778850331902504\n",
      "epoch:  122, loss: 0.017661718651652336\n",
      "epoch:  123, loss: 0.016844939440488815\n",
      "epoch:  124, loss: 0.016632191836833954\n",
      "epoch:  125, loss: 0.016132835298776627\n",
      "epoch:  126, loss: 0.015663091093301773\n",
      "epoch:  127, loss: 0.015555295161902905\n",
      "epoch:  128, loss: 0.014792594127357006\n",
      "epoch:  129, loss: 0.014648277312517166\n",
      "epoch:  130, loss: 0.01403915137052536\n",
      "epoch:  131, loss: 0.013818175531923771\n",
      "epoch:  132, loss: 0.01338373962789774\n",
      "epoch:  133, loss: 0.013066225685179234\n",
      "epoch:  134, loss: 0.012758921831846237\n",
      "epoch:  135, loss: 0.012392816133797169\n",
      "epoch:  136, loss: 0.012103964574635029\n",
      "epoch:  137, loss: 0.01179503370076418\n",
      "epoch:  138, loss: 0.011428522877395153\n",
      "epoch:  139, loss: 0.011272036470472813\n",
      "epoch:  140, loss: 0.010892353951931\n",
      "epoch:  141, loss: 0.010818188078701496\n",
      "epoch:  142, loss: 0.010460464283823967\n",
      "epoch:  143, loss: 0.01042830292135477\n",
      "epoch:  144, loss: 0.010112915188074112\n",
      "epoch:  145, loss: 0.009844937361776829\n",
      "epoch:  146, loss: 0.009817175567150116\n",
      "epoch:  147, loss: 0.009585815481841564\n",
      "epoch:  148, loss: 0.009422977454960346\n",
      "epoch:  149, loss: 0.009366746991872787\n",
      "epoch:  150, loss: 0.009222443215548992\n",
      "epoch:  151, loss: 0.009181767702102661\n",
      "epoch:  152, loss: 0.009057887829840183\n",
      "epoch:  153, loss: 0.009026013314723969\n",
      "epoch:  154, loss: 0.008924132212996483\n",
      "epoch:  155, loss: 0.008893772028386593\n",
      "epoch:  156, loss: 0.008816225454211235\n",
      "epoch:  157, loss: 0.008781769312918186\n",
      "epoch:  158, loss: 0.008729325607419014\n",
      "epoch:  159, loss: 0.008687048219144344\n",
      "epoch:  160, loss: 0.008673124015331268\n",
      "epoch:  161, loss: 0.008607851341366768\n",
      "epoch:  162, loss: 0.008600342087447643\n",
      "epoch:  163, loss: 0.008540933951735497\n",
      "epoch:  164, loss: 0.008534558117389679\n",
      "epoch:  165, loss: 0.008486634120345116\n",
      "epoch:  166, loss: 0.008478611707687378\n",
      "epoch:  167, loss: 0.008444582112133503\n",
      "epoch:  168, loss: 0.008431047201156616\n",
      "epoch:  169, loss: 0.008415616117417812\n",
      "epoch:  170, loss: 0.008389987051486969\n",
      "epoch:  171, loss: 0.008386150002479553\n",
      "epoch:  172, loss: 0.008354827761650085\n",
      "epoch:  173, loss: 0.008350973017513752\n",
      "epoch:  174, loss: 0.00832641776651144\n",
      "epoch:  175, loss: 0.008320684544742107\n",
      "epoch:  176, loss: 0.008305228315293789\n",
      "epoch:  177, loss: 0.008294492028653622\n",
      "epoch:  178, loss: 0.00829423125833273\n",
      "epoch:  179, loss: 0.008271817117929459\n",
      "epoch:  180, loss: 0.008269531652331352\n",
      "epoch:  181, loss: 0.0082527631893754\n",
      "epoch:  182, loss: 0.008249650709331036\n",
      "epoch:  183, loss: 0.00824018195271492\n",
      "epoch:  184, loss: 0.008232254534959793\n",
      "epoch:  185, loss: 0.008230617269873619\n",
      "epoch:  186, loss: 0.008217494003474712\n",
      "epoch:  187, loss: 0.00821556057780981\n",
      "epoch:  188, loss: 0.008205905556678772\n",
      "epoch:  189, loss: 0.008202511817216873\n",
      "epoch:  190, loss: 0.008201071061193943\n",
      "epoch:  191, loss: 0.008191230706870556\n",
      "epoch:  192, loss: 0.008190072141587734\n",
      "epoch:  193, loss: 0.0081820422783494\n",
      "epoch:  194, loss: 0.008179964497685432\n",
      "epoch:  195, loss: 0.008176963776350021\n",
      "epoch:  196, loss: 0.008171121589839458\n",
      "epoch:  197, loss: 0.00817023403942585\n",
      "epoch:  198, loss: 0.00816329661756754\n",
      "epoch:  199, loss: 0.00816232617944479\n",
      "epoch:  200, loss: 0.008156392723321915\n",
      "epoch:  201, loss: 0.008155141025781631\n",
      "epoch:  202, loss: 0.008152123540639877\n",
      "epoch:  203, loss: 0.00814858265221119\n",
      "epoch:  204, loss: 0.008147946558892727\n",
      "epoch:  205, loss: 0.008142589591443539\n",
      "epoch:  206, loss: 0.008141913451254368\n",
      "epoch:  207, loss: 0.008137794211506844\n",
      "epoch:  208, loss: 0.008136536926031113\n",
      "epoch:  209, loss: 0.008135305717587471\n",
      "epoch:  210, loss: 0.008131463080644608\n",
      "epoch:  211, loss: 0.008130907081067562\n",
      "epoch:  212, loss: 0.00812680646777153\n",
      "epoch:  213, loss: 0.008125967346131802\n",
      "epoch:  214, loss: 0.008122880011796951\n",
      "epoch:  215, loss: 0.008121448568999767\n",
      "epoch:  216, loss: 0.008120282553136349\n",
      "epoch:  217, loss: 0.008117103017866611\n",
      "epoch:  218, loss: 0.008116645738482475\n",
      "epoch:  219, loss: 0.008112907409667969\n",
      "epoch:  220, loss: 0.008112363517284393\n",
      "epoch:  221, loss: 0.008108901791274548\n",
      "epoch:  222, loss: 0.008108151145279408\n",
      "epoch:  223, loss: 0.008105387911200523\n",
      "epoch:  224, loss: 0.008104094304144382\n",
      "epoch:  225, loss: 0.008103687316179276\n",
      "epoch:  226, loss: 0.008100142702460289\n",
      "epoch:  227, loss: 0.00809971522539854\n",
      "epoch:  228, loss: 0.008096417412161827\n",
      "epoch:  229, loss: 0.008095970377326012\n",
      "epoch:  230, loss: 0.008092837408185005\n",
      "epoch:  231, loss: 0.0080922432243824\n",
      "epoch:  232, loss: 0.008091351017355919\n",
      "epoch:  233, loss: 0.008088608272373676\n",
      "epoch:  234, loss: 0.008088219910860062\n",
      "epoch:  235, loss: 0.008085327222943306\n",
      "epoch:  236, loss: 0.008084784261882305\n",
      "epoch:  237, loss: 0.008083834312856197\n",
      "epoch:  238, loss: 0.008081653155386448\n",
      "epoch:  239, loss: 0.00808128621429205\n",
      "epoch:  240, loss: 0.008079404942691326\n",
      "epoch:  241, loss: 0.008078052662312984\n",
      "epoch:  242, loss: 0.008077937178313732\n",
      "epoch:  243, loss: 0.008074897341430187\n",
      "epoch:  244, loss: 0.008074531331658363\n",
      "epoch:  245, loss: 0.008072447963058949\n",
      "epoch:  246, loss: 0.008071458898484707\n",
      "epoch:  247, loss: 0.008071161806583405\n",
      "epoch:  248, loss: 0.008068804629147053\n",
      "epoch:  249, loss: 0.008068384602665901\n",
      "epoch:  250, loss: 0.008067171089351177\n",
      "epoch:  251, loss: 0.008065817877650261\n",
      "epoch:  252, loss: 0.00806554313749075\n",
      "epoch:  253, loss: 0.008063470013439655\n",
      "epoch:  254, loss: 0.008063003420829773\n",
      "epoch:  255, loss: 0.008061414584517479\n",
      "epoch:  256, loss: 0.008060548454523087\n",
      "epoch:  257, loss: 0.008060304448008537\n",
      "epoch:  258, loss: 0.008058102801442146\n",
      "epoch:  259, loss: 0.008056427352130413\n",
      "epoch:  260, loss: 0.008055814541876316\n",
      "epoch:  261, loss: 0.008055097423493862\n",
      "epoch:  262, loss: 0.008053653873503208\n",
      "epoch:  263, loss: 0.008053370751440525\n",
      "epoch:  264, loss: 0.008051797747612\n",
      "epoch:  265, loss: 0.008051271550357342\n",
      "epoch:  266, loss: 0.008050995878875256\n",
      "epoch:  267, loss: 0.008049252443015575\n",
      "epoch:  268, loss: 0.008049017749726772\n",
      "epoch:  269, loss: 0.008047887124121189\n",
      "epoch:  270, loss: 0.008047077804803848\n",
      "epoch:  271, loss: 0.008046877570450306\n",
      "epoch:  272, loss: 0.008045158348977566\n",
      "epoch:  273, loss: 0.00804491713643074\n",
      "epoch:  274, loss: 0.008043822832405567\n",
      "epoch:  275, loss: 0.008042984642088413\n",
      "epoch:  276, loss: 0.00804278813302517\n",
      "epoch:  277, loss: 0.00804111734032631\n",
      "epoch:  278, loss: 0.008040876127779484\n",
      "epoch:  279, loss: 0.008039542473852634\n",
      "epoch:  280, loss: 0.008039060980081558\n",
      "epoch:  281, loss: 0.008039042353630066\n",
      "epoch:  282, loss: 0.00803732592612505\n",
      "epoch:  283, loss: 0.008037108927965164\n",
      "epoch:  284, loss: 0.00803658552467823\n",
      "epoch:  285, loss: 0.008035401813685894\n",
      "epoch:  286, loss: 0.00803520530462265\n",
      "epoch:  287, loss: 0.008033734746277332\n",
      "epoch:  288, loss: 0.008033446036279202\n",
      "epoch:  289, loss: 0.008032368496060371\n",
      "epoch:  290, loss: 0.008031683042645454\n",
      "epoch:  291, loss: 0.008031493984162807\n",
      "epoch:  292, loss: 0.00802992656826973\n",
      "epoch:  293, loss: 0.008029699325561523\n",
      "epoch:  294, loss: 0.008028668351471424\n",
      "epoch:  295, loss: 0.008027882315218449\n",
      "epoch:  296, loss: 0.00802768673747778\n",
      "epoch:  297, loss: 0.008026068098843098\n",
      "epoch:  298, loss: 0.008025823161005974\n",
      "epoch:  299, loss: 0.00802474282681942\n",
      "epoch:  300, loss: 0.008024000562727451\n",
      "epoch:  301, loss: 0.008023807778954506\n",
      "epoch:  302, loss: 0.008022218011319637\n",
      "epoch:  303, loss: 0.00802199449390173\n",
      "epoch:  304, loss: 0.008020997047424316\n",
      "epoch:  305, loss: 0.008020204491913319\n",
      "epoch:  306, loss: 0.008020021952688694\n",
      "epoch:  307, loss: 0.008018695749342442\n",
      "epoch:  308, loss: 0.008018259890377522\n",
      "epoch:  309, loss: 0.008018084801733494\n",
      "epoch:  310, loss: 0.008016574196517467\n",
      "epoch:  311, loss: 0.008016356267035007\n",
      "epoch:  312, loss: 0.008015024475753307\n",
      "epoch:  313, loss: 0.008014692924916744\n",
      "epoch:  314, loss: 0.008014259859919548\n",
      "epoch:  315, loss: 0.008013078011572361\n",
      "epoch:  316, loss: 0.008012899197638035\n",
      "epoch:  317, loss: 0.00801177229732275\n",
      "epoch:  318, loss: 0.008011285215616226\n",
      "epoch:  319, loss: 0.00801111664623022\n",
      "epoch:  320, loss: 0.008009707555174828\n",
      "epoch:  321, loss: 0.008009485900402069\n",
      "epoch:  322, loss: 0.008008179254829884\n",
      "epoch:  323, loss: 0.008007834665477276\n",
      "epoch:  324, loss: 0.008007668890058994\n",
      "epoch:  325, loss: 0.008006207644939423\n",
      "epoch:  326, loss: 0.00800603348761797\n",
      "epoch:  327, loss: 0.008004995062947273\n",
      "epoch:  328, loss: 0.008004444651305676\n",
      "epoch:  329, loss: 0.008004275150597095\n",
      "epoch:  330, loss: 0.00800303928554058\n",
      "epoch:  331, loss: 0.008002697490155697\n",
      "epoch:  332, loss: 0.008002512156963348\n",
      "epoch:  333, loss: 0.008001163601875305\n",
      "epoch:  334, loss: 0.008000971749424934\n",
      "epoch:  335, loss: 0.008000016212463379\n",
      "epoch:  336, loss: 0.007999449968338013\n",
      "epoch:  337, loss: 0.00799927394837141\n",
      "epoch:  338, loss: 0.00799831748008728\n",
      "epoch:  339, loss: 0.007997767999768257\n",
      "epoch:  340, loss: 0.007997610606253147\n",
      "epoch:  341, loss: 0.00799631979316473\n",
      "epoch:  342, loss: 0.007996113039553165\n",
      "epoch:  343, loss: 0.007995366118848324\n",
      "epoch:  344, loss: 0.00799466110765934\n",
      "epoch:  345, loss: 0.007994494400918484\n",
      "epoch:  346, loss: 0.007993531413376331\n",
      "epoch:  347, loss: 0.007993005216121674\n",
      "epoch:  348, loss: 0.007992841303348541\n",
      "epoch:  349, loss: 0.007991774939000607\n",
      "epoch:  350, loss: 0.007991353049874306\n",
      "epoch:  351, loss: 0.007991193793714046\n",
      "epoch:  352, loss: 0.007989933714270592\n",
      "epoch:  353, loss: 0.007989688776433468\n",
      "epoch:  354, loss: 0.007989540696144104\n",
      "epoch:  355, loss: 0.007988343015313148\n",
      "epoch:  356, loss: 0.00798803474754095\n",
      "epoch:  357, loss: 0.007987878285348415\n",
      "epoch:  358, loss: 0.007986572571098804\n",
      "epoch:  359, loss: 0.007986352778971195\n",
      "epoch:  360, loss: 0.007985747419297695\n",
      "epoch:  361, loss: 0.007984831929206848\n",
      "epoch:  362, loss: 0.00798465684056282\n",
      "epoch:  363, loss: 0.007983713410794735\n",
      "epoch:  364, loss: 0.007983111776411533\n",
      "epoch:  365, loss: 0.007982947863638401\n",
      "epoch:  366, loss: 0.007981591857969761\n",
      "epoch:  367, loss: 0.00798134133219719\n",
      "epoch:  368, loss: 0.00798078440129757\n",
      "epoch:  369, loss: 0.007979781366884708\n",
      "epoch:  370, loss: 0.00797958206385374\n",
      "epoch:  371, loss: 0.007978850044310093\n",
      "epoch:  372, loss: 0.007977931760251522\n",
      "epoch:  373, loss: 0.007977750152349472\n",
      "epoch:  374, loss: 0.007976633496582508\n",
      "epoch:  375, loss: 0.007976116612553596\n",
      "epoch:  376, loss: 0.007975946180522442\n",
      "epoch:  377, loss: 0.007974638603627682\n",
      "epoch:  378, loss: 0.007974319159984589\n",
      "epoch:  379, loss: 0.007973868399858475\n",
      "epoch:  380, loss: 0.007972737774252892\n",
      "epoch:  381, loss: 0.007972550578415394\n",
      "epoch:  382, loss: 0.007971428334712982\n",
      "epoch:  383, loss: 0.00797098595649004\n",
      "epoch:  384, loss: 0.007970819249749184\n",
      "epoch:  385, loss: 0.007969461381435394\n",
      "epoch:  386, loss: 0.007969247177243233\n",
      "epoch:  387, loss: 0.00796841736882925\n",
      "epoch:  388, loss: 0.007967673242092133\n",
      "epoch:  389, loss: 0.00796749908477068\n",
      "epoch:  390, loss: 0.00796638336032629\n",
      "epoch:  391, loss: 0.007965914905071259\n",
      "epoch:  392, loss: 0.00796575378626585\n",
      "epoch:  393, loss: 0.007965011522173882\n",
      "epoch:  394, loss: 0.007964199408888817\n",
      "epoch:  395, loss: 0.007964033633470535\n",
      "epoch:  396, loss: 0.00796281173825264\n",
      "epoch:  397, loss: 0.007962474599480629\n",
      "epoch:  398, loss: 0.007962392643094063\n",
      "epoch:  399, loss: 0.007960958406329155\n",
      "epoch:  400, loss: 0.007960760034620762\n",
      "epoch:  401, loss: 0.007959919981658459\n",
      "epoch:  402, loss: 0.007959212176501751\n",
      "epoch:  403, loss: 0.00795904640108347\n",
      "epoch:  404, loss: 0.007957715541124344\n",
      "epoch:  405, loss: 0.00795749668031931\n",
      "epoch:  406, loss: 0.00795675814151764\n",
      "epoch:  407, loss: 0.007955968379974365\n",
      "epoch:  408, loss: 0.007955795153975487\n",
      "epoch:  409, loss: 0.007954522036015987\n",
      "epoch:  410, loss: 0.007954264990985394\n",
      "epoch:  411, loss: 0.007953285239636898\n",
      "epoch:  412, loss: 0.007952730171382427\n",
      "epoch:  413, loss: 0.007952569052577019\n",
      "epoch:  414, loss: 0.007951385341584682\n",
      "epoch:  415, loss: 0.007950960658490658\n",
      "epoch:  416, loss: 0.007950790226459503\n",
      "epoch:  417, loss: 0.00794943980872631\n",
      "epoch:  418, loss: 0.00794912874698639\n",
      "epoch:  419, loss: 0.007948159240186214\n",
      "epoch:  420, loss: 0.007947448641061783\n",
      "epoch:  421, loss: 0.007947275415062904\n",
      "epoch:  422, loss: 0.007945824414491653\n",
      "epoch:  423, loss: 0.007945584133267403\n",
      "epoch:  424, loss: 0.007944360375404358\n",
      "epoch:  425, loss: 0.007943886332213879\n",
      "epoch:  426, loss: 0.007943711243569851\n",
      "epoch:  427, loss: 0.007942135445773602\n",
      "epoch:  428, loss: 0.007941925898194313\n",
      "epoch:  429, loss: 0.007940629497170448\n",
      "epoch:  430, loss: 0.007940106093883514\n",
      "epoch:  431, loss: 0.007939908653497696\n",
      "epoch:  432, loss: 0.007938184775412083\n",
      "epoch:  433, loss: 0.007937956601381302\n",
      "epoch:  434, loss: 0.007936347275972366\n",
      "epoch:  435, loss: 0.007935971021652222\n",
      "epoch:  436, loss: 0.007935774512588978\n",
      "epoch:  437, loss: 0.00793403945863247\n",
      "epoch:  438, loss: 0.00793380569666624\n",
      "epoch:  439, loss: 0.007932528853416443\n",
      "epoch:  440, loss: 0.007931827567517757\n",
      "epoch:  441, loss: 0.007931623607873917\n",
      "epoch:  442, loss: 0.007929867133498192\n",
      "epoch:  443, loss: 0.007929607294499874\n",
      "epoch:  444, loss: 0.007928279228508472\n",
      "epoch:  445, loss: 0.007927547208964825\n",
      "epoch:  446, loss: 0.007927333936095238\n",
      "epoch:  447, loss: 0.007925661280751228\n",
      "epoch:  448, loss: 0.007925343699753284\n",
      "epoch:  449, loss: 0.007924960926175117\n",
      "epoch:  450, loss: 0.007923293858766556\n",
      "epoch:  451, loss: 0.00792305450886488\n",
      "epoch:  452, loss: 0.007921439595520496\n",
      "epoch:  453, loss: 0.007921003736555576\n",
      "epoch:  454, loss: 0.007920796982944012\n",
      "epoch:  455, loss: 0.007918949238955975\n",
      "epoch:  456, loss: 0.00791870802640915\n",
      "epoch:  457, loss: 0.00791732408106327\n",
      "epoch:  458, loss: 0.007916576229035854\n",
      "epoch:  459, loss: 0.007916360162198544\n",
      "epoch:  460, loss: 0.007914425805211067\n",
      "epoch:  461, loss: 0.007914148271083832\n",
      "epoch:  462, loss: 0.007912402041256428\n",
      "epoch:  463, loss: 0.007911869324743748\n",
      "epoch:  464, loss: 0.00791101809591055\n",
      "epoch:  465, loss: 0.007909520529210567\n",
      "epoch:  466, loss: 0.00790924858301878\n",
      "epoch:  467, loss: 0.007907148450613022\n",
      "epoch:  468, loss: 0.007906717248260975\n",
      "epoch:  469, loss: 0.007905283011496067\n",
      "epoch:  470, loss: 0.00790429674088955\n",
      "epoch:  471, loss: 0.007904058322310448\n",
      "epoch:  472, loss: 0.007902235724031925\n",
      "epoch:  473, loss: 0.00790178682655096\n",
      "epoch:  474, loss: 0.007901327684521675\n",
      "epoch:  475, loss: 0.007899599149823189\n",
      "epoch:  476, loss: 0.007899320684373379\n",
      "epoch:  477, loss: 0.00789790228009224\n",
      "epoch:  478, loss: 0.00789706315845251\n",
      "epoch:  479, loss: 0.00789682287722826\n",
      "epoch:  480, loss: 0.007894852198660374\n",
      "epoch:  481, loss: 0.007894466631114483\n",
      "epoch:  482, loss: 0.007893509231507778\n",
      "epoch:  483, loss: 0.007892148569226265\n",
      "epoch:  484, loss: 0.007891873829066753\n",
      "epoch:  485, loss: 0.007890140637755394\n",
      "epoch:  486, loss: 0.007889527827501297\n",
      "epoch:  487, loss: 0.007889294065535069\n",
      "epoch:  488, loss: 0.00788733921945095\n",
      "epoch:  489, loss: 0.007886925712227821\n",
      "epoch:  490, loss: 0.007886136882007122\n",
      "epoch:  491, loss: 0.007884515449404716\n",
      "epoch:  492, loss: 0.007884221151471138\n",
      "epoch:  493, loss: 0.0078823771327734\n",
      "epoch:  494, loss: 0.007881689816713333\n",
      "epoch:  495, loss: 0.007881395518779755\n",
      "epoch:  496, loss: 0.007879222743213177\n",
      "epoch:  497, loss: 0.007878893986344337\n",
      "epoch:  498, loss: 0.007877139374613762\n",
      "epoch:  499, loss: 0.007876288145780563\n",
      "epoch:  500, loss: 0.007876024581491947\n",
      "epoch:  501, loss: 0.00787374097853899\n",
      "epoch:  502, loss: 0.007873392663896084\n",
      "epoch:  503, loss: 0.007871568202972412\n",
      "epoch:  504, loss: 0.0078706881031394\n",
      "epoch:  505, loss: 0.007870413362979889\n",
      "epoch:  506, loss: 0.007868023589253426\n",
      "epoch:  507, loss: 0.007867677137255669\n",
      "epoch:  508, loss: 0.007865666411817074\n",
      "epoch:  509, loss: 0.00786499958485365\n",
      "epoch:  510, loss: 0.007864993996918201\n",
      "epoch:  511, loss: 0.007862436585128307\n",
      "epoch:  512, loss: 0.00786210410296917\n",
      "epoch:  513, loss: 0.007860329002141953\n",
      "epoch:  514, loss: 0.007859544828534126\n",
      "epoch:  515, loss: 0.007859288714826107\n",
      "epoch:  516, loss: 0.007857069373130798\n",
      "epoch:  517, loss: 0.007856749929487705\n",
      "epoch:  518, loss: 0.007854877971112728\n",
      "epoch:  519, loss: 0.007854267954826355\n",
      "epoch:  520, loss: 0.007853872142732143\n",
      "epoch:  521, loss: 0.007851890288293362\n",
      "epoch:  522, loss: 0.007851590402424335\n",
      "epoch:  523, loss: 0.007849856279790401\n",
      "epoch:  524, loss: 0.007849151268601418\n",
      "epoch:  525, loss: 0.007848897948861122\n",
      "epoch:  526, loss: 0.007846685126423836\n",
      "epoch:  527, loss: 0.007846342399716377\n",
      "epoch:  528, loss: 0.007844722829759121\n",
      "epoch:  529, loss: 0.007843798957765102\n",
      "epoch:  530, loss: 0.007843531668186188\n",
      "epoch:  531, loss: 0.007841280661523342\n",
      "epoch:  532, loss: 0.007840928621590137\n",
      "epoch:  533, loss: 0.00783934909850359\n",
      "epoch:  534, loss: 0.007838403806090355\n",
      "epoch:  535, loss: 0.007838142104446888\n",
      "epoch:  536, loss: 0.007835949771106243\n",
      "epoch:  537, loss: 0.007835587486624718\n",
      "epoch:  538, loss: 0.00783395767211914\n",
      "epoch:  539, loss: 0.007833058014512062\n",
      "epoch:  540, loss: 0.007832810282707214\n",
      "epoch:  541, loss: 0.007830657996237278\n",
      "epoch:  542, loss: 0.007830305956304073\n",
      "epoch:  543, loss: 0.007828439585864544\n",
      "epoch:  544, loss: 0.007827743887901306\n",
      "epoch:  545, loss: 0.007827491499483585\n",
      "epoch:  546, loss: 0.007825350388884544\n",
      "epoch:  547, loss: 0.00782497227191925\n",
      "epoch:  548, loss: 0.007823317311704159\n",
      "epoch:  549, loss: 0.007822430692613125\n",
      "epoch:  550, loss: 0.007822171784937382\n",
      "epoch:  551, loss: 0.007819971069693565\n",
      "epoch:  552, loss: 0.007819611579179764\n",
      "epoch:  553, loss: 0.007818469777703285\n",
      "epoch:  554, loss: 0.007817148230969906\n",
      "epoch:  555, loss: 0.007816859520971775\n",
      "epoch:  556, loss: 0.007814927957952023\n",
      "epoch:  557, loss: 0.007814392447471619\n",
      "epoch:  558, loss: 0.007814032025635242\n",
      "epoch:  559, loss: 0.00781196029856801\n",
      "epoch:  560, loss: 0.007811657153069973\n",
      "epoch:  561, loss: 0.007809786591678858\n",
      "epoch:  562, loss: 0.007809194270521402\n",
      "epoch:  563, loss: 0.007808943744748831\n",
      "epoch:  564, loss: 0.007806790992617607\n",
      "epoch:  565, loss: 0.007806454785168171\n",
      "epoch:  566, loss: 0.0078047774732112885\n",
      "epoch:  567, loss: 0.007803992833942175\n",
      "epoch:  568, loss: 0.007803741376847029\n",
      "epoch:  569, loss: 0.007801790256053209\n",
      "epoch:  570, loss: 0.007801271975040436\n",
      "epoch:  571, loss: 0.0078010219149291515\n",
      "epoch:  572, loss: 0.007798857521265745\n",
      "epoch:  573, loss: 0.00779851246625185\n",
      "epoch:  574, loss: 0.007797513622790575\n",
      "epoch:  575, loss: 0.007795983459800482\n",
      "epoch:  576, loss: 0.007795690093189478\n",
      "epoch:  577, loss: 0.007795570883899927\n",
      "epoch:  578, loss: 0.0077931988053023815\n",
      "epoch:  579, loss: 0.0077928160317242146\n",
      "epoch:  580, loss: 0.007792224641889334\n",
      "epoch:  581, loss: 0.0077901482582092285\n",
      "epoch:  582, loss: 0.007789812050759792\n",
      "epoch:  583, loss: 0.007788059767335653\n",
      "epoch:  584, loss: 0.007787142880260944\n",
      "epoch:  585, loss: 0.00778681505471468\n",
      "epoch:  586, loss: 0.007785352412611246\n",
      "epoch:  587, loss: 0.0077841030433773994\n",
      "epoch:  588, loss: 0.007783796638250351\n",
      "epoch:  589, loss: 0.007781859952956438\n",
      "epoch:  590, loss: 0.007781016640365124\n",
      "epoch:  591, loss: 0.007780694402754307\n",
      "epoch:  592, loss: 0.00777914933860302\n",
      "epoch:  593, loss: 0.007777709048241377\n",
      "epoch:  594, loss: 0.007777380291372538\n",
      "epoch:  595, loss: 0.007776506245136261\n",
      "epoch:  596, loss: 0.00777448620647192\n",
      "epoch:  597, loss: 0.0077741146087646484\n",
      "epoch:  598, loss: 0.007773812860250473\n",
      "epoch:  599, loss: 0.007771246135234833\n",
      "epoch:  600, loss: 0.007770849391818047\n",
      "epoch:  601, loss: 0.007769289892166853\n",
      "epoch:  602, loss: 0.007767890579998493\n",
      "epoch:  603, loss: 0.0077675203792750835\n",
      "epoch:  604, loss: 0.007765249814838171\n",
      "epoch:  605, loss: 0.007764497771859169\n",
      "epoch:  606, loss: 0.007764196023344994\n",
      "epoch:  607, loss: 0.007762515917420387\n",
      "epoch:  608, loss: 0.007761266082525253\n",
      "epoch:  609, loss: 0.007760925218462944\n",
      "epoch:  610, loss: 0.007758717518299818\n",
      "epoch:  611, loss: 0.0077579733915627\n",
      "epoch:  612, loss: 0.00775768281891942\n",
      "epoch:  613, loss: 0.007755558472126722\n",
      "epoch:  614, loss: 0.007754744496196508\n",
      "epoch:  615, loss: 0.007754429243505001\n",
      "epoch:  616, loss: 0.007752362173050642\n",
      "epoch:  617, loss: 0.0077516138553619385\n",
      "epoch:  618, loss: 0.0077513400465250015\n",
      "epoch:  619, loss: 0.007749408949166536\n",
      "epoch:  620, loss: 0.007748598698526621\n",
      "epoch:  621, loss: 0.007748295087367296\n",
      "epoch:  622, loss: 0.007746413815766573\n",
      "epoch:  623, loss: 0.007745598908513784\n",
      "epoch:  624, loss: 0.007745310664176941\n",
      "epoch:  625, loss: 0.007744002155959606\n",
      "epoch:  626, loss: 0.007742693182080984\n",
      "epoch:  627, loss: 0.007742368616163731\n",
      "epoch:  628, loss: 0.007742105517536402\n",
      "epoch:  629, loss: 0.007740231696516275\n",
      "epoch:  630, loss: 0.0077394722029566765\n",
      "epoch:  631, loss: 0.0077391681261360645\n",
      "epoch:  632, loss: 0.007737389765679836\n",
      "epoch:  633, loss: 0.0077364942990243435\n",
      "epoch:  634, loss: 0.0077362037263810635\n",
      "epoch:  635, loss: 0.007734634913504124\n",
      "epoch:  636, loss: 0.007733606733381748\n",
      "epoch:  637, loss: 0.007733242120593786\n",
      "epoch:  638, loss: 0.0077324360609054565\n",
      "epoch:  639, loss: 0.007730577606707811\n",
      "epoch:  640, loss: 0.007730238139629364\n",
      "epoch:  641, loss: 0.007728347089141607\n",
      "epoch:  642, loss: 0.007727577351033688\n",
      "epoch:  643, loss: 0.0077272397466003895\n",
      "epoch:  644, loss: 0.0077262576669454575\n",
      "epoch:  645, loss: 0.0077245538122951984\n",
      "epoch:  646, loss: 0.007724243216216564\n",
      "epoch:  647, loss: 0.007723968010395765\n",
      "epoch:  648, loss: 0.007721814326941967\n",
      "epoch:  649, loss: 0.007721244357526302\n",
      "epoch:  650, loss: 0.00772096635773778\n",
      "epoch:  651, loss: 0.007720699068158865\n",
      "epoch:  652, loss: 0.0077190217562019825\n",
      "epoch:  653, loss: 0.0077180201187729836\n",
      "epoch:  654, loss: 0.007717700209468603\n",
      "epoch:  655, loss: 0.0077170380391180515\n",
      "epoch:  656, loss: 0.007715040817856789\n",
      "epoch:  657, loss: 0.007714667357504368\n",
      "epoch:  658, loss: 0.007714385632425547\n",
      "epoch:  659, loss: 0.007712088525295258\n",
      "epoch:  660, loss: 0.007711607031524181\n",
      "epoch:  661, loss: 0.007711428217589855\n",
      "epoch:  662, loss: 0.007708950433880091\n",
      "epoch:  663, loss: 0.007708499673753977\n",
      "epoch:  664, loss: 0.007707687560468912\n",
      "epoch:  665, loss: 0.0077055818401277065\n",
      "epoch:  666, loss: 0.007705212105065584\n",
      "epoch:  667, loss: 0.007704920135438442\n",
      "epoch:  668, loss: 0.007702826056629419\n",
      "epoch:  669, loss: 0.007702042814344168\n",
      "epoch:  670, loss: 0.007701742462813854\n",
      "epoch:  671, loss: 0.007700708229094744\n",
      "epoch:  672, loss: 0.0076989722438156605\n",
      "epoch:  673, loss: 0.007698585744947195\n",
      "epoch:  674, loss: 0.007698299828916788\n",
      "epoch:  675, loss: 0.007697945460677147\n",
      "epoch:  676, loss: 0.007695558946579695\n",
      "epoch:  677, loss: 0.007695171516388655\n",
      "epoch:  678, loss: 0.007694861851632595\n",
      "epoch:  679, loss: 0.007692360784858465\n",
      "epoch:  680, loss: 0.0076919603161513805\n",
      "epoch:  681, loss: 0.007691006641834974\n",
      "epoch:  682, loss: 0.007689176592975855\n",
      "epoch:  683, loss: 0.00768877612426877\n",
      "epoch:  684, loss: 0.00768817774951458\n",
      "epoch:  685, loss: 0.007685980759561062\n",
      "epoch:  686, loss: 0.00768556771799922\n",
      "epoch:  687, loss: 0.007685260381549597\n",
      "epoch:  688, loss: 0.00768436910584569\n",
      "epoch:  689, loss: 0.007682500872761011\n",
      "epoch:  690, loss: 0.0076821125112473965\n",
      "epoch:  691, loss: 0.007680533453822136\n",
      "epoch:  692, loss: 0.007679303642362356\n",
      "epoch:  693, loss: 0.007678938563913107\n",
      "epoch:  694, loss: 0.007677140645682812\n",
      "epoch:  695, loss: 0.007676044944673777\n",
      "epoch:  696, loss: 0.007675724569708109\n",
      "epoch:  697, loss: 0.007674647495150566\n",
      "epoch:  698, loss: 0.0076729292050004005\n",
      "epoch:  699, loss: 0.007672531995922327\n",
      "epoch:  700, loss: 0.007672260981053114\n",
      "epoch:  701, loss: 0.007669777609407902\n",
      "epoch:  702, loss: 0.007669359445571899\n",
      "epoch:  703, loss: 0.00766906701028347\n",
      "epoch:  704, loss: 0.007667763624340296\n",
      "epoch:  705, loss: 0.0076662288047373295\n",
      "epoch:  706, loss: 0.00766587583348155\n",
      "epoch:  707, loss: 0.007664914708584547\n",
      "epoch:  708, loss: 0.0076630269177258015\n",
      "epoch:  709, loss: 0.007662639487534761\n",
      "epoch:  710, loss: 0.007662586867809296\n",
      "epoch:  711, loss: 0.00765979615971446\n",
      "epoch:  712, loss: 0.007659364957362413\n",
      "epoch:  713, loss: 0.007659057155251503\n",
      "epoch:  714, loss: 0.007656731642782688\n",
      "epoch:  715, loss: 0.007656019646674395\n",
      "epoch:  716, loss: 0.007655696477741003\n",
      "epoch:  717, loss: 0.007653878070414066\n",
      "epoch:  718, loss: 0.007652736268937588\n",
      "epoch:  719, loss: 0.007652370724827051\n",
      "epoch:  720, loss: 0.007652087602764368\n",
      "epoch:  721, loss: 0.007651374209672213\n",
      "epoch:  722, loss: 0.0076491557992994785\n",
      "epoch:  723, loss: 0.007648782338947058\n",
      "epoch:  724, loss: 0.007648455444723368\n",
      "epoch:  725, loss: 0.0076466822065413\n",
      "epoch:  726, loss: 0.007645490579307079\n",
      "epoch:  727, loss: 0.007645115721970797\n",
      "epoch:  728, loss: 0.007644511293619871\n",
      "epoch:  729, loss: 0.0076421103440225124\n",
      "epoch:  730, loss: 0.007641699630767107\n",
      "epoch:  731, loss: 0.007641369942575693\n",
      "epoch:  732, loss: 0.007638761308044195\n",
      "epoch:  733, loss: 0.007638286333531141\n",
      "epoch:  734, loss: 0.00763794407248497\n",
      "epoch:  735, loss: 0.007635736837983131\n",
      "epoch:  736, loss: 0.007634853012859821\n",
      "epoch:  737, loss: 0.0076344977132976055\n",
      "epoch:  738, loss: 0.007631964050233364\n",
      "epoch:  739, loss: 0.007631336804479361\n",
      "epoch:  740, loss: 0.007631001528352499\n",
      "epoch:  741, loss: 0.007630698382854462\n",
      "epoch:  742, loss: 0.007628960534930229\n",
      "epoch:  743, loss: 0.0076276324689388275\n",
      "epoch:  744, loss: 0.007627235259860754\n",
      "epoch:  745, loss: 0.007625238504260778\n",
      "epoch:  746, loss: 0.007624099031090736\n",
      "epoch:  747, loss: 0.007623703218996525\n",
      "epoch:  748, loss: 0.007622411474585533\n",
      "epoch:  749, loss: 0.007620528806000948\n",
      "epoch:  750, loss: 0.007620098534971476\n",
      "epoch:  751, loss: 0.007619769312441349\n",
      "epoch:  752, loss: 0.007617061957716942\n",
      "epoch:  753, loss: 0.007616500835865736\n",
      "epoch:  754, loss: 0.007616139482706785\n",
      "epoch:  755, loss: 0.007614833768457174\n",
      "epoch:  756, loss: 0.00761289382353425\n",
      "epoch:  757, loss: 0.0076124584302306175\n",
      "epoch:  758, loss: 0.007611486595124006\n",
      "epoch:  759, loss: 0.00760911637917161\n",
      "epoch:  760, loss: 0.007608682848513126\n",
      "epoch:  761, loss: 0.007608323357999325\n",
      "epoch:  762, loss: 0.007605432532727718\n",
      "epoch:  763, loss: 0.00760485278442502\n",
      "epoch:  764, loss: 0.007604467682540417\n",
      "epoch:  765, loss: 0.007602679543197155\n",
      "epoch:  766, loss: 0.007601037621498108\n",
      "epoch:  767, loss: 0.007600597105920315\n",
      "epoch:  768, loss: 0.00759933702647686\n",
      "epoch:  769, loss: 0.007597106508910656\n",
      "epoch:  770, loss: 0.007596640381962061\n",
      "epoch:  771, loss: 0.007596280891448259\n",
      "epoch:  772, loss: 0.007593157235532999\n",
      "epoch:  773, loss: 0.0075927237048745155\n",
      "epoch:  774, loss: 0.0075923483818769455\n",
      "epoch:  775, loss: 0.007589713204652071\n",
      "epoch:  776, loss: 0.007588853593915701\n",
      "epoch:  777, loss: 0.00758845591917634\n",
      "epoch:  778, loss: 0.007586490362882614\n",
      "epoch:  779, loss: 0.00758493784815073\n",
      "epoch:  780, loss: 0.007584514562040567\n",
      "epoch:  781, loss: 0.0075841560028493404\n",
      "epoch:  782, loss: 0.007581473793834448\n",
      "epoch:  783, loss: 0.007580658420920372\n",
      "epoch:  784, loss: 0.0075802612118422985\n",
      "epoch:  785, loss: 0.007578386925160885\n",
      "epoch:  786, loss: 0.007576792035251856\n",
      "epoch:  787, loss: 0.007576318923383951\n",
      "epoch:  788, loss: 0.007574943825602531\n",
      "epoch:  789, loss: 0.007572824601083994\n",
      "epoch:  790, loss: 0.007572359871119261\n",
      "epoch:  791, loss: 0.007571997586637735\n",
      "epoch:  792, loss: 0.007569202221930027\n",
      "epoch:  793, loss: 0.0075684599578380585\n",
      "epoch:  794, loss: 0.0075680832378566265\n",
      "epoch:  795, loss: 0.007566114421933889\n",
      "epoch:  796, loss: 0.007564583793282509\n",
      "epoch:  797, loss: 0.0075641474686563015\n",
      "epoch:  798, loss: 0.007564089726656675\n",
      "epoch:  799, loss: 0.007560585159808397\n",
      "epoch:  800, loss: 0.007560055237263441\n",
      "epoch:  801, loss: 0.007559661753475666\n",
      "epoch:  802, loss: 0.007557067554444075\n",
      "epoch:  803, loss: 0.0075559113174676895\n",
      "epoch:  804, loss: 0.007555464282631874\n",
      "epoch:  805, loss: 0.007553508505225182\n",
      "epoch:  806, loss: 0.0075517138466238976\n",
      "epoch:  807, loss: 0.007551196496933699\n",
      "epoch:  808, loss: 0.00754926260560751\n",
      "epoch:  809, loss: 0.007547383662313223\n",
      "epoch:  810, loss: 0.007546856068074703\n",
      "epoch:  811, loss: 0.007545929867774248\n",
      "epoch:  812, loss: 0.007543002720922232\n",
      "epoch:  813, loss: 0.007542431354522705\n",
      "epoch:  814, loss: 0.00754201877862215\n",
      "epoch:  815, loss: 0.007538739126175642\n",
      "epoch:  816, loss: 0.007537925150245428\n",
      "epoch:  817, loss: 0.007537487428635359\n",
      "epoch:  818, loss: 0.007535369135439396\n",
      "epoch:  819, loss: 0.007533457595854998\n",
      "epoch:  820, loss: 0.007532927673310041\n",
      "epoch:  821, loss: 0.007530666887760162\n",
      "epoch:  822, loss: 0.007528887130320072\n",
      "epoch:  823, loss: 0.007528353948146105\n",
      "epoch:  824, loss: 0.007527512032538652\n",
      "epoch:  825, loss: 0.0075243436731398106\n",
      "epoch:  826, loss: 0.007523749489337206\n",
      "epoch:  827, loss: 0.007523319683969021\n",
      "epoch:  828, loss: 0.007520945277065039\n",
      "epoch:  829, loss: 0.007519252132624388\n",
      "epoch:  830, loss: 0.007518718484789133\n",
      "epoch:  831, loss: 0.007517009973526001\n",
      "epoch:  832, loss: 0.00751462671905756\n",
      "epoch:  833, loss: 0.007514033000916243\n",
      "epoch:  834, loss: 0.007513607386499643\n",
      "epoch:  835, loss: 0.007511134725064039\n",
      "epoch:  836, loss: 0.007509429473429918\n",
      "epoch:  837, loss: 0.0075089167803525925\n",
      "epoch:  838, loss: 0.007507036905735731\n",
      "epoch:  839, loss: 0.007504602894186974\n",
      "epoch:  840, loss: 0.007504051551222801\n",
      "epoch:  841, loss: 0.007503611966967583\n",
      "epoch:  842, loss: 0.007499930914491415\n",
      "epoch:  843, loss: 0.00749920355156064\n",
      "epoch:  844, loss: 0.007498736027628183\n",
      "epoch:  845, loss: 0.007495108526200056\n",
      "epoch:  846, loss: 0.007494216784834862\n",
      "epoch:  847, loss: 0.007493704557418823\n",
      "epoch:  848, loss: 0.007489877752959728\n",
      "epoch:  849, loss: 0.0074889264069497585\n",
      "epoch:  850, loss: 0.007488405331969261\n",
      "epoch:  851, loss: 0.007484458386898041\n",
      "epoch:  852, loss: 0.007483587134629488\n",
      "epoch:  853, loss: 0.00748302461579442\n",
      "epoch:  854, loss: 0.007478951010853052\n",
      "epoch:  855, loss: 0.0074779014103114605\n",
      "epoch:  856, loss: 0.00747735146433115\n",
      "epoch:  857, loss: 0.007473694626241922\n",
      "epoch:  858, loss: 0.007472319062799215\n",
      "epoch:  859, loss: 0.007471716962754726\n",
      "epoch:  860, loss: 0.007469873875379562\n",
      "epoch:  861, loss: 0.007466751616448164\n",
      "epoch:  862, loss: 0.007466072682291269\n",
      "epoch:  863, loss: 0.007465557660907507\n",
      "epoch:  864, loss: 0.007461239118129015\n",
      "epoch:  865, loss: 0.007460477761924267\n",
      "epoch:  866, loss: 0.007459976710379124\n",
      "epoch:  867, loss: 0.007456531748175621\n",
      "epoch:  868, loss: 0.007454988081008196\n",
      "epoch:  869, loss: 0.007454430218786001\n",
      "epoch:  870, loss: 0.007451735436916351\n",
      "epoch:  871, loss: 0.0074494825676083565\n",
      "epoch:  872, loss: 0.007448878139257431\n",
      "epoch:  873, loss: 0.007448380813002586\n",
      "epoch:  874, loss: 0.007444058079272509\n",
      "epoch:  875, loss: 0.007443299517035484\n",
      "epoch:  876, loss: 0.007442794740200043\n",
      "epoch:  877, loss: 0.007438866887241602\n",
      "epoch:  878, loss: 0.007437680382281542\n",
      "epoch:  879, loss: 0.007437126245349646\n",
      "epoch:  880, loss: 0.00743435462936759\n",
      "epoch:  881, loss: 0.007432084996253252\n",
      "epoch:  882, loss: 0.00743143493309617\n",
      "epoch:  883, loss: 0.007429620251059532\n",
      "epoch:  884, loss: 0.007426423951983452\n",
      "epoch:  885, loss: 0.0074257319793105125\n",
      "epoch:  886, loss: 0.007425213698297739\n",
      "epoch:  887, loss: 0.007421430665999651\n",
      "epoch:  888, loss: 0.007420137990266085\n",
      "epoch:  889, loss: 0.0074195521883666515\n",
      "epoch:  890, loss: 0.0074159312061965466\n",
      "epoch:  891, loss: 0.007414399180561304\n",
      "epoch:  892, loss: 0.007413760293275118\n",
      "epoch:  893, loss: 0.007410905323922634\n",
      "epoch:  894, loss: 0.007408553268760443\n",
      "epoch:  895, loss: 0.007407910656183958\n",
      "epoch:  896, loss: 0.007407395634800196\n",
      "epoch:  897, loss: 0.007403497584164143\n",
      "epoch:  898, loss: 0.0074022202752530575\n",
      "epoch:  899, loss: 0.0074016498401761055\n",
      "epoch:  900, loss: 0.007397961337119341\n",
      "epoch:  901, loss: 0.007396472617983818\n",
      "epoch:  902, loss: 0.007395871914923191\n",
      "epoch:  903, loss: 0.007393098436295986\n",
      "epoch:  904, loss: 0.0073906551115214825\n",
      "epoch:  905, loss: 0.007390027865767479\n",
      "epoch:  906, loss: 0.00738950539380312\n",
      "epoch:  907, loss: 0.007385043893009424\n",
      "epoch:  908, loss: 0.007384194061160088\n",
      "epoch:  909, loss: 0.007383640389889479\n",
      "epoch:  910, loss: 0.007379567250609398\n",
      "epoch:  911, loss: 0.007378321141004562\n",
      "epoch:  912, loss: 0.007377732079476118\n",
      "epoch:  913, loss: 0.00737394904717803\n",
      "epoch:  914, loss: 0.007372359279543161\n",
      "epoch:  915, loss: 0.007371779531240463\n",
      "epoch:  916, loss: 0.007369792088866234\n",
      "epoch:  917, loss: 0.007366547826677561\n",
      "epoch:  918, loss: 0.007365760859102011\n",
      "epoch:  919, loss: 0.007365226745605469\n",
      "epoch:  920, loss: 0.007362043019384146\n",
      "epoch:  921, loss: 0.007359873503446579\n",
      "epoch:  922, loss: 0.007359267212450504\n",
      "epoch:  923, loss: 0.007358921226114035\n",
      "epoch:  924, loss: 0.007354081608355045\n",
      "epoch:  925, loss: 0.007353277411311865\n",
      "epoch:  926, loss: 0.007352720480412245\n",
      "epoch:  927, loss: 0.0073484075255692005\n",
      "epoch:  928, loss: 0.007347223814576864\n",
      "epoch:  929, loss: 0.0073466431349515915\n",
      "epoch:  930, loss: 0.0073435320518910885\n",
      "epoch:  931, loss: 0.007341180928051472\n",
      "epoch:  932, loss: 0.007340502459555864\n",
      "epoch:  933, loss: 0.0073392074555158615\n",
      "epoch:  934, loss: 0.007335084956139326\n",
      "epoch:  935, loss: 0.007334274705499411\n",
      "epoch:  936, loss: 0.00733276829123497\n",
      "epoch:  937, loss: 0.00732885766774416\n",
      "epoch:  938, loss: 0.007328021340072155\n",
      "epoch:  939, loss: 0.007327476050704718\n",
      "epoch:  940, loss: 0.007323982659727335\n",
      "epoch:  941, loss: 0.007321885786950588\n",
      "epoch:  942, loss: 0.007321228738874197\n",
      "epoch:  943, loss: 0.007320114877074957\n",
      "epoch:  944, loss: 0.007315839175134897\n",
      "epoch:  945, loss: 0.007314975839108229\n",
      "epoch:  946, loss: 0.007314416114240885\n",
      "epoch:  947, loss: 0.007309736218303442\n",
      "epoch:  948, loss: 0.007308675907552242\n",
      "epoch:  949, loss: 0.0073081012815237045\n",
      "epoch:  950, loss: 0.0073052882216870785\n",
      "epoch:  951, loss: 0.007302542217075825\n",
      "epoch:  952, loss: 0.007301821373403072\n",
      "epoch:  953, loss: 0.007301253732293844\n",
      "epoch:  954, loss: 0.007296476047486067\n",
      "epoch:  955, loss: 0.007295507006347179\n",
      "epoch:  956, loss: 0.007294944487512112\n",
      "epoch:  957, loss: 0.007290829438716173\n",
      "epoch:  958, loss: 0.007289214059710503\n",
      "epoch:  959, loss: 0.007288598921149969\n",
      "epoch:  960, loss: 0.007284841500222683\n",
      "epoch:  961, loss: 0.0072829024866223335\n",
      "epoch:  962, loss: 0.007282235659658909\n",
      "epoch:  963, loss: 0.007281683385372162\n",
      "epoch:  964, loss: 0.0072783371433615685\n",
      "epoch:  965, loss: 0.007275985088199377\n",
      "epoch:  966, loss: 0.007275296375155449\n",
      "epoch:  967, loss: 0.007274672854691744\n",
      "epoch:  968, loss: 0.007269816938787699\n",
      "epoch:  969, loss: 0.007268870249390602\n",
      "epoch:  970, loss: 0.007268281187862158\n",
      "epoch:  971, loss: 0.007263444364070892\n",
      "epoch:  972, loss: 0.007262341678142548\n",
      "epoch:  973, loss: 0.0072617423720657825\n",
      "epoch:  974, loss: 0.007257082965224981\n",
      "epoch:  975, loss: 0.007255828008055687\n",
      "epoch:  976, loss: 0.007255177944898605\n",
      "epoch:  977, loss: 0.007250950671732426\n",
      "epoch:  978, loss: 0.007249256130307913\n",
      "epoch:  979, loss: 0.007248566020280123\n",
      "epoch:  980, loss: 0.007246432360261679\n",
      "epoch:  981, loss: 0.007242788095027208\n",
      "epoch:  982, loss: 0.007241937331855297\n",
      "epoch:  983, loss: 0.007241324055939913\n",
      "epoch:  984, loss: 0.007236150559037924\n",
      "epoch:  985, loss: 0.00723515497520566\n",
      "epoch:  986, loss: 0.007234550081193447\n",
      "epoch:  987, loss: 0.007229338865727186\n",
      "epoch:  988, loss: 0.007228344678878784\n",
      "epoch:  989, loss: 0.00722773652523756\n",
      "epoch:  990, loss: 0.007222571410238743\n",
      "epoch:  991, loss: 0.007221551146358252\n",
      "epoch:  992, loss: 0.007220927160233259\n",
      "epoch:  993, loss: 0.007216756697744131\n",
      "epoch:  994, loss: 0.007214922923594713\n",
      "epoch:  995, loss: 0.007214186247438192\n",
      "epoch:  996, loss: 0.007213967852294445\n",
      "epoch:  997, loss: 0.007208494935184717\n",
      "epoch:  998, loss: 0.0072074588388204575\n",
      "epoch:  999, loss: 0.0072068157605826855\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size=X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "opt = torch_numopt.GradientDescentLS(model=model, lr=1, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"armijo\")\n",
    "# opt = torch_numopt.GradientDescentLS(model=model, lr=1, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"wolfe\")\n",
    "# opt = torch_numopt.GradientDescentLS(model=model, lr=1, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"strong-wolfe\")\n",
    "# opt = torch_numopt.GradientDescentLS(model=model, lr=1, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"goldstein\")\n",
    "\n",
    "all_loss = {}\n",
    "for epoch in range(1000):\n",
    "    print(\"epoch: \", epoch, end=\"\")\n",
    "    all_loss[epoch + 1] = 0\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y, loss_fn)\n",
    "\n",
    "        all_loss[epoch + 1] += loss\n",
    "    all_loss[epoch + 1] /= len(data_loader)\n",
    "    print(\", loss: {}\".format(all_loss[epoch + 1].cpu().detach().numpy().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics: R2 = 0.7501114946120745\n",
      "Test metrics:  R2 = 0.7733097966885041\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "pred_train = model.forward(X_train).cpu().detach()\n",
    "pred_test = model.forward(X_test).cpu().detach()\n",
    "print(f\"Train metrics: R2 = {r2_score(pred_train, y_train.cpu())}\")\n",
    "print(f\"Test metrics:  R2 = {r2_score(pred_test, y_test.cpu())}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_numopt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
