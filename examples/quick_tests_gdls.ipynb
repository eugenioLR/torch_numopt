{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch_numopt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import *\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.f1 = nn.Linear(input_size, 10, device=device)\n",
    "        self.f2 = nn.Linear(10, 20, device=device)\n",
    "        self.f3 = nn.Linear(20, 20, device=device)\n",
    "        self.f4 = nn.Linear(20, 10, device=device)\n",
    "        self.f5 = nn.Linear(10, 1, device=device)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        # self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.f1(x))\n",
    "        x = self.activation(self.f2(x))\n",
    "        x = self.activation(self.f3(x))\n",
    "        x = self.activation(self.f4(x))\n",
    "        x = self.f5(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([800, 10]) torch.Size([800, 1])\n",
      "torch.Size([200, 10]) torch.Size([200, 1])\n"
     ]
    }
   ],
   "source": [
    "# X, y = load_diabetes(return_X_y=True, scaled=False)\n",
    "# X, y = make_regression(n_samples=1000, n_features=100)\n",
    "X, y = make_friedman1(n_samples=1000, noise=1e-2)\n",
    "\n",
    "X_scaler = MinMaxScaler()\n",
    "X = X_scaler.fit_transform(X)\n",
    "X = torch.Tensor(X).to(device)\n",
    "\n",
    "y_scaler = MinMaxScaler()\n",
    "y = y_scaler.fit_transform(y.reshape((-1, 1)))\n",
    "y = torch.Tensor(y).to(device)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "torch_data = TensorDataset(X_train, y_train)\n",
    "data_loader = DataLoader(torch_data, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.5746150016784668\n",
      "epoch:  1, loss: 0.34389716386795044\n",
      "epoch:  2, loss: 0.20839425921440125\n",
      "epoch:  3, loss: 0.12964080274105072\n",
      "epoch:  4, loss: 0.08500199019908905\n",
      "epoch:  5, loss: 0.06045519933104515\n",
      "epoch:  6, loss: 0.04735987260937691\n",
      "epoch:  7, loss: 0.04056306183338165\n",
      "epoch:  8, loss: 0.037114664912223816\n",
      "epoch:  9, loss: 0.035395897924900055\n",
      "epoch:  10, loss: 0.034550007432699203\n",
      "epoch:  11, loss: 0.03413689136505127\n",
      "epoch:  12, loss: 0.03393560275435448\n",
      "epoch:  13, loss: 0.03383691608905792\n",
      "epoch:  14, loss: 0.033787596970796585\n",
      "epoch:  15, loss: 0.033761993050575256\n",
      "epoch:  16, loss: 0.033747609704732895\n",
      "epoch:  17, loss: 0.0337366946041584\n",
      "epoch:  18, loss: 0.03370966389775276\n",
      "epoch:  19, loss: 0.03369445353746414\n",
      "epoch:  20, loss: 0.03368062898516655\n",
      "epoch:  21, loss: 0.033653561025857925\n",
      "epoch:  22, loss: 0.0336383618414402\n",
      "epoch:  23, loss: 0.03362689167261124\n",
      "epoch:  24, loss: 0.033599015325307846\n",
      "epoch:  25, loss: 0.03358340635895729\n",
      "epoch:  26, loss: 0.03357093408703804\n",
      "epoch:  27, loss: 0.033542320132255554\n",
      "epoch:  28, loss: 0.03352624922990799\n",
      "epoch:  29, loss: 0.03351227194070816\n",
      "epoch:  30, loss: 0.03348229452967644\n",
      "epoch:  31, loss: 0.03346551954746246\n",
      "epoch:  32, loss: 0.03344995155930519\n",
      "epoch:  33, loss: 0.033418938517570496\n",
      "epoch:  34, loss: 0.03340153768658638\n",
      "epoch:  35, loss: 0.03338473662734032\n",
      "epoch:  36, loss: 0.03335180878639221\n",
      "epoch:  37, loss: 0.03333349898457527\n",
      "epoch:  38, loss: 0.033313628286123276\n",
      "epoch:  39, loss: 0.03327944129705429\n",
      "epoch:  40, loss: 0.03326018899679184\n",
      "epoch:  41, loss: 0.033238645642995834\n",
      "epoch:  42, loss: 0.033202189952135086\n",
      "epoch:  43, loss: 0.033181820064783096\n",
      "epoch:  44, loss: 0.0331571027636528\n",
      "epoch:  45, loss: 0.033118780702352524\n",
      "epoch:  46, loss: 0.03309714049100876\n",
      "epoch:  47, loss: 0.033069923520088196\n",
      "epoch:  48, loss: 0.03302856162190437\n",
      "epoch:  49, loss: 0.03300541266798973\n",
      "epoch:  50, loss: 0.032974012196063995\n",
      "epoch:  51, loss: 0.03293014317750931\n",
      "epoch:  52, loss: 0.032905369997024536\n",
      "epoch:  53, loss: 0.03287139907479286\n",
      "epoch:  54, loss: 0.032823532819747925\n",
      "epoch:  55, loss: 0.03279680758714676\n",
      "epoch:  56, loss: 0.032757386565208435\n",
      "epoch:  57, loss: 0.03270623832941055\n",
      "epoch:  58, loss: 0.03267728164792061\n",
      "epoch:  59, loss: 0.032633207738399506\n",
      "epoch:  60, loss: 0.03257659450173378\n",
      "epoch:  61, loss: 0.03254500776529312\n",
      "epoch:  62, loss: 0.03249575197696686\n",
      "epoch:  63, loss: 0.03243490308523178\n",
      "epoch:  64, loss: 0.03240077942609787\n",
      "epoch:  65, loss: 0.032350316643714905\n",
      "epoch:  66, loss: 0.032282330095767975\n",
      "epoch:  67, loss: 0.03224494308233261\n",
      "epoch:  68, loss: 0.032189369201660156\n",
      "epoch:  69, loss: 0.03211556747555733\n",
      "epoch:  70, loss: 0.03207483887672424\n",
      "epoch:  71, loss: 0.03202064707875252\n",
      "epoch:  72, loss: 0.03193707764148712\n",
      "epoch:  73, loss: 0.03189215064048767\n",
      "epoch:  74, loss: 0.03183506056666374\n",
      "epoch:  75, loss: 0.03174347057938576\n",
      "epoch:  76, loss: 0.03169405087828636\n",
      "epoch:  77, loss: 0.031641751527786255\n",
      "epoch:  78, loss: 0.03153618052601814\n",
      "epoch:  79, loss: 0.03148122876882553\n",
      "epoch:  80, loss: 0.0314285010099411\n",
      "epoch:  81, loss: 0.0313117541372776\n",
      "epoch:  82, loss: 0.03125077113509178\n",
      "epoch:  83, loss: 0.03120732493698597\n",
      "epoch:  84, loss: 0.03107031062245369\n",
      "epoch:  85, loss: 0.031001567840576172\n",
      "epoch:  86, loss: 0.030960287898778915\n",
      "epoch:  87, loss: 0.03080727346241474\n",
      "epoch:  88, loss: 0.030730104073882103\n",
      "epoch:  89, loss: 0.03070922940969467\n",
      "epoch:  90, loss: 0.03052525594830513\n",
      "epoch:  91, loss: 0.030437279492616653\n",
      "epoch:  92, loss: 0.030425911769270897\n",
      "epoch:  93, loss: 0.0302187018096447\n",
      "epoch:  94, loss: 0.030118828639388084\n",
      "epoch:  95, loss: 0.030061950907111168\n",
      "epoch:  96, loss: 0.02989480458199978\n",
      "epoch:  97, loss: 0.029775839298963547\n",
      "epoch:  98, loss: 0.029711473733186722\n",
      "epoch:  99, loss: 0.029536589980125427\n",
      "epoch:  100, loss: 0.02940136380493641\n",
      "epoch:  101, loss: 0.02932882122695446\n",
      "epoch:  102, loss: 0.029162617400288582\n",
      "epoch:  103, loss: 0.028997626155614853\n",
      "epoch:  104, loss: 0.02891458570957184\n",
      "epoch:  105, loss: 0.028743566945195198\n",
      "epoch:  106, loss: 0.028555285185575485\n",
      "epoch:  107, loss: 0.028461260721087456\n",
      "epoch:  108, loss: 0.028311926871538162\n",
      "epoch:  109, loss: 0.028078490868210793\n",
      "epoch:  110, loss: 0.027970483526587486\n",
      "epoch:  111, loss: 0.02782410942018032\n",
      "epoch:  112, loss: 0.027556587010622025\n",
      "epoch:  113, loss: 0.02743356302380562\n",
      "epoch:  114, loss: 0.027329787611961365\n",
      "epoch:  115, loss: 0.02699238620698452\n",
      "epoch:  116, loss: 0.026851171627640724\n",
      "epoch:  117, loss: 0.0267585888504982\n",
      "epoch:  118, loss: 0.026377098634839058\n",
      "epoch:  119, loss: 0.026217130944132805\n",
      "epoch:  120, loss: 0.026193005964159966\n",
      "epoch:  121, loss: 0.02571185864508152\n",
      "epoch:  122, loss: 0.025531869381666183\n",
      "epoch:  123, loss: 0.02550756372511387\n",
      "epoch:  124, loss: 0.024986358359456062\n",
      "epoch:  125, loss: 0.02478860877454281\n",
      "epoch:  126, loss: 0.02468542754650116\n",
      "epoch:  127, loss: 0.024221710860729218\n",
      "epoch:  128, loss: 0.023990923538804054\n",
      "epoch:  129, loss: 0.023879801854491234\n",
      "epoch:  130, loss: 0.023372318595647812\n",
      "epoch:  131, loss: 0.023131081834435463\n",
      "epoch:  132, loss: 0.023014603182673454\n",
      "epoch:  133, loss: 0.02248399332165718\n",
      "epoch:  134, loss: 0.02221912331879139\n",
      "epoch:  135, loss: 0.022100046277046204\n",
      "epoch:  136, loss: 0.02150600217282772\n",
      "epoch:  137, loss: 0.021254949271678925\n",
      "epoch:  138, loss: 0.021137448027729988\n",
      "epoch:  139, loss: 0.020502015948295593\n",
      "epoch:  140, loss: 0.02025485970079899\n",
      "epoch:  141, loss: 0.02018112689256668\n",
      "epoch:  142, loss: 0.019430214539170265\n",
      "epoch:  143, loss: 0.01922776736319065\n",
      "epoch:  144, loss: 0.018976103514432907\n",
      "epoch:  145, loss: 0.018352573737502098\n",
      "epoch:  146, loss: 0.018196692690253258\n",
      "epoch:  147, loss: 0.017653238028287888\n",
      "epoch:  148, loss: 0.017294200137257576\n",
      "epoch:  149, loss: 0.01717628911137581\n",
      "epoch:  150, loss: 0.016510438174009323\n",
      "epoch:  151, loss: 0.01628655008971691\n",
      "epoch:  152, loss: 0.016033178195357323\n",
      "epoch:  153, loss: 0.015459767542779446\n",
      "epoch:  154, loss: 0.015340450219810009\n",
      "epoch:  155, loss: 0.014766809530556202\n",
      "epoch:  156, loss: 0.014547484926879406\n",
      "epoch:  157, loss: 0.014297882094979286\n",
      "epoch:  158, loss: 0.013828122988343239\n",
      "epoch:  159, loss: 0.013743121176958084\n",
      "epoch:  160, loss: 0.013202399015426636\n",
      "epoch:  161, loss: 0.01309044100344181\n",
      "epoch:  162, loss: 0.0126627953723073\n",
      "epoch:  163, loss: 0.012511782348155975\n",
      "epoch:  164, loss: 0.012217015959322453\n",
      "epoch:  165, loss: 0.012004250660538673\n",
      "epoch:  166, loss: 0.011766847223043442\n",
      "epoch:  167, loss: 0.011564581654965878\n",
      "epoch:  168, loss: 0.011378327384591103\n",
      "epoch:  169, loss: 0.011187786236405373\n",
      "epoch:  170, loss: 0.010968012735247612\n",
      "epoch:  171, loss: 0.01086598914116621\n",
      "epoch:  172, loss: 0.010642919689416885\n",
      "epoch:  173, loss: 0.010592167265713215\n",
      "epoch:  174, loss: 0.010382184758782387\n",
      "epoch:  175, loss: 0.01035819761455059\n",
      "epoch:  176, loss: 0.010172558948397636\n",
      "epoch:  177, loss: 0.01004528533667326\n",
      "epoch:  178, loss: 0.009993931278586388\n",
      "epoch:  179, loss: 0.009852859191596508\n",
      "epoch:  180, loss: 0.009771429002285004\n",
      "epoch:  181, loss: 0.009712625294923782\n",
      "epoch:  182, loss: 0.009599444456398487\n",
      "epoch:  183, loss: 0.009501122869551182\n",
      "epoch:  184, loss: 0.009488883428275585\n",
      "epoch:  185, loss: 0.00939940195530653\n",
      "epoch:  186, loss: 0.009346282109618187\n",
      "epoch:  187, loss: 0.00931159034371376\n",
      "epoch:  188, loss: 0.00924434419721365\n",
      "epoch:  189, loss: 0.009233846329152584\n",
      "epoch:  190, loss: 0.009171088226139545\n",
      "epoch:  191, loss: 0.009164584800601006\n",
      "epoch:  192, loss: 0.009108223021030426\n",
      "epoch:  193, loss: 0.009075630456209183\n",
      "epoch:  194, loss: 0.009052296169102192\n",
      "epoch:  195, loss: 0.009015349671244621\n",
      "epoch:  196, loss: 0.009002386592328548\n",
      "epoch:  197, loss: 0.008965411223471165\n",
      "epoch:  198, loss: 0.008957631886005402\n",
      "epoch:  199, loss: 0.008923130109906197\n",
      "epoch:  200, loss: 0.008917707949876785\n",
      "epoch:  201, loss: 0.008885657414793968\n",
      "epoch:  202, loss: 0.008881818503141403\n",
      "epoch:  203, loss: 0.008852441795170307\n",
      "epoch:  204, loss: 0.008849237114191055\n",
      "epoch:  205, loss: 0.008822501637041569\n",
      "epoch:  206, loss: 0.008820679038763046\n",
      "epoch:  207, loss: 0.0087958462536335\n",
      "epoch:  208, loss: 0.008787975646555424\n",
      "epoch:  209, loss: 0.008771885186433792\n",
      "epoch:  210, loss: 0.008762017823755741\n",
      "epoch:  211, loss: 0.00875021517276764\n",
      "epoch:  212, loss: 0.00873933918774128\n",
      "epoch:  213, loss: 0.008730723522603512\n",
      "epoch:  214, loss: 0.00872060190886259\n",
      "epoch:  215, loss: 0.008713306859135628\n",
      "epoch:  216, loss: 0.00870425719767809\n",
      "epoch:  217, loss: 0.008697647601366043\n",
      "epoch:  218, loss: 0.008689142763614655\n",
      "epoch:  219, loss: 0.008683518506586552\n",
      "epoch:  220, loss: 0.00867568701505661\n",
      "epoch:  221, loss: 0.008670872077345848\n",
      "epoch:  222, loss: 0.008664345368742943\n",
      "epoch:  223, loss: 0.008659450337290764\n",
      "epoch:  224, loss: 0.008654058910906315\n",
      "epoch:  225, loss: 0.008649192750453949\n",
      "epoch:  226, loss: 0.008645116351544857\n",
      "epoch:  227, loss: 0.00863993912935257\n",
      "epoch:  228, loss: 0.00863700732588768\n",
      "epoch:  229, loss: 0.008631556294858456\n",
      "epoch:  230, loss: 0.008630403317511082\n",
      "epoch:  231, loss: 0.008623968809843063\n",
      "epoch:  232, loss: 0.008623259142041206\n",
      "epoch:  233, loss: 0.008617062121629715\n",
      "epoch:  234, loss: 0.008616410195827484\n",
      "epoch:  235, loss: 0.008610759861767292\n",
      "epoch:  236, loss: 0.00861075147986412\n",
      "epoch:  237, loss: 0.008604907430708408\n",
      "epoch:  238, loss: 0.008604338392615318\n",
      "epoch:  239, loss: 0.008599594235420227\n",
      "epoch:  240, loss: 0.00859905406832695\n",
      "epoch:  241, loss: 0.008594809100031853\n",
      "epoch:  242, loss: 0.00859428197145462\n",
      "epoch:  243, loss: 0.008590434677898884\n",
      "epoch:  244, loss: 0.008589929901063442\n",
      "epoch:  245, loss: 0.008586464449763298\n",
      "epoch:  246, loss: 0.008585939183831215\n",
      "epoch:  247, loss: 0.00858272798359394\n",
      "epoch:  248, loss: 0.008582256734371185\n",
      "epoch:  249, loss: 0.008579417131841183\n",
      "epoch:  250, loss: 0.00857886765152216\n",
      "epoch:  251, loss: 0.008576647378504276\n",
      "epoch:  252, loss: 0.008575746789574623\n",
      "epoch:  253, loss: 0.008574018254876137\n",
      "epoch:  254, loss: 0.00857293326407671\n",
      "epoch:  255, loss: 0.008572057820856571\n",
      "epoch:  256, loss: 0.008570341393351555\n",
      "epoch:  257, loss: 0.008570270612835884\n",
      "epoch:  258, loss: 0.008567924611270428\n",
      "epoch:  259, loss: 0.008567685261368752\n",
      "epoch:  260, loss: 0.008565671741962433\n",
      "epoch:  261, loss: 0.008565433323383331\n",
      "epoch:  262, loss: 0.008563564158976078\n",
      "epoch:  263, loss: 0.008563315495848656\n",
      "epoch:  264, loss: 0.008561592549085617\n",
      "epoch:  265, loss: 0.00856130849570036\n",
      "epoch:  266, loss: 0.008559728041291237\n",
      "epoch:  267, loss: 0.008559422567486763\n",
      "epoch:  268, loss: 0.008557978086173534\n",
      "epoch:  269, loss: 0.008557640016078949\n",
      "epoch:  270, loss: 0.008556191809475422\n",
      "epoch:  271, loss: 0.008555897511541843\n",
      "epoch:  272, loss: 0.008554563857614994\n",
      "epoch:  273, loss: 0.00855424627661705\n",
      "epoch:  274, loss: 0.008553027175366879\n",
      "epoch:  275, loss: 0.008552666753530502\n",
      "epoch:  276, loss: 0.008551543578505516\n",
      "epoch:  277, loss: 0.00855113286525011\n",
      "epoch:  278, loss: 0.008550172671675682\n",
      "epoch:  279, loss: 0.00854964554309845\n",
      "epoch:  280, loss: 0.008548805490136147\n",
      "epoch:  281, loss: 0.008548193611204624\n",
      "epoch:  282, loss: 0.00854767020791769\n",
      "epoch:  283, loss: 0.008546808734536171\n",
      "epoch:  284, loss: 0.008546793833374977\n",
      "epoch:  285, loss: 0.00854549091309309\n",
      "epoch:  286, loss: 0.008545354939997196\n",
      "epoch:  287, loss: 0.00854422152042389\n",
      "epoch:  288, loss: 0.008544085547327995\n",
      "epoch:  289, loss: 0.008543017320334911\n",
      "epoch:  290, loss: 0.008542853407561779\n",
      "epoch:  291, loss: 0.008541825227439404\n",
      "epoch:  292, loss: 0.008541611954569817\n",
      "epoch:  293, loss: 0.008540657348930836\n",
      "epoch:  294, loss: 0.008540414273738861\n",
      "epoch:  295, loss: 0.00853960495442152\n",
      "epoch:  296, loss: 0.008539247326552868\n",
      "epoch:  297, loss: 0.008538556285202503\n",
      "epoch:  298, loss: 0.008538085967302322\n",
      "epoch:  299, loss: 0.008537625893950462\n",
      "epoch:  300, loss: 0.008536937646567822\n",
      "epoch:  301, loss: 0.008536825887858868\n",
      "epoch:  302, loss: 0.008535848930478096\n",
      "epoch:  303, loss: 0.008535733446478844\n",
      "epoch:  304, loss: 0.008534741587936878\n",
      "epoch:  305, loss: 0.008534621447324753\n",
      "epoch:  306, loss: 0.008533687330782413\n",
      "epoch:  307, loss: 0.008533529005944729\n",
      "epoch:  308, loss: 0.008532638661563396\n",
      "epoch:  309, loss: 0.008532432839274406\n",
      "epoch:  310, loss: 0.00853165052831173\n",
      "epoch:  311, loss: 0.008531349711120129\n",
      "epoch:  312, loss: 0.008530765771865845\n",
      "epoch:  313, loss: 0.008530268445611\n",
      "epoch:  314, loss: 0.008529883809387684\n",
      "epoch:  315, loss: 0.008529224433004856\n",
      "epoch:  316, loss: 0.008529121056199074\n",
      "epoch:  317, loss: 0.008528221398591995\n",
      "epoch:  318, loss: 0.008528118021786213\n",
      "epoch:  319, loss: 0.0085272416472435\n",
      "epoch:  320, loss: 0.008527123369276524\n",
      "epoch:  321, loss: 0.008526298217475414\n",
      "epoch:  322, loss: 0.008526135236024857\n",
      "epoch:  323, loss: 0.00852537713944912\n",
      "epoch:  324, loss: 0.008525144308805466\n",
      "epoch:  325, loss: 0.008524488657712936\n",
      "epoch:  326, loss: 0.008524157106876373\n",
      "epoch:  327, loss: 0.00852380134165287\n",
      "epoch:  328, loss: 0.008523181080818176\n",
      "epoch:  329, loss: 0.008523179218173027\n",
      "epoch:  330, loss: 0.008522211574018002\n",
      "epoch:  331, loss: 0.008522105403244495\n",
      "epoch:  332, loss: 0.00852123647928238\n",
      "epoch:  333, loss: 0.00852111540734768\n",
      "epoch:  334, loss: 0.008520295843482018\n",
      "epoch:  335, loss: 0.008520123548805714\n",
      "epoch:  336, loss: 0.008519357070326805\n",
      "epoch:  337, loss: 0.008519127033650875\n",
      "epoch:  338, loss: 0.008518487215042114\n",
      "epoch:  339, loss: 0.008518162183463573\n",
      "epoch:  340, loss: 0.008517651818692684\n",
      "epoch:  341, loss: 0.008517131209373474\n",
      "epoch:  342, loss: 0.008517035283148289\n",
      "epoch:  343, loss: 0.008516186848282814\n",
      "epoch:  344, loss: 0.00851606484502554\n",
      "epoch:  345, loss: 0.008515207096934319\n",
      "epoch:  346, loss: 0.008515048772096634\n",
      "epoch:  347, loss: 0.008514289744198322\n",
      "epoch:  348, loss: 0.008514064364135265\n",
      "epoch:  349, loss: 0.00851354468613863\n",
      "epoch:  350, loss: 0.008513147942721844\n",
      "epoch:  351, loss: 0.008512461557984352\n",
      "epoch:  352, loss: 0.008512199856340885\n",
      "epoch:  353, loss: 0.008511506021022797\n",
      "epoch:  354, loss: 0.008511118590831757\n",
      "epoch:  355, loss: 0.008510714396834373\n",
      "epoch:  356, loss: 0.008510064333677292\n",
      "epoch:  357, loss: 0.008509958162903786\n",
      "epoch:  358, loss: 0.008509021252393723\n",
      "epoch:  359, loss: 0.008508915081620216\n",
      "epoch:  360, loss: 0.008508012630045414\n",
      "epoch:  361, loss: 0.008507889695465565\n",
      "epoch:  362, loss: 0.008507016114890575\n",
      "epoch:  363, loss: 0.008506879210472107\n",
      "epoch:  364, loss: 0.00850614719092846\n",
      "epoch:  365, loss: 0.008505912497639656\n",
      "epoch:  366, loss: 0.008505385369062424\n",
      "epoch:  367, loss: 0.008504953235387802\n",
      "epoch:  368, loss: 0.008504655212163925\n",
      "epoch:  369, loss: 0.008503993041813374\n",
      "epoch:  370, loss: 0.008503896184265614\n",
      "epoch:  371, loss: 0.008503058925271034\n",
      "epoch:  372, loss: 0.008502939715981483\n",
      "epoch:  373, loss: 0.008502138778567314\n",
      "epoch:  374, loss: 0.008501977659761906\n",
      "epoch:  375, loss: 0.00850124005228281\n",
      "epoch:  376, loss: 0.008501005358994007\n",
      "epoch:  377, loss: 0.008500415831804276\n",
      "epoch:  378, loss: 0.008500008843839169\n",
      "epoch:  379, loss: 0.008499355986714363\n",
      "epoch:  380, loss: 0.008498823270201683\n",
      "epoch:  381, loss: 0.008498412556946278\n",
      "epoch:  382, loss: 0.008497652597725391\n",
      "epoch:  383, loss: 0.00849753525108099\n",
      "epoch:  384, loss: 0.008496526628732681\n",
      "epoch:  385, loss: 0.008496400900185108\n",
      "epoch:  386, loss: 0.008495447225868702\n",
      "epoch:  387, loss: 0.008495278656482697\n",
      "epoch:  388, loss: 0.008494353853166103\n",
      "epoch:  389, loss: 0.008494107984006405\n",
      "epoch:  390, loss: 0.008493088185787201\n",
      "epoch:  391, loss: 0.008492766879498959\n",
      "epoch:  392, loss: 0.008491966873407364\n",
      "epoch:  393, loss: 0.008491545915603638\n",
      "epoch:  394, loss: 0.008490991778671741\n",
      "epoch:  395, loss: 0.008490361273288727\n",
      "epoch:  396, loss: 0.008490263484418392\n",
      "epoch:  397, loss: 0.008489184081554413\n",
      "epoch:  398, loss: 0.008489060215651989\n",
      "epoch:  399, loss: 0.008487957529723644\n",
      "epoch:  400, loss: 0.00848781131207943\n",
      "epoch:  401, loss: 0.008486619219183922\n",
      "epoch:  402, loss: 0.00848644133657217\n",
      "epoch:  403, loss: 0.008485324680805206\n",
      "epoch:  404, loss: 0.008485117927193642\n",
      "epoch:  405, loss: 0.008484143763780594\n",
      "epoch:  406, loss: 0.008483819663524628\n",
      "epoch:  407, loss: 0.00848312396556139\n",
      "epoch:  408, loss: 0.008482546545565128\n",
      "epoch:  409, loss: 0.008482381701469421\n",
      "epoch:  410, loss: 0.008481311611831188\n",
      "epoch:  411, loss: 0.008481180295348167\n",
      "epoch:  412, loss: 0.008480150252580643\n",
      "epoch:  413, loss: 0.00848002452403307\n",
      "epoch:  414, loss: 0.008479121141135693\n",
      "epoch:  415, loss: 0.008478955365717411\n",
      "epoch:  416, loss: 0.008478367701172829\n",
      "epoch:  417, loss: 0.008477980270981789\n",
      "epoch:  418, loss: 0.008477586321532726\n",
      "epoch:  419, loss: 0.008477013558149338\n",
      "epoch:  420, loss: 0.008476914837956429\n",
      "epoch:  421, loss: 0.008476048707962036\n",
      "epoch:  422, loss: 0.008475947193801403\n",
      "epoch:  423, loss: 0.008475091308355331\n",
      "epoch:  424, loss: 0.008474969305098057\n",
      "epoch:  425, loss: 0.008474193513393402\n",
      "epoch:  426, loss: 0.008473990485072136\n",
      "epoch:  427, loss: 0.008473298512399197\n",
      "epoch:  428, loss: 0.008472990244626999\n",
      "epoch:  429, loss: 0.008472559042274952\n",
      "epoch:  430, loss: 0.008471931330859661\n",
      "epoch:  431, loss: 0.00847190897911787\n",
      "epoch:  432, loss: 0.008470851927995682\n",
      "epoch:  433, loss: 0.00847073644399643\n",
      "epoch:  434, loss: 0.008469793014228344\n",
      "epoch:  435, loss: 0.0084696589037776\n",
      "epoch:  436, loss: 0.00846879556775093\n",
      "epoch:  437, loss: 0.008468590676784515\n",
      "epoch:  438, loss: 0.00846783071756363\n",
      "epoch:  439, loss: 0.008467522449791431\n",
      "epoch:  440, loss: 0.008466949686408043\n",
      "epoch:  441, loss: 0.00846647284924984\n",
      "epoch:  442, loss: 0.008466372266411781\n",
      "epoch:  443, loss: 0.00846545398235321\n",
      "epoch:  444, loss: 0.008465342223644257\n",
      "epoch:  445, loss: 0.008464437909424305\n",
      "epoch:  446, loss: 0.008464301005005836\n",
      "epoch:  447, loss: 0.008463451638817787\n",
      "epoch:  448, loss: 0.008463259786367416\n",
      "epoch:  449, loss: 0.008462519384920597\n",
      "epoch:  450, loss: 0.008462224155664444\n",
      "epoch:  451, loss: 0.008461658842861652\n",
      "epoch:  452, loss: 0.008461189456284046\n",
      "epoch:  453, loss: 0.00846101250499487\n",
      "epoch:  454, loss: 0.008460158482193947\n",
      "epoch:  455, loss: 0.008460051380097866\n",
      "epoch:  456, loss: 0.008459117263555527\n",
      "epoch:  457, loss: 0.008459001779556274\n",
      "epoch:  458, loss: 0.008458053693175316\n",
      "epoch:  459, loss: 0.008457920514047146\n",
      "epoch:  460, loss: 0.008456872776150703\n",
      "epoch:  461, loss: 0.008456743322312832\n",
      "epoch:  462, loss: 0.008455735631287098\n",
      "epoch:  463, loss: 0.008455593138933182\n",
      "epoch:  464, loss: 0.008454619906842709\n",
      "epoch:  465, loss: 0.008454421535134315\n",
      "epoch:  466, loss: 0.008453559130430222\n",
      "epoch:  467, loss: 0.008453240618109703\n",
      "epoch:  468, loss: 0.008452634327113628\n",
      "epoch:  469, loss: 0.00845206156373024\n",
      "epoch:  470, loss: 0.008451967500150204\n",
      "epoch:  471, loss: 0.008450891822576523\n",
      "epoch:  472, loss: 0.008450763300061226\n",
      "epoch:  473, loss: 0.008449717424809933\n",
      "epoch:  474, loss: 0.008449578657746315\n",
      "epoch:  475, loss: 0.008448567241430283\n",
      "epoch:  476, loss: 0.008448394946753979\n",
      "epoch:  477, loss: 0.008447456173598766\n",
      "epoch:  478, loss: 0.008447214029729366\n",
      "epoch:  479, loss: 0.008446376770734787\n",
      "epoch:  480, loss: 0.008446017280220985\n",
      "epoch:  481, loss: 0.008445310406386852\n",
      "epoch:  482, loss: 0.008444838225841522\n",
      "epoch:  483, loss: 0.008444231934845448\n",
      "epoch:  484, loss: 0.008443637751042843\n",
      "epoch:  485, loss: 0.00844347383826971\n",
      "epoch:  486, loss: 0.008442445658147335\n",
      "epoch:  487, loss: 0.008442317135632038\n",
      "epoch:  488, loss: 0.00844122190028429\n",
      "epoch:  489, loss: 0.008441050536930561\n",
      "epoch:  490, loss: 0.00844002515077591\n",
      "epoch:  491, loss: 0.008439779281616211\n",
      "epoch:  492, loss: 0.00843876227736473\n",
      "epoch:  493, loss: 0.008438433520495892\n",
      "epoch:  494, loss: 0.00843767635524273\n",
      "epoch:  495, loss: 0.008437097072601318\n",
      "epoch:  496, loss: 0.008436708711087704\n",
      "epoch:  497, loss: 0.008435741998255253\n",
      "epoch:  498, loss: 0.008435598574578762\n",
      "epoch:  499, loss: 0.008434268645942211\n",
      "epoch:  500, loss: 0.00843410287052393\n",
      "epoch:  501, loss: 0.008432796224951744\n",
      "epoch:  502, loss: 0.008432629518210888\n",
      "epoch:  503, loss: 0.00843146350234747\n",
      "epoch:  504, loss: 0.00843126978725195\n",
      "epoch:  505, loss: 0.00843023881316185\n",
      "epoch:  506, loss: 0.008429997600615025\n",
      "epoch:  507, loss: 0.008429083041846752\n",
      "epoch:  508, loss: 0.008428731001913548\n",
      "epoch:  509, loss: 0.00842798501253128\n",
      "epoch:  510, loss: 0.008427455089986324\n",
      "epoch:  511, loss: 0.008427087217569351\n",
      "epoch:  512, loss: 0.008426186628639698\n",
      "epoch:  513, loss: 0.008426057174801826\n",
      "epoch:  514, loss: 0.008424945175647736\n",
      "epoch:  515, loss: 0.008424794301390648\n",
      "epoch:  516, loss: 0.00842377170920372\n",
      "epoch:  517, loss: 0.008423534221947193\n",
      "epoch:  518, loss: 0.008422710932791233\n",
      "epoch:  519, loss: 0.008422273211181164\n",
      "epoch:  520, loss: 0.008421813137829304\n",
      "epoch:  521, loss: 0.00842101126909256\n",
      "epoch:  522, loss: 0.008420883677899837\n",
      "epoch:  523, loss: 0.008419763296842575\n",
      "epoch:  524, loss: 0.008419614285230637\n",
      "epoch:  525, loss: 0.008418537676334381\n",
      "epoch:  526, loss: 0.00841833557933569\n",
      "epoch:  527, loss: 0.00841746386140585\n",
      "epoch:  528, loss: 0.00841706432402134\n",
      "epoch:  529, loss: 0.00841650739312172\n",
      "epoch:  530, loss: 0.00841578934341669\n",
      "epoch:  531, loss: 0.008415657095611095\n",
      "epoch:  532, loss: 0.008414531126618385\n",
      "epoch:  533, loss: 0.008414375595748425\n",
      "epoch:  534, loss: 0.008413277566432953\n",
      "epoch:  535, loss: 0.008413088507950306\n",
      "epoch:  536, loss: 0.008412049151957035\n",
      "epoch:  537, loss: 0.008411801420152187\n",
      "epoch:  538, loss: 0.008410838432610035\n",
      "epoch:  539, loss: 0.00841050036251545\n",
      "epoch:  540, loss: 0.00840974785387516\n",
      "epoch:  541, loss: 0.008409184403717518\n",
      "epoch:  542, loss: 0.008409031666815281\n",
      "epoch:  543, loss: 0.008407861925661564\n",
      "epoch:  544, loss: 0.008407719433307648\n",
      "epoch:  545, loss: 0.008406582288444042\n",
      "epoch:  546, loss: 0.008406413719058037\n",
      "epoch:  547, loss: 0.00840530451387167\n",
      "epoch:  548, loss: 0.008405085653066635\n",
      "epoch:  549, loss: 0.008404143154621124\n",
      "epoch:  550, loss: 0.008403797633945942\n",
      "epoch:  551, loss: 0.008403066545724869\n",
      "epoch:  552, loss: 0.008402492851018906\n",
      "epoch:  553, loss: 0.008402466773986816\n",
      "epoch:  554, loss: 0.008401178754866123\n",
      "epoch:  555, loss: 0.00840103067457676\n",
      "epoch:  556, loss: 0.008399879559874535\n",
      "epoch:  557, loss: 0.008399704471230507\n",
      "epoch:  558, loss: 0.008398599922657013\n",
      "epoch:  559, loss: 0.008398373611271381\n",
      "epoch:  560, loss: 0.008397353813052177\n",
      "epoch:  561, loss: 0.008397013880312443\n",
      "epoch:  562, loss: 0.00839623250067234\n",
      "epoch:  563, loss: 0.008395673707127571\n",
      "epoch:  564, loss: 0.008395331911742687\n",
      "epoch:  565, loss: 0.008394313044846058\n",
      "epoch:  566, loss: 0.008394171483814716\n",
      "epoch:  567, loss: 0.008393051102757454\n",
      "epoch:  568, loss: 0.008392837829887867\n",
      "epoch:  569, loss: 0.008391780778765678\n",
      "epoch:  570, loss: 0.00839146412909031\n",
      "epoch:  571, loss: 0.008390825241804123\n",
      "epoch:  572, loss: 0.008390119299292564\n",
      "epoch:  573, loss: 0.008390086703002453\n",
      "epoch:  574, loss: 0.008388682268559933\n",
      "epoch:  575, loss: 0.008388513699173927\n",
      "epoch:  576, loss: 0.008387301117181778\n",
      "epoch:  577, loss: 0.008387072943150997\n",
      "epoch:  578, loss: 0.008386068977415562\n",
      "epoch:  579, loss: 0.008385637775063515\n",
      "epoch:  580, loss: 0.008384964428842068\n",
      "epoch:  581, loss: 0.008384209126234055\n",
      "epoch:  582, loss: 0.00838406290858984\n",
      "epoch:  583, loss: 0.008382773026823997\n",
      "epoch:  584, loss: 0.008382598869502544\n",
      "epoch:  585, loss: 0.008381352759897709\n",
      "epoch:  586, loss: 0.008381105959415436\n",
      "epoch:  587, loss: 0.008380117826163769\n",
      "epoch:  588, loss: 0.0083796177059412\n",
      "epoch:  589, loss: 0.008379319682717323\n",
      "epoch:  590, loss: 0.008378086611628532\n",
      "epoch:  591, loss: 0.00837791059166193\n",
      "epoch:  592, loss: 0.008376486599445343\n",
      "epoch:  593, loss: 0.00837627798318863\n",
      "epoch:  594, loss: 0.008374976925551891\n",
      "epoch:  595, loss: 0.008374654687941074\n",
      "epoch:  596, loss: 0.008373702876269817\n",
      "epoch:  597, loss: 0.008373028598725796\n",
      "epoch:  598, loss: 0.008372864685952663\n",
      "epoch:  599, loss: 0.008371380157768726\n",
      "epoch:  600, loss: 0.008371162228286266\n",
      "epoch:  601, loss: 0.00836972612887621\n",
      "epoch:  602, loss: 0.00836941134184599\n",
      "epoch:  603, loss: 0.008368519134819508\n",
      "epoch:  604, loss: 0.008367696776986122\n",
      "epoch:  605, loss: 0.008367522619664669\n",
      "epoch:  606, loss: 0.008366039022803307\n",
      "epoch:  607, loss: 0.008365800604224205\n",
      "epoch:  608, loss: 0.008364665322005749\n",
      "epoch:  609, loss: 0.008364085108041763\n",
      "epoch:  610, loss: 0.00836362224072218\n",
      "epoch:  611, loss: 0.00836238358169794\n",
      "epoch:  612, loss: 0.00836218986660242\n",
      "epoch:  613, loss: 0.00836064014583826\n",
      "epoch:  614, loss: 0.008360431529581547\n",
      "epoch:  615, loss: 0.00835908018052578\n",
      "epoch:  616, loss: 0.008358698338270187\n",
      "epoch:  617, loss: 0.008357800543308258\n",
      "epoch:  618, loss: 0.008356973528862\n",
      "epoch:  619, loss: 0.008356795646250248\n",
      "epoch:  620, loss: 0.008355273865163326\n",
      "epoch:  621, loss: 0.008355062454938889\n",
      "epoch:  622, loss: 0.008353743702173233\n",
      "epoch:  623, loss: 0.008353338576853275\n",
      "epoch:  624, loss: 0.008352412842214108\n",
      "epoch:  625, loss: 0.008351624943315983\n",
      "epoch:  626, loss: 0.008351454511284828\n",
      "epoch:  627, loss: 0.008350036107003689\n",
      "epoch:  628, loss: 0.008349735289812088\n",
      "epoch:  629, loss: 0.008348684757947922\n",
      "epoch:  630, loss: 0.00834791548550129\n",
      "epoch:  631, loss: 0.008347743190824986\n",
      "epoch:  632, loss: 0.008346267975866795\n",
      "epoch:  633, loss: 0.008345997892320156\n",
      "epoch:  634, loss: 0.008344645611941814\n",
      "epoch:  635, loss: 0.008344248868525028\n",
      "epoch:  636, loss: 0.008343237452208996\n",
      "epoch:  637, loss: 0.00834250170737505\n",
      "epoch:  638, loss: 0.008342329412698746\n",
      "epoch:  639, loss: 0.008340829983353615\n",
      "epoch:  640, loss: 0.008340585045516491\n",
      "epoch:  641, loss: 0.008339432068169117\n",
      "epoch:  642, loss: 0.0083388052880764\n",
      "epoch:  643, loss: 0.008338617160916328\n",
      "epoch:  644, loss: 0.008336988277733326\n",
      "epoch:  645, loss: 0.008336699567735195\n",
      "epoch:  646, loss: 0.008335240185260773\n",
      "epoch:  647, loss: 0.008334782905876637\n",
      "epoch:  648, loss: 0.008334033191204071\n",
      "epoch:  649, loss: 0.00833286251872778\n",
      "epoch:  650, loss: 0.008332653902471066\n",
      "epoch:  651, loss: 0.00833086296916008\n",
      "epoch:  652, loss: 0.008330614306032658\n",
      "epoch:  653, loss: 0.008328991010785103\n",
      "epoch:  654, loss: 0.008328587748110294\n",
      "epoch:  655, loss: 0.008327540941536427\n",
      "epoch:  656, loss: 0.008326578885316849\n",
      "epoch:  657, loss: 0.00832636933773756\n",
      "epoch:  658, loss: 0.008324521593749523\n",
      "epoch:  659, loss: 0.008324261754751205\n",
      "epoch:  660, loss: 0.008322595618665218\n",
      "epoch:  661, loss: 0.008322157897055149\n",
      "epoch:  662, loss: 0.008320864289999008\n",
      "epoch:  663, loss: 0.008320041000843048\n",
      "epoch:  664, loss: 0.008320004679262638\n",
      "epoch:  665, loss: 0.008317970670759678\n",
      "epoch:  666, loss: 0.008317728526890278\n",
      "epoch:  667, loss: 0.008315840736031532\n",
      "epoch:  668, loss: 0.008315524086356163\n",
      "epoch:  669, loss: 0.008313710801303387\n",
      "epoch:  670, loss: 0.00831318274140358\n",
      "epoch:  671, loss: 0.008311980403959751\n",
      "epoch:  672, loss: 0.008310803212225437\n",
      "epoch:  673, loss: 0.008310554549098015\n",
      "epoch:  674, loss: 0.008308595046401024\n",
      "epoch:  675, loss: 0.008308317512273788\n",
      "epoch:  676, loss: 0.00830660667270422\n",
      "epoch:  677, loss: 0.008306190371513367\n",
      "epoch:  678, loss: 0.008304813876748085\n",
      "epoch:  679, loss: 0.008304021321237087\n",
      "epoch:  680, loss: 0.008303587324917316\n",
      "epoch:  681, loss: 0.008301781490445137\n",
      "epoch:  682, loss: 0.008301529102027416\n",
      "epoch:  683, loss: 0.008299464359879494\n",
      "epoch:  684, loss: 0.008299203589558601\n",
      "epoch:  685, loss: 0.008297251537442207\n",
      "epoch:  686, loss: 0.0082969069480896\n",
      "epoch:  687, loss: 0.008295283652842045\n",
      "epoch:  688, loss: 0.008294626139104366\n",
      "epoch:  689, loss: 0.008293776772916317\n",
      "epoch:  690, loss: 0.008292417041957378\n",
      "epoch:  691, loss: 0.0082921851426363\n",
      "epoch:  692, loss: 0.00829010084271431\n",
      "epoch:  693, loss: 0.008289779536426067\n",
      "epoch:  694, loss: 0.008287828415632248\n",
      "epoch:  695, loss: 0.008287318982183933\n",
      "epoch:  696, loss: 0.008285898715257645\n",
      "epoch:  697, loss: 0.008284872397780418\n",
      "epoch:  698, loss: 0.008284511975944042\n",
      "epoch:  699, loss: 0.008282388560473919\n",
      "epoch:  700, loss: 0.008282115682959557\n",
      "epoch:  701, loss: 0.008279871195554733\n",
      "epoch:  702, loss: 0.008279506117105484\n",
      "epoch:  703, loss: 0.008277305401861668\n",
      "epoch:  704, loss: 0.008276775479316711\n",
      "epoch:  705, loss: 0.008275049738585949\n",
      "epoch:  706, loss: 0.00827400479465723\n",
      "epoch:  707, loss: 0.008273115381598473\n",
      "epoch:  708, loss: 0.008271260187029839\n",
      "epoch:  709, loss: 0.008270977064967155\n",
      "epoch:  710, loss: 0.008268546313047409\n",
      "epoch:  711, loss: 0.00826821569353342\n",
      "epoch:  712, loss: 0.008266002871096134\n",
      "epoch:  713, loss: 0.008265446871519089\n",
      "epoch:  714, loss: 0.008263505063951015\n",
      "epoch:  715, loss: 0.008262560702860355\n",
      "epoch:  716, loss: 0.008261953480541706\n",
      "epoch:  717, loss: 0.008259803056716919\n",
      "epoch:  718, loss: 0.008259500376880169\n",
      "epoch:  719, loss: 0.008257218636572361\n",
      "epoch:  720, loss: 0.00825676042586565\n",
      "epoch:  721, loss: 0.00825495459139347\n",
      "epoch:  722, loss: 0.008253995329141617\n",
      "epoch:  723, loss: 0.008253676816821098\n",
      "epoch:  724, loss: 0.008251278661191463\n",
      "epoch:  725, loss: 0.008250975050032139\n",
      "epoch:  726, loss: 0.008248727768659592\n",
      "epoch:  727, loss: 0.008248265832662582\n",
      "epoch:  728, loss: 0.008246462792158127\n",
      "epoch:  729, loss: 0.008245661854743958\n",
      "epoch:  730, loss: 0.00824480876326561\n",
      "epoch:  731, loss: 0.008243194781243801\n",
      "epoch:  732, loss: 0.008242925629019737\n",
      "epoch:  733, loss: 0.00824076496064663\n",
      "epoch:  734, loss: 0.00824044644832611\n",
      "epoch:  735, loss: 0.008238483220338821\n",
      "epoch:  736, loss: 0.008237939327955246\n",
      "epoch:  737, loss: 0.008236483670771122\n",
      "epoch:  738, loss: 0.008235410787165165\n",
      "epoch:  739, loss: 0.00823515560477972\n",
      "epoch:  740, loss: 0.008232896216213703\n",
      "epoch:  741, loss: 0.008232604712247849\n",
      "epoch:  742, loss: 0.008230477571487427\n",
      "epoch:  743, loss: 0.008230057545006275\n",
      "epoch:  744, loss: 0.008228260092437267\n",
      "epoch:  745, loss: 0.008227517828345299\n",
      "epoch:  746, loss: 0.0082270922139287\n",
      "epoch:  747, loss: 0.00822500605136156\n",
      "epoch:  748, loss: 0.008224726654589176\n",
      "epoch:  749, loss: 0.008222537115216255\n",
      "epoch:  750, loss: 0.00822215061634779\n",
      "epoch:  751, loss: 0.00822021160274744\n",
      "epoch:  752, loss: 0.008219486102461815\n",
      "epoch:  753, loss: 0.008219009265303612\n",
      "epoch:  754, loss: 0.008216851390898228\n",
      "epoch:  755, loss: 0.008216549642384052\n",
      "epoch:  756, loss: 0.008214305154979229\n",
      "epoch:  757, loss: 0.008213885128498077\n",
      "epoch:  758, loss: 0.008212045766413212\n",
      "epoch:  759, loss: 0.008211202919483185\n",
      "epoch:  760, loss: 0.008211133070290089\n",
      "epoch:  761, loss: 0.008208537474274635\n",
      "epoch:  762, loss: 0.008208218030631542\n",
      "epoch:  763, loss: 0.008205903694033623\n",
      "epoch:  764, loss: 0.008205456659197807\n",
      "epoch:  765, loss: 0.008203635923564434\n",
      "epoch:  766, loss: 0.008202631957828999\n",
      "epoch:  767, loss: 0.008202222175896168\n",
      "epoch:  768, loss: 0.00819979514926672\n",
      "epoch:  769, loss: 0.008199470117688179\n",
      "epoch:  770, loss: 0.008197053335607052\n",
      "epoch:  771, loss: 0.00819658488035202\n",
      "epoch:  772, loss: 0.008194859139621258\n",
      "epoch:  773, loss: 0.008193697780370712\n",
      "epoch:  774, loss: 0.008193404413759708\n",
      "epoch:  775, loss: 0.008190837688744068\n",
      "epoch:  776, loss: 0.008190478198230267\n",
      "epoch:  777, loss: 0.008188009262084961\n",
      "epoch:  778, loss: 0.008187461644411087\n",
      "epoch:  779, loss: 0.008186107501387596\n",
      "epoch:  780, loss: 0.008184471167623997\n",
      "epoch:  781, loss: 0.008184152655303478\n",
      "epoch:  782, loss: 0.008181513287127018\n",
      "epoch:  783, loss: 0.008181089535355568\n",
      "epoch:  784, loss: 0.008178777992725372\n",
      "epoch:  785, loss: 0.00817799847573042\n",
      "epoch:  786, loss: 0.008177224546670914\n",
      "epoch:  787, loss: 0.008174932561814785\n",
      "epoch:  788, loss: 0.008174585178494453\n",
      "epoch:  789, loss: 0.008172010071575642\n",
      "epoch:  790, loss: 0.008171472698450089\n",
      "epoch:  791, loss: 0.008169719949364662\n",
      "epoch:  792, loss: 0.008168370462954044\n",
      "epoch:  793, loss: 0.008168204687535763\n",
      "epoch:  794, loss: 0.008165145292878151\n",
      "epoch:  795, loss: 0.008164772763848305\n",
      "epoch:  796, loss: 0.00816203374415636\n",
      "epoch:  797, loss: 0.008161459118127823\n",
      "epoch:  798, loss: 0.008159046992659569\n",
      "epoch:  799, loss: 0.008158104494214058\n",
      "epoch:  800, loss: 0.008157544769346714\n",
      "epoch:  801, loss: 0.008154802024364471\n",
      "epoch:  802, loss: 0.008154397830367088\n",
      "epoch:  803, loss: 0.008151315152645111\n",
      "epoch:  804, loss: 0.008150603622198105\n",
      "epoch:  805, loss: 0.008148141205310822\n",
      "epoch:  806, loss: 0.008146444335579872\n",
      "epoch:  807, loss: 0.008146020583808422\n",
      "epoch:  808, loss: 0.008142366074025631\n",
      "epoch:  809, loss: 0.008141862228512764\n",
      "epoch:  810, loss: 0.008138640783727169\n",
      "epoch:  811, loss: 0.008137882687151432\n",
      "epoch:  812, loss: 0.008135671727359295\n",
      "epoch:  813, loss: 0.008133942261338234\n",
      "epoch:  814, loss: 0.008133540861308575\n",
      "epoch:  815, loss: 0.00813016016036272\n",
      "epoch:  816, loss: 0.008129584603011608\n",
      "epoch:  817, loss: 0.008126777596771717\n",
      "epoch:  818, loss: 0.00812563020735979\n",
      "epoch:  819, loss: 0.008125422522425652\n",
      "epoch:  820, loss: 0.008121694438159466\n",
      "epoch:  821, loss: 0.008121206425130367\n",
      "epoch:  822, loss: 0.008117877878248692\n",
      "epoch:  823, loss: 0.008117101155221462\n",
      "epoch:  824, loss: 0.00811484269797802\n",
      "epoch:  825, loss: 0.008112865500152111\n",
      "epoch:  826, loss: 0.008112423121929169\n",
      "epoch:  827, loss: 0.008108626119792461\n",
      "epoch:  828, loss: 0.008108042180538177\n",
      "epoch:  829, loss: 0.008104512467980385\n",
      "epoch:  830, loss: 0.008103519678115845\n",
      "epoch:  831, loss: 0.008101615123450756\n",
      "epoch:  832, loss: 0.008099074475467205\n",
      "epoch:  833, loss: 0.008098598569631577\n",
      "epoch:  834, loss: 0.008094890974462032\n",
      "epoch:  835, loss: 0.008094183169305325\n",
      "epoch:  836, loss: 0.008091500960290432\n",
      "epoch:  837, loss: 0.00808981154114008\n",
      "epoch:  838, loss: 0.00808936357498169\n",
      "epoch:  839, loss: 0.008085398934781551\n",
      "epoch:  840, loss: 0.008084774017333984\n",
      "epoch:  841, loss: 0.008081058040261269\n",
      "epoch:  842, loss: 0.008079806342720985\n",
      "epoch:  843, loss: 0.008078105747699738\n",
      "epoch:  844, loss: 0.008074444718658924\n",
      "epoch:  845, loss: 0.008073845878243446\n",
      "epoch:  846, loss: 0.008069422096014023\n",
      "epoch:  847, loss: 0.008068466559052467\n",
      "epoch:  848, loss: 0.008066214621067047\n",
      "epoch:  849, loss: 0.008063211105763912\n",
      "epoch:  850, loss: 0.00806262157857418\n",
      "epoch:  851, loss: 0.008058136329054832\n",
      "epoch:  852, loss: 0.008057251572608948\n",
      "epoch:  853, loss: 0.008053336292505264\n",
      "epoch:  854, loss: 0.008051387034356594\n",
      "epoch:  855, loss: 0.008050743490457535\n",
      "epoch:  856, loss: 0.008045267313718796\n",
      "epoch:  857, loss: 0.008044508285820484\n",
      "epoch:  858, loss: 0.008039796724915504\n",
      "epoch:  859, loss: 0.008038627915084362\n",
      "epoch:  860, loss: 0.008037286810576916\n",
      "epoch:  861, loss: 0.008033248595893383\n",
      "epoch:  862, loss: 0.008032617159187794\n",
      "epoch:  863, loss: 0.0080282436683774\n",
      "epoch:  864, loss: 0.008027139119803905\n",
      "epoch:  865, loss: 0.008025170303881168\n",
      "epoch:  866, loss: 0.008021842688322067\n",
      "epoch:  867, loss: 0.008021257817745209\n",
      "epoch:  868, loss: 0.008016865700483322\n",
      "epoch:  869, loss: 0.008015933446586132\n",
      "epoch:  870, loss: 0.008014146238565445\n",
      "epoch:  871, loss: 0.008010716177523136\n",
      "epoch:  872, loss: 0.008010116405785084\n",
      "epoch:  873, loss: 0.008006011135876179\n",
      "epoch:  874, loss: 0.008004863746464252\n",
      "epoch:  875, loss: 0.008003096096217632\n",
      "epoch:  876, loss: 0.00799970980733633\n",
      "epoch:  877, loss: 0.007999114692211151\n",
      "epoch:  878, loss: 0.007994775660336018\n",
      "epoch:  879, loss: 0.007993812672793865\n",
      "epoch:  880, loss: 0.007991504855453968\n",
      "epoch:  881, loss: 0.007988635450601578\n",
      "epoch:  882, loss: 0.007988042198121548\n",
      "epoch:  883, loss: 0.007983582094311714\n",
      "epoch:  884, loss: 0.007982701994478703\n",
      "epoch:  885, loss: 0.007980707101523876\n",
      "epoch:  886, loss: 0.007977467030286789\n",
      "epoch:  887, loss: 0.007976870983839035\n",
      "epoch:  888, loss: 0.007972795516252518\n",
      "epoch:  889, loss: 0.007971529848873615\n",
      "epoch:  890, loss: 0.00797069352120161\n",
      "epoch:  891, loss: 0.007966252975165844\n",
      "epoch:  892, loss: 0.007965617813169956\n",
      "epoch:  893, loss: 0.007962482050061226\n",
      "epoch:  894, loss: 0.007960350252687931\n",
      "epoch:  895, loss: 0.007959788665175438\n",
      "epoch:  896, loss: 0.007955465465784073\n",
      "epoch:  897, loss: 0.007954497821629047\n",
      "epoch:  898, loss: 0.007952149026095867\n",
      "epoch:  899, loss: 0.007949240505695343\n",
      "epoch:  900, loss: 0.007948637008666992\n",
      "epoch:  901, loss: 0.007944408804178238\n",
      "epoch:  902, loss: 0.007943320088088512\n",
      "epoch:  903, loss: 0.007942788302898407\n",
      "epoch:  904, loss: 0.007938186638057232\n",
      "epoch:  905, loss: 0.007937433198094368\n",
      "epoch:  906, loss: 0.007934407331049442\n",
      "epoch:  907, loss: 0.00793219543993473\n",
      "epoch:  908, loss: 0.007931624539196491\n",
      "epoch:  909, loss: 0.007927081547677517\n",
      "epoch:  910, loss: 0.007926194928586483\n",
      "epoch:  911, loss: 0.007923820987343788\n",
      "epoch:  912, loss: 0.007920823059976101\n",
      "epoch:  913, loss: 0.00792019721120596\n",
      "epoch:  914, loss: 0.007915858179330826\n",
      "epoch:  915, loss: 0.00791461206972599\n",
      "epoch:  916, loss: 0.007913061417639256\n",
      "epoch:  917, loss: 0.007909247651696205\n",
      "epoch:  918, loss: 0.007908585481345654\n",
      "epoch:  919, loss: 0.007904520258307457\n",
      "epoch:  920, loss: 0.007902918383479118\n",
      "epoch:  921, loss: 0.007902305573225021\n",
      "epoch:  922, loss: 0.007897180505096912\n",
      "epoch:  923, loss: 0.007896359078586102\n",
      "epoch:  924, loss: 0.007892359048128128\n",
      "epoch:  925, loss: 0.007890298031270504\n",
      "epoch:  926, loss: 0.00788966752588749\n",
      "epoch:  927, loss: 0.007884704507887363\n",
      "epoch:  928, loss: 0.007883692160248756\n",
      "epoch:  929, loss: 0.007880999706685543\n",
      "epoch:  930, loss: 0.007877642288804054\n",
      "epoch:  931, loss: 0.007876915857195854\n",
      "epoch:  932, loss: 0.007872073911130428\n",
      "epoch:  933, loss: 0.007870743982493877\n",
      "epoch:  934, loss: 0.007870347239077091\n",
      "epoch:  935, loss: 0.00786472950130701\n",
      "epoch:  936, loss: 0.007863907143473625\n",
      "epoch:  937, loss: 0.007859786041080952\n",
      "epoch:  938, loss: 0.007857627235352993\n",
      "epoch:  939, loss: 0.007856948301196098\n",
      "epoch:  940, loss: 0.007851618342101574\n",
      "epoch:  941, loss: 0.00785036664456129\n",
      "epoch:  942, loss: 0.007849307730793953\n",
      "epoch:  943, loss: 0.007843933068215847\n",
      "epoch:  944, loss: 0.007843081839382648\n",
      "epoch:  945, loss: 0.007838004268705845\n",
      "epoch:  946, loss: 0.007836139760911465\n",
      "epoch:  947, loss: 0.007835444994270802\n",
      "epoch:  948, loss: 0.007829923182725906\n",
      "epoch:  949, loss: 0.007828542031347752\n",
      "epoch:  950, loss: 0.007826360873878002\n",
      "epoch:  951, loss: 0.007821809500455856\n",
      "epoch:  952, loss: 0.007820994593203068\n",
      "epoch:  953, loss: 0.007815960794687271\n",
      "epoch:  954, loss: 0.007814208045601845\n",
      "epoch:  955, loss: 0.007813523523509502\n",
      "epoch:  956, loss: 0.0078077311627566814\n",
      "epoch:  957, loss: 0.007806680630892515\n",
      "epoch:  958, loss: 0.007802779786288738\n",
      "epoch:  959, loss: 0.007799860090017319\n",
      "epoch:  960, loss: 0.007799109444022179\n",
      "epoch:  961, loss: 0.00779373524710536\n",
      "epoch:  962, loss: 0.007792224176228046\n",
      "epoch:  963, loss: 0.00779219763353467\n",
      "epoch:  964, loss: 0.007785587105900049\n",
      "epoch:  965, loss: 0.007784618064761162\n",
      "epoch:  966, loss: 0.00778014725074172\n",
      "epoch:  967, loss: 0.007777659688144922\n",
      "epoch:  968, loss: 0.007776920683681965\n",
      "epoch:  969, loss: 0.007771186996251345\n",
      "epoch:  970, loss: 0.0077698491513729095\n",
      "epoch:  971, loss: 0.007768752984702587\n",
      "epoch:  972, loss: 0.007762898225337267\n",
      "epoch:  973, loss: 0.007761916611343622\n",
      "epoch:  974, loss: 0.007757212966680527\n",
      "epoch:  975, loss: 0.007754503283649683\n",
      "epoch:  976, loss: 0.007753717713057995\n",
      "epoch:  977, loss: 0.007747805677354336\n",
      "epoch:  978, loss: 0.007746217772364616\n",
      "epoch:  979, loss: 0.0077460408210754395\n",
      "epoch:  980, loss: 0.00773888872936368\n",
      "epoch:  981, loss: 0.007737811654806137\n",
      "epoch:  982, loss: 0.007733992300927639\n",
      "epoch:  983, loss: 0.007730099372565746\n",
      "epoch:  984, loss: 0.007729196920990944\n",
      "epoch:  985, loss: 0.00772311445325613\n",
      "epoch:  986, loss: 0.00772114796563983\n",
      "epoch:  987, loss: 0.007720321416854858\n",
      "epoch:  988, loss: 0.00771373463794589\n",
      "epoch:  989, loss: 0.00771215558052063\n",
      "epoch:  990, loss: 0.007711328100413084\n",
      "epoch:  991, loss: 0.007704076822847128\n",
      "epoch:  992, loss: 0.007702915463596582\n",
      "epoch:  993, loss: 0.007697882130742073\n",
      "epoch:  994, loss: 0.007694465108215809\n",
      "epoch:  995, loss: 0.0076935128308832645\n",
      "epoch:  996, loss: 0.0076867262832820415\n",
      "epoch:  997, loss: 0.007684956770390272\n",
      "epoch:  998, loss: 0.0076835076324641705\n",
      "epoch:  999, loss: 0.007676627486944199\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size=X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "opt = torch_numopt.GradientDescentLS(model=model, lr=1, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"armijo\")\n",
    "# opt = torch_numopt.GradientDescentLS(model=model, lr=1, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"wolfe\")\n",
    "# opt = torch_numopt.GradientDescentLS(model=model, lr=1, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"strong-wolfe\")\n",
    "# opt = torch_numopt.GradientDescentLS(model=model, lr=1, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"goldstein\")\n",
    "\n",
    "all_loss = {}\n",
    "for epoch in range(1000):\n",
    "    print(\"epoch: \", epoch, end=\"\")\n",
    "    all_loss[epoch + 1] = 0\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y, loss_fn)\n",
    "\n",
    "        all_loss[epoch + 1] += loss\n",
    "    all_loss[epoch + 1] /= len(data_loader)\n",
    "    print(\", loss: {}\".format(all_loss[epoch + 1].detach().numpy().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics: R2 = 0.7071237841988433\n",
      "Test metrics:  R2 = 0.6868923986851438\n"
     ]
    }
   ],
   "source": [
    "pred_train = model.forward(X_train).detach()\n",
    "pred_test = model.forward(X_test).detach()\n",
    "print(f\"Train metrics: R2 = {r2_score(pred_train, y_train)}\")\n",
    "print(f\"Test metrics:  R2 = {r2_score(pred_test, y_test)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
