{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch_numopt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import *\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.f1 = nn.Linear(input_size, 10, device=device)\n",
    "        self.f2 = nn.Linear(10, 20, device=device)\n",
    "        self.f3 = nn.Linear(20, 20, device=device)\n",
    "        self.f4 = nn.Linear(20, 10, device=device)\n",
    "        self.f5 = nn.Linear(10, 1, device=device)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        # self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.f1(x))\n",
    "        x = self.activation(self.f2(x))\n",
    "        x = self.activation(self.f3(x))\n",
    "        x = self.activation(self.f4(x))\n",
    "        x = self.f5(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([800, 10]) torch.Size([800, 1])\n",
      "torch.Size([200, 10]) torch.Size([200, 1])\n"
     ]
    }
   ],
   "source": [
    "# X, y = load_diabetes(return_X_y=True, scaled=False)\n",
    "# X, y = make_regression(n_samples=1000, n_features=100)\n",
    "X, y = make_friedman1(n_samples=1000, noise=1e-2)\n",
    "\n",
    "X_scaler = MinMaxScaler()\n",
    "X = X_scaler.fit_transform(X)\n",
    "X = torch.Tensor(X).to(device)\n",
    "\n",
    "y_scaler = MinMaxScaler()\n",
    "y = y_scaler.fit_transform(y.reshape((-1, 1)))\n",
    "y = torch.Tensor(y).to(device)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "torch_data = TensorDataset(X_train, y_train)\n",
    "data_loader = DataLoader(torch_data, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.2528630793094635\n",
      "epoch:  1, loss: 0.15897618234157562\n",
      "epoch:  2, loss: 0.10568813979625702\n",
      "epoch:  3, loss: 0.07483512908220291\n",
      "epoch:  4, loss: 0.056644998490810394\n",
      "epoch:  5, loss: 0.045877814292907715\n",
      "epoch:  6, loss: 0.03950885683298111\n",
      "epoch:  7, loss: 0.03574659675359726\n",
      "epoch:  8, loss: 0.03352739289402962\n",
      "epoch:  9, loss: 0.032219771295785904\n",
      "epoch:  10, loss: 0.03145007789134979\n",
      "epoch:  11, loss: 0.0309968963265419\n",
      "epoch:  12, loss: 0.030729608610272408\n",
      "epoch:  13, loss: 0.030571434646844864\n",
      "epoch:  14, loss: 0.030477138236165047\n",
      "epoch:  15, loss: 0.03042026236653328\n",
      "epoch:  16, loss: 0.03038528747856617\n",
      "epoch:  17, loss: 0.03038029745221138\n",
      "epoch:  18, loss: 0.030342860147356987\n",
      "epoch:  19, loss: 0.03034265711903572\n",
      "epoch:  20, loss: 0.030302628874778748\n",
      "epoch:  21, loss: 0.030277609825134277\n",
      "epoch:  22, loss: 0.03026353381574154\n",
      "epoch:  23, loss: 0.030236603692173958\n",
      "epoch:  24, loss: 0.030224870890378952\n",
      "epoch:  25, loss: 0.030195973813533783\n",
      "epoch:  26, loss: 0.030186668038368225\n",
      "epoch:  27, loss: 0.030155446380376816\n",
      "epoch:  28, loss: 0.030149852856993675\n",
      "epoch:  29, loss: 0.030116049572825432\n",
      "epoch:  30, loss: 0.03011324256658554\n",
      "epoch:  31, loss: 0.030076351016759872\n",
      "epoch:  32, loss: 0.03005318157374859\n",
      "epoch:  33, loss: 0.03003721870481968\n",
      "epoch:  34, loss: 0.030012035742402077\n",
      "epoch:  35, loss: 0.029998326674103737\n",
      "epoch:  36, loss: 0.029970591887831688\n",
      "epoch:  37, loss: 0.029958749189972878\n",
      "epoch:  38, loss: 0.029928235337138176\n",
      "epoch:  39, loss: 0.029918381944298744\n",
      "epoch:  40, loss: 0.029884466901421547\n",
      "epoch:  41, loss: 0.02987830713391304\n",
      "epoch:  42, loss: 0.029840536415576935\n",
      "epoch:  43, loss: 0.029839370399713516\n",
      "epoch:  44, loss: 0.02979685366153717\n",
      "epoch:  45, loss: 0.02977037988603115\n",
      "epoch:  46, loss: 0.02975296415388584\n",
      "epoch:  47, loss: 0.02972330152988434\n",
      "epoch:  48, loss: 0.02970900759100914\n",
      "epoch:  49, loss: 0.0296754352748394\n",
      "epoch:  50, loss: 0.029665881767868996\n",
      "epoch:  51, loss: 0.029627593234181404\n",
      "epoch:  52, loss: 0.029623165726661682\n",
      "epoch:  53, loss: 0.029579034075140953\n",
      "epoch:  54, loss: 0.029551522806286812\n",
      "epoch:  55, loss: 0.029530685395002365\n",
      "epoch:  56, loss: 0.029499053955078125\n",
      "epoch:  57, loss: 0.029481783509254456\n",
      "epoch:  58, loss: 0.029444975778460503\n",
      "epoch:  59, loss: 0.029433149844408035\n",
      "epoch:  60, loss: 0.029389848932623863\n",
      "epoch:  61, loss: 0.029385235160589218\n",
      "epoch:  62, loss: 0.029333550482988358\n",
      "epoch:  63, loss: 0.029301414266228676\n",
      "epoch:  64, loss: 0.029276467859745026\n",
      "epoch:  65, loss: 0.02923821285367012\n",
      "epoch:  66, loss: 0.029219448566436768\n",
      "epoch:  67, loss: 0.02917340025305748\n",
      "epoch:  68, loss: 0.02916422113776207\n",
      "epoch:  69, loss: 0.029108010232448578\n",
      "epoch:  70, loss: 0.029073024168610573\n",
      "epoch:  71, loss: 0.029042387381196022\n",
      "epoch:  72, loss: 0.02899940498173237\n",
      "epoch:  73, loss: 0.028978364542126656\n",
      "epoch:  74, loss: 0.028924815356731415\n",
      "epoch:  75, loss: 0.028916949406266212\n",
      "epoch:  76, loss: 0.028848862275481224\n",
      "epoch:  77, loss: 0.02880694717168808\n",
      "epoch:  78, loss: 0.028773415833711624\n",
      "epoch:  79, loss: 0.02872025966644287\n",
      "epoch:  80, loss: 0.028700461611151695\n",
      "epoch:  81, loss: 0.028631335124373436\n",
      "epoch:  82, loss: 0.02858849987387657\n",
      "epoch:  83, loss: 0.028543589636683464\n",
      "epoch:  84, loss: 0.02848810888826847\n",
      "epoch:  85, loss: 0.028460698202252388\n",
      "epoch:  86, loss: 0.028386330232024193\n",
      "epoch:  87, loss: 0.0283402968198061\n",
      "epoch:  88, loss: 0.028286565095186234\n",
      "epoch:  89, loss: 0.028225217014551163\n",
      "epoch:  90, loss: 0.028193721547722816\n",
      "epoch:  91, loss: 0.028108688071370125\n",
      "epoch:  92, loss: 0.028056416660547256\n",
      "epoch:  93, loss: 0.027995767071843147\n",
      "epoch:  94, loss: 0.027923960238695145\n",
      "epoch:  95, loss: 0.027894800528883934\n",
      "epoch:  96, loss: 0.027791500091552734\n",
      "epoch:  97, loss: 0.027728930115699768\n",
      "epoch:  98, loss: 0.027668196707963943\n",
      "epoch:  99, loss: 0.02757819928228855\n",
      "epoch:  100, loss: 0.027567174285650253\n",
      "epoch:  101, loss: 0.027430953457951546\n",
      "epoch:  102, loss: 0.02735031582415104\n",
      "epoch:  103, loss: 0.027302520349621773\n",
      "epoch:  104, loss: 0.027180053293704987\n",
      "epoch:  105, loss: 0.027105942368507385\n",
      "epoch:  106, loss: 0.027025222778320312\n",
      "epoch:  107, loss: 0.026910867542028427\n",
      "epoch:  108, loss: 0.026840901002287865\n",
      "epoch:  109, loss: 0.0267278291285038\n",
      "epoch:  110, loss: 0.02662021666765213\n",
      "epoch:  111, loss: 0.026589587330818176\n",
      "epoch:  112, loss: 0.026409879326820374\n",
      "epoch:  113, loss: 0.026305818930268288\n",
      "epoch:  114, loss: 0.0262395478785038\n",
      "epoch:  115, loss: 0.02606387808918953\n",
      "epoch:  116, loss: 0.025961074978113174\n",
      "epoch:  117, loss: 0.02586311660706997\n",
      "epoch:  118, loss: 0.02568669244647026\n",
      "epoch:  119, loss: 0.025583412498235703\n",
      "epoch:  120, loss: 0.02546468749642372\n",
      "epoch:  121, loss: 0.025281395763158798\n",
      "epoch:  122, loss: 0.02517443709075451\n",
      "epoch:  123, loss: 0.025038892403244972\n",
      "epoch:  124, loss: 0.024842746555805206\n",
      "epoch:  125, loss: 0.02473023347556591\n",
      "epoch:  126, loss: 0.024575361981987953\n",
      "epoch:  127, loss: 0.024365734308958054\n",
      "epoch:  128, loss: 0.024245642125606537\n",
      "epoch:  129, loss: 0.02409277856349945\n",
      "epoch:  130, loss: 0.023855270817875862\n",
      "epoch:  131, loss: 0.02372383512556553\n",
      "epoch:  132, loss: 0.023567616939544678\n",
      "epoch:  133, loss: 0.02329907938838005\n",
      "epoch:  134, loss: 0.023154057562351227\n",
      "epoch:  135, loss: 0.02301151677966118\n",
      "epoch:  136, loss: 0.022702133283019066\n",
      "epoch:  137, loss: 0.022542068734765053\n",
      "epoch:  138, loss: 0.022433340549468994\n",
      "epoch:  139, loss: 0.02206822670996189\n",
      "epoch:  140, loss: 0.021885745227336884\n",
      "epoch:  141, loss: 0.021858422085642815\n",
      "epoch:  142, loss: 0.021408360451459885\n",
      "epoch:  143, loss: 0.02119714766740799\n",
      "epoch:  144, loss: 0.021079260855913162\n",
      "epoch:  145, loss: 0.02072182111442089\n",
      "epoch:  146, loss: 0.02047421969473362\n",
      "epoch:  147, loss: 0.02034410461783409\n",
      "epoch:  148, loss: 0.02004307694733143\n",
      "epoch:  149, loss: 0.019738420844078064\n",
      "epoch:  150, loss: 0.019591093063354492\n",
      "epoch:  151, loss: 0.01935497671365738\n",
      "epoch:  152, loss: 0.018980037420988083\n",
      "epoch:  153, loss: 0.018817085772752762\n",
      "epoch:  154, loss: 0.018676109611988068\n",
      "epoch:  155, loss: 0.01823331043124199\n",
      "epoch:  156, loss: 0.018057934939861298\n",
      "epoch:  157, loss: 0.018017318099737167\n",
      "epoch:  158, loss: 0.017490772530436516\n",
      "epoch:  159, loss: 0.017307709902524948\n",
      "epoch:  160, loss: 0.01721513830125332\n",
      "epoch:  161, loss: 0.01679910719394684\n",
      "epoch:  162, loss: 0.016593346372246742\n",
      "epoch:  163, loss: 0.016498617827892303\n",
      "epoch:  164, loss: 0.01610664092004299\n",
      "epoch:  165, loss: 0.015902908518910408\n",
      "epoch:  166, loss: 0.015815705060958862\n",
      "epoch:  167, loss: 0.015475989319384098\n",
      "epoch:  168, loss: 0.015267003327608109\n",
      "epoch:  169, loss: 0.015184801071882248\n",
      "epoch:  170, loss: 0.014844302088022232\n",
      "epoch:  171, loss: 0.014665271155536175\n",
      "epoch:  172, loss: 0.014593146741390228\n",
      "epoch:  173, loss: 0.014272404834628105\n",
      "epoch:  174, loss: 0.014109492301940918\n",
      "epoch:  175, loss: 0.01404784806072712\n",
      "epoch:  176, loss: 0.013711635954678059\n",
      "epoch:  177, loss: 0.013596649281680584\n",
      "epoch:  178, loss: 0.013573221862316132\n",
      "epoch:  179, loss: 0.013206672854721546\n",
      "epoch:  180, loss: 0.013126772828400135\n",
      "epoch:  181, loss: 0.012962989509105682\n",
      "epoch:  182, loss: 0.012747743166983128\n",
      "epoch:  183, loss: 0.012687836773693562\n",
      "epoch:  184, loss: 0.012459900230169296\n",
      "epoch:  185, loss: 0.012326929718255997\n",
      "epoch:  186, loss: 0.01228351704776287\n",
      "epoch:  187, loss: 0.01201771292835474\n",
      "epoch:  188, loss: 0.011943294666707516\n",
      "epoch:  189, loss: 0.011821197345852852\n",
      "epoch:  190, loss: 0.011633485555648804\n",
      "epoch:  191, loss: 0.011591428890824318\n",
      "epoch:  192, loss: 0.011367087252438068\n",
      "epoch:  193, loss: 0.011293092742562294\n",
      "epoch:  194, loss: 0.011196109466254711\n",
      "epoch:  195, loss: 0.011024050414562225\n",
      "epoch:  196, loss: 0.010989315807819366\n",
      "epoch:  197, loss: 0.010791981592774391\n",
      "epoch:  198, loss: 0.010735851712524891\n",
      "epoch:  199, loss: 0.010615624487400055\n",
      "epoch:  200, loss: 0.010501213371753693\n",
      "epoch:  201, loss: 0.01047543901950121\n",
      "epoch:  202, loss: 0.010288284160196781\n",
      "epoch:  203, loss: 0.010255604982376099\n",
      "epoch:  204, loss: 0.010100736282765865\n",
      "epoch:  205, loss: 0.010053429752588272\n",
      "epoch:  206, loss: 0.009945300407707691\n",
      "epoch:  207, loss: 0.009867544285953045\n",
      "epoch:  208, loss: 0.009830431081354618\n",
      "epoch:  209, loss: 0.009698253124952316\n",
      "epoch:  210, loss: 0.00967978686094284\n",
      "epoch:  211, loss: 0.009541868232190609\n",
      "epoch:  212, loss: 0.009522836655378342\n",
      "epoch:  213, loss: 0.009400257840752602\n",
      "epoch:  214, loss: 0.009378543123602867\n",
      "epoch:  215, loss: 0.009271570481359959\n",
      "epoch:  216, loss: 0.009245523251593113\n",
      "epoch:  217, loss: 0.009151244536042213\n",
      "epoch:  218, loss: 0.009123826399445534\n",
      "epoch:  219, loss: 0.009044995531439781\n",
      "epoch:  220, loss: 0.009013790637254715\n",
      "epoch:  221, loss: 0.008947551250457764\n",
      "epoch:  222, loss: 0.008912387304008007\n",
      "epoch:  223, loss: 0.008852648548781872\n",
      "epoch:  224, loss: 0.008820406161248684\n",
      "epoch:  225, loss: 0.008764133788645267\n",
      "epoch:  226, loss: 0.008738318458199501\n",
      "epoch:  227, loss: 0.00868254341185093\n",
      "epoch:  228, loss: 0.008662543259561062\n",
      "epoch:  229, loss: 0.008608993142843246\n",
      "epoch:  230, loss: 0.008592721074819565\n",
      "epoch:  231, loss: 0.008538790978491306\n",
      "epoch:  232, loss: 0.008528686128556728\n",
      "epoch:  233, loss: 0.008477190509438515\n",
      "epoch:  234, loss: 0.00846982654184103\n",
      "epoch:  235, loss: 0.008420297876000404\n",
      "epoch:  236, loss: 0.008414031006395817\n",
      "epoch:  237, loss: 0.008364751935005188\n",
      "epoch:  238, loss: 0.008350957185029984\n",
      "epoch:  239, loss: 0.008312255144119263\n",
      "epoch:  240, loss: 0.008276842534542084\n",
      "epoch:  241, loss: 0.008264120668172836\n",
      "epoch:  242, loss: 0.008226540870964527\n",
      "epoch:  243, loss: 0.00822161789983511\n",
      "epoch:  244, loss: 0.00818608794361353\n",
      "epoch:  245, loss: 0.008170250803232193\n",
      "epoch:  246, loss: 0.008149866946041584\n",
      "epoch:  247, loss: 0.008121260441839695\n",
      "epoch:  248, loss: 0.008117293938994408\n",
      "epoch:  249, loss: 0.008089354261755943\n",
      "epoch:  250, loss: 0.008074039593338966\n",
      "epoch:  251, loss: 0.008061593398451805\n",
      "epoch:  252, loss: 0.008039756678044796\n",
      "epoch:  253, loss: 0.008037225343286991\n",
      "epoch:  254, loss: 0.008016422390937805\n",
      "epoch:  255, loss: 0.008004956878721714\n",
      "epoch:  256, loss: 0.007994815707206726\n",
      "epoch:  257, loss: 0.007977994158864021\n",
      "epoch:  258, loss: 0.007970768958330154\n",
      "epoch:  259, loss: 0.007961082272231579\n",
      "epoch:  260, loss: 0.007947107776999474\n",
      "epoch:  261, loss: 0.007945576682686806\n",
      "epoch:  262, loss: 0.007932346314191818\n",
      "epoch:  263, loss: 0.0079202177003026\n",
      "epoch:  264, loss: 0.007915149442851543\n",
      "epoch:  265, loss: 0.007907760329544544\n",
      "epoch:  266, loss: 0.007897227071225643\n",
      "epoch:  267, loss: 0.007888244464993477\n",
      "epoch:  268, loss: 0.0078856460750103\n",
      "epoch:  269, loss: 0.007874937728047371\n",
      "epoch:  270, loss: 0.007865161634981632\n",
      "epoch:  271, loss: 0.007863078266382217\n",
      "epoch:  272, loss: 0.007852412760257721\n",
      "epoch:  273, loss: 0.007843703031539917\n",
      "epoch:  274, loss: 0.007840623147785664\n",
      "epoch:  275, loss: 0.00782986544072628\n",
      "epoch:  276, loss: 0.0078202523291111\n",
      "epoch:  277, loss: 0.007818259298801422\n",
      "epoch:  278, loss: 0.007808491121977568\n",
      "epoch:  279, loss: 0.00780321704223752\n",
      "epoch:  280, loss: 0.007798188831657171\n",
      "epoch:  281, loss: 0.007789707276970148\n",
      "epoch:  282, loss: 0.007788406684994698\n",
      "epoch:  283, loss: 0.007780383341014385\n",
      "epoch:  284, loss: 0.007779187057167292\n",
      "epoch:  285, loss: 0.00777084194123745\n",
      "epoch:  286, loss: 0.00776312779635191\n",
      "epoch:  287, loss: 0.007761533372104168\n",
      "epoch:  288, loss: 0.0077539910562336445\n",
      "epoch:  289, loss: 0.007752633653581142\n",
      "epoch:  290, loss: 0.007745230104774237\n",
      "epoch:  291, loss: 0.007744130678474903\n",
      "epoch:  292, loss: 0.007737451232969761\n",
      "epoch:  293, loss: 0.007736308500170708\n",
      "epoch:  294, loss: 0.007729625795036554\n",
      "epoch:  295, loss: 0.007728646509349346\n",
      "epoch:  296, loss: 0.007721788715571165\n",
      "epoch:  297, loss: 0.0077186282724142075\n",
      "epoch:  298, loss: 0.007714392151683569\n",
      "epoch:  299, loss: 0.007708665914833546\n",
      "epoch:  300, loss: 0.00770709989592433\n",
      "epoch:  301, loss: 0.00770169124007225\n",
      "epoch:  302, loss: 0.007700047921389341\n",
      "epoch:  303, loss: 0.007693682797253132\n",
      "epoch:  304, loss: 0.0076929545029997826\n",
      "epoch:  305, loss: 0.007686430122703314\n",
      "epoch:  306, loss: 0.007684222888201475\n",
      "epoch:  307, loss: 0.007679408416152\n",
      "epoch:  308, loss: 0.00767379067838192\n",
      "epoch:  309, loss: 0.007672573439776897\n",
      "epoch:  310, loss: 0.007667060475796461\n",
      "epoch:  311, loss: 0.007665842771530151\n",
      "epoch:  312, loss: 0.007659956347197294\n",
      "epoch:  313, loss: 0.0076590897515416145\n",
      "epoch:  314, loss: 0.00765322195366025\n",
      "epoch:  315, loss: 0.007652514614164829\n",
      "epoch:  316, loss: 0.0076466822065413\n",
      "epoch:  317, loss: 0.007646047044545412\n",
      "epoch:  318, loss: 0.0076403808780014515\n",
      "epoch:  319, loss: 0.00763974292203784\n",
      "epoch:  320, loss: 0.007634159177541733\n",
      "epoch:  321, loss: 0.007633472792804241\n",
      "epoch:  322, loss: 0.0076277730986475945\n",
      "epoch:  323, loss: 0.0076224543154239655\n",
      "epoch:  324, loss: 0.007621250115334988\n",
      "epoch:  325, loss: 0.007615708280354738\n",
      "epoch:  326, loss: 0.007614722941070795\n",
      "epoch:  327, loss: 0.007608907762914896\n",
      "epoch:  328, loss: 0.0076082325540483\n",
      "epoch:  329, loss: 0.0076020448468625546\n",
      "epoch:  330, loss: 0.0075986008159816265\n",
      "epoch:  331, loss: 0.007594237104058266\n",
      "epoch:  332, loss: 0.0075887273997068405\n",
      "epoch:  333, loss: 0.007586830295622349\n",
      "epoch:  334, loss: 0.007580987177789211\n",
      "epoch:  335, loss: 0.007579382508993149\n",
      "epoch:  336, loss: 0.007572675123810768\n",
      "epoch:  337, loss: 0.0075716315768659115\n",
      "epoch:  338, loss: 0.007565015461295843\n",
      "epoch:  339, loss: 0.007564001716673374\n",
      "epoch:  340, loss: 0.0075570885092020035\n",
      "epoch:  341, loss: 0.007556270342320204\n",
      "epoch:  342, loss: 0.00754935760051012\n",
      "epoch:  343, loss: 0.00754633080214262\n",
      "epoch:  344, loss: 0.007541456259787083\n",
      "epoch:  345, loss: 0.007539358921349049\n",
      "epoch:  346, loss: 0.007533838041126728\n",
      "epoch:  347, loss: 0.007530517410486937\n",
      "epoch:  348, loss: 0.007526461035013199\n",
      "epoch:  349, loss: 0.00752251734957099\n",
      "epoch:  350, loss: 0.007518974132835865\n",
      "epoch:  351, loss: 0.007515564560890198\n",
      "epoch:  352, loss: 0.00751159293577075\n",
      "epoch:  353, loss: 0.007507277652621269\n",
      "epoch:  354, loss: 0.007504111155867577\n",
      "epoch:  355, loss: 0.007499940227717161\n",
      "epoch:  356, loss: 0.007496635429561138\n",
      "epoch:  357, loss: 0.0074918316677212715\n",
      "epoch:  358, loss: 0.007489223964512348\n",
      "epoch:  359, loss: 0.0074842567555606365\n",
      "epoch:  360, loss: 0.007481572683900595\n",
      "epoch:  361, loss: 0.0074769360944628716\n",
      "epoch:  362, loss: 0.007473973091691732\n",
      "epoch:  363, loss: 0.007467756979167461\n",
      "epoch:  364, loss: 0.007466000970453024\n",
      "epoch:  365, loss: 0.007459757383912802\n",
      "epoch:  366, loss: 0.007458091247826815\n",
      "epoch:  367, loss: 0.007452476769685745\n",
      "epoch:  368, loss: 0.007450144737958908\n",
      "epoch:  369, loss: 0.007443986367434263\n",
      "epoch:  370, loss: 0.00744227459654212\n",
      "epoch:  371, loss: 0.007436465471982956\n",
      "epoch:  372, loss: 0.007434488739818335\n",
      "epoch:  373, loss: 0.007430121302604675\n",
      "epoch:  374, loss: 0.0074266064912080765\n",
      "epoch:  375, loss: 0.007422979921102524\n",
      "epoch:  376, loss: 0.00741874473169446\n",
      "epoch:  377, loss: 0.007415044121444225\n",
      "epoch:  378, loss: 0.007410754449665546\n",
      "epoch:  379, loss: 0.007408086210489273\n",
      "epoch:  380, loss: 0.007402966730296612\n",
      "epoch:  381, loss: 0.007401019800454378\n",
      "epoch:  382, loss: 0.00739526329562068\n",
      "epoch:  383, loss: 0.007391757797449827\n",
      "epoch:  384, loss: 0.007387529127299786\n",
      "epoch:  385, loss: 0.007385355420410633\n",
      "epoch:  386, loss: 0.0073799071833491325\n",
      "epoch:  387, loss: 0.007378248497843742\n",
      "epoch:  388, loss: 0.007372242398560047\n",
      "epoch:  389, loss: 0.007370867766439915\n",
      "epoch:  390, loss: 0.007364579010754824\n",
      "epoch:  391, loss: 0.0073638055473566055\n",
      "epoch:  392, loss: 0.007357522379606962\n",
      "epoch:  393, loss: 0.0073562790639698505\n",
      "epoch:  394, loss: 0.007350596133619547\n",
      "epoch:  395, loss: 0.007348734885454178\n",
      "epoch:  396, loss: 0.0073435548692941666\n",
      "epoch:  397, loss: 0.007341212127357721\n",
      "epoch:  398, loss: 0.007337065879255533\n",
      "epoch:  399, loss: 0.007333546876907349\n",
      "epoch:  400, loss: 0.007329764775931835\n",
      "epoch:  401, loss: 0.007325883023440838\n",
      "epoch:  402, loss: 0.007325133308768272\n",
      "epoch:  403, loss: 0.0073185814544558525\n",
      "epoch:  404, loss: 0.0073175085708498955\n",
      "epoch:  405, loss: 0.0073119597509503365\n",
      "epoch:  406, loss: 0.007309896405786276\n",
      "epoch:  407, loss: 0.007304253056645393\n",
      "epoch:  408, loss: 0.007302211597561836\n",
      "epoch:  409, loss: 0.0072983624413609505\n",
      "epoch:  410, loss: 0.007294418755918741\n",
      "epoch:  411, loss: 0.00729239359498024\n",
      "epoch:  412, loss: 0.0072866021655499935\n",
      "epoch:  413, loss: 0.00728580541908741\n",
      "epoch:  414, loss: 0.007278946693986654\n",
      "epoch:  415, loss: 0.007277855183929205\n",
      "epoch:  416, loss: 0.007271966896951199\n",
      "epoch:  417, loss: 0.007269717287272215\n",
      "epoch:  418, loss: 0.0072648595087230206\n",
      "epoch:  419, loss: 0.007261533755809069\n",
      "epoch:  420, loss: 0.007259659003466368\n",
      "epoch:  421, loss: 0.00725338002666831\n",
      "epoch:  422, loss: 0.0072525436989963055\n",
      "epoch:  423, loss: 0.007245300337672234\n",
      "epoch:  424, loss: 0.00724434619769454\n",
      "epoch:  425, loss: 0.007237558253109455\n",
      "epoch:  426, loss: 0.007236172445118427\n",
      "epoch:  427, loss: 0.007231452502310276\n",
      "epoch:  428, loss: 0.007228034548461437\n",
      "epoch:  429, loss: 0.007226727437227964\n",
      "epoch:  430, loss: 0.007219604216516018\n",
      "epoch:  431, loss: 0.007218711543828249\n",
      "epoch:  432, loss: 0.007211463525891304\n",
      "epoch:  433, loss: 0.007209980394691229\n",
      "epoch:  434, loss: 0.007203520741313696\n",
      "epoch:  435, loss: 0.0072013093158602715\n",
      "epoch:  436, loss: 0.007196300197392702\n",
      "epoch:  437, loss: 0.0071924710646271706\n",
      "epoch:  438, loss: 0.007190108764916658\n",
      "epoch:  439, loss: 0.007183265872299671\n",
      "epoch:  440, loss: 0.007182278670370579\n",
      "epoch:  441, loss: 0.007173795253038406\n",
      "epoch:  442, loss: 0.007172651123255491\n",
      "epoch:  443, loss: 0.007165058981627226\n",
      "epoch:  444, loss: 0.0071629988960921764\n",
      "epoch:  445, loss: 0.007156802341341972\n",
      "epoch:  446, loss: 0.0071531860157847404\n",
      "epoch:  447, loss: 0.007149762939661741\n",
      "epoch:  448, loss: 0.007143569644540548\n",
      "epoch:  449, loss: 0.007142605260014534\n",
      "epoch:  450, loss: 0.007134194485843182\n",
      "epoch:  451, loss: 0.007133173290640116\n",
      "epoch:  452, loss: 0.007125184405595064\n",
      "epoch:  453, loss: 0.007123755291104317\n",
      "epoch:  454, loss: 0.007117369212210178\n",
      "epoch:  455, loss: 0.007114587351679802\n",
      "epoch:  456, loss: 0.007112137041985989\n",
      "epoch:  457, loss: 0.007105428725481033\n",
      "epoch:  458, loss: 0.007104477379471064\n",
      "epoch:  459, loss: 0.007096290588378906\n",
      "epoch:  460, loss: 0.0070952982641756535\n",
      "epoch:  461, loss: 0.007088177837431431\n",
      "epoch:  462, loss: 0.007086205296218395\n",
      "epoch:  463, loss: 0.007079713512212038\n",
      "epoch:  464, loss: 0.0070771723985672\n",
      "epoch:  465, loss: 0.0070760780945420265\n",
      "epoch:  466, loss: 0.007068103179335594\n",
      "epoch:  467, loss: 0.007067122496664524\n",
      "epoch:  468, loss: 0.007059225346893072\n",
      "epoch:  469, loss: 0.007058105897158384\n",
      "epoch:  470, loss: 0.007051465567201376\n",
      "epoch:  471, loss: 0.007049372419714928\n",
      "epoch:  472, loss: 0.007045590318739414\n",
      "epoch:  473, loss: 0.007040060590952635\n",
      "epoch:  474, loss: 0.00703907897695899\n",
      "epoch:  475, loss: 0.007030576001852751\n",
      "epoch:  476, loss: 0.007029522676020861\n",
      "epoch:  477, loss: 0.007021474651992321\n",
      "epoch:  478, loss: 0.007020034361630678\n",
      "epoch:  479, loss: 0.0070133209228515625\n",
      "epoch:  480, loss: 0.007010756526142359\n",
      "epoch:  481, loss: 0.007010402157902718\n",
      "epoch:  482, loss: 0.007001656107604504\n",
      "epoch:  483, loss: 0.0070006148889660835\n",
      "epoch:  484, loss: 0.00699251051992178\n",
      "epoch:  485, loss: 0.006991126574575901\n",
      "epoch:  486, loss: 0.006984284147620201\n",
      "epoch:  487, loss: 0.006981638725847006\n",
      "epoch:  488, loss: 0.006978173740208149\n",
      "epoch:  489, loss: 0.006972321309149265\n",
      "epoch:  490, loss: 0.006971315015107393\n",
      "epoch:  491, loss: 0.006963048130273819\n",
      "epoch:  492, loss: 0.006961934734135866\n",
      "epoch:  493, loss: 0.0069539486430585384\n",
      "epoch:  494, loss: 0.0069525958970189095\n",
      "epoch:  495, loss: 0.006945079192519188\n",
      "epoch:  496, loss: 0.006943398620933294\n",
      "epoch:  497, loss: 0.006937673781067133\n",
      "epoch:  498, loss: 0.006934227887541056\n",
      "epoch:  499, loss: 0.006932470016181469\n",
      "epoch:  500, loss: 0.00692499615252018\n",
      "epoch:  501, loss: 0.006923967506736517\n",
      "epoch:  502, loss: 0.006915244273841381\n",
      "epoch:  503, loss: 0.006914079189300537\n",
      "epoch:  504, loss: 0.006906281691044569\n",
      "epoch:  505, loss: 0.006904091686010361\n",
      "epoch:  506, loss: 0.00690298480913043\n",
      "epoch:  507, loss: 0.0068944101221859455\n",
      "epoch:  508, loss: 0.006893323268741369\n",
      "epoch:  509, loss: 0.006886130664497614\n",
      "epoch:  510, loss: 0.0068837720900774\n",
      "epoch:  511, loss: 0.00687838951125741\n",
      "epoch:  512, loss: 0.006874325685203075\n",
      "epoch:  513, loss: 0.006873358506709337\n",
      "epoch:  514, loss: 0.006865179166197777\n",
      "epoch:  515, loss: 0.006863895803689957\n",
      "epoch:  516, loss: 0.006856999825686216\n",
      "epoch:  517, loss: 0.006854457780718803\n",
      "epoch:  518, loss: 0.006851116195321083\n",
      "epoch:  519, loss: 0.006845138501375914\n",
      "epoch:  520, loss: 0.006844064686447382\n",
      "epoch:  521, loss: 0.006836427375674248\n",
      "epoch:  522, loss: 0.006834793835878372\n",
      "epoch:  523, loss: 0.006830321624875069\n",
      "epoch:  524, loss: 0.006825632881373167\n",
      "epoch:  525, loss: 0.006824687588959932\n",
      "epoch:  526, loss: 0.006816932465881109\n",
      "epoch:  527, loss: 0.006815389264374971\n",
      "epoch:  528, loss: 0.006808798294514418\n",
      "epoch:  529, loss: 0.006806052755564451\n",
      "epoch:  530, loss: 0.006805100478231907\n",
      "epoch:  531, loss: 0.006797158624976873\n",
      "epoch:  532, loss: 0.006795861292630434\n",
      "epoch:  533, loss: 0.0067892735823988914\n",
      "epoch:  534, loss: 0.006786646321415901\n",
      "epoch:  535, loss: 0.006785282399505377\n",
      "epoch:  536, loss: 0.006777633912861347\n",
      "epoch:  537, loss: 0.006776530295610428\n",
      "epoch:  538, loss: 0.006769159808754921\n",
      "epoch:  539, loss: 0.0067673628218472\n",
      "epoch:  540, loss: 0.006765260826796293\n",
      "epoch:  541, loss: 0.006758443545550108\n",
      "epoch:  542, loss: 0.006757314782589674\n",
      "epoch:  543, loss: 0.006750470958650112\n",
      "epoch:  544, loss: 0.006748200859874487\n",
      "epoch:  545, loss: 0.0067472695372998714\n",
      "epoch:  546, loss: 0.006739391479641199\n",
      "epoch:  547, loss: 0.006738247815519571\n",
      "epoch:  548, loss: 0.006731047760695219\n",
      "epoch:  549, loss: 0.006729298736900091\n",
      "epoch:  550, loss: 0.006725696846842766\n",
      "epoch:  551, loss: 0.006720511708408594\n",
      "epoch:  552, loss: 0.0067194788716733456\n",
      "epoch:  553, loss: 0.006712250877171755\n",
      "epoch:  554, loss: 0.006710618734359741\n",
      "epoch:  555, loss: 0.006705926731228828\n",
      "epoch:  556, loss: 0.006701701786369085\n",
      "epoch:  557, loss: 0.00670077558606863\n",
      "epoch:  558, loss: 0.006693131756037474\n",
      "epoch:  559, loss: 0.0066917152144014835\n",
      "epoch:  560, loss: 0.0066870879381895065\n",
      "epoch:  561, loss: 0.006682781968265772\n",
      "epoch:  562, loss: 0.006681814324110746\n",
      "epoch:  563, loss: 0.00667407875880599\n",
      "epoch:  564, loss: 0.00667275907471776\n",
      "epoch:  565, loss: 0.006666845176368952\n",
      "epoch:  566, loss: 0.006663651671260595\n",
      "epoch:  567, loss: 0.006662700790911913\n",
      "epoch:  568, loss: 0.006654807366430759\n",
      "epoch:  569, loss: 0.006653393618762493\n",
      "epoch:  570, loss: 0.006648246198892593\n",
      "epoch:  571, loss: 0.006644282955676317\n",
      "epoch:  572, loss: 0.006643346976488829\n",
      "epoch:  573, loss: 0.006635997444391251\n",
      "epoch:  574, loss: 0.006634282413870096\n",
      "epoch:  575, loss: 0.006629853043705225\n",
      "epoch:  576, loss: 0.006625363603234291\n",
      "epoch:  577, loss: 0.006624377798289061\n",
      "epoch:  578, loss: 0.006616729311645031\n",
      "epoch:  579, loss: 0.006615329999476671\n",
      "epoch:  580, loss: 0.006610725540667772\n",
      "epoch:  581, loss: 0.006606416776776314\n",
      "epoch:  582, loss: 0.0066054267808794975\n",
      "epoch:  583, loss: 0.0065980954095721245\n",
      "epoch:  584, loss: 0.006596504244953394\n",
      "epoch:  585, loss: 0.0065926443785429\n",
      "epoch:  586, loss: 0.006587398238480091\n",
      "epoch:  587, loss: 0.0065863062627613544\n",
      "epoch:  588, loss: 0.006577959284186363\n",
      "epoch:  589, loss: 0.006576347164809704\n",
      "epoch:  590, loss: 0.0065697007812559605\n",
      "epoch:  591, loss: 0.0065659950487315655\n",
      "epoch:  592, loss: 0.00656488910317421\n",
      "epoch:  593, loss: 0.006556546315550804\n",
      "epoch:  594, loss: 0.006554365158081055\n",
      "epoch:  595, loss: 0.006550540216267109\n",
      "epoch:  596, loss: 0.0065438407473266125\n",
      "epoch:  597, loss: 0.006542597431689501\n",
      "epoch:  598, loss: 0.006535620894283056\n",
      "epoch:  599, loss: 0.006532332859933376\n",
      "epoch:  600, loss: 0.006531264167279005\n",
      "epoch:  601, loss: 0.0065237474627792835\n",
      "epoch:  602, loss: 0.0065210675820708275\n",
      "epoch:  603, loss: 0.0065200249664485455\n",
      "epoch:  604, loss: 0.006511632353067398\n",
      "epoch:  605, loss: 0.006509801372885704\n",
      "epoch:  606, loss: 0.006508806720376015\n",
      "epoch:  607, loss: 0.006501428317278624\n",
      "epoch:  608, loss: 0.006498771719634533\n",
      "epoch:  609, loss: 0.006497737020254135\n",
      "epoch:  610, loss: 0.00649156142026186\n",
      "epoch:  611, loss: 0.006487617269158363\n",
      "epoch:  612, loss: 0.006486536469310522\n",
      "epoch:  613, loss: 0.0064785294234752655\n",
      "epoch:  614, loss: 0.006476498208940029\n",
      "epoch:  615, loss: 0.00647547235712409\n",
      "epoch:  616, loss: 0.006467344705015421\n",
      "epoch:  617, loss: 0.006465303245931864\n",
      "epoch:  618, loss: 0.0064642769284546375\n",
      "epoch:  619, loss: 0.006455825641751289\n",
      "epoch:  620, loss: 0.006453932728618383\n",
      "epoch:  621, loss: 0.006453629117459059\n",
      "epoch:  622, loss: 0.006444064900279045\n",
      "epoch:  623, loss: 0.006442623678594828\n",
      "epoch:  624, loss: 0.006440793164074421\n",
      "epoch:  625, loss: 0.006432699505239725\n",
      "epoch:  626, loss: 0.006431316025555134\n",
      "epoch:  627, loss: 0.006427264306694269\n",
      "epoch:  628, loss: 0.006421155761927366\n",
      "epoch:  629, loss: 0.006419910117983818\n",
      "epoch:  630, loss: 0.0064163957722485065\n",
      "epoch:  631, loss: 0.006410068832337856\n",
      "epoch:  632, loss: 0.006408737972378731\n",
      "epoch:  633, loss: 0.006402994971722364\n",
      "epoch:  634, loss: 0.006398810539394617\n",
      "epoch:  635, loss: 0.006397636607289314\n",
      "epoch:  636, loss: 0.006391637492924929\n",
      "epoch:  637, loss: 0.006387435365468264\n",
      "epoch:  638, loss: 0.006386244669556618\n",
      "epoch:  639, loss: 0.006381863262504339\n",
      "epoch:  640, loss: 0.006376328878104687\n",
      "epoch:  641, loss: 0.006375025026500225\n",
      "epoch:  642, loss: 0.0063703167252242565\n",
      "epoch:  643, loss: 0.006365232635289431\n",
      "epoch:  644, loss: 0.006363861728459597\n",
      "epoch:  645, loss: 0.006360524799674749\n",
      "epoch:  646, loss: 0.006354484707117081\n",
      "epoch:  647, loss: 0.006353149190545082\n",
      "epoch:  648, loss: 0.006349464878439903\n",
      "epoch:  649, loss: 0.006343848071992397\n",
      "epoch:  650, loss: 0.006342551205307245\n",
      "epoch:  651, loss: 0.006336999591439962\n",
      "epoch:  652, loss: 0.006332924589514732\n",
      "epoch:  653, loss: 0.00633181631565094\n",
      "epoch:  654, loss: 0.00632669311016798\n",
      "epoch:  655, loss: 0.006322429981082678\n",
      "epoch:  656, loss: 0.006321245338767767\n",
      "epoch:  657, loss: 0.006317870691418648\n",
      "epoch:  658, loss: 0.006311893463134766\n",
      "epoch:  659, loss: 0.00631069066002965\n",
      "epoch:  660, loss: 0.006307512521743774\n",
      "epoch:  661, loss: 0.006301453337073326\n",
      "epoch:  662, loss: 0.006300121545791626\n",
      "epoch:  663, loss: 0.006295698694884777\n",
      "epoch:  664, loss: 0.006290761288255453\n",
      "epoch:  665, loss: 0.006289609242230654\n",
      "epoch:  666, loss: 0.006288368254899979\n",
      "epoch:  667, loss: 0.0062806918285787106\n",
      "epoch:  668, loss: 0.006279240362346172\n",
      "epoch:  669, loss: 0.006278672721236944\n",
      "epoch:  670, loss: 0.006270194426178932\n",
      "epoch:  671, loss: 0.006268810480833054\n",
      "epoch:  672, loss: 0.006267875898629427\n",
      "epoch:  673, loss: 0.006261669564992189\n",
      "epoch:  674, loss: 0.0062584406696259975\n",
      "epoch:  675, loss: 0.006257395725697279\n",
      "epoch:  676, loss: 0.006251560524106026\n",
      "epoch:  677, loss: 0.006248003337532282\n",
      "epoch:  678, loss: 0.006246873643249273\n",
      "epoch:  679, loss: 0.006242305506020784\n",
      "epoch:  680, loss: 0.006237394642084837\n",
      "epoch:  681, loss: 0.006236228160560131\n",
      "epoch:  682, loss: 0.00623256154358387\n",
      "epoch:  683, loss: 0.006226918660104275\n",
      "epoch:  684, loss: 0.006225587800145149\n",
      "epoch:  685, loss: 0.006224650889635086\n",
      "epoch:  686, loss: 0.006218171678483486\n",
      "epoch:  687, loss: 0.006215198431164026\n",
      "epoch:  688, loss: 0.006214085966348648\n",
      "epoch:  689, loss: 0.00620722770690918\n",
      "epoch:  690, loss: 0.00620450871065259\n",
      "epoch:  691, loss: 0.006203487049788237\n",
      "epoch:  692, loss: 0.00619687931612134\n",
      "epoch:  693, loss: 0.006193842273205519\n",
      "epoch:  694, loss: 0.006192734465003014\n",
      "epoch:  695, loss: 0.006187967024743557\n",
      "epoch:  696, loss: 0.006183180958032608\n",
      "epoch:  697, loss: 0.006181949283927679\n",
      "epoch:  698, loss: 0.00617963494732976\n",
      "epoch:  699, loss: 0.006172504276037216\n",
      "epoch:  700, loss: 0.006171060260385275\n",
      "epoch:  701, loss: 0.006170075386762619\n",
      "epoch:  702, loss: 0.006162807811051607\n",
      "epoch:  703, loss: 0.0061602224595844746\n",
      "epoch:  704, loss: 0.006159101612865925\n",
      "epoch:  705, loss: 0.006153361406177282\n",
      "epoch:  706, loss: 0.006149415858089924\n",
      "epoch:  707, loss: 0.006148259621113539\n",
      "epoch:  708, loss: 0.0061431205831468105\n",
      "epoch:  709, loss: 0.006138703320175409\n",
      "epoch:  710, loss: 0.006137477699667215\n",
      "epoch:  711, loss: 0.006136000622063875\n",
      "epoch:  712, loss: 0.0061279963701963425\n",
      "epoch:  713, loss: 0.006126509979367256\n",
      "epoch:  714, loss: 0.006125509738922119\n",
      "epoch:  715, loss: 0.006117464974522591\n",
      "epoch:  716, loss: 0.0061155869625508785\n",
      "epoch:  717, loss: 0.00611456111073494\n",
      "epoch:  718, loss: 0.006109393201768398\n",
      "epoch:  719, loss: 0.006105046719312668\n",
      "epoch:  720, loss: 0.0061037479899823666\n",
      "epoch:  721, loss: 0.006102777551859617\n",
      "epoch:  722, loss: 0.006094560492783785\n",
      "epoch:  723, loss: 0.006092874798923731\n",
      "epoch:  724, loss: 0.006091884337365627\n",
      "epoch:  725, loss: 0.006085316650569439\n",
      "epoch:  726, loss: 0.00608224468305707\n",
      "epoch:  727, loss: 0.006081102881580591\n",
      "epoch:  728, loss: 0.006076927296817303\n",
      "epoch:  729, loss: 0.006071749143302441\n",
      "epoch:  730, loss: 0.006070373579859734\n",
      "epoch:  731, loss: 0.006069428753107786\n",
      "epoch:  732, loss: 0.006061975844204426\n",
      "epoch:  733, loss: 0.006059968378394842\n",
      "epoch:  734, loss: 0.006058961618691683\n",
      "epoch:  735, loss: 0.006055137608200312\n",
      "epoch:  736, loss: 0.00604986073449254\n",
      "epoch:  737, loss: 0.006048562470823526\n",
      "epoch:  738, loss: 0.006047602277249098\n",
      "epoch:  739, loss: 0.006040293723344803\n",
      "epoch:  740, loss: 0.006038160994648933\n",
      "epoch:  741, loss: 0.006037132814526558\n",
      "epoch:  742, loss: 0.006032805889844894\n",
      "epoch:  743, loss: 0.006027941592037678\n",
      "epoch:  744, loss: 0.006026533432304859\n",
      "epoch:  745, loss: 0.006025566253811121\n",
      "epoch:  746, loss: 0.006017542909830809\n",
      "epoch:  747, loss: 0.006015919614583254\n",
      "epoch:  748, loss: 0.0060149566270411015\n",
      "epoch:  749, loss: 0.006007493939250708\n",
      "epoch:  750, loss: 0.006005372852087021\n",
      "epoch:  751, loss: 0.006004358641803265\n",
      "epoch:  752, loss: 0.005999257788062096\n",
      "epoch:  753, loss: 0.005994956009089947\n",
      "epoch:  754, loss: 0.005993701051920652\n",
      "epoch:  755, loss: 0.0059906900860369205\n",
      "epoch:  756, loss: 0.00598436314612627\n",
      "epoch:  757, loss: 0.005982937756925821\n",
      "epoch:  758, loss: 0.005981937050819397\n",
      "epoch:  759, loss: 0.00597505085170269\n",
      "epoch:  760, loss: 0.005972177255898714\n",
      "epoch:  761, loss: 0.0059710233472287655\n",
      "epoch:  762, loss: 0.005967675242573023\n",
      "epoch:  763, loss: 0.005961771123111248\n",
      "epoch:  764, loss: 0.0059601604007184505\n",
      "epoch:  765, loss: 0.005959168076515198\n",
      "epoch:  766, loss: 0.005951947066932917\n",
      "epoch:  767, loss: 0.005949307233095169\n",
      "epoch:  768, loss: 0.005948211997747421\n",
      "epoch:  769, loss: 0.0059436894953250885\n",
      "epoch:  770, loss: 0.00593870272859931\n",
      "epoch:  771, loss: 0.005937335081398487\n",
      "epoch:  772, loss: 0.005936331581324339\n",
      "epoch:  773, loss: 0.005930537357926369\n",
      "epoch:  774, loss: 0.005926747340708971\n",
      "epoch:  775, loss: 0.005925483535975218\n",
      "epoch:  776, loss: 0.005924496799707413\n",
      "epoch:  777, loss: 0.005916561931371689\n",
      "epoch:  778, loss: 0.005914655979722738\n",
      "epoch:  779, loss: 0.005913597531616688\n",
      "epoch:  780, loss: 0.005911856424063444\n",
      "epoch:  781, loss: 0.005904612131416798\n",
      "epoch:  782, loss: 0.005902920383960009\n",
      "epoch:  783, loss: 0.00590188754722476\n",
      "epoch:  784, loss: 0.005897674709558487\n",
      "epoch:  785, loss: 0.0058927168138325214\n",
      "epoch:  786, loss: 0.005891157314181328\n",
      "epoch:  787, loss: 0.005890197586268187\n",
      "epoch:  788, loss: 0.005884495563805103\n",
      "epoch:  789, loss: 0.0058806645683944225\n",
      "epoch:  790, loss: 0.005879344884306192\n",
      "epoch:  791, loss: 0.005878343712538481\n",
      "epoch:  792, loss: 0.005870161112397909\n",
      "epoch:  793, loss: 0.005868327803909779\n",
      "epoch:  794, loss: 0.0058672428131103516\n",
      "epoch:  795, loss: 0.005865078419446945\n",
      "epoch:  796, loss: 0.005858083255589008\n",
      "epoch:  797, loss: 0.005856458563357592\n",
      "epoch:  798, loss: 0.005855392199009657\n",
      "epoch:  799, loss: 0.005850638262927532\n",
      "epoch:  800, loss: 0.005845995154231787\n",
      "epoch:  801, loss: 0.005844561383128166\n",
      "epoch:  802, loss: 0.00584358861669898\n",
      "epoch:  803, loss: 0.005836218595504761\n",
      "epoch:  804, loss: 0.005834008567035198\n",
      "epoch:  805, loss: 0.005832866299897432\n",
      "epoch:  806, loss: 0.005828895606100559\n",
      "epoch:  807, loss: 0.005823799408972263\n",
      "epoch:  808, loss: 0.005822283681482077\n",
      "epoch:  809, loss: 0.005821311380714178\n",
      "epoch:  810, loss: 0.0058164214715361595\n",
      "epoch:  811, loss: 0.005812323186546564\n",
      "epoch:  812, loss: 0.005810908041894436\n",
      "epoch:  813, loss: 0.0058099376037716866\n",
      "epoch:  814, loss: 0.005803149193525314\n",
      "epoch:  815, loss: 0.005800669081509113\n",
      "epoch:  816, loss: 0.005799494218081236\n",
      "epoch:  817, loss: 0.00579854566603899\n",
      "epoch:  818, loss: 0.005791874136775732\n",
      "epoch:  819, loss: 0.0057893963530659676\n",
      "epoch:  820, loss: 0.005788247566670179\n",
      "epoch:  821, loss: 0.005787291098386049\n",
      "epoch:  822, loss: 0.005779807455837727\n",
      "epoch:  823, loss: 0.005778051447123289\n",
      "epoch:  824, loss: 0.005776953883469105\n",
      "epoch:  825, loss: 0.0057762740179896355\n",
      "epoch:  826, loss: 0.005768694914877415\n",
      "epoch:  827, loss: 0.005766890477389097\n",
      "epoch:  828, loss: 0.005765844136476517\n",
      "epoch:  829, loss: 0.005764007568359375\n",
      "epoch:  830, loss: 0.005757336504757404\n",
      "epoch:  831, loss: 0.005755640100687742\n",
      "epoch:  832, loss: 0.005754677578806877\n",
      "epoch:  833, loss: 0.005749406758695841\n",
      "epoch:  834, loss: 0.005745574366301298\n",
      "epoch:  835, loss: 0.005744253285229206\n",
      "epoch:  836, loss: 0.005743319168686867\n",
      "epoch:  837, loss: 0.005736137740314007\n",
      "epoch:  838, loss: 0.005734010133892298\n",
      "epoch:  839, loss: 0.005732945166528225\n",
      "epoch:  840, loss: 0.005729808006435633\n",
      "epoch:  841, loss: 0.005724238697439432\n",
      "epoch:  842, loss: 0.005722705274820328\n",
      "epoch:  843, loss: 0.0057217120192945\n",
      "epoch:  844, loss: 0.005717229098081589\n",
      "epoch:  845, loss: 0.005712860729545355\n",
      "epoch:  846, loss: 0.005711441859602928\n",
      "epoch:  847, loss: 0.005710488185286522\n",
      "epoch:  848, loss: 0.005705120973289013\n",
      "epoch:  849, loss: 0.005701593589037657\n",
      "epoch:  850, loss: 0.0057002343237400055\n",
      "epoch:  851, loss: 0.005699293687939644\n",
      "epoch:  852, loss: 0.005694366060197353\n",
      "epoch:  853, loss: 0.005690480582416058\n",
      "epoch:  854, loss: 0.0056890565901994705\n",
      "epoch:  855, loss: 0.005688079632818699\n",
      "epoch:  856, loss: 0.005682759452611208\n",
      "epoch:  857, loss: 0.005679217167198658\n",
      "epoch:  858, loss: 0.005677889101207256\n",
      "epoch:  859, loss: 0.005676900967955589\n",
      "epoch:  860, loss: 0.005673207808285952\n",
      "epoch:  861, loss: 0.005668271332979202\n",
      "epoch:  862, loss: 0.005666675046086311\n",
      "epoch:  863, loss: 0.005665702745318413\n",
      "epoch:  864, loss: 0.005661621689796448\n",
      "epoch:  865, loss: 0.005657099653035402\n",
      "epoch:  866, loss: 0.005655584391206503\n",
      "epoch:  867, loss: 0.005654586479067802\n",
      "epoch:  868, loss: 0.005652494262903929\n",
      "epoch:  869, loss: 0.005646343808621168\n",
      "epoch:  870, loss: 0.0056445542722940445\n",
      "epoch:  871, loss: 0.0056435102596879005\n",
      "epoch:  872, loss: 0.005642618052661419\n",
      "epoch:  873, loss: 0.005636497866362333\n",
      "epoch:  874, loss: 0.0056336671113967896\n",
      "epoch:  875, loss: 0.005632443819195032\n",
      "epoch:  876, loss: 0.00563150504603982\n",
      "epoch:  877, loss: 0.005628873594105244\n",
      "epoch:  878, loss: 0.005623097997158766\n",
      "epoch:  879, loss: 0.0056213922798633575\n",
      "epoch:  880, loss: 0.00562031427398324\n",
      "epoch:  881, loss: 0.005619400180876255\n",
      "epoch:  882, loss: 0.0056161945685744286\n",
      "epoch:  883, loss: 0.005610938183963299\n",
      "epoch:  884, loss: 0.005609353072941303\n",
      "epoch:  885, loss: 0.005608308129012585\n",
      "epoch:  886, loss: 0.005607397761195898\n",
      "epoch:  887, loss: 0.005600851960480213\n",
      "epoch:  888, loss: 0.0055984691716730595\n",
      "epoch:  889, loss: 0.005597241688519716\n",
      "epoch:  890, loss: 0.005596318282186985\n",
      "epoch:  891, loss: 0.005594763904809952\n",
      "epoch:  892, loss: 0.0055882795713841915\n",
      "epoch:  893, loss: 0.005586417391896248\n",
      "epoch:  894, loss: 0.005585340782999992\n",
      "epoch:  895, loss: 0.005584460683166981\n",
      "epoch:  896, loss: 0.005583886057138443\n",
      "epoch:  897, loss: 0.005576667841523886\n",
      "epoch:  898, loss: 0.005574729293584824\n",
      "epoch:  899, loss: 0.005573558621108532\n",
      "epoch:  900, loss: 0.005572606809437275\n",
      "epoch:  901, loss: 0.005568122956901789\n",
      "epoch:  902, loss: 0.0055641187354922295\n",
      "epoch:  903, loss: 0.005562501028180122\n",
      "epoch:  904, loss: 0.005561438854783773\n",
      "epoch:  905, loss: 0.005560541525483131\n",
      "epoch:  906, loss: 0.005559378769248724\n",
      "epoch:  907, loss: 0.005552651360630989\n",
      "epoch:  908, loss: 0.005550688598304987\n",
      "epoch:  909, loss: 0.005549534689635038\n",
      "epoch:  910, loss: 0.005548613611608744\n",
      "epoch:  911, loss: 0.005545066669583321\n",
      "epoch:  912, loss: 0.005540396086871624\n",
      "epoch:  913, loss: 0.005538790952414274\n",
      "epoch:  914, loss: 0.005537748336791992\n",
      "epoch:  915, loss: 0.005536860786378384\n",
      "epoch:  916, loss: 0.005535947158932686\n",
      "epoch:  917, loss: 0.005529158283025026\n",
      "epoch:  918, loss: 0.005527236964553595\n",
      "epoch:  919, loss: 0.005526101216673851\n",
      "epoch:  920, loss: 0.005525190848857164\n",
      "epoch:  921, loss: 0.005524703301489353\n",
      "epoch:  922, loss: 0.005517631769180298\n",
      "epoch:  923, loss: 0.005515633150935173\n",
      "epoch:  924, loss: 0.005514482967555523\n",
      "epoch:  925, loss: 0.0055135879665613174\n",
      "epoch:  926, loss: 0.005510973744094372\n",
      "epoch:  927, loss: 0.005505723878741264\n",
      "epoch:  928, loss: 0.005504029802978039\n",
      "epoch:  929, loss: 0.00550295552238822\n",
      "epoch:  930, loss: 0.0055020833387970924\n",
      "epoch:  931, loss: 0.00550047168508172\n",
      "epoch:  932, loss: 0.005494475830346346\n",
      "epoch:  933, loss: 0.005492674186825752\n",
      "epoch:  934, loss: 0.00549154169857502\n",
      "epoch:  935, loss: 0.005490642040967941\n",
      "epoch:  936, loss: 0.005489808972924948\n",
      "epoch:  937, loss: 0.005486064124852419\n",
      "epoch:  938, loss: 0.005481956992298365\n",
      "epoch:  939, loss: 0.005480323452502489\n",
      "epoch:  940, loss: 0.0054793222807347775\n",
      "epoch:  941, loss: 0.005478449631482363\n",
      "epoch:  942, loss: 0.005477601196616888\n",
      "epoch:  943, loss: 0.005476764403283596\n",
      "epoch:  944, loss: 0.005470242351293564\n",
      "epoch:  945, loss: 0.005468243267387152\n",
      "epoch:  946, loss: 0.005467087030410767\n",
      "epoch:  947, loss: 0.00546618178486824\n",
      "epoch:  948, loss: 0.005465307272970676\n",
      "epoch:  949, loss: 0.00546101201325655\n",
      "epoch:  950, loss: 0.005457261577248573\n",
      "epoch:  951, loss: 0.005455690436065197\n",
      "epoch:  952, loss: 0.0054546743631362915\n",
      "epoch:  953, loss: 0.005453793331980705\n",
      "epoch:  954, loss: 0.005450775846838951\n",
      "epoch:  955, loss: 0.00544603168964386\n",
      "epoch:  956, loss: 0.005444269627332687\n",
      "epoch:  957, loss: 0.005443200934678316\n",
      "epoch:  958, loss: 0.005442263092845678\n",
      "epoch:  959, loss: 0.005441394634544849\n",
      "epoch:  960, loss: 0.00544011453166604\n",
      "epoch:  961, loss: 0.005433868151158094\n",
      "epoch:  962, loss: 0.0054318541660904884\n",
      "epoch:  963, loss: 0.005430663004517555\n",
      "epoch:  964, loss: 0.005429720971733332\n",
      "epoch:  965, loss: 0.005426803603768349\n",
      "epoch:  966, loss: 0.005421923007816076\n",
      "epoch:  967, loss: 0.005420118570327759\n",
      "epoch:  968, loss: 0.00541903730481863\n",
      "epoch:  969, loss: 0.005418082699179649\n",
      "epoch:  970, loss: 0.005415849853307009\n",
      "epoch:  971, loss: 0.005410375073552132\n",
      "epoch:  972, loss: 0.0054085333831608295\n",
      "epoch:  973, loss: 0.005407297518104315\n",
      "epoch:  974, loss: 0.005406353622674942\n",
      "epoch:  975, loss: 0.0054054562933743\n",
      "epoch:  976, loss: 0.005404287949204445\n",
      "epoch:  977, loss: 0.00539808627218008\n",
      "epoch:  978, loss: 0.005396037828177214\n",
      "epoch:  979, loss: 0.005394728388637304\n",
      "epoch:  980, loss: 0.005393771454691887\n",
      "epoch:  981, loss: 0.005392868537455797\n",
      "epoch:  982, loss: 0.00539226271212101\n",
      "epoch:  983, loss: 0.005385942291468382\n",
      "epoch:  984, loss: 0.005383500829339027\n",
      "epoch:  985, loss: 0.005382224917411804\n",
      "epoch:  986, loss: 0.005381257738918066\n",
      "epoch:  987, loss: 0.005380374379456043\n",
      "epoch:  988, loss: 0.005376788787543774\n",
      "epoch:  989, loss: 0.005372581537812948\n",
      "epoch:  990, loss: 0.0053708297200500965\n",
      "epoch:  991, loss: 0.005369699560105801\n",
      "epoch:  992, loss: 0.005368801299482584\n",
      "epoch:  993, loss: 0.005367919337004423\n",
      "epoch:  994, loss: 0.005366743076592684\n",
      "epoch:  995, loss: 0.005360811948776245\n",
      "epoch:  996, loss: 0.005358579568564892\n",
      "epoch:  997, loss: 0.005357363261282444\n",
      "epoch:  998, loss: 0.005356415640562773\n",
      "epoch:  999, loss: 0.005355544853955507\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size=X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "opt = torch_numopt.GradientDescentLS(model.parameters(), lr=1, model=model, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"armijo\")\n",
    "# opt = torch_numopt.GradientDescentLS(model.parameters(), lr=1, model=model, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"wolfe\")\n",
    "# opt = torch_numopt.GradientDescentLS(model.parameters(), lr=1, model=model, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"strong-wolfe\")\n",
    "# opt = torch_numopt.GradientDescentLS(model.parameters(), lr=1, model=model, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"goldstein\")\n",
    "\n",
    "\n",
    "all_loss = {}\n",
    "for epoch in range(1000):\n",
    "    print(\"epoch: \", epoch, end=\"\")\n",
    "    all_loss[epoch + 1] = 0\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y, loss_fn)\n",
    "\n",
    "        all_loss[epoch + 1] += loss\n",
    "    all_loss[epoch + 1] /= len(data_loader)\n",
    "    print(\", loss: {}\".format(all_loss[epoch + 1].detach().numpy().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics: R2 = 0.7819220835915011\n",
      "Test metrics:  R2 = 0.8194488293559897\n"
     ]
    }
   ],
   "source": [
    "pred_train = model.forward(X_train).detach()\n",
    "pred_test = model.forward(X_test).detach()\n",
    "print(f\"Train metrics: R2 = {r2_score(pred_train, y_train)}\")\n",
    "print(f\"Test metrics:  R2 = {r2_score(pred_test, y_test)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
