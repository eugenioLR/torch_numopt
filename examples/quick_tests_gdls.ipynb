{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch_numopt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import *\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.f1 = nn.Linear(input_size, 10, device=device)\n",
    "        self.f2 = nn.Linear(10, 20, device=device)\n",
    "        self.f3 = nn.Linear(20, 20, device=device)\n",
    "        self.f4 = nn.Linear(20, 10, device=device)\n",
    "        self.f5 = nn.Linear(10, 1, device=device)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        # self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.f1(x))\n",
    "        x = self.activation(self.f2(x))\n",
    "        x = self.activation(self.f3(x))\n",
    "        x = self.activation(self.f4(x))\n",
    "        x = self.f5(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([800, 10]) torch.Size([800, 1])\n",
      "torch.Size([200, 10]) torch.Size([200, 1])\n"
     ]
    }
   ],
   "source": [
    "# X, y = load_diabetes(return_X_y=True, scaled=False)\n",
    "# X, y = make_regression(n_samples=1000, n_features=100)\n",
    "X, y = make_friedman1(n_samples=1000, noise=1e-2)\n",
    "\n",
    "X_scaler = MinMaxScaler()\n",
    "X = X_scaler.fit_transform(X)\n",
    "X = torch.Tensor(X).to(device)\n",
    "\n",
    "y_scaler = MinMaxScaler()\n",
    "y = y_scaler.fit_transform(y.reshape((-1, 1)))\n",
    "y = torch.Tensor(y).to(device)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "torch_data = TensorDataset(X_train, y_train)\n",
    "data_loader = DataLoader(torch_data, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.2912001311779022\n",
      "epoch:  1, loss: 0.16991253197193146\n",
      "epoch:  2, loss: 0.10898474603891373\n",
      "epoch:  3, loss: 0.07597718387842178\n",
      "epoch:  4, loss: 0.05781695619225502\n",
      "epoch:  5, loss: 0.04778759926557541\n",
      "epoch:  6, loss: 0.04224797338247299\n",
      "epoch:  7, loss: 0.03919154033064842\n",
      "epoch:  8, loss: 0.03750688582658768\n",
      "epoch:  9, loss: 0.036577802151441574\n",
      "epoch:  10, loss: 0.036063339561223984\n",
      "epoch:  11, loss: 0.03577562794089317\n",
      "epoch:  12, loss: 0.035612016916275024\n",
      "epoch:  13, loss: 0.035515863448381424\n",
      "epoch:  14, loss: 0.03549443185329437\n",
      "epoch:  15, loss: 0.035353973507881165\n",
      "epoch:  16, loss: 0.035267796367406845\n",
      "epoch:  17, loss: 0.03524455428123474\n",
      "epoch:  18, loss: 0.03512768819928169\n",
      "epoch:  19, loss: 0.035059552639722824\n",
      "epoch:  20, loss: 0.0350586399435997\n",
      "epoch:  21, loss: 0.03496047109365463\n",
      "epoch:  22, loss: 0.034902554005384445\n",
      "epoch:  23, loss: 0.03488832339644432\n",
      "epoch:  24, loss: 0.03480549156665802\n",
      "epoch:  25, loss: 0.034756142646074295\n",
      "epoch:  26, loss: 0.03473096713423729\n",
      "epoch:  27, loss: 0.03466034308075905\n",
      "epoch:  28, loss: 0.034618087112903595\n",
      "epoch:  29, loss: 0.03458508849143982\n",
      "epoch:  30, loss: 0.034524474292993546\n",
      "epoch:  31, loss: 0.03448796272277832\n",
      "epoch:  32, loss: 0.034447573125362396\n",
      "epoch:  33, loss: 0.03439471498131752\n",
      "epoch:  34, loss: 0.0343928188085556\n",
      "epoch:  35, loss: 0.03431128337979317\n",
      "epoch:  36, loss: 0.034263454377651215\n",
      "epoch:  37, loss: 0.03424183651804924\n",
      "epoch:  38, loss: 0.034168459475040436\n",
      "epoch:  39, loss: 0.034124694764614105\n",
      "epoch:  40, loss: 0.034086525440216064\n",
      "epoch:  41, loss: 0.034018661826848984\n",
      "epoch:  42, loss: 0.033977340906858444\n",
      "epoch:  43, loss: 0.03391849622130394\n",
      "epoch:  44, loss: 0.03385530784726143\n",
      "epoch:  45, loss: 0.03383883461356163\n",
      "epoch:  46, loss: 0.03373687341809273\n",
      "epoch:  47, loss: 0.03367720916867256\n",
      "epoch:  48, loss: 0.03363422676920891\n",
      "epoch:  49, loss: 0.03353795409202576\n",
      "epoch:  50, loss: 0.033480651676654816\n",
      "epoch:  51, loss: 0.03341123089194298\n",
      "epoch:  52, loss: 0.03331839293241501\n",
      "epoch:  53, loss: 0.03326207399368286\n",
      "epoch:  54, loss: 0.03316101059317589\n",
      "epoch:  55, loss: 0.03307193145155907\n",
      "epoch:  56, loss: 0.03304247185587883\n",
      "epoch:  57, loss: 0.03288756310939789\n",
      "epoch:  58, loss: 0.03279944509267807\n",
      "epoch:  59, loss: 0.03273352235555649\n",
      "epoch:  60, loss: 0.03258126974105835\n",
      "epoch:  61, loss: 0.03249260410666466\n",
      "epoch:  62, loss: 0.03239276632666588\n",
      "epoch:  63, loss: 0.03223562240600586\n",
      "epoch:  64, loss: 0.0321439653635025\n",
      "epoch:  65, loss: 0.03200216963887215\n",
      "epoch:  66, loss: 0.031838662922382355\n",
      "epoch:  67, loss: 0.031742461025714874\n",
      "epoch:  68, loss: 0.031575024127960205\n",
      "epoch:  69, loss: 0.03139990195631981\n",
      "epoch:  70, loss: 0.031296905130147934\n",
      "epoch:  71, loss: 0.03109782189130783\n",
      "epoch:  72, loss: 0.030905723571777344\n",
      "epoch:  73, loss: 0.03079386241734028\n",
      "epoch:  74, loss: 0.030565474182367325\n",
      "epoch:  75, loss: 0.030351120978593826\n",
      "epoch:  76, loss: 0.030227001756429672\n",
      "epoch:  77, loss: 0.0299641452729702\n",
      "epoch:  78, loss: 0.02971656806766987\n",
      "epoch:  79, loss: 0.029577460139989853\n",
      "epoch:  80, loss: 0.029283994808793068\n",
      "epoch:  81, loss: 0.028997372835874557\n",
      "epoch:  82, loss: 0.028837349265813828\n",
      "epoch:  83, loss: 0.028506126254796982\n",
      "epoch:  84, loss: 0.02816561795771122\n",
      "epoch:  85, loss: 0.02798333577811718\n",
      "epoch:  86, loss: 0.027668436989188194\n",
      "epoch:  87, loss: 0.027251262217760086\n",
      "epoch:  88, loss: 0.027038531377911568\n",
      "epoch:  89, loss: 0.0267409048974514\n",
      "epoch:  90, loss: 0.026218000799417496\n",
      "epoch:  91, loss: 0.025970591232180595\n",
      "epoch:  92, loss: 0.025767631828784943\n",
      "epoch:  93, loss: 0.02510109357535839\n",
      "epoch:  94, loss: 0.024808505550026894\n",
      "epoch:  95, loss: 0.024704013019800186\n",
      "epoch:  96, loss: 0.023858394473791122\n",
      "epoch:  97, loss: 0.02352382056415081\n",
      "epoch:  98, loss: 0.023342985659837723\n",
      "epoch:  99, loss: 0.02254309318959713\n",
      "epoch:  100, loss: 0.022127922624349594\n",
      "epoch:  101, loss: 0.021926697343587875\n",
      "epoch:  102, loss: 0.02105526812374592\n",
      "epoch:  103, loss: 0.020599564537405968\n",
      "epoch:  104, loss: 0.020393414422869682\n",
      "epoch:  105, loss: 0.019483650103211403\n",
      "epoch:  106, loss: 0.019002122804522514\n",
      "epoch:  107, loss: 0.018803121522068977\n",
      "epoch:  108, loss: 0.01783267967402935\n",
      "epoch:  109, loss: 0.017407020553946495\n",
      "epoch:  110, loss: 0.0172364991158247\n",
      "epoch:  111, loss: 0.016217615455389023\n",
      "epoch:  112, loss: 0.015909599140286446\n",
      "epoch:  113, loss: 0.015595467761158943\n",
      "epoch:  114, loss: 0.014717970974743366\n",
      "epoch:  115, loss: 0.014549016952514648\n",
      "epoch:  116, loss: 0.013790390454232693\n",
      "epoch:  117, loss: 0.013463237322866917\n",
      "epoch:  118, loss: 0.01316940039396286\n",
      "epoch:  119, loss: 0.012528232298791409\n",
      "epoch:  120, loss: 0.012431883253157139\n",
      "epoch:  121, loss: 0.011746859177947044\n",
      "epoch:  122, loss: 0.011643895879387856\n",
      "epoch:  123, loss: 0.011079538613557816\n",
      "epoch:  124, loss: 0.010990621522068977\n",
      "epoch:  125, loss: 0.010535290464758873\n",
      "epoch:  126, loss: 0.010458789765834808\n",
      "epoch:  127, loss: 0.010064695030450821\n",
      "epoch:  128, loss: 0.010025185532867908\n",
      "epoch:  129, loss: 0.00969525147229433\n",
      "epoch:  130, loss: 0.009489062242209911\n",
      "epoch:  131, loss: 0.009386779740452766\n",
      "epoch:  132, loss: 0.009147647768259048\n",
      "epoch:  133, loss: 0.009075147099792957\n",
      "epoch:  134, loss: 0.008925387635827065\n",
      "epoch:  135, loss: 0.008784768171608448\n",
      "epoch:  136, loss: 0.008738677017390728\n",
      "epoch:  137, loss: 0.008599855937063694\n",
      "epoch:  138, loss: 0.008582116104662418\n",
      "epoch:  139, loss: 0.008463065139949322\n",
      "epoch:  140, loss: 0.008424007333815098\n",
      "epoch:  141, loss: 0.008351574651896954\n",
      "epoch:  142, loss: 0.008310064673423767\n",
      "epoch:  143, loss: 0.008253572508692741\n",
      "epoch:  144, loss: 0.008217600174248219\n",
      "epoch:  145, loss: 0.008166474290192127\n",
      "epoch:  146, loss: 0.008133706636726856\n",
      "epoch:  147, loss: 0.008089699782431126\n",
      "epoch:  148, loss: 0.008059730753302574\n",
      "epoch:  149, loss: 0.008020172826945782\n",
      "epoch:  150, loss: 0.008013895712792873\n",
      "epoch:  151, loss: 0.007959499023854733\n",
      "epoch:  152, loss: 0.007953579537570477\n",
      "epoch:  153, loss: 0.00790499709546566\n",
      "epoch:  154, loss: 0.007899225689470768\n",
      "epoch:  155, loss: 0.007855566218495369\n",
      "epoch:  156, loss: 0.007849935442209244\n",
      "epoch:  157, loss: 0.007813816890120506\n",
      "epoch:  158, loss: 0.007807646878063679\n",
      "epoch:  159, loss: 0.007778600323945284\n",
      "epoch:  160, loss: 0.007770238909870386\n",
      "epoch:  161, loss: 0.007748227566480637\n",
      "epoch:  162, loss: 0.007736253552138805\n",
      "epoch:  163, loss: 0.007715502753853798\n",
      "epoch:  164, loss: 0.007704263553023338\n",
      "epoch:  165, loss: 0.007694193162024021\n",
      "epoch:  166, loss: 0.007676586974412203\n",
      "epoch:  167, loss: 0.007674034219235182\n",
      "epoch:  168, loss: 0.007652624510228634\n",
      "epoch:  169, loss: 0.00764877675101161\n",
      "epoch:  170, loss: 0.007633280474692583\n",
      "epoch:  171, loss: 0.007625877857208252\n",
      "epoch:  172, loss: 0.00762431463226676\n",
      "epoch:  173, loss: 0.007605801802128553\n",
      "epoch:  174, loss: 0.007603599689900875\n",
      "epoch:  175, loss: 0.0075868163257837296\n",
      "epoch:  176, loss: 0.007584352511912584\n",
      "epoch:  177, loss: 0.007573784328997135\n",
      "epoch:  178, loss: 0.0075665912590920925\n",
      "epoch:  179, loss: 0.007564932107925415\n",
      "epoch:  180, loss: 0.007553629577159882\n",
      "epoch:  181, loss: 0.007549392990767956\n",
      "epoch:  182, loss: 0.0075479792430996895\n",
      "epoch:  183, loss: 0.007537170313298702\n",
      "epoch:  184, loss: 0.007534568198025227\n",
      "epoch:  185, loss: 0.007534070406109095\n",
      "epoch:  186, loss: 0.0075219557620584965\n",
      "epoch:  187, loss: 0.007520617451518774\n",
      "epoch:  188, loss: 0.007510217372328043\n",
      "epoch:  189, loss: 0.007508359849452972\n",
      "epoch:  190, loss: 0.00750621547922492\n",
      "epoch:  191, loss: 0.0074975560419261456\n",
      "epoch:  192, loss: 0.007496261969208717\n",
      "epoch:  193, loss: 0.007489354349672794\n",
      "epoch:  194, loss: 0.007486231159418821\n",
      "epoch:  195, loss: 0.007485227193683386\n",
      "epoch:  196, loss: 0.007479729596525431\n",
      "epoch:  197, loss: 0.007475554011762142\n",
      "epoch:  198, loss: 0.0074745845049619675\n",
      "epoch:  199, loss: 0.007467006333172321\n",
      "epoch:  200, loss: 0.007465289440006018\n",
      "epoch:  201, loss: 0.007464403752237558\n",
      "epoch:  202, loss: 0.007457301486283541\n",
      "epoch:  203, loss: 0.007455734070390463\n",
      "epoch:  204, loss: 0.007455639075487852\n",
      "epoch:  205, loss: 0.00744762783870101\n",
      "epoch:  206, loss: 0.00744652422145009\n",
      "epoch:  207, loss: 0.007441078312695026\n",
      "epoch:  208, loss: 0.007438329979777336\n",
      "epoch:  209, loss: 0.007437525317072868\n",
      "epoch:  210, loss: 0.007430847268551588\n",
      "epoch:  211, loss: 0.007429859600961208\n",
      "epoch:  212, loss: 0.007424477953463793\n",
      "epoch:  213, loss: 0.007422562688589096\n",
      "epoch:  214, loss: 0.007421830669045448\n",
      "epoch:  215, loss: 0.007415801286697388\n",
      "epoch:  216, loss: 0.007414494641125202\n",
      "epoch:  217, loss: 0.007409690413624048\n",
      "epoch:  218, loss: 0.007407082710415125\n",
      "epoch:  219, loss: 0.007405266165733337\n",
      "epoch:  220, loss: 0.007399674970656633\n",
      "epoch:  221, loss: 0.00739894388243556\n",
      "epoch:  222, loss: 0.007393558043986559\n",
      "epoch:  223, loss: 0.007391809020191431\n",
      "epoch:  224, loss: 0.007390030659735203\n",
      "epoch:  225, loss: 0.007384613621979952\n",
      "epoch:  226, loss: 0.007383686024695635\n",
      "epoch:  227, loss: 0.007378654088824987\n",
      "epoch:  228, loss: 0.007375733461230993\n",
      "epoch:  229, loss: 0.007374797947704792\n",
      "epoch:  230, loss: 0.007368824444711208\n",
      "epoch:  231, loss: 0.0073671722784638405\n",
      "epoch:  232, loss: 0.007366272155195475\n",
      "epoch:  233, loss: 0.007359724026173353\n",
      "epoch:  234, loss: 0.007358502130955458\n",
      "epoch:  235, loss: 0.007356063928455114\n",
      "epoch:  236, loss: 0.007351084612309933\n",
      "epoch:  237, loss: 0.007350125350058079\n",
      "epoch:  238, loss: 0.007344331592321396\n",
      "epoch:  239, loss: 0.007342302706092596\n",
      "epoch:  240, loss: 0.007341512478888035\n",
      "epoch:  241, loss: 0.007334867492318153\n",
      "epoch:  242, loss: 0.007333817426115274\n",
      "epoch:  243, loss: 0.007330984342843294\n",
      "epoch:  244, loss: 0.007326692342758179\n",
      "epoch:  245, loss: 0.007325825747102499\n",
      "epoch:  246, loss: 0.007321374025195837\n",
      "epoch:  247, loss: 0.007318906486034393\n",
      "epoch:  248, loss: 0.007318144664168358\n",
      "epoch:  249, loss: 0.007312254048883915\n",
      "epoch:  250, loss: 0.007310883142054081\n",
      "epoch:  251, loss: 0.007310133893042803\n",
      "epoch:  252, loss: 0.007303672842681408\n",
      "epoch:  253, loss: 0.007302687503397465\n",
      "epoch:  254, loss: 0.007298613432794809\n",
      "epoch:  255, loss: 0.007295583374798298\n",
      "epoch:  256, loss: 0.007294772192835808\n",
      "epoch:  257, loss: 0.007289659231901169\n",
      "epoch:  258, loss: 0.0072878203354775906\n",
      "epoch:  259, loss: 0.007287037093192339\n",
      "epoch:  260, loss: 0.007282601203769445\n",
      "epoch:  261, loss: 0.007280255202203989\n",
      "epoch:  262, loss: 0.007279431447386742\n",
      "epoch:  263, loss: 0.0072752428241074085\n",
      "epoch:  264, loss: 0.007272901479154825\n",
      "epoch:  265, loss: 0.007272207643836737\n",
      "epoch:  266, loss: 0.007267557084560394\n",
      "epoch:  267, loss: 0.007266075816005468\n",
      "epoch:  268, loss: 0.007265392225235701\n",
      "epoch:  269, loss: 0.007260913494974375\n",
      "epoch:  270, loss: 0.007259222213178873\n",
      "epoch:  271, loss: 0.00725855678319931\n",
      "epoch:  272, loss: 0.007253638468682766\n",
      "epoch:  273, loss: 0.007252232637256384\n",
      "epoch:  274, loss: 0.00725159514695406\n",
      "epoch:  275, loss: 0.00724643561989069\n",
      "epoch:  276, loss: 0.007245271001011133\n",
      "epoch:  277, loss: 0.0072446358390152454\n",
      "epoch:  278, loss: 0.00723922997713089\n",
      "epoch:  279, loss: 0.007238267455250025\n",
      "epoch:  280, loss: 0.007236598525196314\n",
      "epoch:  281, loss: 0.007232205010950565\n",
      "epoch:  282, loss: 0.007231227122247219\n",
      "epoch:  283, loss: 0.007229761220514774\n",
      "epoch:  284, loss: 0.007224565371870995\n",
      "epoch:  285, loss: 0.007223535794764757\n",
      "epoch:  286, loss: 0.007220266852527857\n",
      "epoch:  287, loss: 0.007216603960841894\n",
      "epoch:  288, loss: 0.007215636782348156\n",
      "epoch:  289, loss: 0.007214014418423176\n",
      "epoch:  290, loss: 0.007208871655166149\n",
      "epoch:  291, loss: 0.007207851391285658\n",
      "epoch:  292, loss: 0.007207153830677271\n",
      "epoch:  293, loss: 0.00720218988135457\n",
      "epoch:  294, loss: 0.007200194522738457\n",
      "epoch:  295, loss: 0.0071994285099208355\n",
      "epoch:  296, loss: 0.007198760751634836\n",
      "epoch:  297, loss: 0.007194008678197861\n",
      "epoch:  298, loss: 0.00719193322584033\n",
      "epoch:  299, loss: 0.007191178854554892\n",
      "epoch:  300, loss: 0.007189755327999592\n",
      "epoch:  301, loss: 0.007184517104178667\n",
      "epoch:  302, loss: 0.007183453068137169\n",
      "epoch:  303, loss: 0.007182787638157606\n",
      "epoch:  304, loss: 0.007177983410656452\n",
      "epoch:  305, loss: 0.007175740320235491\n",
      "epoch:  306, loss: 0.007174949627369642\n",
      "epoch:  307, loss: 0.007169951684772968\n",
      "epoch:  308, loss: 0.007167852483689785\n",
      "epoch:  309, loss: 0.007167082279920578\n",
      "epoch:  310, loss: 0.007164382841438055\n",
      "epoch:  311, loss: 0.007160109002143145\n",
      "epoch:  312, loss: 0.007159150671213865\n",
      "epoch:  313, loss: 0.007156759966164827\n",
      "epoch:  314, loss: 0.007152504753321409\n",
      "epoch:  315, loss: 0.00715154642239213\n",
      "epoch:  316, loss: 0.00715145468711853\n",
      "epoch:  317, loss: 0.0071450029499828815\n",
      "epoch:  318, loss: 0.007143933791667223\n",
      "epoch:  319, loss: 0.007143253926187754\n",
      "epoch:  320, loss: 0.007137345615774393\n",
      "epoch:  321, loss: 0.007136380299925804\n",
      "epoch:  322, loss: 0.0071345362812280655\n",
      "epoch:  323, loss: 0.007129886653274298\n",
      "epoch:  324, loss: 0.007128942292183638\n",
      "epoch:  325, loss: 0.007128279190510511\n",
      "epoch:  326, loss: 0.00712243840098381\n",
      "epoch:  327, loss: 0.0071215545758605\n",
      "epoch:  328, loss: 0.007119256537407637\n",
      "epoch:  329, loss: 0.007115217391401529\n",
      "epoch:  330, loss: 0.007114302832633257\n",
      "epoch:  331, loss: 0.007114030420780182\n",
      "epoch:  332, loss: 0.007108195684850216\n",
      "epoch:  333, loss: 0.007107137236744165\n",
      "epoch:  334, loss: 0.007106294389814138\n",
      "epoch:  335, loss: 0.007101274095475674\n",
      "epoch:  336, loss: 0.00710023520514369\n",
      "epoch:  337, loss: 0.007099620066583157\n",
      "epoch:  338, loss: 0.007094278000295162\n",
      "epoch:  339, loss: 0.007093267980962992\n",
      "epoch:  340, loss: 0.007092613261193037\n",
      "epoch:  341, loss: 0.00708722835406661\n",
      "epoch:  342, loss: 0.00708614569157362\n",
      "epoch:  343, loss: 0.0070854914374649525\n",
      "epoch:  344, loss: 0.007080857641994953\n",
      "epoch:  345, loss: 0.007079438306391239\n",
      "epoch:  346, loss: 0.007078766822814941\n",
      "epoch:  347, loss: 0.0070767467841506\n",
      "epoch:  348, loss: 0.007072873413562775\n",
      "epoch:  349, loss: 0.007072065956890583\n",
      "epoch:  350, loss: 0.007071433588862419\n",
      "epoch:  351, loss: 0.007067215163260698\n",
      "epoch:  352, loss: 0.007065275218337774\n",
      "epoch:  353, loss: 0.007064529694616795\n",
      "epoch:  354, loss: 0.00706394761800766\n",
      "epoch:  355, loss: 0.007061852142214775\n",
      "epoch:  356, loss: 0.007058008573949337\n",
      "epoch:  357, loss: 0.007057156413793564\n",
      "epoch:  358, loss: 0.007056484464555979\n",
      "epoch:  359, loss: 0.007053751964122057\n",
      "epoch:  360, loss: 0.007049971260130405\n",
      "epoch:  361, loss: 0.007049120031297207\n",
      "epoch:  362, loss: 0.007048448547720909\n",
      "epoch:  363, loss: 0.007045597769320011\n",
      "epoch:  364, loss: 0.007041945122182369\n",
      "epoch:  365, loss: 0.007041108328849077\n",
      "epoch:  366, loss: 0.007040423806756735\n",
      "epoch:  367, loss: 0.007034420967102051\n",
      "epoch:  368, loss: 0.0070336139760911465\n",
      "epoch:  369, loss: 0.007031737361103296\n",
      "epoch:  370, loss: 0.007027107290923595\n",
      "epoch:  371, loss: 0.007026156410574913\n",
      "epoch:  372, loss: 0.0070255170576274395\n",
      "epoch:  373, loss: 0.007022225763648748\n",
      "epoch:  374, loss: 0.0070189014077186584\n",
      "epoch:  375, loss: 0.007018023636192083\n",
      "epoch:  376, loss: 0.00701538659632206\n",
      "epoch:  377, loss: 0.007011381909251213\n",
      "epoch:  378, loss: 0.007010440807789564\n",
      "epoch:  379, loss: 0.007009756285697222\n",
      "epoch:  380, loss: 0.007004031911492348\n",
      "epoch:  381, loss: 0.007002959027886391\n",
      "epoch:  382, loss: 0.007002226077020168\n",
      "epoch:  383, loss: 0.006997651420533657\n",
      "epoch:  384, loss: 0.006995576433837414\n",
      "epoch:  385, loss: 0.0069947694428265095\n",
      "epoch:  386, loss: 0.006994109600782394\n",
      "epoch:  387, loss: 0.006990392226725817\n",
      "epoch:  388, loss: 0.006987545173615217\n",
      "epoch:  389, loss: 0.00698669720441103\n",
      "epoch:  390, loss: 0.006985975429415703\n",
      "epoch:  391, loss: 0.0069809830747544765\n",
      "epoch:  392, loss: 0.006979078520089388\n",
      "epoch:  393, loss: 0.006978229153901339\n",
      "epoch:  394, loss: 0.006977524142712355\n",
      "epoch:  395, loss: 0.006971593946218491\n",
      "epoch:  396, loss: 0.0069704363122582436\n",
      "epoch:  397, loss: 0.006969717796891928\n",
      "epoch:  398, loss: 0.006967238616198301\n",
      "epoch:  399, loss: 0.0069630262441933155\n",
      "epoch:  400, loss: 0.006961923558264971\n",
      "epoch:  401, loss: 0.006961194332689047\n",
      "epoch:  402, loss: 0.006956690456718206\n",
      "epoch:  403, loss: 0.00695417495444417\n",
      "epoch:  404, loss: 0.006953359115868807\n",
      "epoch:  405, loss: 0.006952654104679823\n",
      "epoch:  406, loss: 0.006947359070181847\n",
      "epoch:  407, loss: 0.006945609115064144\n",
      "epoch:  408, loss: 0.006944825407117605\n",
      "epoch:  409, loss: 0.006944131571799517\n",
      "epoch:  410, loss: 0.006939977407455444\n",
      "epoch:  411, loss: 0.006937215570360422\n",
      "epoch:  412, loss: 0.006936374586075544\n",
      "epoch:  413, loss: 0.0069341170601546764\n",
      "epoch:  414, loss: 0.006929730996489525\n",
      "epoch:  415, loss: 0.00692864740267396\n",
      "epoch:  416, loss: 0.006927939597517252\n",
      "epoch:  417, loss: 0.006922850385308266\n",
      "epoch:  418, loss: 0.006920853164047003\n",
      "epoch:  419, loss: 0.006920027546584606\n",
      "epoch:  420, loss: 0.0069173239171504974\n",
      "epoch:  421, loss: 0.006913057528436184\n",
      "epoch:  422, loss: 0.00691204285249114\n",
      "epoch:  423, loss: 0.0069113001227378845\n",
      "epoch:  424, loss: 0.006906135939061642\n",
      "epoch:  425, loss: 0.006903993897140026\n",
      "epoch:  426, loss: 0.0069031380116939545\n",
      "epoch:  427, loss: 0.006897856015712023\n",
      "epoch:  428, loss: 0.006895958445966244\n",
      "epoch:  429, loss: 0.006895104423165321\n",
      "epoch:  430, loss: 0.006889707874506712\n",
      "epoch:  431, loss: 0.006887994706630707\n",
      "epoch:  432, loss: 0.006887235678732395\n",
      "epoch:  433, loss: 0.006886546965688467\n",
      "epoch:  434, loss: 0.006883549503982067\n",
      "epoch:  435, loss: 0.006879739463329315\n",
      "epoch:  436, loss: 0.00687879603356123\n",
      "epoch:  437, loss: 0.0068780481815338135\n",
      "epoch:  438, loss: 0.006873250938951969\n",
      "epoch:  439, loss: 0.006870970129966736\n",
      "epoch:  440, loss: 0.0068700844421982765\n",
      "epoch:  441, loss: 0.006869410164654255\n",
      "epoch:  442, loss: 0.006867066491395235\n",
      "epoch:  443, loss: 0.0068625458516180515\n",
      "epoch:  444, loss: 0.006861529313027859\n",
      "epoch:  445, loss: 0.006860771216452122\n",
      "epoch:  446, loss: 0.006856325548142195\n",
      "epoch:  447, loss: 0.006853776052594185\n",
      "epoch:  448, loss: 0.006852814927697182\n",
      "epoch:  449, loss: 0.006852489430457354\n",
      "epoch:  450, loss: 0.006845894735306501\n",
      "epoch:  451, loss: 0.0068444316275417805\n",
      "epoch:  452, loss: 0.006843597162514925\n",
      "epoch:  453, loss: 0.006842249073088169\n",
      "epoch:  454, loss: 0.006836197338998318\n",
      "epoch:  455, loss: 0.006834886968135834\n",
      "epoch:  456, loss: 0.006834038533270359\n",
      "epoch:  457, loss: 0.006828411016613245\n",
      "epoch:  458, loss: 0.0068263523280620575\n",
      "epoch:  459, loss: 0.00682541960850358\n",
      "epoch:  460, loss: 0.006824648007750511\n",
      "epoch:  461, loss: 0.006820568814873695\n",
      "epoch:  462, loss: 0.006817281246185303\n",
      "epoch:  463, loss: 0.006816220004111528\n",
      "epoch:  464, loss: 0.0068154167383909225\n",
      "epoch:  465, loss: 0.006810554303228855\n",
      "epoch:  466, loss: 0.006807887461036444\n",
      "epoch:  467, loss: 0.006806916557252407\n",
      "epoch:  468, loss: 0.0068061319179832935\n",
      "epoch:  469, loss: 0.006801329553127289\n",
      "epoch:  470, loss: 0.006798195652663708\n",
      "epoch:  471, loss: 0.0067971860989928246\n",
      "epoch:  472, loss: 0.006795919965952635\n",
      "epoch:  473, loss: 0.00678978068754077\n",
      "epoch:  474, loss: 0.006788251921534538\n",
      "epoch:  475, loss: 0.006787445396184921\n",
      "epoch:  476, loss: 0.006784840486943722\n",
      "epoch:  477, loss: 0.006779860705137253\n",
      "epoch:  478, loss: 0.006778497714549303\n",
      "epoch:  479, loss: 0.006777692120522261\n",
      "epoch:  480, loss: 0.006776880938559771\n",
      "epoch:  481, loss: 0.006769941188395023\n",
      "epoch:  482, loss: 0.006768697872757912\n",
      "epoch:  483, loss: 0.006767814978957176\n",
      "epoch:  484, loss: 0.006765368860214949\n",
      "epoch:  485, loss: 0.006760255433619022\n",
      "epoch:  486, loss: 0.006758871953934431\n",
      "epoch:  487, loss: 0.006758050061762333\n",
      "epoch:  488, loss: 0.006751949433237314\n",
      "epoch:  489, loss: 0.006750068627297878\n",
      "epoch:  490, loss: 0.006749058607965708\n",
      "epoch:  491, loss: 0.006747457664459944\n",
      "epoch:  492, loss: 0.006741509307175875\n",
      "epoch:  493, loss: 0.006740097887814045\n",
      "epoch:  494, loss: 0.0067392317578196526\n",
      "epoch:  495, loss: 0.006734204012900591\n",
      "epoch:  496, loss: 0.006731335539370775\n",
      "epoch:  497, loss: 0.006730280816555023\n",
      "epoch:  498, loss: 0.006729482673108578\n",
      "epoch:  499, loss: 0.006723803002387285\n",
      "epoch:  500, loss: 0.006721442565321922\n",
      "epoch:  501, loss: 0.006720494478940964\n",
      "epoch:  502, loss: 0.0067179459147155285\n",
      "epoch:  503, loss: 0.006712586618959904\n",
      "epoch:  504, loss: 0.006711240392178297\n",
      "epoch:  505, loss: 0.006710353773087263\n",
      "epoch:  506, loss: 0.006703568622469902\n",
      "epoch:  507, loss: 0.006701475940644741\n",
      "epoch:  508, loss: 0.006700510624796152\n",
      "epoch:  509, loss: 0.006695910356938839\n",
      "epoch:  510, loss: 0.006691838148981333\n",
      "epoch:  511, loss: 0.006690614856779575\n",
      "epoch:  512, loss: 0.00668971287086606\n",
      "epoch:  513, loss: 0.006682904437184334\n",
      "epoch:  514, loss: 0.006680650636553764\n",
      "epoch:  515, loss: 0.006679624319076538\n",
      "epoch:  516, loss: 0.006675112061202526\n",
      "epoch:  517, loss: 0.006670796312391758\n",
      "epoch:  518, loss: 0.0066694109700620174\n",
      "epoch:  519, loss: 0.0066684577614068985\n",
      "epoch:  520, loss: 0.006661159917712212\n",
      "epoch:  521, loss: 0.006658842787146568\n",
      "epoch:  522, loss: 0.006657756399363279\n",
      "epoch:  523, loss: 0.0066536590456962585\n",
      "epoch:  524, loss: 0.006648363545536995\n",
      "epoch:  525, loss: 0.0066468678414821625\n",
      "epoch:  526, loss: 0.006645881570875645\n",
      "epoch:  527, loss: 0.006638727150857449\n",
      "epoch:  528, loss: 0.006635939236730337\n",
      "epoch:  529, loss: 0.006634780205786228\n",
      "epoch:  530, loss: 0.006629890762269497\n",
      "epoch:  531, loss: 0.006625078152865171\n",
      "epoch:  532, loss: 0.006623533088713884\n",
      "epoch:  533, loss: 0.006622524932026863\n",
      "epoch:  534, loss: 0.006616240367293358\n",
      "epoch:  535, loss: 0.006612782366573811\n",
      "epoch:  536, loss: 0.0066114854998886585\n",
      "epoch:  537, loss: 0.006610489450395107\n",
      "epoch:  538, loss: 0.006603865418583155\n",
      "epoch:  539, loss: 0.006600865628570318\n",
      "epoch:  540, loss: 0.0065995678305625916\n",
      "epoch:  541, loss: 0.006598604843020439\n",
      "epoch:  542, loss: 0.006591195240616798\n",
      "epoch:  543, loss: 0.006588914431631565\n",
      "epoch:  544, loss: 0.006587795447558165\n",
      "epoch:  545, loss: 0.0065855649299919605\n",
      "epoch:  546, loss: 0.006578873377293348\n",
      "epoch:  547, loss: 0.0065771425142884254\n",
      "epoch:  548, loss: 0.00657608499750495\n",
      "epoch:  549, loss: 0.006571096833795309\n",
      "epoch:  550, loss: 0.00656655291095376\n",
      "epoch:  551, loss: 0.00656507583335042\n",
      "epoch:  552, loss: 0.006564059294760227\n",
      "epoch:  553, loss: 0.006557514891028404\n",
      "epoch:  554, loss: 0.0065543148666620255\n",
      "epoch:  555, loss: 0.00655302032828331\n",
      "epoch:  556, loss: 0.0065520405769348145\n",
      "epoch:  557, loss: 0.006544316653162241\n",
      "epoch:  558, loss: 0.006542157847434282\n",
      "epoch:  559, loss: 0.00654102535918355\n",
      "epoch:  560, loss: 0.006537934765219688\n",
      "epoch:  561, loss: 0.006531922612339258\n",
      "epoch:  562, loss: 0.0065302192233502865\n",
      "epoch:  563, loss: 0.006529154721647501\n",
      "epoch:  564, loss: 0.006525265984237194\n",
      "epoch:  565, loss: 0.006519954185932875\n",
      "epoch:  566, loss: 0.006518370471894741\n",
      "epoch:  567, loss: 0.006517329253256321\n",
      "epoch:  568, loss: 0.006511582061648369\n",
      "epoch:  569, loss: 0.006507907994091511\n",
      "epoch:  570, loss: 0.006506522186100483\n",
      "epoch:  571, loss: 0.006505535915493965\n",
      "epoch:  572, loss: 0.006498690694570541\n",
      "epoch:  573, loss: 0.006495763082057238\n",
      "epoch:  574, loss: 0.006494548171758652\n",
      "epoch:  575, loss: 0.006493531167507172\n",
      "epoch:  576, loss: 0.006484900135546923\n",
      "epoch:  577, loss: 0.0064828586764633656\n",
      "epoch:  578, loss: 0.006481701508164406\n",
      "epoch:  579, loss: 0.006479290779680014\n",
      "epoch:  580, loss: 0.006471975240856409\n",
      "epoch:  581, loss: 0.006470001768320799\n",
      "epoch:  582, loss: 0.006468859035521746\n",
      "epoch:  583, loss: 0.006464713253080845\n",
      "epoch:  584, loss: 0.006458894349634647\n",
      "epoch:  585, loss: 0.006457220297306776\n",
      "epoch:  586, loss: 0.006456123664975166\n",
      "epoch:  587, loss: 0.006450401619076729\n",
      "epoch:  588, loss: 0.006445998791605234\n",
      "epoch:  589, loss: 0.00644449470564723\n",
      "epoch:  590, loss: 0.006443443242460489\n",
      "epoch:  591, loss: 0.006436640862375498\n",
      "epoch:  592, loss: 0.0064333039335906506\n",
      "epoch:  593, loss: 0.006431965157389641\n",
      "epoch:  594, loss: 0.006430939305573702\n",
      "epoch:  595, loss: 0.006423060782253742\n",
      "epoch:  596, loss: 0.006420668680220842\n",
      "epoch:  597, loss: 0.006419428624212742\n",
      "epoch:  598, loss: 0.0064183976501226425\n",
      "epoch:  599, loss: 0.006410351954400539\n",
      "epoch:  600, loss: 0.006407944019883871\n",
      "epoch:  601, loss: 0.006406702101230621\n",
      "epoch:  602, loss: 0.006405646447092295\n",
      "epoch:  603, loss: 0.006399084348231554\n",
      "epoch:  604, loss: 0.006395306903868914\n",
      "epoch:  605, loss: 0.006393886171281338\n",
      "epoch:  606, loss: 0.0063928100280463696\n",
      "epoch:  607, loss: 0.006389029324054718\n",
      "epoch:  608, loss: 0.006383225321769714\n",
      "epoch:  609, loss: 0.006381219252943993\n",
      "epoch:  610, loss: 0.006380039267241955\n",
      "epoch:  611, loss: 0.006379005964845419\n",
      "epoch:  612, loss: 0.00637131929397583\n",
      "epoch:  613, loss: 0.006368563044816256\n",
      "epoch:  614, loss: 0.006367271766066551\n",
      "epoch:  615, loss: 0.006366225425153971\n",
      "epoch:  616, loss: 0.0063598728738725185\n",
      "epoch:  617, loss: 0.006356169003993273\n",
      "epoch:  618, loss: 0.006354752462357283\n",
      "epoch:  619, loss: 0.006353710312396288\n",
      "epoch:  620, loss: 0.006349034141749144\n",
      "epoch:  621, loss: 0.006344163324683905\n",
      "epoch:  622, loss: 0.006342561915516853\n",
      "epoch:  623, loss: 0.006341477390378714\n",
      "epoch:  624, loss: 0.00633601751178503\n",
      "epoch:  625, loss: 0.006331735290586948\n",
      "epoch:  626, loss: 0.006330298725515604\n",
      "epoch:  627, loss: 0.006329203024506569\n",
      "epoch:  628, loss: 0.006327504757791758\n",
      "epoch:  629, loss: 0.006320042070001364\n",
      "epoch:  630, loss: 0.006318049505352974\n",
      "epoch:  631, loss: 0.006316859740763903\n",
      "epoch:  632, loss: 0.006315841339528561\n",
      "epoch:  633, loss: 0.006307984236627817\n",
      "epoch:  634, loss: 0.006305554881691933\n",
      "epoch:  635, loss: 0.00630431342869997\n",
      "epoch:  636, loss: 0.006303293164819479\n",
      "epoch:  637, loss: 0.006296063307672739\n",
      "epoch:  638, loss: 0.006293191574513912\n",
      "epoch:  639, loss: 0.006291824858635664\n",
      "epoch:  640, loss: 0.006290755234658718\n",
      "epoch:  641, loss: 0.006283990107476711\n",
      "epoch:  642, loss: 0.006280684377998114\n",
      "epoch:  643, loss: 0.006279217544943094\n",
      "epoch:  644, loss: 0.006278154905885458\n",
      "epoch:  645, loss: 0.006273704580962658\n",
      "epoch:  646, loss: 0.006268750410526991\n",
      "epoch:  647, loss: 0.0062669143080711365\n",
      "epoch:  648, loss: 0.00626574270427227\n",
      "epoch:  649, loss: 0.006265617907047272\n",
      "epoch:  650, loss: 0.006257038563489914\n",
      "epoch:  651, loss: 0.006254665087908506\n",
      "epoch:  652, loss: 0.006253333762288094\n",
      "epoch:  653, loss: 0.00625230185687542\n",
      "epoch:  654, loss: 0.0062491679564118385\n",
      "epoch:  655, loss: 0.006243029143661261\n",
      "epoch:  656, loss: 0.006240983959287405\n",
      "epoch:  657, loss: 0.006239746697247028\n",
      "epoch:  658, loss: 0.006238690111786127\n",
      "epoch:  659, loss: 0.006230818573385477\n",
      "epoch:  660, loss: 0.006228206679224968\n",
      "epoch:  661, loss: 0.006226814351975918\n",
      "epoch:  662, loss: 0.006225715856999159\n",
      "epoch:  663, loss: 0.006219057831913233\n",
      "epoch:  664, loss: 0.006215328816324472\n",
      "epoch:  665, loss: 0.006213738117367029\n",
      "epoch:  666, loss: 0.006212600972503424\n",
      "epoch:  667, loss: 0.006209034007042646\n",
      "epoch:  668, loss: 0.006202859804034233\n",
      "epoch:  669, loss: 0.006200823001563549\n",
      "epoch:  670, loss: 0.0061995661817491055\n",
      "epoch:  671, loss: 0.00619850168004632\n",
      "epoch:  672, loss: 0.006198047194629908\n",
      "epoch:  673, loss: 0.00618945574387908\n",
      "epoch:  674, loss: 0.006186865270137787\n",
      "epoch:  675, loss: 0.006185464095324278\n",
      "epoch:  676, loss: 0.006184351164847612\n",
      "epoch:  677, loss: 0.006183315999805927\n",
      "epoch:  678, loss: 0.006177858449518681\n",
      "epoch:  679, loss: 0.006173276342451572\n",
      "epoch:  680, loss: 0.006171511486172676\n",
      "epoch:  681, loss: 0.006170294247567654\n",
      "epoch:  682, loss: 0.0061692167073488235\n",
      "epoch:  683, loss: 0.0061660632491111755\n",
      "epoch:  684, loss: 0.006159558892250061\n",
      "epoch:  685, loss: 0.0061574457213282585\n",
      "epoch:  686, loss: 0.00615611532703042\n",
      "epoch:  687, loss: 0.0061550447717309\n",
      "epoch:  688, loss: 0.00615273229777813\n",
      "epoch:  689, loss: 0.0061460272409021854\n",
      "epoch:  690, loss: 0.006143489386886358\n",
      "epoch:  691, loss: 0.006142166908830404\n",
      "epoch:  692, loss: 0.006141018588095903\n",
      "epoch:  693, loss: 0.006140011828392744\n",
      "epoch:  694, loss: 0.00613732635974884\n",
      "epoch:  695, loss: 0.006130891852080822\n",
      "epoch:  696, loss: 0.006128301378339529\n",
      "epoch:  697, loss: 0.006126896478235722\n",
      "epoch:  698, loss: 0.006125733722001314\n",
      "epoch:  699, loss: 0.006124884821474552\n",
      "epoch:  700, loss: 0.006116420030593872\n",
      "epoch:  701, loss: 0.006113606505095959\n",
      "epoch:  702, loss: 0.006112098693847656\n",
      "epoch:  703, loss: 0.006110952235758305\n",
      "epoch:  704, loss: 0.006106709595769644\n",
      "epoch:  705, loss: 0.006101374980062246\n",
      "epoch:  706, loss: 0.006099019665271044\n",
      "epoch:  707, loss: 0.006097681354731321\n",
      "epoch:  708, loss: 0.006096578203141689\n",
      "epoch:  709, loss: 0.006093666888773441\n",
      "epoch:  710, loss: 0.006087158340960741\n",
      "epoch:  711, loss: 0.006084749940782785\n",
      "epoch:  712, loss: 0.006083416752517223\n",
      "epoch:  713, loss: 0.006082281935960054\n",
      "epoch:  714, loss: 0.006078141741454601\n",
      "epoch:  715, loss: 0.006072611548006535\n",
      "epoch:  716, loss: 0.006070475559681654\n",
      "epoch:  717, loss: 0.006069159600883722\n",
      "epoch:  718, loss: 0.006068054120987654\n",
      "epoch:  719, loss: 0.006061783526092768\n",
      "epoch:  720, loss: 0.006057868245989084\n",
      "epoch:  721, loss: 0.0060560801066458225\n",
      "epoch:  722, loss: 0.006054845172911882\n",
      "epoch:  723, loss: 0.00605378020554781\n",
      "epoch:  724, loss: 0.0060458979569375515\n",
      "epoch:  725, loss: 0.0060432469472289085\n",
      "epoch:  726, loss: 0.006041810382157564\n",
      "epoch:  727, loss: 0.00604073703289032\n",
      "epoch:  728, loss: 0.006035094149410725\n",
      "epoch:  729, loss: 0.006030541379004717\n",
      "epoch:  730, loss: 0.006028755567967892\n",
      "epoch:  731, loss: 0.006027456372976303\n",
      "epoch:  732, loss: 0.006026326213032007\n",
      "epoch:  733, loss: 0.006021976936608553\n",
      "epoch:  734, loss: 0.006016337778419256\n",
      "epoch:  735, loss: 0.006014037411659956\n",
      "epoch:  736, loss: 0.006012695841491222\n",
      "epoch:  737, loss: 0.006011519115418196\n",
      "epoch:  738, loss: 0.006010863464325666\n",
      "epoch:  739, loss: 0.006002515554428101\n",
      "epoch:  740, loss: 0.0059996661730110645\n",
      "epoch:  741, loss: 0.005998045671731234\n",
      "epoch:  742, loss: 0.005996877793222666\n",
      "epoch:  743, loss: 0.005995794199407101\n",
      "epoch:  744, loss: 0.005990948062390089\n",
      "epoch:  745, loss: 0.005985940340906382\n",
      "epoch:  746, loss: 0.005983987357467413\n",
      "epoch:  747, loss: 0.00598259037360549\n",
      "epoch:  748, loss: 0.005981462076306343\n",
      "epoch:  749, loss: 0.005980405490845442\n",
      "epoch:  750, loss: 0.005974020808935165\n",
      "epoch:  751, loss: 0.005970127414911985\n",
      "epoch:  752, loss: 0.005968343932181597\n",
      "epoch:  753, loss: 0.005967055447399616\n",
      "epoch:  754, loss: 0.00596591318026185\n",
      "epoch:  755, loss: 0.005965388845652342\n",
      "epoch:  756, loss: 0.005957146640866995\n",
      "epoch:  757, loss: 0.0059541380032896996\n",
      "epoch:  758, loss: 0.005952469073235989\n",
      "epoch:  759, loss: 0.005951257888227701\n",
      "epoch:  760, loss: 0.005950129125267267\n",
      "epoch:  761, loss: 0.005945868324488401\n",
      "epoch:  762, loss: 0.005940527655184269\n",
      "epoch:  763, loss: 0.005938259419053793\n",
      "epoch:  764, loss: 0.005936841946095228\n",
      "epoch:  765, loss: 0.005935692694038153\n",
      "epoch:  766, loss: 0.005934617482125759\n",
      "epoch:  767, loss: 0.005932912230491638\n",
      "epoch:  768, loss: 0.00592587236315012\n",
      "epoch:  769, loss: 0.005923116579651833\n",
      "epoch:  770, loss: 0.005921574775129557\n",
      "epoch:  771, loss: 0.005920397583395243\n",
      "epoch:  772, loss: 0.005919319111853838\n",
      "epoch:  773, loss: 0.005919043440371752\n",
      "epoch:  774, loss: 0.005911088548600674\n",
      "epoch:  775, loss: 0.005908028222620487\n",
      "epoch:  776, loss: 0.005906364414840937\n",
      "epoch:  777, loss: 0.005905182100832462\n",
      "epoch:  778, loss: 0.005904083140194416\n",
      "epoch:  779, loss: 0.005899934563785791\n",
      "epoch:  780, loss: 0.005894643720239401\n",
      "epoch:  781, loss: 0.0058923326432704926\n",
      "epoch:  782, loss: 0.005890867672860622\n",
      "epoch:  783, loss: 0.0058896588161587715\n",
      "epoch:  784, loss: 0.005888490471988916\n",
      "epoch:  785, loss: 0.005882476456463337\n",
      "epoch:  786, loss: 0.005877941846847534\n",
      "epoch:  787, loss: 0.005875794682651758\n",
      "epoch:  788, loss: 0.005874385591596365\n",
      "epoch:  789, loss: 0.005873199552297592\n",
      "epoch:  790, loss: 0.005872087553143501\n",
      "epoch:  791, loss: 0.005865146405994892\n",
      "epoch:  792, loss: 0.005861193407326937\n",
      "epoch:  793, loss: 0.005859119817614555\n",
      "epoch:  794, loss: 0.005857653450220823\n",
      "epoch:  795, loss: 0.005856378003954887\n",
      "epoch:  796, loss: 0.005855170544236898\n",
      "epoch:  797, loss: 0.005848809145390987\n",
      "epoch:  798, loss: 0.005844227969646454\n",
      "epoch:  799, loss: 0.0058420151472091675\n",
      "epoch:  800, loss: 0.005840523634105921\n",
      "epoch:  801, loss: 0.005839271936565638\n",
      "epoch:  802, loss: 0.005838092416524887\n",
      "epoch:  803, loss: 0.005834923125803471\n",
      "epoch:  804, loss: 0.005828327499330044\n",
      "epoch:  805, loss: 0.00582543620839715\n",
      "epoch:  806, loss: 0.00582372909411788\n",
      "epoch:  807, loss: 0.005822407081723213\n",
      "epoch:  808, loss: 0.005821209866553545\n",
      "epoch:  809, loss: 0.005820057354867458\n",
      "epoch:  810, loss: 0.005813873838633299\n",
      "epoch:  811, loss: 0.005809250753372908\n",
      "epoch:  812, loss: 0.00580701045691967\n",
      "epoch:  813, loss: 0.005805454216897488\n",
      "epoch:  814, loss: 0.005804189946502447\n",
      "epoch:  815, loss: 0.005802998784929514\n",
      "epoch:  816, loss: 0.005801855120807886\n",
      "epoch:  817, loss: 0.00579639058560133\n",
      "epoch:  818, loss: 0.005791445728391409\n",
      "epoch:  819, loss: 0.005789097398519516\n",
      "epoch:  820, loss: 0.005787556059658527\n",
      "epoch:  821, loss: 0.005786329507827759\n",
      "epoch:  822, loss: 0.005785169545561075\n",
      "epoch:  823, loss: 0.005781116429716349\n",
      "epoch:  824, loss: 0.005775353871285915\n",
      "epoch:  825, loss: 0.0057726954109966755\n",
      "epoch:  826, loss: 0.005771002732217312\n",
      "epoch:  827, loss: 0.005769694689661264\n",
      "epoch:  828, loss: 0.0057684979401528835\n",
      "epoch:  829, loss: 0.005767356604337692\n",
      "epoch:  830, loss: 0.005762466229498386\n",
      "epoch:  831, loss: 0.005757340230047703\n",
      "epoch:  832, loss: 0.005754797253757715\n",
      "epoch:  833, loss: 0.005753210745751858\n",
      "epoch:  834, loss: 0.005751900374889374\n",
      "epoch:  835, loss: 0.005750703159719706\n",
      "epoch:  836, loss: 0.005749570205807686\n",
      "epoch:  837, loss: 0.005745946429669857\n",
      "epoch:  838, loss: 0.005740053486078978\n",
      "epoch:  839, loss: 0.005737335421144962\n",
      "epoch:  840, loss: 0.005735552869737148\n",
      "epoch:  841, loss: 0.005734279751777649\n",
      "epoch:  842, loss: 0.005733071360737085\n",
      "epoch:  843, loss: 0.005731921177357435\n",
      "epoch:  844, loss: 0.005728455726057291\n",
      "epoch:  845, loss: 0.005722532048821449\n",
      "epoch:  846, loss: 0.005719728302210569\n",
      "epoch:  847, loss: 0.00571803655475378\n",
      "epoch:  848, loss: 0.0057166991755366325\n",
      "epoch:  849, loss: 0.005715543404221535\n",
      "epoch:  850, loss: 0.005714399740099907\n",
      "epoch:  851, loss: 0.005708748009055853\n",
      "epoch:  852, loss: 0.005704253911972046\n",
      "epoch:  853, loss: 0.005701965186744928\n",
      "epoch:  854, loss: 0.0057004461996257305\n",
      "epoch:  855, loss: 0.005699189845472574\n",
      "epoch:  856, loss: 0.005698017310351133\n",
      "epoch:  857, loss: 0.0056951818987727165\n",
      "epoch:  858, loss: 0.005688984412699938\n",
      "epoch:  859, loss: 0.0056860968470573425\n",
      "epoch:  860, loss: 0.005684379022568464\n",
      "epoch:  861, loss: 0.005683037918061018\n",
      "epoch:  862, loss: 0.005681883078068495\n",
      "epoch:  863, loss: 0.0056807659566402435\n",
      "epoch:  864, loss: 0.005679266527295113\n",
      "epoch:  865, loss: 0.005672447383403778\n",
      "epoch:  866, loss: 0.0056692492216825485\n",
      "epoch:  867, loss: 0.005667431280016899\n",
      "epoch:  868, loss: 0.005666038952767849\n",
      "epoch:  869, loss: 0.0056648231111466885\n",
      "epoch:  870, loss: 0.005663683172315359\n",
      "epoch:  871, loss: 0.00566092599183321\n",
      "epoch:  872, loss: 0.005655175540596247\n",
      "epoch:  873, loss: 0.005652217660099268\n",
      "epoch:  874, loss: 0.005650452338159084\n",
      "epoch:  875, loss: 0.005649087484925985\n",
      "epoch:  876, loss: 0.00564788980409503\n",
      "epoch:  877, loss: 0.00564677594229579\n",
      "epoch:  878, loss: 0.005645699333399534\n",
      "epoch:  879, loss: 0.005644634831696749\n",
      "epoch:  880, loss: 0.005639228504151106\n",
      "epoch:  881, loss: 0.005635228008031845\n",
      "epoch:  882, loss: 0.005632943939417601\n",
      "epoch:  883, loss: 0.005631428211927414\n",
      "epoch:  884, loss: 0.005630179308354855\n",
      "epoch:  885, loss: 0.0056290519423782825\n",
      "epoch:  886, loss: 0.005627977661788464\n",
      "epoch:  887, loss: 0.0056279730051755905\n",
      "epoch:  888, loss: 0.005620845593512058\n",
      "epoch:  889, loss: 0.005617250222712755\n",
      "epoch:  890, loss: 0.005615160334855318\n",
      "epoch:  891, loss: 0.0056137447245419025\n",
      "epoch:  892, loss: 0.005612518638372421\n",
      "epoch:  893, loss: 0.0056114112958312035\n",
      "epoch:  894, loss: 0.0056103430688381195\n",
      "epoch:  895, loss: 0.005607367493212223\n",
      "epoch:  896, loss: 0.005602134857326746\n",
      "epoch:  897, loss: 0.0055992803536355495\n",
      "epoch:  898, loss: 0.005597575567662716\n",
      "epoch:  899, loss: 0.005596193019300699\n",
      "epoch:  900, loss: 0.005595028400421143\n",
      "epoch:  901, loss: 0.005593958776444197\n",
      "epoch:  902, loss: 0.005592903587967157\n",
      "epoch:  903, loss: 0.005587159190326929\n",
      "epoch:  904, loss: 0.005583765916526318\n",
      "epoch:  905, loss: 0.005581592209637165\n",
      "epoch:  906, loss: 0.005580087658017874\n",
      "epoch:  907, loss: 0.005578828975558281\n",
      "epoch:  908, loss: 0.005577703472226858\n",
      "epoch:  909, loss: 0.005576649215072393\n",
      "epoch:  910, loss: 0.005575618706643581\n",
      "epoch:  911, loss: 0.005571385379880667\n",
      "epoch:  912, loss: 0.005567197222262621\n",
      "epoch:  913, loss: 0.005564747843891382\n",
      "epoch:  914, loss: 0.0055631049908697605\n",
      "epoch:  915, loss: 0.00556185282766819\n",
      "epoch:  916, loss: 0.0055607077665627\n",
      "epoch:  917, loss: 0.005559656303375959\n",
      "epoch:  918, loss: 0.005558620672672987\n",
      "epoch:  919, loss: 0.0055573247373104095\n",
      "epoch:  920, loss: 0.005551534704864025\n",
      "epoch:  921, loss: 0.005548391025513411\n",
      "epoch:  922, loss: 0.0055464147590100765\n",
      "epoch:  923, loss: 0.005544972140341997\n",
      "epoch:  924, loss: 0.0055437516421079636\n",
      "epoch:  925, loss: 0.005542672239243984\n",
      "epoch:  926, loss: 0.005541617516428232\n",
      "epoch:  927, loss: 0.0055405995808541775\n",
      "epoch:  928, loss: 0.0055378801189363\n",
      "epoch:  929, loss: 0.005532907322049141\n",
      "epoch:  930, loss: 0.005530240945518017\n",
      "epoch:  931, loss: 0.005528478883206844\n",
      "epoch:  932, loss: 0.005527062341570854\n",
      "epoch:  933, loss: 0.0055258688516914845\n",
      "epoch:  934, loss: 0.005524786654859781\n",
      "epoch:  935, loss: 0.005523746367543936\n",
      "epoch:  936, loss: 0.005522726569324732\n",
      "epoch:  937, loss: 0.005521808750927448\n",
      "epoch:  938, loss: 0.0055160620249807835\n",
      "epoch:  939, loss: 0.005512935575097799\n",
      "epoch:  940, loss: 0.005510943941771984\n",
      "epoch:  941, loss: 0.005509538110345602\n",
      "epoch:  942, loss: 0.005508283153176308\n",
      "epoch:  943, loss: 0.0055071827955543995\n",
      "epoch:  944, loss: 0.005506127141416073\n",
      "epoch:  945, loss: 0.005505112931132317\n",
      "epoch:  946, loss: 0.005504115484654903\n",
      "epoch:  947, loss: 0.005500131752341986\n",
      "epoch:  948, loss: 0.005496207159012556\n",
      "epoch:  949, loss: 0.005493811331689358\n",
      "epoch:  950, loss: 0.005492121912539005\n",
      "epoch:  951, loss: 0.005490847863256931\n",
      "epoch:  952, loss: 0.00548963900655508\n",
      "epoch:  953, loss: 0.005488553550094366\n",
      "epoch:  954, loss: 0.005487506277859211\n",
      "epoch:  955, loss: 0.005486494395881891\n",
      "epoch:  956, loss: 0.00548548111692071\n",
      "epoch:  957, loss: 0.0054809460416436195\n",
      "epoch:  958, loss: 0.005477369297295809\n",
      "epoch:  959, loss: 0.0054751206189394\n",
      "epoch:  960, loss: 0.005473438184708357\n",
      "epoch:  961, loss: 0.005472153425216675\n",
      "epoch:  962, loss: 0.005470998119562864\n",
      "epoch:  963, loss: 0.005469919182360172\n",
      "epoch:  964, loss: 0.0054689026437699795\n",
      "epoch:  965, loss: 0.005467889364808798\n",
      "epoch:  966, loss: 0.005467045120894909\n",
      "epoch:  967, loss: 0.005461548920720816\n",
      "epoch:  968, loss: 0.00545847462490201\n",
      "epoch:  969, loss: 0.00545635586604476\n",
      "epoch:  970, loss: 0.005454810336232185\n",
      "epoch:  971, loss: 0.005453556776046753\n",
      "epoch:  972, loss: 0.005452424753457308\n",
      "epoch:  973, loss: 0.005451377481222153\n",
      "epoch:  974, loss: 0.005450352095067501\n",
      "epoch:  975, loss: 0.005449350923299789\n",
      "epoch:  976, loss: 0.005448353011161089\n",
      "epoch:  977, loss: 0.0054441289976239204\n",
      "epoch:  978, loss: 0.005440174136310816\n",
      "epoch:  979, loss: 0.005437982734292746\n",
      "epoch:  980, loss: 0.005436433479189873\n",
      "epoch:  981, loss: 0.0054351151920855045\n",
      "epoch:  982, loss: 0.005433989688754082\n",
      "epoch:  983, loss: 0.005432906094938517\n",
      "epoch:  984, loss: 0.005431887228041887\n",
      "epoch:  985, loss: 0.005430894438177347\n",
      "epoch:  986, loss: 0.005429896991699934\n",
      "epoch:  987, loss: 0.005428926087915897\n",
      "epoch:  988, loss: 0.005428669508546591\n",
      "epoch:  989, loss: 0.005423098802566528\n",
      "epoch:  990, loss: 0.005419570486992598\n",
      "epoch:  991, loss: 0.005417586304247379\n",
      "epoch:  992, loss: 0.005416105501353741\n",
      "epoch:  993, loss: 0.005414831452071667\n",
      "epoch:  994, loss: 0.005413717124611139\n",
      "epoch:  995, loss: 0.005412654019892216\n",
      "epoch:  996, loss: 0.0054116216488182545\n",
      "epoch:  997, loss: 0.005410634912550449\n",
      "epoch:  998, loss: 0.005409657955169678\n",
      "epoch:  999, loss: 0.005408685188740492\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size=X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "opt = torch_numopt.GradientDescentLS(model=model, lr=1, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"armijo\")\n",
    "# opt = torch_numopt.GradientDescentLS(model=model, lr=1, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"wolfe\")\n",
    "# opt = torch_numopt.GradientDescentLS(model=model, lr=1, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"strong-wolfe\")\n",
    "# opt = torch_numopt.GradientDescentLS(model=model, lr=1, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"goldstein\")\n",
    "\n",
    "all_loss = {}\n",
    "for epoch in range(1000):\n",
    "    print(\"epoch: \", epoch, end=\"\")\n",
    "    all_loss[epoch + 1] = 0\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y, loss_fn)\n",
    "\n",
    "        all_loss[epoch + 1] += loss\n",
    "    all_loss[epoch + 1] /= len(data_loader)\n",
    "    print(\", loss: {}\".format(all_loss[epoch + 1].cpu().detach().numpy().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics: R2 = 0.8188966220469371\n",
      "Test metrics:  R2 = 0.8179251142007707\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "pred_train = model.forward(X_train).cpu().detach()\n",
    "pred_test = model.forward(X_test).cpu().detach()\n",
    "print(f\"Train metrics: R2 = {r2_score(pred_train, y_train.cpu())}\")\n",
    "print(f\"Test metrics:  R2 = {r2_score(pred_test, y_test.cpu())}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_numopt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
