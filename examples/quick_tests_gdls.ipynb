{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch_numopt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import *\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.f1 = nn.Linear(input_size, 10, device=device)\n",
    "        self.f2 = nn.Linear(10, 20, device=device)\n",
    "        self.f3 = nn.Linear(20, 20, device=device)\n",
    "        self.f4 = nn.Linear(20, 10, device=device)\n",
    "        self.f5 = nn.Linear(10, 1, device=device)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        # self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.f1(x))\n",
    "        x = self.activation(self.f2(x))\n",
    "        x = self.activation(self.f3(x))\n",
    "        x = self.activation(self.f4(x))\n",
    "        x = self.f5(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([800, 10]) torch.Size([800, 1])\n",
      "torch.Size([200, 10]) torch.Size([200, 1])\n"
     ]
    }
   ],
   "source": [
    "# X, y = load_diabetes(return_X_y=True, scaled=False)\n",
    "# X, y = make_regression(n_samples=1000, n_features=100)\n",
    "X, y = make_friedman1(n_samples=1000, noise=1e-2)\n",
    "\n",
    "X_scaler = MinMaxScaler()\n",
    "X = X_scaler.fit_transform(X)\n",
    "X = torch.Tensor(X).to(device)\n",
    "\n",
    "y_scaler = MinMaxScaler()\n",
    "y = y_scaler.fit_transform(y.reshape((-1, 1)))\n",
    "y = torch.Tensor(y).to(device)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "torch_data = TensorDataset(X_train, y_train)\n",
    "data_loader = DataLoader(torch_data, batch_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No Line search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.6761481165885925\n",
      "epoch:  1, loss: 0.657284140586853\n",
      "epoch:  2, loss: 0.6391590237617493\n",
      "epoch:  3, loss: 0.6217473745346069\n",
      "epoch:  4, loss: 0.6050043702125549\n",
      "epoch:  5, loss: 0.5888789296150208\n",
      "epoch:  6, loss: 0.5733609199523926\n",
      "epoch:  7, loss: 0.5584023594856262\n",
      "epoch:  8, loss: 0.5439767241477966\n",
      "epoch:  9, loss: 0.5300385355949402\n",
      "epoch:  10, loss: 0.5165624022483826\n",
      "epoch:  11, loss: 0.5035180449485779\n",
      "epoch:  12, loss: 0.49087804555892944\n",
      "epoch:  13, loss: 0.4786328673362732\n",
      "epoch:  14, loss: 0.46675780415534973\n",
      "epoch:  15, loss: 0.455228328704834\n",
      "epoch:  16, loss: 0.44402986764907837\n",
      "epoch:  17, loss: 0.43314898014068604\n",
      "epoch:  18, loss: 0.42257553339004517\n",
      "epoch:  19, loss: 0.41229602694511414\n",
      "epoch:  20, loss: 0.40230056643486023\n",
      "epoch:  21, loss: 0.3925756812095642\n",
      "epoch:  22, loss: 0.38311567902565\n",
      "epoch:  23, loss: 0.3739073872566223\n",
      "epoch:  24, loss: 0.3649451434612274\n",
      "epoch:  25, loss: 0.3562214970588684\n",
      "epoch:  26, loss: 0.34772858023643494\n",
      "epoch:  27, loss: 0.33945852518081665\n",
      "epoch:  28, loss: 0.33140289783477783\n",
      "epoch:  29, loss: 0.3235568404197693\n",
      "epoch:  30, loss: 0.3159152865409851\n",
      "epoch:  31, loss: 0.3084714710712433\n",
      "epoch:  32, loss: 0.30122077465057373\n",
      "epoch:  33, loss: 0.29415836930274963\n",
      "epoch:  34, loss: 0.28727805614471436\n",
      "epoch:  35, loss: 0.28057661652565\n",
      "epoch:  36, loss: 0.2740480601787567\n",
      "epoch:  37, loss: 0.26768893003463745\n",
      "epoch:  38, loss: 0.2614954710006714\n",
      "epoch:  39, loss: 0.255462646484375\n",
      "epoch:  40, loss: 0.24958528578281403\n",
      "epoch:  41, loss: 0.2438586801290512\n",
      "epoch:  42, loss: 0.23827961087226868\n",
      "epoch:  43, loss: 0.23284472525119781\n",
      "epoch:  44, loss: 0.22755032777786255\n",
      "epoch:  45, loss: 0.22239238023757935\n",
      "epoch:  46, loss: 0.2173672765493393\n",
      "epoch:  47, loss: 0.21247157454490662\n",
      "epoch:  48, loss: 0.20770224928855896\n",
      "epoch:  49, loss: 0.20305591821670532\n",
      "epoch:  50, loss: 0.19852951169013977\n",
      "epoch:  51, loss: 0.19412007927894592\n",
      "epoch:  52, loss: 0.18982508778572083\n",
      "epoch:  53, loss: 0.18564124405384064\n",
      "epoch:  54, loss: 0.18156535923480988\n",
      "epoch:  55, loss: 0.17759495973587036\n",
      "epoch:  56, loss: 0.1737273782491684\n",
      "epoch:  57, loss: 0.16996002197265625\n",
      "epoch:  58, loss: 0.16629058122634888\n",
      "epoch:  59, loss: 0.16271667182445526\n",
      "epoch:  60, loss: 0.1592358946800232\n",
      "epoch:  61, loss: 0.15584561228752136\n",
      "epoch:  62, loss: 0.15254369378089905\n",
      "epoch:  63, loss: 0.1493283361196518\n",
      "epoch:  64, loss: 0.14619706571102142\n",
      "epoch:  65, loss: 0.1431475430727005\n",
      "epoch:  66, loss: 0.14017780125141144\n",
      "epoch:  67, loss: 0.13728587329387665\n",
      "epoch:  68, loss: 0.13446970283985138\n",
      "epoch:  69, loss: 0.13172754645347595\n",
      "epoch:  70, loss: 0.1290576457977295\n",
      "epoch:  71, loss: 0.1264583021402359\n",
      "epoch:  72, loss: 0.12392783910036087\n",
      "epoch:  73, loss: 0.12146420031785965\n",
      "epoch:  74, loss: 0.1190657764673233\n",
      "epoch:  75, loss: 0.11673086136579514\n",
      "epoch:  76, loss: 0.11445819586515427\n",
      "epoch:  77, loss: 0.11224625259637833\n",
      "epoch:  78, loss: 0.11009327322244644\n",
      "epoch:  79, loss: 0.10799796134233475\n",
      "epoch:  80, loss: 0.10595844686031342\n",
      "epoch:  81, loss: 0.10397356003522873\n",
      "epoch:  82, loss: 0.10204185545444489\n",
      "epoch:  83, loss: 0.10016223043203354\n",
      "epoch:  84, loss: 0.09833288937807083\n",
      "epoch:  85, loss: 0.09655298292636871\n",
      "epoch:  86, loss: 0.09482163190841675\n",
      "epoch:  87, loss: 0.09313720464706421\n",
      "epoch:  88, loss: 0.09149811416864395\n",
      "epoch:  89, loss: 0.0899030789732933\n",
      "epoch:  90, loss: 0.08835110813379288\n",
      "epoch:  91, loss: 0.08684104681015015\n",
      "epoch:  92, loss: 0.0853717178106308\n",
      "epoch:  93, loss: 0.08394241333007812\n",
      "epoch:  94, loss: 0.08255196362733841\n",
      "epoch:  95, loss: 0.0811995416879654\n",
      "epoch:  96, loss: 0.07988394796848297\n",
      "epoch:  97, loss: 0.07860387861728668\n",
      "epoch:  98, loss: 0.07735852152109146\n",
      "epoch:  99, loss: 0.07614713162183762\n",
      "epoch:  100, loss: 0.07496865838766098\n",
      "epoch:  101, loss: 0.07382233440876007\n",
      "epoch:  102, loss: 0.07270728051662445\n",
      "epoch:  103, loss: 0.07162250578403473\n",
      "epoch:  104, loss: 0.07056721299886703\n",
      "epoch:  105, loss: 0.06954047083854675\n",
      "epoch:  106, loss: 0.06854163110256195\n",
      "epoch:  107, loss: 0.06757032871246338\n",
      "epoch:  108, loss: 0.06662577390670776\n",
      "epoch:  109, loss: 0.06570707261562347\n",
      "epoch:  110, loss: 0.0648135244846344\n",
      "epoch:  111, loss: 0.0639444962143898\n",
      "epoch:  112, loss: 0.06309937685728073\n",
      "epoch:  113, loss: 0.06227750703692436\n",
      "epoch:  114, loss: 0.061478350311517715\n",
      "epoch:  115, loss: 0.06070125475525856\n",
      "epoch:  116, loss: 0.05994560942053795\n",
      "epoch:  117, loss: 0.05921086296439171\n",
      "epoch:  118, loss: 0.05849650129675865\n",
      "epoch:  119, loss: 0.0578019842505455\n",
      "epoch:  120, loss: 0.057126715779304504\n",
      "epoch:  121, loss: 0.05647021159529686\n",
      "epoch:  122, loss: 0.05583195388317108\n",
      "epoch:  123, loss: 0.05521141737699509\n",
      "epoch:  124, loss: 0.05460797995328903\n",
      "epoch:  125, loss: 0.054021261632442474\n",
      "epoch:  126, loss: 0.05345088988542557\n",
      "epoch:  127, loss: 0.05289645120501518\n",
      "epoch:  128, loss: 0.052357569336891174\n",
      "epoch:  129, loss: 0.05183372274041176\n",
      "epoch:  130, loss: 0.05132446810603142\n",
      "epoch:  131, loss: 0.05082947015762329\n",
      "epoch:  132, loss: 0.05034850165247917\n",
      "epoch:  133, loss: 0.04988093674182892\n",
      "epoch:  134, loss: 0.049426451325416565\n",
      "epoch:  135, loss: 0.04898468777537346\n",
      "epoch:  136, loss: 0.048555295914411545\n",
      "epoch:  137, loss: 0.048137910664081573\n",
      "epoch:  138, loss: 0.0477321520447731\n",
      "epoch:  139, loss: 0.04733777046203613\n",
      "epoch:  140, loss: 0.04695446044206619\n",
      "epoch:  141, loss: 0.0465819351375103\n",
      "epoch:  142, loss: 0.04621991142630577\n",
      "epoch:  143, loss: 0.045868054032325745\n",
      "epoch:  144, loss: 0.045526083558797836\n",
      "epoch:  145, loss: 0.04519372060894966\n",
      "epoch:  146, loss: 0.04487072303891182\n",
      "epoch:  147, loss: 0.04455689340829849\n",
      "epoch:  148, loss: 0.04425206407904625\n",
      "epoch:  149, loss: 0.04395585134625435\n",
      "epoch:  150, loss: 0.04366806894540787\n",
      "epoch:  151, loss: 0.043388500809669495\n",
      "epoch:  152, loss: 0.043116889894008636\n",
      "epoch:  153, loss: 0.042853016406297684\n",
      "epoch:  154, loss: 0.04259663447737694\n",
      "epoch:  155, loss: 0.042347535490989685\n",
      "epoch:  156, loss: 0.04210551083087921\n",
      "epoch:  157, loss: 0.041870396584272385\n",
      "epoch:  158, loss: 0.0416419692337513\n",
      "epoch:  159, loss: 0.04142000898718834\n",
      "epoch:  160, loss: 0.041204337030649185\n",
      "epoch:  161, loss: 0.04099487140774727\n",
      "epoch:  162, loss: 0.0407913476228714\n",
      "epoch:  163, loss: 0.04059359058737755\n",
      "epoch:  164, loss: 0.04040146246552467\n",
      "epoch:  165, loss: 0.04021478071808815\n",
      "epoch:  166, loss: 0.040033381432294846\n",
      "epoch:  167, loss: 0.0398571714758873\n",
      "epoch:  168, loss: 0.03968597948551178\n",
      "epoch:  169, loss: 0.039519667625427246\n",
      "epoch:  170, loss: 0.03935810178518295\n",
      "epoch:  171, loss: 0.039201173931360245\n",
      "epoch:  172, loss: 0.039048705250024796\n",
      "epoch:  173, loss: 0.03890061751008034\n",
      "epoch:  174, loss: 0.03875676542520523\n",
      "epoch:  175, loss: 0.03861701861023903\n",
      "epoch:  176, loss: 0.03848128020763397\n",
      "epoch:  177, loss: 0.03834942728281021\n",
      "epoch:  178, loss: 0.038221295922994614\n",
      "epoch:  179, loss: 0.038096833974123\n",
      "epoch:  180, loss: 0.037975940853357315\n",
      "epoch:  181, loss: 0.0378585122525692\n",
      "epoch:  182, loss: 0.0377444289624691\n",
      "epoch:  183, loss: 0.03763361647725105\n",
      "epoch:  184, loss: 0.037525974214076996\n",
      "epoch:  185, loss: 0.03742139786481857\n",
      "epoch:  186, loss: 0.03731982409954071\n",
      "epoch:  187, loss: 0.03722114861011505\n",
      "epoch:  188, loss: 0.03712528571486473\n",
      "epoch:  189, loss: 0.03703216463327408\n",
      "epoch:  190, loss: 0.03694172948598862\n",
      "epoch:  191, loss: 0.036853890866041183\n",
      "epoch:  192, loss: 0.03676857426762581\n",
      "epoch:  193, loss: 0.03668570518493652\n",
      "epoch:  194, loss: 0.036605190485715866\n",
      "epoch:  195, loss: 0.036527011543512344\n",
      "epoch:  196, loss: 0.036451082676649094\n",
      "epoch:  197, loss: 0.03637733682990074\n",
      "epoch:  198, loss: 0.03630569204688072\n",
      "epoch:  199, loss: 0.03623609617352486\n",
      "epoch:  200, loss: 0.036168523132801056\n",
      "epoch:  201, loss: 0.03610286861658096\n",
      "epoch:  202, loss: 0.036039046943187714\n",
      "epoch:  203, loss: 0.03597703203558922\n",
      "epoch:  204, loss: 0.035916782915592194\n",
      "epoch:  205, loss: 0.03585820272564888\n",
      "epoch:  206, loss: 0.03580131381750107\n",
      "epoch:  207, loss: 0.03574610501527786\n",
      "epoch:  208, loss: 0.03569246828556061\n",
      "epoch:  209, loss: 0.03564035892486572\n",
      "epoch:  210, loss: 0.03558973968029022\n",
      "epoch:  211, loss: 0.035540565848350525\n",
      "epoch:  212, loss: 0.035492777824401855\n",
      "epoch:  213, loss: 0.03544636443257332\n",
      "epoch:  214, loss: 0.035401295870542526\n",
      "epoch:  215, loss: 0.035357534885406494\n",
      "epoch:  216, loss: 0.03531506285071373\n",
      "epoch:  217, loss: 0.03527381643652916\n",
      "epoch:  218, loss: 0.0352337546646595\n",
      "epoch:  219, loss: 0.035194844007492065\n",
      "epoch:  220, loss: 0.03515704721212387\n",
      "epoch:  221, loss: 0.03512030467391014\n",
      "epoch:  222, loss: 0.03508460894227028\n",
      "epoch:  223, loss: 0.03504996374249458\n",
      "epoch:  224, loss: 0.03501631319522858\n",
      "epoch:  225, loss: 0.03498362377285957\n",
      "epoch:  226, loss: 0.03495185077190399\n",
      "epoch:  227, loss: 0.034920964390039444\n",
      "epoch:  228, loss: 0.034890953451395035\n",
      "epoch:  229, loss: 0.03486179932951927\n",
      "epoch:  230, loss: 0.034833475947380066\n",
      "epoch:  231, loss: 0.03480595722794533\n",
      "epoch:  232, loss: 0.034779198467731476\n",
      "epoch:  233, loss: 0.03475319221615791\n",
      "epoch:  234, loss: 0.03472791612148285\n",
      "epoch:  235, loss: 0.034703340381383896\n",
      "epoch:  236, loss: 0.03467945381999016\n",
      "epoch:  237, loss: 0.03465624153614044\n",
      "epoch:  238, loss: 0.03463369235396385\n",
      "epoch:  239, loss: 0.03461175411939621\n",
      "epoch:  240, loss: 0.03459042310714722\n",
      "epoch:  241, loss: 0.03456968069076538\n",
      "epoch:  242, loss: 0.03454950824379921\n",
      "epoch:  243, loss: 0.03452988713979721\n",
      "epoch:  244, loss: 0.0345107726752758\n",
      "epoch:  245, loss: 0.03449217602610588\n",
      "epoch:  246, loss: 0.03447408229112625\n",
      "epoch:  247, loss: 0.03445649519562721\n",
      "epoch:  248, loss: 0.03443940728902817\n",
      "epoch:  249, loss: 0.034422773867845535\n",
      "epoch:  250, loss: 0.03440659120678902\n",
      "epoch:  251, loss: 0.03439084440469742\n",
      "epoch:  252, loss: 0.03437551483511925\n",
      "epoch:  253, loss: 0.034360602498054504\n",
      "epoch:  254, loss: 0.0343460850417614\n",
      "epoch:  255, loss: 0.034331951290369034\n",
      "epoch:  256, loss: 0.03431818261742592\n",
      "epoch:  257, loss: 0.034304797649383545\n",
      "epoch:  258, loss: 0.03429179638624191\n",
      "epoch:  259, loss: 0.03427916765213013\n",
      "epoch:  260, loss: 0.0342668853700161\n",
      "epoch:  261, loss: 0.03425493463873863\n",
      "epoch:  262, loss: 0.03424330800771713\n",
      "epoch:  263, loss: 0.03423197567462921\n",
      "epoch:  264, loss: 0.03422093763947487\n",
      "epoch:  265, loss: 0.03421017900109291\n",
      "epoch:  266, loss: 0.03419969603419304\n",
      "epoch:  267, loss: 0.034189481288194656\n",
      "epoch:  268, loss: 0.03417953848838806\n",
      "epoch:  269, loss: 0.034169867634773254\n",
      "epoch:  270, loss: 0.03416045010089874\n",
      "epoch:  271, loss: 0.03415125608444214\n",
      "epoch:  272, loss: 0.03414229676127434\n",
      "epoch:  273, loss: 0.034133560955524445\n",
      "epoch:  274, loss: 0.03412504121661186\n",
      "epoch:  275, loss: 0.03411673381924629\n",
      "epoch:  276, loss: 0.034108635038137436\n",
      "epoch:  277, loss: 0.0341007262468338\n",
      "epoch:  278, loss: 0.03409300744533539\n",
      "epoch:  279, loss: 0.0340854711830616\n",
      "epoch:  280, loss: 0.03407811000943184\n",
      "epoch:  281, loss: 0.034070935100317\n",
      "epoch:  282, loss: 0.03406393527984619\n",
      "epoch:  283, loss: 0.03405711054801941\n",
      "epoch:  284, loss: 0.034050457179546356\n",
      "epoch:  285, loss: 0.03404398262500763\n",
      "epoch:  286, loss: 0.03403767570853233\n",
      "epoch:  287, loss: 0.03403155133128166\n",
      "epoch:  288, loss: 0.03402557596564293\n",
      "epoch:  289, loss: 0.034019749611616135\n",
      "epoch:  290, loss: 0.034014057368040085\n",
      "epoch:  291, loss: 0.03400849923491478\n",
      "epoch:  292, loss: 0.034003086388111115\n",
      "epoch:  293, loss: 0.0339978002011776\n",
      "epoch:  294, loss: 0.03399263322353363\n",
      "epoch:  295, loss: 0.033987585455179214\n",
      "epoch:  296, loss: 0.03398265317082405\n",
      "epoch:  297, loss: 0.03397782891988754\n",
      "epoch:  298, loss: 0.03397310525178909\n",
      "epoch:  299, loss: 0.0339684896171093\n",
      "epoch:  300, loss: 0.03396398574113846\n",
      "epoch:  301, loss: 0.03395958989858627\n",
      "epoch:  302, loss: 0.03395529091358185\n",
      "epoch:  303, loss: 0.033951085060834885\n",
      "epoch:  304, loss: 0.03394696116447449\n",
      "epoch:  305, loss: 0.033942919224500656\n",
      "epoch:  306, loss: 0.03393895924091339\n",
      "epoch:  307, loss: 0.033935096114873886\n",
      "epoch:  308, loss: 0.03393131494522095\n",
      "epoch:  309, loss: 0.03392761945724487\n",
      "epoch:  310, loss: 0.033924005925655365\n",
      "epoch:  311, loss: 0.03392046317458153\n",
      "epoch:  312, loss: 0.03391699120402336\n",
      "epoch:  313, loss: 0.03391358256340027\n",
      "epoch:  314, loss: 0.03391023725271225\n",
      "epoch:  315, loss: 0.0339069589972496\n",
      "epoch:  316, loss: 0.033903732895851135\n",
      "epoch:  317, loss: 0.03390057012438774\n",
      "epoch:  318, loss: 0.033897459506988525\n",
      "epoch:  319, loss: 0.03389440476894379\n",
      "epoch:  320, loss: 0.03389139473438263\n",
      "epoch:  321, loss: 0.033888451755046844\n",
      "epoch:  322, loss: 0.03388555347919464\n",
      "epoch:  323, loss: 0.03388270363211632\n",
      "epoch:  324, loss: 0.033879898488521576\n",
      "epoch:  325, loss: 0.033877138048410416\n",
      "epoch:  326, loss: 0.033874426037073135\n",
      "epoch:  327, loss: 0.03387175127863884\n",
      "epoch:  328, loss: 0.03386912867426872\n",
      "epoch:  329, loss: 0.033866558223962784\n",
      "epoch:  330, loss: 0.03386402502655983\n",
      "epoch:  331, loss: 0.03386153280735016\n",
      "epoch:  332, loss: 0.033859074115753174\n",
      "epoch:  333, loss: 0.03385666012763977\n",
      "epoch:  334, loss: 0.03385427966713905\n",
      "epoch:  335, loss: 0.03385193645954132\n",
      "epoch:  336, loss: 0.03384963423013687\n",
      "epoch:  337, loss: 0.03384735807776451\n",
      "epoch:  338, loss: 0.03384511545300484\n",
      "epoch:  339, loss: 0.03384290263056755\n",
      "epoch:  340, loss: 0.03384071961045265\n",
      "epoch:  341, loss: 0.03383857384324074\n",
      "epoch:  342, loss: 0.03383646532893181\n",
      "epoch:  343, loss: 0.033834390342235565\n",
      "epoch:  344, loss: 0.03383234515786171\n",
      "epoch:  345, loss: 0.03383033350110054\n",
      "epoch:  346, loss: 0.03382834792137146\n",
      "epoch:  347, loss: 0.03382638469338417\n",
      "epoch:  348, loss: 0.03382444381713867\n",
      "epoch:  349, loss: 0.033822521567344666\n",
      "epoch:  350, loss: 0.03382062166929245\n",
      "epoch:  351, loss: 0.033818744122982025\n",
      "epoch:  352, loss: 0.033816900104284286\n",
      "epoch:  353, loss: 0.03381507098674774\n",
      "epoch:  354, loss: 0.03381325677037239\n",
      "epoch:  355, loss: 0.03381146490573883\n",
      "epoch:  356, loss: 0.03380969539284706\n",
      "epoch:  357, loss: 0.033807940781116486\n",
      "epoch:  358, loss: 0.033806212246418\n",
      "epoch:  359, loss: 0.033804502338171005\n",
      "epoch:  360, loss: 0.033802807331085205\n",
      "epoch:  361, loss: 0.0338011309504509\n",
      "epoch:  362, loss: 0.033799465745687485\n",
      "epoch:  363, loss: 0.033797815442085266\n",
      "epoch:  364, loss: 0.03379618749022484\n",
      "epoch:  365, loss: 0.033794570714235306\n",
      "epoch:  366, loss: 0.03379296883940697\n",
      "epoch:  367, loss: 0.033791378140449524\n",
      "epoch:  368, loss: 0.033789802342653275\n",
      "epoch:  369, loss: 0.03378824517130852\n",
      "epoch:  370, loss: 0.033786702901124954\n",
      "epoch:  371, loss: 0.03378516435623169\n",
      "epoch:  372, loss: 0.03378364443778992\n",
      "epoch:  373, loss: 0.03378213196992874\n",
      "epoch:  374, loss: 0.03378063067793846\n",
      "epoch:  375, loss: 0.03377913683652878\n",
      "epoch:  376, loss: 0.03377764672040939\n",
      "epoch:  377, loss: 0.03377616032958031\n",
      "epoch:  378, loss: 0.03377467393875122\n",
      "epoch:  379, loss: 0.03377319872379303\n",
      "epoch:  380, loss: 0.033771730959415436\n",
      "epoch:  381, loss: 0.03377027064561844\n",
      "epoch:  382, loss: 0.03376881405711174\n",
      "epoch:  383, loss: 0.03376736491918564\n",
      "epoch:  384, loss: 0.033765923231840134\n",
      "epoch:  385, loss: 0.03376448526978493\n",
      "epoch:  386, loss: 0.03376305475831032\n",
      "epoch:  387, loss: 0.0337616391479969\n",
      "epoch:  388, loss: 0.03376024216413498\n",
      "epoch:  389, loss: 0.033758848905563354\n",
      "epoch:  390, loss: 0.03375746309757233\n",
      "epoch:  391, loss: 0.0337560810148716\n",
      "epoch:  392, loss: 0.03375471383333206\n",
      "epoch:  393, loss: 0.033753350377082825\n",
      "epoch:  394, loss: 0.033751990646123886\n",
      "epoch:  395, loss: 0.033750638365745544\n",
      "epoch:  396, loss: 0.0337492898106575\n",
      "epoch:  397, loss: 0.033747948706150055\n",
      "epoch:  398, loss: 0.033746618777513504\n",
      "epoch:  399, loss: 0.03374529257416725\n",
      "epoch:  400, loss: 0.033743977546691895\n",
      "epoch:  401, loss: 0.033742666244506836\n",
      "epoch:  402, loss: 0.033741362392902374\n",
      "epoch:  403, loss: 0.03374005854129791\n",
      "epoch:  404, loss: 0.033738765865564346\n",
      "epoch:  405, loss: 0.03373747318983078\n",
      "epoch:  406, loss: 0.03373619168996811\n",
      "epoch:  407, loss: 0.03373490646481514\n",
      "epoch:  408, loss: 0.03373362496495247\n",
      "epoch:  409, loss: 0.033732347190380096\n",
      "epoch:  410, loss: 0.03373106196522713\n",
      "epoch:  411, loss: 0.03372977674007416\n",
      "epoch:  412, loss: 0.03372849151492119\n",
      "epoch:  413, loss: 0.033727217465639114\n",
      "epoch:  414, loss: 0.03372593969106674\n",
      "epoch:  415, loss: 0.03372465446591377\n",
      "epoch:  416, loss: 0.0337233729660511\n",
      "epoch:  417, loss: 0.03372209146618843\n",
      "epoch:  418, loss: 0.03372081741690636\n",
      "epoch:  419, loss: 0.033719539642333984\n",
      "epoch:  420, loss: 0.03371826186776161\n",
      "epoch:  421, loss: 0.03371698781847954\n",
      "epoch:  422, loss: 0.033715713769197464\n",
      "epoch:  423, loss: 0.03371443971991539\n",
      "epoch:  424, loss: 0.033713169395923615\n",
      "epoch:  425, loss: 0.03371189162135124\n",
      "epoch:  426, loss: 0.03371062129735947\n",
      "epoch:  427, loss: 0.03370935097336769\n",
      "epoch:  428, loss: 0.03370809182524681\n",
      "epoch:  429, loss: 0.033706825226545334\n",
      "epoch:  430, loss: 0.033705566078424454\n",
      "epoch:  431, loss: 0.033704306930303574\n",
      "epoch:  432, loss: 0.03370305523276329\n",
      "epoch:  433, loss: 0.03370179980993271\n",
      "epoch:  434, loss: 0.033700548112392426\n",
      "epoch:  435, loss: 0.03369930759072304\n",
      "epoch:  436, loss: 0.03369806334376335\n",
      "epoch:  437, loss: 0.033696822822093964\n",
      "epoch:  438, loss: 0.033695586025714874\n",
      "epoch:  439, loss: 0.033694349229335785\n",
      "epoch:  440, loss: 0.033693112432956696\n",
      "epoch:  441, loss: 0.033691879361867905\n",
      "epoch:  442, loss: 0.033690646290779114\n",
      "epoch:  443, loss: 0.03368941321969032\n",
      "epoch:  444, loss: 0.033688176423311234\n",
      "epoch:  445, loss: 0.03368692845106125\n",
      "epoch:  446, loss: 0.03368568792939186\n",
      "epoch:  447, loss: 0.03368444740772247\n",
      "epoch:  448, loss: 0.03368320316076279\n",
      "epoch:  449, loss: 0.0336819626390934\n",
      "epoch:  450, loss: 0.03368072584271431\n",
      "epoch:  451, loss: 0.03367948904633522\n",
      "epoch:  452, loss: 0.03367824852466583\n",
      "epoch:  453, loss: 0.033677008002996445\n",
      "epoch:  454, loss: 0.03367576748132706\n",
      "epoch:  455, loss: 0.03367453068494797\n",
      "epoch:  456, loss: 0.03367328643798828\n",
      "epoch:  457, loss: 0.03367204964160919\n",
      "epoch:  458, loss: 0.0336708128452301\n",
      "epoch:  459, loss: 0.03366957604885101\n",
      "epoch:  460, loss: 0.033668335527181625\n",
      "epoch:  461, loss: 0.03366709500551224\n",
      "epoch:  462, loss: 0.03366586193442345\n",
      "epoch:  463, loss: 0.03366462513804436\n",
      "epoch:  464, loss: 0.033663392066955566\n",
      "epoch:  465, loss: 0.03366215527057648\n",
      "epoch:  466, loss: 0.03366093710064888\n",
      "epoch:  467, loss: 0.03365971893072128\n",
      "epoch:  468, loss: 0.033658504486083984\n",
      "epoch:  469, loss: 0.033657293766736984\n",
      "epoch:  470, loss: 0.033656083047389984\n",
      "epoch:  471, loss: 0.03365487605333328\n",
      "epoch:  472, loss: 0.03365367278456688\n",
      "epoch:  473, loss: 0.03365246579051018\n",
      "epoch:  474, loss: 0.03365126624703407\n",
      "epoch:  475, loss: 0.03365006670355797\n",
      "epoch:  476, loss: 0.03364887088537216\n",
      "epoch:  477, loss: 0.03364767134189606\n",
      "epoch:  478, loss: 0.033646468073129654\n",
      "epoch:  479, loss: 0.03364526852965355\n",
      "epoch:  480, loss: 0.033644068986177444\n",
      "epoch:  481, loss: 0.03364286944270134\n",
      "epoch:  482, loss: 0.033641666173934937\n",
      "epoch:  483, loss: 0.03364046663045883\n",
      "epoch:  484, loss: 0.03363926708698273\n",
      "epoch:  485, loss: 0.03363807499408722\n",
      "epoch:  486, loss: 0.03363687917590141\n",
      "epoch:  487, loss: 0.03363568335771561\n",
      "epoch:  488, loss: 0.0336344838142395\n",
      "epoch:  489, loss: 0.033633291721343994\n",
      "epoch:  490, loss: 0.033632099628448486\n",
      "epoch:  491, loss: 0.03363090381026268\n",
      "epoch:  492, loss: 0.03362971171736717\n",
      "epoch:  493, loss: 0.033628515899181366\n",
      "epoch:  494, loss: 0.03362732380628586\n",
      "epoch:  495, loss: 0.03362612798810005\n",
      "epoch:  496, loss: 0.03362493962049484\n",
      "epoch:  497, loss: 0.03362375125288963\n",
      "epoch:  498, loss: 0.03362257033586502\n",
      "epoch:  499, loss: 0.03362138941884041\n",
      "epoch:  500, loss: 0.033620208501815796\n",
      "epoch:  501, loss: 0.033619027584791183\n",
      "epoch:  502, loss: 0.03361785411834717\n",
      "epoch:  503, loss: 0.03361668810248375\n",
      "epoch:  504, loss: 0.03361551836133003\n",
      "epoch:  505, loss: 0.03361434489488602\n",
      "epoch:  506, loss: 0.0336131751537323\n",
      "epoch:  507, loss: 0.03361200913786888\n",
      "epoch:  508, loss: 0.033610835671424866\n",
      "epoch:  509, loss: 0.03360966593027115\n",
      "epoch:  510, loss: 0.03360849618911743\n",
      "epoch:  511, loss: 0.033607326447963715\n",
      "epoch:  512, loss: 0.03360615670681\n",
      "epoch:  513, loss: 0.03360499441623688\n",
      "epoch:  514, loss: 0.03360382840037346\n",
      "epoch:  515, loss: 0.03360266238451004\n",
      "epoch:  516, loss: 0.03360150009393692\n",
      "epoch:  517, loss: 0.0336003378033638\n",
      "epoch:  518, loss: 0.03359917923808098\n",
      "epoch:  519, loss: 0.03359802067279816\n",
      "epoch:  520, loss: 0.033596862107515335\n",
      "epoch:  521, loss: 0.03359570726752281\n",
      "epoch:  522, loss: 0.033594563603401184\n",
      "epoch:  523, loss: 0.033593423664569855\n",
      "epoch:  524, loss: 0.03359229117631912\n",
      "epoch:  525, loss: 0.03359115868806839\n",
      "epoch:  526, loss: 0.03359002247452736\n",
      "epoch:  527, loss: 0.03358888626098633\n",
      "epoch:  528, loss: 0.0335877500474453\n",
      "epoch:  529, loss: 0.03358660638332367\n",
      "epoch:  530, loss: 0.033585455268621445\n",
      "epoch:  531, loss: 0.033584319055080414\n",
      "epoch:  532, loss: 0.033583179116249084\n",
      "epoch:  533, loss: 0.033582039177417755\n",
      "epoch:  534, loss: 0.03358089551329613\n",
      "epoch:  535, loss: 0.0335797555744648\n",
      "epoch:  536, loss: 0.03357860818505287\n",
      "epoch:  537, loss: 0.033577460795640945\n",
      "epoch:  538, loss: 0.03357630968093872\n",
      "epoch:  539, loss: 0.033575162291526794\n",
      "epoch:  540, loss: 0.03357401490211487\n",
      "epoch:  541, loss: 0.03357286751270294\n",
      "epoch:  542, loss: 0.033571723848581314\n",
      "epoch:  543, loss: 0.033570580184459686\n",
      "epoch:  544, loss: 0.03356943279504776\n",
      "epoch:  545, loss: 0.03356828913092613\n",
      "epoch:  546, loss: 0.033567145466804504\n",
      "epoch:  547, loss: 0.03356599807739258\n",
      "epoch:  548, loss: 0.03356485441327095\n",
      "epoch:  549, loss: 0.033563703298568726\n",
      "epoch:  550, loss: 0.0335625521838665\n",
      "epoch:  551, loss: 0.033561404794454575\n",
      "epoch:  552, loss: 0.03356025740504265\n",
      "epoch:  553, loss: 0.033559106290340424\n",
      "epoch:  554, loss: 0.033557962626218796\n",
      "epoch:  555, loss: 0.03355681523680687\n",
      "epoch:  556, loss: 0.03355566784739494\n",
      "epoch:  557, loss: 0.033554527908563614\n",
      "epoch:  558, loss: 0.033553387969732285\n",
      "epoch:  559, loss: 0.033552251756191254\n",
      "epoch:  560, loss: 0.033551108092069626\n",
      "epoch:  561, loss: 0.033549971878528595\n",
      "epoch:  562, loss: 0.03354883939027786\n",
      "epoch:  563, loss: 0.03354770690202713\n",
      "epoch:  564, loss: 0.0335465744137764\n",
      "epoch:  565, loss: 0.033545445650815964\n",
      "epoch:  566, loss: 0.033544305711984634\n",
      "epoch:  567, loss: 0.033543169498443604\n",
      "epoch:  568, loss: 0.03354203328490257\n",
      "epoch:  569, loss: 0.03354089707136154\n",
      "epoch:  570, loss: 0.03353976085782051\n",
      "epoch:  571, loss: 0.03353862091898918\n",
      "epoch:  572, loss: 0.03353748098015785\n",
      "epoch:  573, loss: 0.03353634104132652\n",
      "epoch:  574, loss: 0.03353520482778549\n",
      "epoch:  575, loss: 0.03353406488895416\n",
      "epoch:  576, loss: 0.03353292867541313\n",
      "epoch:  577, loss: 0.0335317887365818\n",
      "epoch:  578, loss: 0.03353064879775047\n",
      "epoch:  579, loss: 0.03352951630949974\n",
      "epoch:  580, loss: 0.03352838382124901\n",
      "epoch:  581, loss: 0.033527255058288574\n",
      "epoch:  582, loss: 0.03352612257003784\n",
      "epoch:  583, loss: 0.033524997532367706\n",
      "epoch:  584, loss: 0.033523865044116974\n",
      "epoch:  585, loss: 0.03352272883057594\n",
      "epoch:  586, loss: 0.03352159261703491\n",
      "epoch:  587, loss: 0.03352046012878418\n",
      "epoch:  588, loss: 0.03351932391524315\n",
      "epoch:  589, loss: 0.033518191426992416\n",
      "epoch:  590, loss: 0.033517058938741684\n",
      "epoch:  591, loss: 0.03351593390107155\n",
      "epoch:  592, loss: 0.033514805138111115\n",
      "epoch:  593, loss: 0.03351368010044098\n",
      "epoch:  594, loss: 0.033512555062770844\n",
      "epoch:  595, loss: 0.03351142257452011\n",
      "epoch:  596, loss: 0.03351029381155968\n",
      "epoch:  597, loss: 0.03350916877388954\n",
      "epoch:  598, loss: 0.033508043736219406\n",
      "epoch:  599, loss: 0.033506911247968674\n",
      "epoch:  600, loss: 0.03350578621029854\n",
      "epoch:  601, loss: 0.0335046611726284\n",
      "epoch:  602, loss: 0.03350353240966797\n",
      "epoch:  603, loss: 0.03350240737199783\n",
      "epoch:  604, loss: 0.0335012748837471\n",
      "epoch:  605, loss: 0.033500149846076965\n",
      "epoch:  606, loss: 0.03349902108311653\n",
      "epoch:  607, loss: 0.0334978923201561\n",
      "epoch:  608, loss: 0.033496759831905365\n",
      "epoch:  609, loss: 0.03349563106894493\n",
      "epoch:  610, loss: 0.0334945023059845\n",
      "epoch:  611, loss: 0.03349336236715317\n",
      "epoch:  612, loss: 0.03349222242832184\n",
      "epoch:  613, loss: 0.03349107503890991\n",
      "epoch:  614, loss: 0.03348992392420769\n",
      "epoch:  615, loss: 0.03348877280950546\n",
      "epoch:  616, loss: 0.033487625420093536\n",
      "epoch:  617, loss: 0.03348648175597191\n",
      "epoch:  618, loss: 0.033485326915979385\n",
      "epoch:  619, loss: 0.03348417207598686\n",
      "epoch:  620, loss: 0.03348301723599434\n",
      "epoch:  621, loss: 0.03348185867071152\n",
      "epoch:  622, loss: 0.0334806926548481\n",
      "epoch:  623, loss: 0.03347952663898468\n",
      "epoch:  624, loss: 0.033478353172540665\n",
      "epoch:  625, loss: 0.033477190881967545\n",
      "epoch:  626, loss: 0.03347602114081383\n",
      "epoch:  627, loss: 0.03347485139966011\n",
      "epoch:  628, loss: 0.033473677933216095\n",
      "epoch:  629, loss: 0.03347250446677208\n",
      "epoch:  630, loss: 0.03347133472561836\n",
      "epoch:  631, loss: 0.03347016125917435\n",
      "epoch:  632, loss: 0.03346899524331093\n",
      "epoch:  633, loss: 0.03346782177686691\n",
      "epoch:  634, loss: 0.0334666483104229\n",
      "epoch:  635, loss: 0.03346548229455948\n",
      "epoch:  636, loss: 0.03346431627869606\n",
      "epoch:  637, loss: 0.03346314653754234\n",
      "epoch:  638, loss: 0.033461980521678925\n",
      "epoch:  639, loss: 0.033460814505815506\n",
      "epoch:  640, loss: 0.03345964848995209\n",
      "epoch:  641, loss: 0.03345847129821777\n",
      "epoch:  642, loss: 0.033457301557064056\n",
      "epoch:  643, loss: 0.03345612436532974\n",
      "epoch:  644, loss: 0.03345495089888573\n",
      "epoch:  645, loss: 0.03345377743244171\n",
      "epoch:  646, loss: 0.033452607691287994\n",
      "epoch:  647, loss: 0.03345143422484398\n",
      "epoch:  648, loss: 0.033450253307819366\n",
      "epoch:  649, loss: 0.033449068665504456\n",
      "epoch:  650, loss: 0.03344787284731865\n",
      "epoch:  651, loss: 0.03344667702913284\n",
      "epoch:  652, loss: 0.03344547376036644\n",
      "epoch:  653, loss: 0.03344426676630974\n",
      "epoch:  654, loss: 0.03344305604696274\n",
      "epoch:  655, loss: 0.03344184160232544\n",
      "epoch:  656, loss: 0.03344062343239784\n",
      "epoch:  657, loss: 0.033439405262470245\n",
      "epoch:  658, loss: 0.03343818336725235\n",
      "epoch:  659, loss: 0.03343695402145386\n",
      "epoch:  660, loss: 0.033435724675655365\n",
      "epoch:  661, loss: 0.03343448415398598\n",
      "epoch:  662, loss: 0.03343324735760689\n",
      "epoch:  663, loss: 0.0334320031106472\n",
      "epoch:  664, loss: 0.033430762588977814\n",
      "epoch:  665, loss: 0.03342951089143753\n",
      "epoch:  666, loss: 0.033428262919187546\n",
      "epoch:  667, loss: 0.033427007496356964\n",
      "epoch:  668, loss: 0.03342575579881668\n",
      "epoch:  669, loss: 0.0334245041012764\n",
      "epoch:  670, loss: 0.033423252403736115\n",
      "epoch:  671, loss: 0.033421989530324936\n",
      "epoch:  672, loss: 0.03342072665691376\n",
      "epoch:  673, loss: 0.03341946378350258\n",
      "epoch:  674, loss: 0.0334181971848011\n",
      "epoch:  675, loss: 0.03341692313551903\n",
      "epoch:  676, loss: 0.03341564163565636\n",
      "epoch:  677, loss: 0.03341436758637428\n",
      "epoch:  678, loss: 0.03341308981180191\n",
      "epoch:  679, loss: 0.03341181203722954\n",
      "epoch:  680, loss: 0.033410537987947464\n",
      "epoch:  681, loss: 0.03340926393866539\n",
      "epoch:  682, loss: 0.03340798243880272\n",
      "epoch:  683, loss: 0.033406682312488556\n",
      "epoch:  684, loss: 0.03340539708733559\n",
      "epoch:  685, loss: 0.03340411186218262\n",
      "epoch:  686, loss: 0.03340282291173935\n",
      "epoch:  687, loss: 0.03340153396129608\n",
      "epoch:  688, loss: 0.033400245010852814\n",
      "epoch:  689, loss: 0.03339895233511925\n",
      "epoch:  690, loss: 0.03339766338467598\n",
      "epoch:  691, loss: 0.033396363258361816\n",
      "epoch:  692, loss: 0.03339504078030586\n",
      "epoch:  693, loss: 0.03339372202754021\n",
      "epoch:  694, loss: 0.03339239954948425\n",
      "epoch:  695, loss: 0.033391073346138\n",
      "epoch:  696, loss: 0.03338974714279175\n",
      "epoch:  697, loss: 0.03338842839002609\n",
      "epoch:  698, loss: 0.03338710218667984\n",
      "epoch:  699, loss: 0.03338577225804329\n",
      "epoch:  700, loss: 0.03338443115353584\n",
      "epoch:  701, loss: 0.033383093774318695\n",
      "epoch:  702, loss: 0.03338174149394035\n",
      "epoch:  703, loss: 0.03338038921356201\n",
      "epoch:  704, loss: 0.03337903320789337\n",
      "epoch:  705, loss: 0.03337768092751503\n",
      "epoch:  706, loss: 0.03337632119655609\n",
      "epoch:  707, loss: 0.03337496146559715\n",
      "epoch:  708, loss: 0.033373601734638214\n",
      "epoch:  709, loss: 0.03337223827838898\n",
      "epoch:  710, loss: 0.03337087482213974\n",
      "epoch:  711, loss: 0.033369503915309906\n",
      "epoch:  712, loss: 0.033368129283189774\n",
      "epoch:  713, loss: 0.03336673602461815\n",
      "epoch:  714, loss: 0.03336533531546593\n",
      "epoch:  715, loss: 0.03336392715573311\n",
      "epoch:  716, loss: 0.03336251154541969\n",
      "epoch:  717, loss: 0.03336109593510628\n",
      "epoch:  718, loss: 0.03335968032479286\n",
      "epoch:  719, loss: 0.03335825726389885\n",
      "epoch:  720, loss: 0.03335683047771454\n",
      "epoch:  721, loss: 0.03335539624094963\n",
      "epoch:  722, loss: 0.033353958278894424\n",
      "epoch:  723, loss: 0.033352501690387726\n",
      "epoch:  724, loss: 0.03335104510188103\n",
      "epoch:  725, loss: 0.033349573612213135\n",
      "epoch:  726, loss: 0.03334809094667435\n",
      "epoch:  727, loss: 0.03334660828113556\n",
      "epoch:  728, loss: 0.03334512934088707\n",
      "epoch:  729, loss: 0.03334365412592888\n",
      "epoch:  730, loss: 0.03334217891097069\n",
      "epoch:  731, loss: 0.0333406925201416\n",
      "epoch:  732, loss: 0.03333920240402222\n",
      "epoch:  733, loss: 0.03333770111203194\n",
      "epoch:  734, loss: 0.03333618864417076\n",
      "epoch:  735, loss: 0.03333467245101929\n",
      "epoch:  736, loss: 0.033333152532577515\n",
      "epoch:  737, loss: 0.033331628888845444\n",
      "epoch:  738, loss: 0.03333010897040367\n",
      "epoch:  739, loss: 0.0333285890519619\n",
      "epoch:  740, loss: 0.03332706168293953\n",
      "epoch:  741, loss: 0.03332553058862686\n",
      "epoch:  742, loss: 0.03332400321960449\n",
      "epoch:  743, loss: 0.03332247585058212\n",
      "epoch:  744, loss: 0.03332095220685005\n",
      "epoch:  745, loss: 0.03331942483782768\n",
      "epoch:  746, loss: 0.033317890018224716\n",
      "epoch:  747, loss: 0.03331635147333145\n",
      "epoch:  748, loss: 0.033314794301986694\n",
      "epoch:  749, loss: 0.03331321105360985\n",
      "epoch:  750, loss: 0.0333116240799427\n",
      "epoch:  751, loss: 0.03331003710627556\n",
      "epoch:  752, loss: 0.033308420330286026\n",
      "epoch:  753, loss: 0.033306803554296494\n",
      "epoch:  754, loss: 0.03330518305301666\n",
      "epoch:  755, loss: 0.033303551375865936\n",
      "epoch:  756, loss: 0.033301904797554016\n",
      "epoch:  757, loss: 0.033300261944532394\n",
      "epoch:  758, loss: 0.03329859673976898\n",
      "epoch:  759, loss: 0.03329692780971527\n",
      "epoch:  760, loss: 0.033295247703790665\n",
      "epoch:  761, loss: 0.03329354152083397\n",
      "epoch:  762, loss: 0.03329180181026459\n",
      "epoch:  763, loss: 0.03329005837440491\n",
      "epoch:  764, loss: 0.033288318663835526\n",
      "epoch:  765, loss: 0.033286578953266144\n",
      "epoch:  766, loss: 0.033284831792116165\n",
      "epoch:  767, loss: 0.03328306972980499\n",
      "epoch:  768, loss: 0.03328130394220352\n",
      "epoch:  769, loss: 0.03327953815460205\n",
      "epoch:  770, loss: 0.0332777239382267\n",
      "epoch:  771, loss: 0.03327589109539986\n",
      "epoch:  772, loss: 0.03327404335141182\n",
      "epoch:  773, loss: 0.03327219560742378\n",
      "epoch:  774, loss: 0.033270347863435745\n",
      "epoch:  775, loss: 0.03326849639415741\n",
      "epoch:  776, loss: 0.033266644924879074\n",
      "epoch:  777, loss: 0.033264778554439545\n",
      "epoch:  778, loss: 0.033262889832258224\n",
      "epoch:  779, loss: 0.03326099365949631\n",
      "epoch:  780, loss: 0.033259067684412\n",
      "epoch:  781, loss: 0.0332571417093277\n",
      "epoch:  782, loss: 0.0332551933825016\n",
      "epoch:  783, loss: 0.03325323015451431\n",
      "epoch:  784, loss: 0.03325122222304344\n",
      "epoch:  785, loss: 0.033249206840991974\n",
      "epoch:  786, loss: 0.033247169107198715\n",
      "epoch:  787, loss: 0.03324512019753456\n",
      "epoch:  788, loss: 0.03324306383728981\n",
      "epoch:  789, loss: 0.03324098885059357\n",
      "epoch:  790, loss: 0.03323892131447792\n",
      "epoch:  791, loss: 0.03323681652545929\n",
      "epoch:  792, loss: 0.03323466703295708\n",
      "epoch:  793, loss: 0.033232446759939194\n",
      "epoch:  794, loss: 0.03323015943169594\n",
      "epoch:  795, loss: 0.03322784975171089\n",
      "epoch:  796, loss: 0.033225543797016144\n",
      "epoch:  797, loss: 0.03322325274348259\n",
      "epoch:  798, loss: 0.03322095051407814\n",
      "epoch:  799, loss: 0.0332186184823513\n",
      "epoch:  800, loss: 0.03321623429656029\n",
      "epoch:  801, loss: 0.03321385011076927\n",
      "epoch:  802, loss: 0.03321145847439766\n",
      "epoch:  803, loss: 0.03320905938744545\n",
      "epoch:  804, loss: 0.03320665657520294\n",
      "epoch:  805, loss: 0.03320422023534775\n",
      "epoch:  806, loss: 0.03320177644491196\n",
      "epoch:  807, loss: 0.03319932520389557\n",
      "epoch:  808, loss: 0.03319690749049187\n",
      "epoch:  809, loss: 0.033194493502378464\n",
      "epoch:  810, loss: 0.03319208696484566\n",
      "epoch:  811, loss: 0.03318967670202255\n",
      "epoch:  812, loss: 0.03318721055984497\n",
      "epoch:  813, loss: 0.033184714615345\n",
      "epoch:  814, loss: 0.03318220004439354\n",
      "epoch:  815, loss: 0.03317964822053909\n",
      "epoch:  816, loss: 0.03317707031965256\n",
      "epoch:  817, loss: 0.03317447751760483\n",
      "epoch:  818, loss: 0.033171866089105606\n",
      "epoch:  819, loss: 0.03316926583647728\n",
      "epoch:  820, loss: 0.03316667303442955\n",
      "epoch:  821, loss: 0.03316409885883331\n",
      "epoch:  822, loss: 0.033161506056785583\n",
      "epoch:  823, loss: 0.03315892070531845\n",
      "epoch:  824, loss: 0.03315632790327072\n",
      "epoch:  825, loss: 0.03315373882651329\n",
      "epoch:  826, loss: 0.033151157200336456\n",
      "epoch:  827, loss: 0.033148568123579025\n",
      "epoch:  828, loss: 0.03314594179391861\n",
      "epoch:  829, loss: 0.03314327821135521\n",
      "epoch:  830, loss: 0.03314060717821121\n",
      "epoch:  831, loss: 0.03313792496919632\n",
      "epoch:  832, loss: 0.033135246485471725\n",
      "epoch:  833, loss: 0.033132556825876236\n",
      "epoch:  834, loss: 0.03312985971570015\n",
      "epoch:  835, loss: 0.03312717750668526\n",
      "epoch:  836, loss: 0.03312453255057335\n",
      "epoch:  837, loss: 0.03312191739678383\n",
      "epoch:  838, loss: 0.03311927244067192\n",
      "epoch:  839, loss: 0.03311656042933464\n",
      "epoch:  840, loss: 0.03311384469270706\n",
      "epoch:  841, loss: 0.03311115503311157\n",
      "epoch:  842, loss: 0.03310849145054817\n",
      "epoch:  843, loss: 0.03310583531856537\n",
      "epoch:  844, loss: 0.03310316801071167\n",
      "epoch:  845, loss: 0.03310050442814827\n",
      "epoch:  846, loss: 0.03309784084558487\n",
      "epoch:  847, loss: 0.03309517726302147\n",
      "epoch:  848, loss: 0.03309248760342598\n",
      "epoch:  849, loss: 0.03308981657028198\n",
      "epoch:  850, loss: 0.033087108284235\n",
      "epoch:  851, loss: 0.03308442607522011\n",
      "epoch:  852, loss: 0.033081743866205215\n",
      "epoch:  853, loss: 0.033079035580158234\n",
      "epoch:  854, loss: 0.033076267689466476\n",
      "epoch:  855, loss: 0.03307345137000084\n",
      "epoch:  856, loss: 0.0330706387758255\n",
      "epoch:  857, loss: 0.033067844808101654\n",
      "epoch:  858, loss: 0.03306508809328079\n",
      "epoch:  859, loss: 0.03306230157613754\n",
      "epoch:  860, loss: 0.033059488981962204\n",
      "epoch:  861, loss: 0.03305670991539955\n",
      "epoch:  862, loss: 0.03305395320057869\n",
      "epoch:  863, loss: 0.03305119276046753\n",
      "epoch:  864, loss: 0.03304843232035637\n",
      "epoch:  865, loss: 0.03304563835263252\n",
      "epoch:  866, loss: 0.03304280340671539\n",
      "epoch:  867, loss: 0.03303996101021767\n",
      "epoch:  868, loss: 0.03303707391023636\n",
      "epoch:  869, loss: 0.03303416073322296\n",
      "epoch:  870, loss: 0.03303125128149986\n",
      "epoch:  871, loss: 0.033028360456228256\n",
      "epoch:  872, loss: 0.03302546218037605\n",
      "epoch:  873, loss: 0.03302254527807236\n",
      "epoch:  874, loss: 0.03301960229873657\n",
      "epoch:  875, loss: 0.0330166295170784\n",
      "epoch:  876, loss: 0.03301361948251724\n",
      "epoch:  877, loss: 0.03301060199737549\n",
      "epoch:  878, loss: 0.033007606863975525\n",
      "epoch:  879, loss: 0.033004630357027054\n",
      "epoch:  880, loss: 0.03300163894891739\n",
      "epoch:  881, loss: 0.03299867361783981\n",
      "epoch:  882, loss: 0.03299574926495552\n",
      "epoch:  883, loss: 0.03299277275800705\n",
      "epoch:  884, loss: 0.03298979252576828\n",
      "epoch:  885, loss: 0.03298681229352951\n",
      "epoch:  886, loss: 0.03298381716012955\n",
      "epoch:  887, loss: 0.03298081085085869\n",
      "epoch:  888, loss: 0.03297782316803932\n",
      "epoch:  889, loss: 0.032974861562252045\n",
      "epoch:  890, loss: 0.032971885055303574\n",
      "epoch:  891, loss: 0.03296888247132301\n",
      "epoch:  892, loss: 0.03296588733792305\n",
      "epoch:  893, loss: 0.03296288847923279\n",
      "epoch:  894, loss: 0.03295990452170372\n",
      "epoch:  895, loss: 0.03295690938830376\n",
      "epoch:  896, loss: 0.032953891903162\n",
      "epoch:  897, loss: 0.03295086696743965\n",
      "epoch:  898, loss: 0.032947853207588196\n",
      "epoch:  899, loss: 0.032944824546575546\n",
      "epoch:  900, loss: 0.032941784709692\n",
      "epoch:  901, loss: 0.03293871507048607\n",
      "epoch:  902, loss: 0.032935597002506256\n",
      "epoch:  903, loss: 0.03293248638510704\n",
      "epoch:  904, loss: 0.032929375767707825\n",
      "epoch:  905, loss: 0.0329262837767601\n",
      "epoch:  906, loss: 0.03292318433523178\n",
      "epoch:  907, loss: 0.03292009234428406\n",
      "epoch:  908, loss: 0.03291701152920723\n",
      "epoch:  909, loss: 0.0329139307141304\n",
      "epoch:  910, loss: 0.03291083872318268\n",
      "epoch:  911, loss: 0.03290773183107376\n",
      "epoch:  912, loss: 0.032904621213674545\n",
      "epoch:  913, loss: 0.03290149196982384\n",
      "epoch:  914, loss: 0.03289838135242462\n",
      "epoch:  915, loss: 0.03289526328444481\n",
      "epoch:  916, loss: 0.032892148941755295\n",
      "epoch:  917, loss: 0.032889045774936676\n",
      "epoch:  918, loss: 0.03288593143224716\n",
      "epoch:  919, loss: 0.03288283571600914\n",
      "epoch:  920, loss: 0.03287973999977112\n",
      "epoch:  921, loss: 0.032876625657081604\n",
      "epoch:  922, loss: 0.0328734889626503\n",
      "epoch:  923, loss: 0.032870352268218994\n",
      "epoch:  924, loss: 0.032867174595594406\n",
      "epoch:  925, loss: 0.03286397457122803\n",
      "epoch:  926, loss: 0.03286081179976463\n",
      "epoch:  927, loss: 0.03285769000649452\n",
      "epoch:  928, loss: 0.03285456448793411\n",
      "epoch:  929, loss: 0.032851461321115494\n",
      "epoch:  930, loss: 0.032848380506038666\n",
      "epoch:  931, loss: 0.03284529969096184\n",
      "epoch:  932, loss: 0.03284222632646561\n",
      "epoch:  933, loss: 0.03283914178609848\n",
      "epoch:  934, loss: 0.032836053520441055\n",
      "epoch:  935, loss: 0.03283301368355751\n",
      "epoch:  936, loss: 0.032830022275447845\n",
      "epoch:  937, loss: 0.03282706066966057\n",
      "epoch:  938, loss: 0.03282411769032478\n",
      "epoch:  939, loss: 0.0328211709856987\n",
      "epoch:  940, loss: 0.03281821310520172\n",
      "epoch:  941, loss: 0.032815273851156235\n",
      "epoch:  942, loss: 0.032812297344207764\n",
      "epoch:  943, loss: 0.03280933201313019\n",
      "epoch:  944, loss: 0.032806385308504105\n",
      "epoch:  945, loss: 0.03280346468091011\n",
      "epoch:  946, loss: 0.03280052915215492\n",
      "epoch:  947, loss: 0.03279756382107735\n",
      "epoch:  948, loss: 0.03279457986354828\n",
      "epoch:  949, loss: 0.03279158100485802\n",
      "epoch:  950, loss: 0.03278858959674835\n",
      "epoch:  951, loss: 0.032785605639219284\n",
      "epoch:  952, loss: 0.03278259187936783\n",
      "epoch:  953, loss: 0.0327795073390007\n",
      "epoch:  954, loss: 0.032776426523923874\n",
      "epoch:  955, loss: 0.032773345708847046\n",
      "epoch:  956, loss: 0.03277023881673813\n",
      "epoch:  957, loss: 0.03276710957288742\n",
      "epoch:  958, loss: 0.03276398405432701\n",
      "epoch:  959, loss: 0.03276083245873451\n",
      "epoch:  960, loss: 0.03275768831372261\n",
      "epoch:  961, loss: 0.032754555344581604\n",
      "epoch:  962, loss: 0.03275148570537567\n",
      "epoch:  963, loss: 0.03274846822023392\n",
      "epoch:  964, loss: 0.03274543210864067\n",
      "epoch:  965, loss: 0.03274243324995041\n",
      "epoch:  966, loss: 0.03273944929242134\n",
      "epoch:  967, loss: 0.03273645415902138\n",
      "epoch:  968, loss: 0.0327334962785244\n",
      "epoch:  969, loss: 0.03273060545325279\n",
      "epoch:  970, loss: 0.032727811485528946\n",
      "epoch:  971, loss: 0.0327250100672245\n",
      "epoch:  972, loss: 0.03272224962711334\n",
      "epoch:  973, loss: 0.032719485461711884\n",
      "epoch:  974, loss: 0.032716721296310425\n",
      "epoch:  975, loss: 0.03271394968032837\n",
      "epoch:  976, loss: 0.03271118551492691\n",
      "epoch:  977, loss: 0.03270845487713814\n",
      "epoch:  978, loss: 0.03270578011870384\n",
      "epoch:  979, loss: 0.03270313888788223\n",
      "epoch:  980, loss: 0.03270050138235092\n",
      "epoch:  981, loss: 0.03269786387681961\n",
      "epoch:  982, loss: 0.032695222645998\n",
      "epoch:  983, loss: 0.03269258141517639\n",
      "epoch:  984, loss: 0.032689936459064484\n",
      "epoch:  985, loss: 0.032687295228242874\n",
      "epoch:  986, loss: 0.03268466889858246\n",
      "epoch:  987, loss: 0.03268204256892204\n",
      "epoch:  988, loss: 0.03267941251397133\n",
      "epoch:  989, loss: 0.03267679736018181\n",
      "epoch:  990, loss: 0.03267420828342438\n",
      "epoch:  991, loss: 0.03267165273427963\n",
      "epoch:  992, loss: 0.032669082283973694\n",
      "epoch:  993, loss: 0.03266653046011925\n",
      "epoch:  994, loss: 0.0326639823615551\n",
      "epoch:  995, loss: 0.032661452889442444\n",
      "epoch:  996, loss: 0.03265894204378128\n",
      "epoch:  997, loss: 0.032656434923410416\n",
      "epoch:  998, loss: 0.032653968781232834\n",
      "epoch:  999, loss: 0.032651543617248535\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size=X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "opt = torch_numopt.GradientDescentLS(model=model, lr=0.005, line_search_method=\"const\")\n",
    "\n",
    "all_loss = {}\n",
    "for epoch in range(1000):\n",
    "    print(\"epoch: \", epoch, end=\"\")\n",
    "    all_loss[epoch + 1] = 0\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y, loss_fn)\n",
    "\n",
    "        all_loss[epoch + 1] += loss\n",
    "    all_loss[epoch + 1] /= len(data_loader)\n",
    "    print(\", loss: {}\".format(all_loss[epoch + 1].cpu().detach().numpy().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics: R2 = -190.5641404760785\n",
      "Test metrics:  R2 = -193.2321866439647\n"
     ]
    }
   ],
   "source": [
    "pred_train = model.forward(X_train).cpu().detach()\n",
    "pred_test = model.forward(X_test).cpu().detach()\n",
    "print(f\"Train metrics: R2 = {r2_score(pred_train, y_train.cpu())}\")\n",
    "print(f\"Test metrics:  R2 = {r2_score(pred_test, y_test.cpu())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backtrack line search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.4261147975921631\n",
      "epoch:  1, loss: 0.2413555085659027\n",
      "epoch:  2, loss: 0.13654690980911255\n",
      "epoch:  3, loss: 0.0811065286397934\n",
      "epoch:  4, loss: 0.05408589541912079\n",
      "epoch:  5, loss: 0.04195704311132431\n",
      "epoch:  6, loss: 0.03688735142350197\n",
      "epoch:  7, loss: 0.03487382456660271\n",
      "epoch:  8, loss: 0.03409208357334137\n",
      "epoch:  9, loss: 0.033784303814172745\n",
      "epoch:  10, loss: 0.03364982455968857\n",
      "epoch:  11, loss: 0.03357938677072525\n",
      "epoch:  12, loss: 0.03342160955071449\n",
      "epoch:  13, loss: 0.03326766565442085\n",
      "epoch:  14, loss: 0.033196352422237396\n",
      "epoch:  15, loss: 0.033091507852077484\n",
      "epoch:  16, loss: 0.03292396292090416\n",
      "epoch:  17, loss: 0.03284898027777672\n",
      "epoch:  18, loss: 0.032813310623168945\n",
      "epoch:  19, loss: 0.0326102189719677\n",
      "epoch:  20, loss: 0.03252800554037094\n",
      "epoch:  21, loss: 0.03248398378491402\n",
      "epoch:  22, loss: 0.03230336681008339\n",
      "epoch:  23, loss: 0.032213058322668076\n",
      "epoch:  24, loss: 0.0321655198931694\n",
      "epoch:  25, loss: 0.031997907906770706\n",
      "epoch:  26, loss: 0.031891897320747375\n",
      "epoch:  27, loss: 0.03184044361114502\n",
      "epoch:  28, loss: 0.03167784586548805\n",
      "epoch:  29, loss: 0.031561970710754395\n",
      "epoch:  30, loss: 0.03150635585188866\n",
      "epoch:  31, loss: 0.03135574981570244\n",
      "epoch:  32, loss: 0.03121843747794628\n",
      "epoch:  33, loss: 0.031157299876213074\n",
      "epoch:  34, loss: 0.030998514965176582\n",
      "epoch:  35, loss: 0.030849775299429893\n",
      "epoch:  36, loss: 0.03078303672373295\n",
      "epoch:  37, loss: 0.030627304688096046\n",
      "epoch:  38, loss: 0.030453667044639587\n",
      "epoch:  39, loss: 0.03038114495575428\n",
      "epoch:  40, loss: 0.030206875875592232\n",
      "epoch:  41, loss: 0.030024275183677673\n",
      "epoch:  42, loss: 0.02994665503501892\n",
      "epoch:  43, loss: 0.029768353328108788\n",
      "epoch:  44, loss: 0.029560549184679985\n",
      "epoch:  45, loss: 0.029478101059794426\n",
      "epoch:  46, loss: 0.02926664799451828\n",
      "epoch:  47, loss: 0.029057731851935387\n",
      "epoch:  48, loss: 0.028971651569008827\n",
      "epoch:  49, loss: 0.028746996074914932\n",
      "epoch:  50, loss: 0.028516806662082672\n",
      "epoch:  51, loss: 0.028427984565496445\n",
      "epoch:  52, loss: 0.028148965910077095\n",
      "epoch:  53, loss: 0.027929751202464104\n",
      "epoch:  54, loss: 0.02783985808491707\n",
      "epoch:  55, loss: 0.027523357421159744\n",
      "epoch:  56, loss: 0.027296194806694984\n",
      "epoch:  57, loss: 0.027206411585211754\n",
      "epoch:  58, loss: 0.026813218370079994\n",
      "epoch:  59, loss: 0.026611050590872765\n",
      "epoch:  60, loss: 0.026523133739829063\n",
      "epoch:  61, loss: 0.026074837893247604\n",
      "epoch:  62, loss: 0.02588195726275444\n",
      "epoch:  63, loss: 0.025848500430583954\n",
      "epoch:  64, loss: 0.02526448667049408\n",
      "epoch:  65, loss: 0.02510443702340126\n",
      "epoch:  66, loss: 0.024900874122977257\n",
      "epoch:  67, loss: 0.024408061057329178\n",
      "epoch:  68, loss: 0.024278150871396065\n",
      "epoch:  69, loss: 0.023817280307412148\n",
      "epoch:  70, loss: 0.023512819781899452\n",
      "epoch:  71, loss: 0.02340538054704666\n",
      "epoch:  72, loss: 0.02280861884355545\n",
      "epoch:  73, loss: 0.022589175030589104\n",
      "epoch:  74, loss: 0.022356214001774788\n",
      "epoch:  75, loss: 0.021769557148218155\n",
      "epoch:  76, loss: 0.021630892530083656\n",
      "epoch:  77, loss: 0.021055450662970543\n",
      "epoch:  78, loss: 0.020749911665916443\n",
      "epoch:  79, loss: 0.020643675699830055\n",
      "epoch:  80, loss: 0.01987908035516739\n",
      "epoch:  81, loss: 0.019737882539629936\n",
      "epoch:  82, loss: 0.019088130444288254\n",
      "epoch:  83, loss: 0.018822962418198586\n",
      "epoch:  84, loss: 0.018433040007948875\n",
      "epoch:  85, loss: 0.01791895553469658\n",
      "epoch:  86, loss: 0.01781376637518406\n",
      "epoch:  87, loss: 0.01704622432589531\n",
      "epoch:  88, loss: 0.016914784908294678\n",
      "epoch:  89, loss: 0.01620369590818882\n",
      "epoch:  90, loss: 0.01604360155761242\n",
      "epoch:  91, loss: 0.01541592925786972\n",
      "epoch:  92, loss: 0.015217022970318794\n",
      "epoch:  93, loss: 0.014604744501411915\n",
      "epoch:  94, loss: 0.014440562576055527\n",
      "epoch:  95, loss: 0.013852261006832123\n",
      "epoch:  96, loss: 0.013722886331379414\n",
      "epoch:  97, loss: 0.013138704933226109\n",
      "epoch:  98, loss: 0.013071814551949501\n",
      "epoch:  99, loss: 0.012538411654531956\n",
      "epoch:  100, loss: 0.012161160819232464\n",
      "epoch:  101, loss: 0.012013127095997334\n",
      "epoch:  102, loss: 0.011590084992349148\n",
      "epoch:  103, loss: 0.011325957253575325\n",
      "epoch:  104, loss: 0.011180558241903782\n",
      "epoch:  105, loss: 0.010925553739070892\n",
      "epoch:  106, loss: 0.010825409553945065\n",
      "epoch:  107, loss: 0.010616912506520748\n",
      "epoch:  108, loss: 0.010517291724681854\n",
      "epoch:  109, loss: 0.010384459048509598\n",
      "epoch:  110, loss: 0.01025134976953268\n",
      "epoch:  111, loss: 0.010227124206721783\n",
      "epoch:  112, loss: 0.010019035078585148\n",
      "epoch:  113, loss: 0.00999613106250763\n",
      "epoch:  114, loss: 0.009819583967328072\n",
      "epoch:  115, loss: 0.009794963523745537\n",
      "epoch:  116, loss: 0.009654926136136055\n",
      "epoch:  117, loss: 0.00961916521191597\n",
      "epoch:  118, loss: 0.009548047557473183\n",
      "epoch:  119, loss: 0.009465891867876053\n",
      "epoch:  120, loss: 0.00945084635168314\n",
      "epoch:  121, loss: 0.009335745126008987\n",
      "epoch:  122, loss: 0.009315451607108116\n",
      "epoch:  123, loss: 0.009246550500392914\n",
      "epoch:  124, loss: 0.009196439757943153\n",
      "epoch:  125, loss: 0.009184653870761395\n",
      "epoch:  126, loss: 0.009097120724618435\n",
      "epoch:  127, loss: 0.009078852832317352\n",
      "epoch:  128, loss: 0.009040288627147675\n",
      "epoch:  129, loss: 0.008984011597931385\n",
      "epoch:  130, loss: 0.008973917923867702\n",
      "epoch:  131, loss: 0.00891145970672369\n",
      "epoch:  132, loss: 0.008890929631888866\n",
      "epoch:  133, loss: 0.008883146569132805\n",
      "epoch:  134, loss: 0.008819300681352615\n",
      "epoch:  135, loss: 0.008808426558971405\n",
      "epoch:  136, loss: 0.008782990276813507\n",
      "epoch:  137, loss: 0.008743179962038994\n",
      "epoch:  138, loss: 0.00873573962599039\n",
      "epoch:  139, loss: 0.008696072734892368\n",
      "epoch:  140, loss: 0.008677329868078232\n",
      "epoch:  141, loss: 0.008671421557664871\n",
      "epoch:  142, loss: 0.008629829622805119\n",
      "epoch:  143, loss: 0.008618212305009365\n",
      "epoch:  144, loss: 0.008613189682364464\n",
      "epoch:  145, loss: 0.008573854342103004\n",
      "epoch:  146, loss: 0.008565409108996391\n",
      "epoch:  147, loss: 0.008561152033507824\n",
      "epoch:  148, loss: 0.008526107296347618\n",
      "epoch:  149, loss: 0.008519486524164677\n",
      "epoch:  150, loss: 0.008516407571732998\n",
      "epoch:  151, loss: 0.008484388701617718\n",
      "epoch:  152, loss: 0.008479438722133636\n",
      "epoch:  153, loss: 0.008466931991279125\n",
      "epoch:  154, loss: 0.008446880616247654\n",
      "epoch:  155, loss: 0.008442649617791176\n",
      "epoch:  156, loss: 0.008425413630902767\n",
      "epoch:  157, loss: 0.008411469869315624\n",
      "epoch:  158, loss: 0.008407962508499622\n",
      "epoch:  159, loss: 0.008388761430978775\n",
      "epoch:  160, loss: 0.008379259146749973\n",
      "epoch:  161, loss: 0.008376237004995346\n",
      "epoch:  162, loss: 0.008356932550668716\n",
      "epoch:  163, loss: 0.008349322713911533\n",
      "epoch:  164, loss: 0.0083464989438653\n",
      "epoch:  165, loss: 0.008325749076902866\n",
      "epoch:  166, loss: 0.008319773711264133\n",
      "epoch:  167, loss: 0.008317171595990658\n",
      "epoch:  168, loss: 0.008296814747154713\n",
      "epoch:  169, loss: 0.008292119018733501\n",
      "epoch:  170, loss: 0.008289751596748829\n",
      "epoch:  171, loss: 0.008269884623587132\n",
      "epoch:  172, loss: 0.008265742100775242\n",
      "epoch:  173, loss: 0.008263584226369858\n",
      "epoch:  174, loss: 0.008246208541095257\n",
      "epoch:  175, loss: 0.008242461830377579\n",
      "epoch:  176, loss: 0.008240394294261932\n",
      "epoch:  177, loss: 0.008222749456763268\n",
      "epoch:  178, loss: 0.008219514973461628\n",
      "epoch:  179, loss: 0.008217236958444118\n",
      "epoch:  180, loss: 0.008201046846807003\n",
      "epoch:  181, loss: 0.008198270574212074\n",
      "epoch:  182, loss: 0.008194138295948505\n",
      "epoch:  183, loss: 0.008181478828191757\n",
      "epoch:  184, loss: 0.008179202675819397\n",
      "epoch:  185, loss: 0.008174508810043335\n",
      "epoch:  186, loss: 0.008164437487721443\n",
      "epoch:  187, loss: 0.008162419311702251\n",
      "epoch:  188, loss: 0.008156358264386654\n",
      "epoch:  189, loss: 0.008148199878633022\n",
      "epoch:  190, loss: 0.008146420121192932\n",
      "epoch:  191, loss: 0.008139747194945812\n",
      "epoch:  192, loss: 0.008133163675665855\n",
      "epoch:  193, loss: 0.008131547830998898\n",
      "epoch:  194, loss: 0.008124344982206821\n",
      "epoch:  195, loss: 0.00811891071498394\n",
      "epoch:  196, loss: 0.008117460645735264\n",
      "epoch:  197, loss: 0.008110959082841873\n",
      "epoch:  198, loss: 0.008105969987809658\n",
      "epoch:  199, loss: 0.00810464471578598\n",
      "epoch:  200, loss: 0.008098565973341465\n",
      "epoch:  201, loss: 0.008094129152595997\n",
      "epoch:  202, loss: 0.008092910051345825\n",
      "epoch:  203, loss: 0.008087504655122757\n",
      "epoch:  204, loss: 0.00808314885944128\n",
      "epoch:  205, loss: 0.008081990294158459\n",
      "epoch:  206, loss: 0.008076979778707027\n",
      "epoch:  207, loss: 0.008072941564023495\n",
      "epoch:  208, loss: 0.008071886375546455\n",
      "epoch:  209, loss: 0.008066670969128609\n",
      "epoch:  210, loss: 0.008063404820859432\n",
      "epoch:  211, loss: 0.00806243997067213\n",
      "epoch:  212, loss: 0.008057322353124619\n",
      "epoch:  213, loss: 0.008054282516241074\n",
      "epoch:  214, loss: 0.008053339086472988\n",
      "epoch:  215, loss: 0.008048191666603088\n",
      "epoch:  216, loss: 0.008045390248298645\n",
      "epoch:  217, loss: 0.00804451946169138\n",
      "epoch:  218, loss: 0.008039861917495728\n",
      "epoch:  219, loss: 0.0080370819196105\n",
      "epoch:  220, loss: 0.008036233484745026\n",
      "epoch:  221, loss: 0.008031691424548626\n",
      "epoch:  222, loss: 0.008029005490243435\n",
      "epoch:  223, loss: 0.008028161711990833\n",
      "epoch:  224, loss: 0.008023506961762905\n",
      "epoch:  225, loss: 0.008020960725843906\n",
      "epoch:  226, loss: 0.008020139299333096\n",
      "epoch:  227, loss: 0.0080155273899436\n",
      "epoch:  228, loss: 0.00801303330808878\n",
      "epoch:  229, loss: 0.008012223057448864\n",
      "epoch:  230, loss: 0.008007648400962353\n",
      "epoch:  231, loss: 0.008005271665751934\n",
      "epoch:  232, loss: 0.008004517294466496\n",
      "epoch:  233, loss: 0.008000297471880913\n",
      "epoch:  234, loss: 0.007998021319508553\n",
      "epoch:  235, loss: 0.007997279986739159\n",
      "epoch:  236, loss: 0.007992863655090332\n",
      "epoch:  237, loss: 0.007990655489265919\n",
      "epoch:  238, loss: 0.007989904843270779\n",
      "epoch:  239, loss: 0.007985519245266914\n",
      "epoch:  240, loss: 0.007983358576893806\n",
      "epoch:  241, loss: 0.007982631213963032\n",
      "epoch:  242, loss: 0.00797827448695898\n",
      "epoch:  243, loss: 0.007976217195391655\n",
      "epoch:  244, loss: 0.007975495420396328\n",
      "epoch:  245, loss: 0.007971154525876045\n",
      "epoch:  246, loss: 0.007969127967953682\n",
      "epoch:  247, loss: 0.007968410849571228\n",
      "epoch:  248, loss: 0.007964138872921467\n",
      "epoch:  249, loss: 0.007962094619870186\n",
      "epoch:  250, loss: 0.007961374707520008\n",
      "epoch:  251, loss: 0.007957383058965206\n",
      "epoch:  252, loss: 0.007954948581755161\n",
      "epoch:  253, loss: 0.007954200729727745\n",
      "epoch:  254, loss: 0.007950486615300179\n",
      "epoch:  255, loss: 0.00794783141463995\n",
      "epoch:  256, loss: 0.007947053760290146\n",
      "epoch:  257, loss: 0.007943808101117611\n",
      "epoch:  258, loss: 0.007940971292555332\n",
      "epoch:  259, loss: 0.007940222509205341\n",
      "epoch:  260, loss: 0.007937452755868435\n",
      "epoch:  261, loss: 0.00793453585356474\n",
      "epoch:  262, loss: 0.007933798246085644\n",
      "epoch:  263, loss: 0.007933374494314194\n",
      "epoch:  264, loss: 0.007928620092570782\n",
      "epoch:  265, loss: 0.00792775396257639\n",
      "epoch:  266, loss: 0.007927218452095985\n",
      "epoch:  267, loss: 0.007922717370092869\n",
      "epoch:  268, loss: 0.007921857759356499\n",
      "epoch:  269, loss: 0.007921340875327587\n",
      "epoch:  270, loss: 0.00791720487177372\n",
      "epoch:  271, loss: 0.007916171103715897\n",
      "epoch:  272, loss: 0.007915645837783813\n",
      "epoch:  273, loss: 0.007911655120551586\n",
      "epoch:  274, loss: 0.007910522632300854\n",
      "epoch:  275, loss: 0.007909983396530151\n",
      "epoch:  276, loss: 0.007906096056103706\n",
      "epoch:  277, loss: 0.007904882542788982\n",
      "epoch:  278, loss: 0.007904334925115108\n",
      "epoch:  279, loss: 0.007900635711848736\n",
      "epoch:  280, loss: 0.007899285294115543\n",
      "epoch:  281, loss: 0.007898743264377117\n",
      "epoch:  282, loss: 0.007895422168076038\n",
      "epoch:  283, loss: 0.007893817499279976\n",
      "epoch:  284, loss: 0.007893254980444908\n",
      "epoch:  285, loss: 0.007890220731496811\n",
      "epoch:  286, loss: 0.007888393476605415\n",
      "epoch:  287, loss: 0.007887819781899452\n",
      "epoch:  288, loss: 0.007885252125561237\n",
      "epoch:  289, loss: 0.007883084006607533\n",
      "epoch:  290, loss: 0.007882478646934032\n",
      "epoch:  291, loss: 0.007880409248173237\n",
      "epoch:  292, loss: 0.007877835072577\n",
      "epoch:  293, loss: 0.00787720549851656\n",
      "epoch:  294, loss: 0.007875757291913033\n",
      "epoch:  295, loss: 0.00787268951535225\n",
      "epoch:  296, loss: 0.007872017100453377\n",
      "epoch:  297, loss: 0.007871130481362343\n",
      "epoch:  298, loss: 0.007867611013352871\n",
      "epoch:  299, loss: 0.007866891101002693\n",
      "epoch:  300, loss: 0.00786642823368311\n",
      "epoch:  301, loss: 0.007862990722060204\n",
      "epoch:  302, loss: 0.00786179956048727\n",
      "epoch:  303, loss: 0.00786129292100668\n",
      "epoch:  304, loss: 0.007858030498027802\n",
      "epoch:  305, loss: 0.007856696844100952\n",
      "epoch:  306, loss: 0.007856175303459167\n",
      "epoch:  307, loss: 0.007853654213249683\n",
      "epoch:  308, loss: 0.007851796224713326\n",
      "epoch:  309, loss: 0.007851257920265198\n",
      "epoch:  310, loss: 0.007849054411053658\n",
      "epoch:  311, loss: 0.007847020402550697\n",
      "epoch:  312, loss: 0.007846463471651077\n",
      "epoch:  313, loss: 0.007844598963856697\n",
      "epoch:  314, loss: 0.007842276245355606\n",
      "epoch:  315, loss: 0.007841693237423897\n",
      "epoch:  316, loss: 0.007840692065656185\n",
      "epoch:  317, loss: 0.007837658748030663\n",
      "epoch:  318, loss: 0.007837005890905857\n",
      "epoch:  319, loss: 0.007836786098778248\n",
      "epoch:  320, loss: 0.007833057083189487\n",
      "epoch:  321, loss: 0.00783233717083931\n",
      "epoch:  322, loss: 0.007831919007003307\n",
      "epoch:  323, loss: 0.00782869104295969\n",
      "epoch:  324, loss: 0.0078277001157403\n",
      "epoch:  325, loss: 0.007827256806194782\n",
      "epoch:  326, loss: 0.007824250496923923\n",
      "epoch:  327, loss: 0.007823051884770393\n",
      "epoch:  328, loss: 0.00782257504761219\n",
      "epoch:  329, loss: 0.007820045575499535\n",
      "epoch:  330, loss: 0.00781838595867157\n",
      "epoch:  331, loss: 0.007817870005965233\n",
      "epoch:  332, loss: 0.00781573262065649\n",
      "epoch:  333, loss: 0.007813695818185806\n",
      "epoch:  334, loss: 0.007813136093318462\n",
      "epoch:  335, loss: 0.007811444811522961\n",
      "epoch:  336, loss: 0.0078089176677167416\n",
      "epoch:  337, loss: 0.007808302529156208\n",
      "epoch:  338, loss: 0.00780789228156209\n",
      "epoch:  339, loss: 0.007804209366440773\n",
      "epoch:  340, loss: 0.007803498767316341\n",
      "epoch:  341, loss: 0.007803063839673996\n",
      "epoch:  342, loss: 0.007799596060067415\n",
      "epoch:  343, loss: 0.0077987490221858025\n",
      "epoch:  344, loss: 0.007798295933753252\n",
      "epoch:  345, loss: 0.007795662619173527\n",
      "epoch:  346, loss: 0.007794083096086979\n",
      "epoch:  347, loss: 0.007793566212058067\n",
      "epoch:  348, loss: 0.007791382726281881\n",
      "epoch:  349, loss: 0.007789341267198324\n",
      "epoch:  350, loss: 0.00778876943513751\n",
      "epoch:  351, loss: 0.007788183633238077\n",
      "epoch:  352, loss: 0.00778468232601881\n",
      "epoch:  353, loss: 0.007783952169120312\n",
      "epoch:  354, loss: 0.00778351491317153\n",
      "epoch:  355, loss: 0.007779723033308983\n",
      "epoch:  356, loss: 0.007779097184538841\n",
      "epoch:  357, loss: 0.007778662256896496\n",
      "epoch:  358, loss: 0.007775289472192526\n",
      "epoch:  359, loss: 0.007774285972118378\n",
      "epoch:  360, loss: 0.007773810066282749\n",
      "epoch:  361, loss: 0.007770814932882786\n",
      "epoch:  362, loss: 0.007769482675939798\n",
      "epoch:  363, loss: 0.007768977899104357\n",
      "epoch:  364, loss: 0.007766548078507185\n",
      "epoch:  365, loss: 0.007764733396470547\n",
      "epoch:  366, loss: 0.007764188107103109\n",
      "epoch:  367, loss: 0.007762643974274397\n",
      "epoch:  368, loss: 0.00776002649217844\n",
      "epoch:  369, loss: 0.007759393192827702\n",
      "epoch:  370, loss: 0.007759050000458956\n",
      "epoch:  371, loss: 0.00775531493127346\n",
      "epoch:  372, loss: 0.007754576858133078\n",
      "epoch:  373, loss: 0.007754133082926273\n",
      "epoch:  374, loss: 0.007750626653432846\n",
      "epoch:  375, loss: 0.007749766111373901\n",
      "epoch:  376, loss: 0.007749308366328478\n",
      "epoch:  377, loss: 0.007746081333607435\n",
      "epoch:  378, loss: 0.00774499261751771\n",
      "epoch:  379, loss: 0.007744521833956242\n",
      "epoch:  380, loss: 0.007741742767393589\n",
      "epoch:  381, loss: 0.007740294560790062\n",
      "epoch:  382, loss: 0.0077397762797772884\n",
      "epoch:  383, loss: 0.007737547159194946\n",
      "epoch:  384, loss: 0.007735553663223982\n",
      "epoch:  385, loss: 0.007734991610050201\n",
      "epoch:  386, loss: 0.007733696140348911\n",
      "epoch:  387, loss: 0.007730917539447546\n",
      "epoch:  388, loss: 0.007730277720838785\n",
      "epoch:  389, loss: 0.007729867473244667\n",
      "epoch:  390, loss: 0.007726478390395641\n",
      "epoch:  391, loss: 0.007725731935352087\n",
      "epoch:  392, loss: 0.007725303061306477\n",
      "epoch:  393, loss: 0.007722203619778156\n",
      "epoch:  394, loss: 0.007721201051026583\n",
      "epoch:  395, loss: 0.007720749359577894\n",
      "epoch:  396, loss: 0.007718212436884642\n",
      "epoch:  397, loss: 0.007716713938862085\n",
      "epoch:  398, loss: 0.007716221734881401\n",
      "epoch:  399, loss: 0.00771428644657135\n",
      "epoch:  400, loss: 0.007712177000939846\n",
      "epoch:  401, loss: 0.007711609825491905\n",
      "epoch:  402, loss: 0.007711230311542749\n",
      "epoch:  403, loss: 0.007707761134952307\n",
      "epoch:  404, loss: 0.00770706357434392\n",
      "epoch:  405, loss: 0.007706650998443365\n",
      "epoch:  406, loss: 0.007703419774770737\n",
      "epoch:  407, loss: 0.0077025676146149635\n",
      "epoch:  408, loss: 0.007702135946601629\n",
      "epoch:  409, loss: 0.007699419744312763\n",
      "epoch:  410, loss: 0.007698136381804943\n",
      "epoch:  411, loss: 0.007697660941630602\n",
      "epoch:  412, loss: 0.0076960125006735325\n",
      "epoch:  413, loss: 0.007693767547607422\n",
      "epoch:  414, loss: 0.007693201303482056\n",
      "epoch:  415, loss: 0.007692806422710419\n",
      "epoch:  416, loss: 0.007689496036618948\n",
      "epoch:  417, loss: 0.007688789628446102\n",
      "epoch:  418, loss: 0.00768837658688426\n",
      "epoch:  419, loss: 0.0076857684180140495\n",
      "epoch:  420, loss: 0.007684414740651846\n",
      "epoch:  421, loss: 0.007683936972171068\n",
      "epoch:  422, loss: 0.007682190742343664\n",
      "epoch:  423, loss: 0.007680041249841452\n",
      "epoch:  424, loss: 0.007679481524974108\n",
      "epoch:  425, loss: 0.0076786610297858715\n",
      "epoch:  426, loss: 0.007675654720515013\n",
      "epoch:  427, loss: 0.00767500139772892\n",
      "epoch:  428, loss: 0.007674590218812227\n",
      "epoch:  429, loss: 0.0076713524758815765\n",
      "epoch:  430, loss: 0.0076704989187419415\n",
      "epoch:  431, loss: 0.0076700556091964245\n",
      "epoch:  432, loss: 0.00766728026792407\n",
      "epoch:  433, loss: 0.007666026242077351\n",
      "epoch:  434, loss: 0.007665551267564297\n",
      "epoch:  435, loss: 0.007663835771381855\n",
      "epoch:  436, loss: 0.007661647628992796\n",
      "epoch:  437, loss: 0.007661064621061087\n",
      "epoch:  438, loss: 0.007660662289708853\n",
      "epoch:  439, loss: 0.007657522801309824\n",
      "epoch:  440, loss: 0.007656619418412447\n",
      "epoch:  441, loss: 0.007656187750399113\n",
      "epoch:  442, loss: 0.007653424050658941\n",
      "epoch:  443, loss: 0.0076521895825862885\n",
      "epoch:  444, loss: 0.007651698309928179\n",
      "epoch:  445, loss: 0.007651329040527344\n",
      "epoch:  446, loss: 0.007647987455129623\n",
      "epoch:  447, loss: 0.007647291291505098\n",
      "epoch:  448, loss: 0.007646872662007809\n",
      "epoch:  449, loss: 0.007644138298928738\n",
      "epoch:  450, loss: 0.007642901968210936\n",
      "epoch:  451, loss: 0.007642433512955904\n",
      "epoch:  452, loss: 0.007641242351382971\n",
      "epoch:  453, loss: 0.007638667710125446\n",
      "epoch:  454, loss: 0.007638052571564913\n",
      "epoch:  455, loss: 0.007637652102857828\n",
      "epoch:  456, loss: 0.007634822744876146\n",
      "epoch:  457, loss: 0.007633720524609089\n",
      "epoch:  458, loss: 0.007633275352418423\n",
      "epoch:  459, loss: 0.007631408981978893\n",
      "epoch:  460, loss: 0.007629456464201212\n",
      "epoch:  461, loss: 0.007628905586898327\n",
      "epoch:  462, loss: 0.0076285116374492645\n",
      "epoch:  463, loss: 0.007625635713338852\n",
      "epoch:  464, loss: 0.007624640129506588\n",
      "epoch:  465, loss: 0.007624192629009485\n",
      "epoch:  466, loss: 0.007622689940035343\n",
      "epoch:  467, loss: 0.007620434742420912\n",
      "epoch:  468, loss: 0.007619882468134165\n",
      "epoch:  469, loss: 0.007619495037943125\n",
      "epoch:  470, loss: 0.007616513874381781\n",
      "epoch:  471, loss: 0.00761560071259737\n",
      "epoch:  472, loss: 0.0076151639223098755\n",
      "epoch:  473, loss: 0.007612795103341341\n",
      "epoch:  474, loss: 0.007611302193254232\n",
      "epoch:  475, loss: 0.00761080626398325\n",
      "epoch:  476, loss: 0.00760941207408905\n",
      "epoch:  477, loss: 0.007607019506394863\n",
      "epoch:  478, loss: 0.00760644068941474\n",
      "epoch:  479, loss: 0.007606047671288252\n",
      "epoch:  480, loss: 0.007603189907968044\n",
      "epoch:  481, loss: 0.007602140307426453\n",
      "epoch:  482, loss: 0.007601706776767969\n",
      "epoch:  483, loss: 0.007600043434649706\n",
      "epoch:  484, loss: 0.007597952149808407\n",
      "epoch:  485, loss: 0.007597379386425018\n",
      "epoch:  486, loss: 0.007596985436975956\n",
      "epoch:  487, loss: 0.007593932561576366\n",
      "epoch:  488, loss: 0.0075930641032755375\n",
      "epoch:  489, loss: 0.007592638488858938\n",
      "epoch:  490, loss: 0.007589975371956825\n",
      "epoch:  491, loss: 0.0075887409038841724\n",
      "epoch:  492, loss: 0.007588270585983992\n",
      "epoch:  493, loss: 0.007587251719087362\n",
      "epoch:  494, loss: 0.0075846100226044655\n",
      "epoch:  495, loss: 0.00758392084389925\n",
      "epoch:  496, loss: 0.00758351432159543\n",
      "epoch:  497, loss: 0.007580578792840242\n",
      "epoch:  498, loss: 0.007579573430120945\n",
      "epoch:  499, loss: 0.007579111494123936\n",
      "epoch:  500, loss: 0.00757698155939579\n",
      "epoch:  501, loss: 0.00757526746019721\n",
      "epoch:  502, loss: 0.007574728690087795\n",
      "epoch:  503, loss: 0.007574551738798618\n",
      "epoch:  504, loss: 0.007571028079837561\n",
      "epoch:  505, loss: 0.007570273708552122\n",
      "epoch:  506, loss: 0.0075698611326515675\n",
      "epoch:  507, loss: 0.007567308377474546\n",
      "epoch:  508, loss: 0.0075659179128706455\n",
      "epoch:  509, loss: 0.007565421983599663\n",
      "epoch:  510, loss: 0.007564513478428125\n",
      "epoch:  511, loss: 0.007561640348285437\n",
      "epoch:  512, loss: 0.007560998201370239\n",
      "epoch:  513, loss: 0.00756059680134058\n",
      "epoch:  514, loss: 0.007557935547083616\n",
      "epoch:  515, loss: 0.007556626573204994\n",
      "epoch:  516, loss: 0.0075561595149338245\n",
      "epoch:  517, loss: 0.007554674055427313\n",
      "epoch:  518, loss: 0.007552289869636297\n",
      "epoch:  519, loss: 0.007551661692559719\n",
      "epoch:  520, loss: 0.0075512477196753025\n",
      "epoch:  521, loss: 0.007547954097390175\n",
      "epoch:  522, loss: 0.0075470698066055775\n",
      "epoch:  523, loss: 0.007546630222350359\n",
      "epoch:  524, loss: 0.007544015999883413\n",
      "epoch:  525, loss: 0.007542514707893133\n",
      "epoch:  526, loss: 0.007541995029896498\n",
      "epoch:  527, loss: 0.0075405254028737545\n",
      "epoch:  528, loss: 0.0075379773043096066\n",
      "epoch:  529, loss: 0.007537335157394409\n",
      "epoch:  530, loss: 0.00753690954297781\n",
      "epoch:  531, loss: 0.007533415220677853\n",
      "epoch:  532, loss: 0.0075325737707316875\n",
      "epoch:  533, loss: 0.007532117422670126\n",
      "epoch:  534, loss: 0.007528912741690874\n",
      "epoch:  535, loss: 0.007527714595198631\n",
      "epoch:  536, loss: 0.007527205627411604\n",
      "epoch:  537, loss: 0.007525008637458086\n",
      "epoch:  538, loss: 0.007522826083004475\n",
      "epoch:  539, loss: 0.00752217648550868\n",
      "epoch:  540, loss: 0.007521603722125292\n",
      "epoch:  541, loss: 0.007517972961068153\n",
      "epoch:  542, loss: 0.007517104037106037\n",
      "epoch:  543, loss: 0.007516614161431789\n",
      "epoch:  544, loss: 0.007513576652854681\n",
      "epoch:  545, loss: 0.0075120264664292336\n",
      "epoch:  546, loss: 0.007511473726481199\n",
      "epoch:  547, loss: 0.007509627379477024\n",
      "epoch:  548, loss: 0.007507123984396458\n",
      "epoch:  549, loss: 0.007506447844207287\n",
      "epoch:  550, loss: 0.0075059994123876095\n",
      "epoch:  551, loss: 0.007502573076635599\n",
      "epoch:  552, loss: 0.007501546759158373\n",
      "epoch:  553, loss: 0.0075010512955486774\n",
      "epoch:  554, loss: 0.007499868515878916\n",
      "epoch:  555, loss: 0.007496919948607683\n",
      "epoch:  556, loss: 0.007496176287531853\n",
      "epoch:  557, loss: 0.007495727390050888\n",
      "epoch:  558, loss: 0.0074927471578121185\n",
      "epoch:  559, loss: 0.0074913823045790195\n",
      "epoch:  560, loss: 0.007490837015211582\n",
      "epoch:  561, loss: 0.007490201853215694\n",
      "epoch:  562, loss: 0.0074867974035441875\n",
      "epoch:  563, loss: 0.0074859862215816975\n",
      "epoch:  564, loss: 0.00748551357537508\n",
      "epoch:  565, loss: 0.007482824847102165\n",
      "epoch:  566, loss: 0.00748114800080657\n",
      "epoch:  567, loss: 0.007480574771761894\n",
      "epoch:  568, loss: 0.0074797580018639565\n",
      "epoch:  569, loss: 0.0074764275923371315\n",
      "epoch:  570, loss: 0.00747560802847147\n",
      "epoch:  571, loss: 0.007475124206393957\n",
      "epoch:  572, loss: 0.007472226396203041\n",
      "epoch:  573, loss: 0.007470686454325914\n",
      "epoch:  574, loss: 0.007470120210200548\n",
      "epoch:  575, loss: 0.007469079922884703\n",
      "epoch:  576, loss: 0.007465988397598267\n",
      "epoch:  577, loss: 0.0074651772156357765\n",
      "epoch:  578, loss: 0.007464684545993805\n",
      "epoch:  579, loss: 0.007462246809154749\n",
      "epoch:  580, loss: 0.007460388820618391\n",
      "epoch:  581, loss: 0.007459778804332018\n",
      "epoch:  582, loss: 0.007459321524947882\n",
      "epoch:  583, loss: 0.007455630227923393\n",
      "epoch:  584, loss: 0.007454769220203161\n",
      "epoch:  585, loss: 0.007454273756593466\n",
      "epoch:  586, loss: 0.0074523999355733395\n",
      "epoch:  587, loss: 0.007449972443282604\n",
      "epoch:  588, loss: 0.007449265569448471\n",
      "epoch:  589, loss: 0.0074487971141934395\n",
      "epoch:  590, loss: 0.0074449568055570126\n",
      "epoch:  591, loss: 0.0074442168697714806\n",
      "epoch:  592, loss: 0.007443722803145647\n",
      "epoch:  593, loss: 0.0074404566548764706\n",
      "epoch:  594, loss: 0.007439129054546356\n",
      "epoch:  595, loss: 0.007438571657985449\n",
      "epoch:  596, loss: 0.007437012158334255\n",
      "epoch:  597, loss: 0.00743416603654623\n",
      "epoch:  598, loss: 0.007433445658534765\n",
      "epoch:  599, loss: 0.007432952988892794\n",
      "epoch:  600, loss: 0.007430592551827431\n",
      "epoch:  601, loss: 0.007428516633808613\n",
      "epoch:  602, loss: 0.007427872624248266\n",
      "epoch:  603, loss: 0.0074274069629609585\n",
      "epoch:  604, loss: 0.007423854898661375\n",
      "epoch:  605, loss: 0.0074228402227163315\n",
      "epoch:  606, loss: 0.007422331254929304\n",
      "epoch:  607, loss: 0.007419836241751909\n",
      "epoch:  608, loss: 0.007417931221425533\n",
      "epoch:  609, loss: 0.007417289074510336\n",
      "epoch:  610, loss: 0.007416837848722935\n",
      "epoch:  611, loss: 0.007413875311613083\n",
      "epoch:  612, loss: 0.00741239171475172\n",
      "epoch:  613, loss: 0.007411823142319918\n",
      "epoch:  614, loss: 0.0074117970652878284\n",
      "epoch:  615, loss: 0.0074077630415558815\n",
      "epoch:  616, loss: 0.007406828459352255\n",
      "epoch:  617, loss: 0.007406326942145824\n",
      "epoch:  618, loss: 0.007404626347124577\n",
      "epoch:  619, loss: 0.007401950657367706\n",
      "epoch:  620, loss: 0.0074011837132275105\n",
      "epoch:  621, loss: 0.00740071153268218\n",
      "epoch:  622, loss: 0.00739798229187727\n",
      "epoch:  623, loss: 0.007396205328404903\n",
      "epoch:  624, loss: 0.007395549677312374\n",
      "epoch:  625, loss: 0.0073950886726379395\n",
      "epoch:  626, loss: 0.007391687482595444\n",
      "epoch:  627, loss: 0.0073904674500226974\n",
      "epoch:  628, loss: 0.00738989794626832\n",
      "epoch:  629, loss: 0.007388504687696695\n",
      "epoch:  630, loss: 0.007385530509054661\n",
      "epoch:  631, loss: 0.007384733762592077\n",
      "epoch:  632, loss: 0.007384241558611393\n",
      "epoch:  633, loss: 0.007381449919193983\n",
      "epoch:  634, loss: 0.007379638962447643\n",
      "epoch:  635, loss: 0.007379017770290375\n",
      "epoch:  636, loss: 0.007378550712019205\n",
      "epoch:  637, loss: 0.0073749530129134655\n",
      "epoch:  638, loss: 0.007373813074082136\n",
      "epoch:  639, loss: 0.007373282220214605\n",
      "epoch:  640, loss: 0.007371832616627216\n",
      "epoch:  641, loss: 0.0073688640259206295\n",
      "epoch:  642, loss: 0.007368060760200024\n",
      "epoch:  643, loss: 0.007367562968283892\n",
      "epoch:  644, loss: 0.00736421812325716\n",
      "epoch:  645, loss: 0.007362877018749714\n",
      "epoch:  646, loss: 0.007362281903624535\n",
      "epoch:  647, loss: 0.007361202035099268\n",
      "epoch:  648, loss: 0.007357823196798563\n",
      "epoch:  649, loss: 0.007356980815529823\n",
      "epoch:  650, loss: 0.007356470450758934\n",
      "epoch:  651, loss: 0.007353387773036957\n",
      "epoch:  652, loss: 0.007351772394031286\n",
      "epoch:  653, loss: 0.007351154461503029\n",
      "epoch:  654, loss: 0.007350686471909285\n",
      "epoch:  655, loss: 0.0073470463976264\n",
      "epoch:  656, loss: 0.0073459031991660595\n",
      "epoch:  657, loss: 0.0073453388176858425\n",
      "epoch:  658, loss: 0.007343363482505083\n",
      "epoch:  659, loss: 0.007340725511312485\n",
      "epoch:  660, loss: 0.007339967880398035\n",
      "epoch:  661, loss: 0.007339467294514179\n",
      "epoch:  662, loss: 0.007336597889661789\n",
      "epoch:  663, loss: 0.0073347389698028564\n",
      "epoch:  664, loss: 0.007334081456065178\n",
      "epoch:  665, loss: 0.007333602290600538\n",
      "epoch:  666, loss: 0.007329663261771202\n",
      "epoch:  667, loss: 0.0073287165723741055\n",
      "epoch:  668, loss: 0.007328179199248552\n",
      "epoch:  669, loss: 0.007325963582843542\n",
      "epoch:  670, loss: 0.007323522586375475\n",
      "epoch:  671, loss: 0.007322784513235092\n",
      "epoch:  672, loss: 0.0073222992941737175\n",
      "epoch:  673, loss: 0.007319183554500341\n",
      "epoch:  674, loss: 0.007317584473639727\n",
      "epoch:  675, loss: 0.007316967006772757\n",
      "epoch:  676, loss: 0.007316485978662968\n",
      "epoch:  677, loss: 0.007312668487429619\n",
      "epoch:  678, loss: 0.00731167197227478\n",
      "epoch:  679, loss: 0.007311137858778238\n",
      "epoch:  680, loss: 0.007308749947696924\n",
      "epoch:  681, loss: 0.0073064654134213924\n",
      "epoch:  682, loss: 0.0073057617992162704\n",
      "epoch:  683, loss: 0.0073052686639130116\n",
      "epoch:  684, loss: 0.007301763631403446\n",
      "epoch:  685, loss: 0.007300462108105421\n",
      "epoch:  686, loss: 0.0072998967953026295\n",
      "epoch:  687, loss: 0.007299426477402449\n",
      "epoch:  688, loss: 0.007296339143067598\n",
      "epoch:  689, loss: 0.007294738199561834\n",
      "epoch:  690, loss: 0.007294128183275461\n",
      "epoch:  691, loss: 0.007293643895536661\n",
      "epoch:  692, loss: 0.007290193811058998\n",
      "epoch:  693, loss: 0.007288846652954817\n",
      "epoch:  694, loss: 0.00728825805708766\n",
      "epoch:  695, loss: 0.007287692744284868\n",
      "epoch:  696, loss: 0.007283905521035194\n",
      "epoch:  697, loss: 0.00728292902931571\n",
      "epoch:  698, loss: 0.007282390259206295\n",
      "epoch:  699, loss: 0.007280043791979551\n",
      "epoch:  700, loss: 0.007277720607817173\n",
      "epoch:  701, loss: 0.007276967633515596\n",
      "epoch:  702, loss: 0.00727645680308342\n",
      "epoch:  703, loss: 0.007273753639310598\n",
      "epoch:  704, loss: 0.007271716371178627\n",
      "epoch:  705, loss: 0.0072710285894572735\n",
      "epoch:  706, loss: 0.007270541042089462\n",
      "epoch:  707, loss: 0.007266721688210964\n",
      "epoch:  708, loss: 0.0072656236588954926\n",
      "epoch:  709, loss: 0.007265076506882906\n",
      "epoch:  710, loss: 0.007263822015374899\n",
      "epoch:  711, loss: 0.007260580081492662\n",
      "epoch:  712, loss: 0.007259697187691927\n",
      "epoch:  713, loss: 0.007259179372340441\n",
      "epoch:  714, loss: 0.007256403565406799\n",
      "epoch:  715, loss: 0.007254415657371283\n",
      "epoch:  716, loss: 0.007253695745021105\n",
      "epoch:  717, loss: 0.0072531932964921\n",
      "epoch:  718, loss: 0.007249447982758284\n",
      "epoch:  719, loss: 0.007248210720717907\n",
      "epoch:  720, loss: 0.007247628178447485\n",
      "epoch:  721, loss: 0.007246616296470165\n",
      "epoch:  722, loss: 0.007243052590638399\n",
      "epoch:  723, loss: 0.007242134772241116\n",
      "epoch:  724, loss: 0.00724160997197032\n",
      "epoch:  725, loss: 0.007240367587655783\n",
      "epoch:  726, loss: 0.007236996665596962\n",
      "epoch:  727, loss: 0.007236119359731674\n",
      "epoch:  728, loss: 0.007235584314912558\n",
      "epoch:  729, loss: 0.007232686039060354\n",
      "epoch:  730, loss: 0.007230741903185844\n",
      "epoch:  731, loss: 0.007230037823319435\n",
      "epoch:  732, loss: 0.007229540031403303\n",
      "epoch:  733, loss: 0.007226112764328718\n",
      "epoch:  734, loss: 0.007224513217806816\n",
      "epoch:  735, loss: 0.007223872933536768\n",
      "epoch:  736, loss: 0.007222161162644625\n",
      "epoch:  737, loss: 0.0072187865152955055\n",
      "epoch:  738, loss: 0.007217806298285723\n",
      "epoch:  739, loss: 0.00721722049638629\n",
      "epoch:  740, loss: 0.007213922683149576\n",
      "epoch:  741, loss: 0.0072117638774216175\n",
      "epoch:  742, loss: 0.00721091590821743\n",
      "epoch:  743, loss: 0.007210325915366411\n",
      "epoch:  744, loss: 0.007205970119684935\n",
      "epoch:  745, loss: 0.007204580586403608\n",
      "epoch:  746, loss: 0.0072039044462144375\n",
      "epoch:  747, loss: 0.007202947046607733\n",
      "epoch:  748, loss: 0.007198845036327839\n",
      "epoch:  749, loss: 0.007197709288448095\n",
      "epoch:  750, loss: 0.0071970876306295395\n",
      "epoch:  751, loss: 0.00719507085159421\n",
      "epoch:  752, loss: 0.007191815413534641\n",
      "epoch:  753, loss: 0.007190891541540623\n",
      "epoch:  754, loss: 0.007190296426415443\n",
      "epoch:  755, loss: 0.007187235169112682\n",
      "epoch:  756, loss: 0.007184953894466162\n",
      "epoch:  757, loss: 0.007184143643826246\n",
      "epoch:  758, loss: 0.007183567155152559\n",
      "epoch:  759, loss: 0.007180125918239355\n",
      "epoch:  760, loss: 0.007178169209510088\n",
      "epoch:  761, loss: 0.007177417632192373\n",
      "epoch:  762, loss: 0.007176857907325029\n",
      "epoch:  763, loss: 0.00717270839959383\n",
      "epoch:  764, loss: 0.007171320728957653\n",
      "epoch:  765, loss: 0.007170645520091057\n",
      "epoch:  766, loss: 0.0071694147773087025\n",
      "epoch:  767, loss: 0.0071655320934951305\n",
      "epoch:  768, loss: 0.007164474576711655\n",
      "epoch:  769, loss: 0.00716386828571558\n",
      "epoch:  770, loss: 0.007161883171647787\n",
      "epoch:  771, loss: 0.007158663589507341\n",
      "epoch:  772, loss: 0.007157709449529648\n",
      "epoch:  773, loss: 0.007157128769904375\n",
      "epoch:  774, loss: 0.00715501606464386\n",
      "epoch:  775, loss: 0.0071518742479383945\n",
      "epoch:  776, loss: 0.007150922901928425\n",
      "epoch:  777, loss: 0.007150322198867798\n",
      "epoch:  778, loss: 0.007148570381104946\n",
      "epoch:  779, loss: 0.007145093288272619\n",
      "epoch:  780, loss: 0.007144066039472818\n",
      "epoch:  781, loss: 0.007143441122025251\n",
      "epoch:  782, loss: 0.007142035290598869\n",
      "epoch:  783, loss: 0.007138276007026434\n",
      "epoch:  784, loss: 0.007137133274227381\n",
      "epoch:  785, loss: 0.007136492058634758\n",
      "epoch:  786, loss: 0.007134689018130302\n",
      "epoch:  787, loss: 0.007131222169846296\n",
      "epoch:  788, loss: 0.007130185607820749\n",
      "epoch:  789, loss: 0.007129563018679619\n",
      "epoch:  790, loss: 0.0071266726590693\n",
      "epoch:  791, loss: 0.007123996037989855\n",
      "epoch:  792, loss: 0.007123075425624847\n",
      "epoch:  793, loss: 0.007122439797967672\n",
      "epoch:  794, loss: 0.007118504028767347\n",
      "epoch:  795, loss: 0.007116349879652262\n",
      "epoch:  796, loss: 0.007115467451512814\n",
      "epoch:  797, loss: 0.007114839274436235\n",
      "epoch:  798, loss: 0.007111181505024433\n",
      "epoch:  799, loss: 0.0071088457480072975\n",
      "epoch:  800, loss: 0.007107943762093782\n",
      "epoch:  801, loss: 0.007107286248356104\n",
      "epoch:  802, loss: 0.007104270160198212\n",
      "epoch:  803, loss: 0.00710135605186224\n",
      "epoch:  804, loss: 0.007100325543433428\n",
      "epoch:  805, loss: 0.0070996517315506935\n",
      "epoch:  806, loss: 0.007098017726093531\n",
      "epoch:  807, loss: 0.007094081025570631\n",
      "epoch:  808, loss: 0.0070928120985627174\n",
      "epoch:  809, loss: 0.007092099171131849\n",
      "epoch:  810, loss: 0.007091341074556112\n",
      "epoch:  811, loss: 0.007086627185344696\n",
      "epoch:  812, loss: 0.007085368502885103\n",
      "epoch:  813, loss: 0.007084613665938377\n",
      "epoch:  814, loss: 0.0070841992273926735\n",
      "epoch:  815, loss: 0.007079159840941429\n",
      "epoch:  816, loss: 0.0070777591317892075\n",
      "epoch:  817, loss: 0.007077014073729515\n",
      "epoch:  818, loss: 0.0070767938159406185\n",
      "epoch:  819, loss: 0.007071508094668388\n",
      "epoch:  820, loss: 0.00707012414932251\n",
      "epoch:  821, loss: 0.007069326471537352\n",
      "epoch:  822, loss: 0.0070685725659132\n",
      "epoch:  823, loss: 0.007063726428896189\n",
      "epoch:  824, loss: 0.0070623611100018024\n",
      "epoch:  825, loss: 0.007061617448925972\n",
      "epoch:  826, loss: 0.00705958716571331\n",
      "epoch:  827, loss: 0.007055692374706268\n",
      "epoch:  828, loss: 0.007054484449326992\n",
      "epoch:  829, loss: 0.00705373752862215\n",
      "epoch:  830, loss: 0.007051368709653616\n",
      "epoch:  831, loss: 0.007047521881759167\n",
      "epoch:  832, loss: 0.007046313025057316\n",
      "epoch:  833, loss: 0.007045578211545944\n",
      "epoch:  834, loss: 0.007044246420264244\n",
      "epoch:  835, loss: 0.0070395865477621555\n",
      "epoch:  836, loss: 0.007038191892206669\n",
      "epoch:  837, loss: 0.007037429604679346\n",
      "epoch:  838, loss: 0.007034590467810631\n",
      "epoch:  839, loss: 0.007031161803752184\n",
      "epoch:  840, loss: 0.007029999513179064\n",
      "epoch:  841, loss: 0.007029280532151461\n",
      "epoch:  842, loss: 0.007027102634310722\n",
      "epoch:  843, loss: 0.00702313520014286\n",
      "epoch:  844, loss: 0.007021906320005655\n",
      "epoch:  845, loss: 0.007021129596978426\n",
      "epoch:  846, loss: 0.0070204767398536205\n",
      "epoch:  847, loss: 0.007016334217041731\n",
      "epoch:  848, loss: 0.0070139383897185326\n",
      "epoch:  849, loss: 0.007012960966676474\n",
      "epoch:  850, loss: 0.007012253627181053\n",
      "epoch:  851, loss: 0.007007782347500324\n",
      "epoch:  852, loss: 0.007005573250353336\n",
      "epoch:  853, loss: 0.007004646118730307\n",
      "epoch:  854, loss: 0.0070039681158959866\n",
      "epoch:  855, loss: 0.00699871638789773\n",
      "epoch:  856, loss: 0.006997203454375267\n",
      "epoch:  857, loss: 0.006996407173573971\n",
      "epoch:  858, loss: 0.006995899602770805\n",
      "epoch:  859, loss: 0.006990288384258747\n",
      "epoch:  860, loss: 0.006988726556301117\n",
      "epoch:  861, loss: 0.006987906526774168\n",
      "epoch:  862, loss: 0.006987282540649176\n",
      "epoch:  863, loss: 0.006981790531426668\n",
      "epoch:  864, loss: 0.006980294827371836\n",
      "epoch:  865, loss: 0.006979483179748058\n",
      "epoch:  866, loss: 0.006978305522352457\n",
      "epoch:  867, loss: 0.0069731720723211765\n",
      "epoch:  868, loss: 0.006971726659685373\n",
      "epoch:  869, loss: 0.0069709038361907005\n",
      "epoch:  870, loss: 0.006970209535211325\n",
      "epoch:  871, loss: 0.0069655440747737885\n",
      "epoch:  872, loss: 0.006963241379708052\n",
      "epoch:  873, loss: 0.00696226442232728\n",
      "epoch:  874, loss: 0.006961538456380367\n",
      "epoch:  875, loss: 0.006956739816814661\n",
      "epoch:  876, loss: 0.006954485084861517\n",
      "epoch:  877, loss: 0.006953456439077854\n",
      "epoch:  878, loss: 0.00695270299911499\n",
      "epoch:  879, loss: 0.006948241963982582\n",
      "epoch:  880, loss: 0.006945564411580563\n",
      "epoch:  881, loss: 0.006944512948393822\n",
      "epoch:  882, loss: 0.006943754386156797\n",
      "epoch:  883, loss: 0.006939935032278299\n",
      "epoch:  884, loss: 0.006936897523701191\n",
      "epoch:  885, loss: 0.0069357105530798435\n",
      "epoch:  886, loss: 0.0069349417462944984\n",
      "epoch:  887, loss: 0.006932104006409645\n",
      "epoch:  888, loss: 0.00692824088037014\n",
      "epoch:  889, loss: 0.006926967296749353\n",
      "epoch:  890, loss: 0.006926192436367273\n",
      "epoch:  891, loss: 0.006925276480615139\n",
      "epoch:  892, loss: 0.0069197979755699635\n",
      "epoch:  893, loss: 0.006918191909790039\n",
      "epoch:  894, loss: 0.00691732345148921\n",
      "epoch:  895, loss: 0.0069166128523647785\n",
      "epoch:  896, loss: 0.006912181153893471\n",
      "epoch:  897, loss: 0.006909751333296299\n",
      "epoch:  898, loss: 0.006908753886818886\n",
      "epoch:  899, loss: 0.00690801627933979\n",
      "epoch:  900, loss: 0.006904623471200466\n",
      "epoch:  901, loss: 0.006901364307850599\n",
      "epoch:  902, loss: 0.006900255102664232\n",
      "epoch:  903, loss: 0.006899511441588402\n",
      "epoch:  904, loss: 0.006895860191434622\n",
      "epoch:  905, loss: 0.006892784498631954\n",
      "epoch:  906, loss: 0.006891659460961819\n",
      "epoch:  907, loss: 0.006890914402902126\n",
      "epoch:  908, loss: 0.006887332070618868\n",
      "epoch:  909, loss: 0.006884187925606966\n",
      "epoch:  910, loss: 0.006883046589791775\n",
      "epoch:  911, loss: 0.0068822940811514854\n",
      "epoch:  912, loss: 0.006878828629851341\n",
      "epoch:  913, loss: 0.006875649560242891\n",
      "epoch:  914, loss: 0.006874506361782551\n",
      "epoch:  915, loss: 0.006873762235045433\n",
      "epoch:  916, loss: 0.006870420649647713\n",
      "epoch:  917, loss: 0.00686714518815279\n",
      "epoch:  918, loss: 0.006865987088531256\n",
      "epoch:  919, loss: 0.006865222938358784\n",
      "epoch:  920, loss: 0.006865156814455986\n",
      "epoch:  921, loss: 0.006859276909381151\n",
      "epoch:  922, loss: 0.006857583299279213\n",
      "epoch:  923, loss: 0.006856744643300772\n",
      "epoch:  924, loss: 0.0068560391664505005\n",
      "epoch:  925, loss: 0.0068519203923642635\n",
      "epoch:  926, loss: 0.006849322002381086\n",
      "epoch:  927, loss: 0.006848318502306938\n",
      "epoch:  928, loss: 0.00684759858995676\n",
      "epoch:  929, loss: 0.006843825336545706\n",
      "epoch:  930, loss: 0.006840952672064304\n",
      "epoch:  931, loss: 0.006839881185442209\n",
      "epoch:  932, loss: 0.006839144509285688\n",
      "epoch:  933, loss: 0.006836415268480778\n",
      "epoch:  934, loss: 0.00683276541531086\n",
      "epoch:  935, loss: 0.006831538397818804\n",
      "epoch:  936, loss: 0.006830775644630194\n",
      "epoch:  937, loss: 0.006829427555203438\n",
      "epoch:  938, loss: 0.006824685260653496\n",
      "epoch:  939, loss: 0.00682325242087245\n",
      "epoch:  940, loss: 0.006822451017796993\n",
      "epoch:  941, loss: 0.006821771617978811\n",
      "epoch:  942, loss: 0.006816835142672062\n",
      "epoch:  943, loss: 0.006815019529312849\n",
      "epoch:  944, loss: 0.006814135238528252\n",
      "epoch:  945, loss: 0.006813441403210163\n",
      "epoch:  946, loss: 0.006809673272073269\n",
      "epoch:  947, loss: 0.006806915160268545\n",
      "epoch:  948, loss: 0.00680586788803339\n",
      "epoch:  949, loss: 0.006805131211876869\n",
      "epoch:  950, loss: 0.0068025141954422\n",
      "epoch:  951, loss: 0.006798815447837114\n",
      "epoch:  952, loss: 0.0067975339479744434\n",
      "epoch:  953, loss: 0.00679676653817296\n",
      "epoch:  954, loss: 0.006795849651098251\n",
      "epoch:  955, loss: 0.006790706887841225\n",
      "epoch:  956, loss: 0.006789174396544695\n",
      "epoch:  957, loss: 0.006788328755646944\n",
      "epoch:  958, loss: 0.006787627004086971\n",
      "epoch:  959, loss: 0.0067825051955878735\n",
      "epoch:  960, loss: 0.006780609954148531\n",
      "epoch:  961, loss: 0.0067797028459608555\n",
      "epoch:  962, loss: 0.0067789810709655285\n",
      "epoch:  963, loss: 0.006774335168302059\n",
      "epoch:  964, loss: 0.006772076245397329\n",
      "epoch:  965, loss: 0.006771031301468611\n",
      "epoch:  966, loss: 0.006770248059183359\n",
      "epoch:  967, loss: 0.006766757927834988\n",
      "epoch:  968, loss: 0.0067632743157446384\n",
      "epoch:  969, loss: 0.006762001197785139\n",
      "epoch:  970, loss: 0.006761164404451847\n",
      "epoch:  971, loss: 0.006760107818990946\n",
      "epoch:  972, loss: 0.006754766684025526\n",
      "epoch:  973, loss: 0.006753019988536835\n",
      "epoch:  974, loss: 0.006752124987542629\n",
      "epoch:  975, loss: 0.006751358974725008\n",
      "epoch:  976, loss: 0.006747582461684942\n",
      "epoch:  977, loss: 0.006744366139173508\n",
      "epoch:  978, loss: 0.006743113975971937\n",
      "epoch:  979, loss: 0.006742282770574093\n",
      "epoch:  980, loss: 0.006742275785654783\n",
      "epoch:  981, loss: 0.006735994480550289\n",
      "epoch:  982, loss: 0.006734111811965704\n",
      "epoch:  983, loss: 0.006733132991939783\n",
      "epoch:  984, loss: 0.0067323422990739346\n",
      "epoch:  985, loss: 0.0067275334149599075\n",
      "epoch:  986, loss: 0.006724858656525612\n",
      "epoch:  987, loss: 0.006723695434629917\n",
      "epoch:  988, loss: 0.006722850725054741\n",
      "epoch:  989, loss: 0.006722127087414265\n",
      "epoch:  990, loss: 0.0067161512561142445\n",
      "epoch:  991, loss: 0.006714319344609976\n",
      "epoch:  992, loss: 0.006713301874697208\n",
      "epoch:  993, loss: 0.006712488830089569\n",
      "epoch:  994, loss: 0.006708428729325533\n",
      "epoch:  995, loss: 0.006705035455524921\n",
      "epoch:  996, loss: 0.006703678052872419\n",
      "epoch:  997, loss: 0.006702761631458998\n",
      "epoch:  998, loss: 0.006701954640448093\n",
      "epoch:  999, loss: 0.006696516647934914\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size=X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "opt = torch_numopt.GradientDescentLS(model=model, lr=1, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"armijo\")\n",
    "# opt = torch_numopt.GradientDescentLS(model=model, lr=1, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"wolfe\")\n",
    "# opt = torch_numopt.GradientDescentLS(model=model, lr=1, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"strong-wolfe\")\n",
    "# opt = torch_numopt.GradientDescentLS(model=model, lr=1, c1=1e-4, tau=0.1, line_search_method=\"backtrack\", line_search_cond=\"goldstein\")\n",
    "\n",
    "all_loss = {}\n",
    "for epoch in range(1000):\n",
    "    print(\"epoch: \", epoch, end=\"\")\n",
    "    all_loss[epoch + 1] = 0\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y, loss_fn)\n",
    "\n",
    "        all_loss[epoch + 1] += loss\n",
    "    all_loss[epoch + 1] /= len(data_loader)\n",
    "    print(\", loss: {}\".format(all_loss[epoch + 1].cpu().detach().numpy().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics: R2 = 0.7621390197729612\n",
      "Test metrics:  R2 = 0.7524372927784362\n"
     ]
    }
   ],
   "source": [
    "pred_train = model.forward(X_train).cpu().detach()\n",
    "pred_test = model.forward(X_test).cpu().detach()\n",
    "print(f\"Train metrics: R2 = {r2_score(pred_train, y_train.cpu())}\")\n",
    "print(f\"Test metrics:  R2 = {r2_score(pred_test, y_test.cpu())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bisection line search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, loss: 0.16318592429161072\n",
      "epoch:  1, loss: 0.048198092728853226\n",
      "epoch:  2, loss: 0.03641186282038689\n",
      "epoch:  3, loss: 0.035575322806835175\n",
      "epoch:  4, loss: 0.03551589697599411\n",
      "epoch:  5, loss: 0.03548041358590126\n",
      "epoch:  6, loss: 0.03545035421848297\n",
      "epoch:  7, loss: 0.03541966527700424\n",
      "epoch:  8, loss: 0.03538878262042999\n",
      "epoch:  9, loss: 0.035365670919418335\n",
      "epoch:  10, loss: 0.035329774022102356\n",
      "epoch:  11, loss: 0.035306416451931\n",
      "epoch:  12, loss: 0.035271935164928436\n",
      "epoch:  13, loss: 0.03524819388985634\n",
      "epoch:  14, loss: 0.0352153517305851\n",
      "epoch:  15, loss: 0.035190921276807785\n",
      "epoch:  16, loss: 0.03515955060720444\n",
      "epoch:  17, loss: 0.03513413667678833\n",
      "epoch:  18, loss: 0.03510410711169243\n",
      "epoch:  19, loss: 0.03507726266980171\n",
      "epoch:  20, loss: 0.035048674792051315\n",
      "epoch:  21, loss: 0.03502007946372032\n",
      "epoch:  22, loss: 0.03499317541718483\n",
      "epoch:  23, loss: 0.03496848791837692\n",
      "epoch:  24, loss: 0.03493164852261543\n",
      "epoch:  25, loss: 0.03490357846021652\n",
      "epoch:  26, loss: 0.034878384321928024\n",
      "epoch:  27, loss: 0.03484022617340088\n",
      "epoch:  28, loss: 0.034810684621334076\n",
      "epoch:  29, loss: 0.03478486090898514\n",
      "epoch:  30, loss: 0.034744590520858765\n",
      "epoch:  31, loss: 0.034712787717580795\n",
      "epoch:  32, loss: 0.034678731113672256\n",
      "epoch:  33, loss: 0.03465237095952034\n",
      "epoch:  34, loss: 0.0346105694770813\n",
      "epoch:  35, loss: 0.034582119435071945\n",
      "epoch:  36, loss: 0.034539878368377686\n",
      "epoch:  37, loss: 0.034508563578128815\n",
      "epoch:  38, loss: 0.034466128796339035\n",
      "epoch:  39, loss: 0.03443107381463051\n",
      "epoch:  40, loss: 0.034389905631542206\n",
      "epoch:  41, loss: 0.03434969484806061\n",
      "epoch:  42, loss: 0.03431687876582146\n",
      "epoch:  43, loss: 0.034263841807842255\n",
      "epoch:  44, loss: 0.03422724828124046\n",
      "epoch:  45, loss: 0.03417364880442619\n",
      "epoch:  46, loss: 0.0341322086751461\n",
      "epoch:  47, loss: 0.03408017009496689\n",
      "epoch:  48, loss: 0.034031543880701065\n",
      "epoch:  49, loss: 0.033990681171417236\n",
      "epoch:  50, loss: 0.03392516076564789\n",
      "epoch:  51, loss: 0.03387872502207756\n",
      "epoch:  52, loss: 0.03381478786468506\n",
      "epoch:  53, loss: 0.0337597131729126\n",
      "epoch:  54, loss: 0.03371156007051468\n",
      "epoch:  55, loss: 0.03363308683037758\n",
      "epoch:  56, loss: 0.03357701376080513\n",
      "epoch:  57, loss: 0.033501237630844116\n",
      "epoch:  58, loss: 0.033448293805122375\n",
      "epoch:  59, loss: 0.033346276730298996\n",
      "epoch:  60, loss: 0.03325525298714638\n",
      "epoch:  61, loss: 0.03318064287304878\n",
      "epoch:  62, loss: 0.03311305120587349\n",
      "epoch:  63, loss: 0.03300738334655762\n",
      "epoch:  64, loss: 0.032925620675086975\n",
      "epoch:  65, loss: 0.03284914419054985\n",
      "epoch:  66, loss: 0.03272852674126625\n",
      "epoch:  67, loss: 0.03263445943593979\n",
      "epoch:  68, loss: 0.032546188682317734\n",
      "epoch:  69, loss: 0.032414235174655914\n",
      "epoch:  70, loss: 0.032326340675354004\n",
      "epoch:  71, loss: 0.03214616701006889\n",
      "epoch:  72, loss: 0.031974852085113525\n",
      "epoch:  73, loss: 0.031856391578912735\n",
      "epoch:  74, loss: 0.03173913061618805\n",
      "epoch:  75, loss: 0.031565722078084946\n",
      "epoch:  76, loss: 0.03143738582730293\n",
      "epoch:  77, loss: 0.031187662854790688\n",
      "epoch:  78, loss: 0.030944522470235825\n",
      "epoch:  79, loss: 0.030785195529460907\n",
      "epoch:  80, loss: 0.030623674392700195\n",
      "epoch:  81, loss: 0.030457811430096626\n",
      "epoch:  82, loss: 0.03021363355219364\n",
      "epoch:  83, loss: 0.03001207485795021\n",
      "epoch:  84, loss: 0.029633907601237297\n",
      "epoch:  85, loss: 0.02923448570072651\n",
      "epoch:  86, loss: 0.029007600620388985\n",
      "epoch:  87, loss: 0.02877243608236313\n",
      "epoch:  88, loss: 0.02852909080684185\n",
      "epoch:  89, loss: 0.028276976197957993\n",
      "epoch:  90, loss: 0.028015604242682457\n",
      "epoch:  91, loss: 0.027748312801122665\n",
      "epoch:  92, loss: 0.027474235743284225\n",
      "epoch:  93, loss: 0.027195395901799202\n",
      "epoch:  94, loss: 0.026913248002529144\n",
      "epoch:  95, loss: 0.026690172031521797\n",
      "epoch:  96, loss: 0.026062697172164917\n",
      "epoch:  97, loss: 0.025481106713414192\n",
      "epoch:  98, loss: 0.025203006342053413\n",
      "epoch:  99, loss: 0.024507340043783188\n",
      "epoch:  100, loss: 0.024165727198123932\n",
      "epoch:  101, loss: 0.02390490472316742\n",
      "epoch:  102, loss: 0.023183034732937813\n",
      "epoch:  103, loss: 0.022918378934264183\n",
      "epoch:  104, loss: 0.022202271968126297\n",
      "epoch:  105, loss: 0.02189941145479679\n",
      "epoch:  106, loss: 0.02132548578083515\n",
      "epoch:  107, loss: 0.02096334472298622\n",
      "epoch:  108, loss: 0.02059691771864891\n",
      "epoch:  109, loss: 0.02032450959086418\n",
      "epoch:  110, loss: 0.019948720932006836\n",
      "epoch:  111, loss: 0.01969614066183567\n",
      "epoch:  112, loss: 0.019326597452163696\n",
      "epoch:  113, loss: 0.01908194273710251\n",
      "epoch:  114, loss: 0.018743352964520454\n",
      "epoch:  115, loss: 0.018493052572011948\n",
      "epoch:  116, loss: 0.01827976666390896\n",
      "epoch:  117, loss: 0.01793978177011013\n",
      "epoch:  118, loss: 0.017720190808176994\n",
      "epoch:  119, loss: 0.01751929707825184\n",
      "epoch:  120, loss: 0.01720387116074562\n",
      "epoch:  121, loss: 0.016994675621390343\n",
      "epoch:  122, loss: 0.016805147752165794\n",
      "epoch:  123, loss: 0.016563640907406807\n",
      "epoch:  124, loss: 0.016362594440579414\n",
      "epoch:  125, loss: 0.015750182792544365\n",
      "epoch:  126, loss: 0.015585801564157009\n",
      "epoch:  127, loss: 0.015428347513079643\n",
      "epoch:  128, loss: 0.015276255086064339\n",
      "epoch:  129, loss: 0.01512798760086298\n",
      "epoch:  130, loss: 0.014983232133090496\n",
      "epoch:  131, loss: 0.014842374250292778\n",
      "epoch:  132, loss: 0.014703955501317978\n",
      "epoch:  133, loss: 0.01451959554105997\n",
      "epoch:  134, loss: 0.014359723776578903\n",
      "epoch:  135, loss: 0.01386075746268034\n",
      "epoch:  136, loss: 0.013489808887243271\n",
      "epoch:  137, loss: 0.013351628556847572\n",
      "epoch:  138, loss: 0.012950347736477852\n",
      "epoch:  139, loss: 0.012860589660704136\n",
      "epoch:  140, loss: 0.012774395756423473\n",
      "epoch:  141, loss: 0.012692559510469437\n",
      "epoch:  142, loss: 0.012626107782125473\n",
      "epoch:  143, loss: 0.012305845506489277\n",
      "epoch:  144, loss: 0.01224873960018158\n",
      "epoch:  145, loss: 0.011958301067352295\n",
      "epoch:  146, loss: 0.011903367936611176\n",
      "epoch:  147, loss: 0.011648878455162048\n",
      "epoch:  148, loss: 0.011587856337428093\n",
      "epoch:  149, loss: 0.011380460113286972\n",
      "epoch:  150, loss: 0.011301805265247822\n",
      "epoch:  151, loss: 0.011208372190594673\n",
      "epoch:  152, loss: 0.011154713109135628\n",
      "epoch:  153, loss: 0.010997271165251732\n",
      "epoch:  154, loss: 0.010930139571428299\n",
      "epoch:  155, loss: 0.010895177721977234\n",
      "epoch:  156, loss: 0.010728208348155022\n",
      "epoch:  157, loss: 0.010672526434063911\n",
      "epoch:  158, loss: 0.010606982745230198\n",
      "epoch:  159, loss: 0.010557408444583416\n",
      "epoch:  160, loss: 0.01049564778804779\n",
      "epoch:  161, loss: 0.010448992252349854\n",
      "epoch:  162, loss: 0.010391934774816036\n",
      "epoch:  163, loss: 0.010346447117626667\n",
      "epoch:  164, loss: 0.010295424610376358\n",
      "epoch:  165, loss: 0.010248920880258083\n",
      "epoch:  166, loss: 0.010214036330580711\n",
      "epoch:  167, loss: 0.010183637030422688\n",
      "epoch:  168, loss: 0.010154691524803638\n",
      "epoch:  169, loss: 0.010096865706145763\n",
      "epoch:  170, loss: 0.010066322050988674\n",
      "epoch:  171, loss: 0.010016651824116707\n",
      "epoch:  172, loss: 0.00998261384665966\n",
      "epoch:  173, loss: 0.009952482767403126\n",
      "epoch:  174, loss: 0.009926723316311836\n",
      "epoch:  175, loss: 0.009901942685246468\n",
      "epoch:  176, loss: 0.00985512975603342\n",
      "epoch:  177, loss: 0.00982675887644291\n",
      "epoch:  178, loss: 0.009792539291083813\n",
      "epoch:  179, loss: 0.009763654321432114\n",
      "epoch:  180, loss: 0.009741001762449741\n",
      "epoch:  181, loss: 0.009720232337713242\n",
      "epoch:  182, loss: 0.00967951025813818\n",
      "epoch:  183, loss: 0.009654570370912552\n",
      "epoch:  184, loss: 0.00963155459612608\n",
      "epoch:  185, loss: 0.009616118855774403\n",
      "epoch:  186, loss: 0.009552729316055775\n",
      "epoch:  187, loss: 0.009518669918179512\n",
      "epoch:  188, loss: 0.009504313580691814\n",
      "epoch:  189, loss: 0.00945561658591032\n",
      "epoch:  190, loss: 0.009417805820703506\n",
      "epoch:  191, loss: 0.009404403157532215\n",
      "epoch:  192, loss: 0.009374520741403103\n",
      "epoch:  193, loss: 0.009357457980513573\n",
      "epoch:  194, loss: 0.009341426193714142\n",
      "epoch:  195, loss: 0.009329848922789097\n",
      "epoch:  196, loss: 0.00929468683898449\n",
      "epoch:  197, loss: 0.009260309860110283\n",
      "epoch:  198, loss: 0.00924907810986042\n",
      "epoch:  199, loss: 0.00922461412847042\n",
      "epoch:  200, loss: 0.009210249409079552\n",
      "epoch:  201, loss: 0.009196526370942593\n",
      "epoch:  202, loss: 0.009187061339616776\n",
      "epoch:  203, loss: 0.009157874621450901\n",
      "epoch:  204, loss: 0.009127832017838955\n",
      "epoch:  205, loss: 0.009118434973061085\n",
      "epoch:  206, loss: 0.009099387563765049\n",
      "epoch:  207, loss: 0.009087926708161831\n",
      "epoch:  208, loss: 0.009076653979718685\n",
      "epoch:  209, loss: 0.009068019688129425\n",
      "epoch:  210, loss: 0.009051105938851833\n",
      "epoch:  211, loss: 0.009040715172886848\n",
      "epoch:  212, loss: 0.009030725806951523\n",
      "epoch:  213, loss: 0.009022899903357029\n",
      "epoch:  214, loss: 0.009007812477648258\n",
      "epoch:  215, loss: 0.00899813324213028\n",
      "epoch:  216, loss: 0.008989174850285053\n",
      "epoch:  217, loss: 0.008982185274362564\n",
      "epoch:  218, loss: 0.008970025926828384\n",
      "epoch:  219, loss: 0.008960064500570297\n",
      "epoch:  220, loss: 0.008952231146395206\n",
      "epoch:  221, loss: 0.008945577777922153\n",
      "epoch:  222, loss: 0.008937530219554901\n",
      "epoch:  223, loss: 0.008931802585721016\n",
      "epoch:  224, loss: 0.008919461630284786\n",
      "epoch:  225, loss: 0.008912482298910618\n",
      "epoch:  226, loss: 0.008905290625989437\n",
      "epoch:  227, loss: 0.008900094777345657\n",
      "epoch:  228, loss: 0.008891461417078972\n",
      "epoch:  229, loss: 0.008883214555680752\n",
      "epoch:  230, loss: 0.008877350017428398\n",
      "epoch:  231, loss: 0.00887218490242958\n",
      "epoch:  232, loss: 0.008864698000252247\n",
      "epoch:  233, loss: 0.008856873027980328\n",
      "epoch:  234, loss: 0.008851705119013786\n",
      "epoch:  235, loss: 0.008846789598464966\n",
      "epoch:  236, loss: 0.008841237984597683\n",
      "epoch:  237, loss: 0.00883711501955986\n",
      "epoch:  238, loss: 0.008829024620354176\n",
      "epoch:  239, loss: 0.008823416195809841\n",
      "epoch:  240, loss: 0.008818387053906918\n",
      "epoch:  241, loss: 0.00881446897983551\n",
      "epoch:  242, loss: 0.008808251470327377\n",
      "epoch:  243, loss: 0.008801666088402271\n",
      "epoch:  244, loss: 0.008797249756753445\n",
      "epoch:  245, loss: 0.00879291258752346\n",
      "epoch:  246, loss: 0.008788028731942177\n",
      "epoch:  247, loss: 0.008784233592450619\n",
      "epoch:  248, loss: 0.008778066374361515\n",
      "epoch:  249, loss: 0.008772025816142559\n",
      "epoch:  250, loss: 0.008767846040427685\n",
      "epoch:  251, loss: 0.008764153346419334\n",
      "epoch:  252, loss: 0.008760045282542706\n",
      "epoch:  253, loss: 0.00875322800129652\n",
      "epoch:  254, loss: 0.008749661035835743\n",
      "epoch:  255, loss: 0.008745934814214706\n",
      "epoch:  256, loss: 0.008742228150367737\n",
      "epoch:  257, loss: 0.008738940581679344\n",
      "epoch:  258, loss: 0.00873512588441372\n",
      "epoch:  259, loss: 0.008732178248465061\n",
      "epoch:  260, loss: 0.008727454580366611\n",
      "epoch:  261, loss: 0.008722511120140553\n",
      "epoch:  262, loss: 0.008719119243323803\n",
      "epoch:  263, loss: 0.008716135285794735\n",
      "epoch:  264, loss: 0.008712961338460445\n",
      "epoch:  265, loss: 0.008707080967724323\n",
      "epoch:  266, loss: 0.008704046718776226\n",
      "epoch:  267, loss: 0.008700934238731861\n",
      "epoch:  268, loss: 0.008697792887687683\n",
      "epoch:  269, loss: 0.008695164695382118\n",
      "epoch:  270, loss: 0.008691108785569668\n",
      "epoch:  271, loss: 0.008686725050210953\n",
      "epoch:  272, loss: 0.008683742955327034\n",
      "epoch:  273, loss: 0.008681138977408409\n",
      "epoch:  274, loss: 0.008678204379975796\n",
      "epoch:  275, loss: 0.008675754070281982\n",
      "epoch:  276, loss: 0.00867187138646841\n",
      "epoch:  277, loss: 0.008668186143040657\n",
      "epoch:  278, loss: 0.008665484376251698\n",
      "epoch:  279, loss: 0.008663194254040718\n",
      "epoch:  280, loss: 0.008660811930894852\n",
      "epoch:  281, loss: 0.008656397461891174\n",
      "epoch:  282, loss: 0.008654321543872356\n",
      "epoch:  283, loss: 0.00865213293582201\n",
      "epoch:  284, loss: 0.008650013245642185\n",
      "epoch:  285, loss: 0.008648017421364784\n",
      "epoch:  286, loss: 0.008645959198474884\n",
      "epoch:  287, loss: 0.008644088171422482\n",
      "epoch:  288, loss: 0.008642122149467468\n",
      "epoch:  289, loss: 0.008640270680189133\n",
      "epoch:  290, loss: 0.00863836519420147\n",
      "epoch:  291, loss: 0.008636544458568096\n",
      "epoch:  292, loss: 0.008634719997644424\n",
      "epoch:  293, loss: 0.008632921613752842\n",
      "epoch:  294, loss: 0.008631013333797455\n",
      "epoch:  295, loss: 0.008629304356873035\n",
      "epoch:  296, loss: 0.008627462200820446\n",
      "epoch:  297, loss: 0.008625754155218601\n",
      "epoch:  298, loss: 0.00862405076622963\n",
      "epoch:  299, loss: 0.008622252382338047\n",
      "epoch:  300, loss: 0.008620630018413067\n",
      "epoch:  301, loss: 0.0086188530549407\n",
      "epoch:  302, loss: 0.008617428131401539\n",
      "epoch:  303, loss: 0.008615572936832905\n",
      "epoch:  304, loss: 0.008614188991487026\n",
      "epoch:  305, loss: 0.008612253703176975\n",
      "epoch:  306, loss: 0.008611124940216541\n",
      "epoch:  307, loss: 0.008609033189713955\n",
      "epoch:  308, loss: 0.008608005940914154\n",
      "epoch:  309, loss: 0.008605863898992538\n",
      "epoch:  310, loss: 0.00860497448593378\n",
      "epoch:  311, loss: 0.008602771908044815\n",
      "epoch:  312, loss: 0.008601948618888855\n",
      "epoch:  313, loss: 0.008599836379289627\n",
      "epoch:  314, loss: 0.00859904009848833\n",
      "epoch:  315, loss: 0.008596978150308132\n",
      "epoch:  316, loss: 0.00859629176557064\n",
      "epoch:  317, loss: 0.008594181388616562\n",
      "epoch:  318, loss: 0.008593437261879444\n",
      "epoch:  319, loss: 0.008591481484472752\n",
      "epoch:  320, loss: 0.00859068613499403\n",
      "epoch:  321, loss: 0.008588748052716255\n",
      "epoch:  322, loss: 0.008588054217398167\n",
      "epoch:  323, loss: 0.008586330339312553\n",
      "epoch:  324, loss: 0.008585122413933277\n",
      "epoch:  325, loss: 0.008582576178014278\n",
      "epoch:  326, loss: 0.00858212634921074\n",
      "epoch:  327, loss: 0.008579044602811337\n",
      "epoch:  328, loss: 0.008578076027333736\n",
      "epoch:  329, loss: 0.008576720952987671\n",
      "epoch:  330, loss: 0.008575852029025555\n",
      "epoch:  331, loss: 0.008574461564421654\n",
      "epoch:  332, loss: 0.008573763072490692\n",
      "epoch:  333, loss: 0.008572286926209927\n",
      "epoch:  334, loss: 0.008571724407374859\n",
      "epoch:  335, loss: 0.008570178411900997\n",
      "epoch:  336, loss: 0.008569229394197464\n",
      "epoch:  337, loss: 0.008568931370973587\n",
      "epoch:  338, loss: 0.008566235192120075\n",
      "epoch:  339, loss: 0.008565417490899563\n",
      "epoch:  340, loss: 0.008564265444874763\n",
      "epoch:  341, loss: 0.008563540875911713\n",
      "epoch:  342, loss: 0.00856239628046751\n",
      "epoch:  343, loss: 0.008561884984374046\n",
      "epoch:  344, loss: 0.00856064073741436\n",
      "epoch:  345, loss: 0.00856023095548153\n",
      "epoch:  346, loss: 0.008558922447264194\n",
      "epoch:  347, loss: 0.008558647707104683\n",
      "epoch:  348, loss: 0.008557258173823357\n",
      "epoch:  349, loss: 0.008556466549634933\n",
      "epoch:  350, loss: 0.008556089363992214\n",
      "epoch:  351, loss: 0.008554055355489254\n",
      "epoch:  352, loss: 0.008553423918783665\n",
      "epoch:  353, loss: 0.00855251308530569\n",
      "epoch:  354, loss: 0.008552051149308681\n",
      "epoch:  355, loss: 0.008551009930670261\n",
      "epoch:  356, loss: 0.0085507333278656\n",
      "epoch:  357, loss: 0.00854948628693819\n",
      "epoch:  358, loss: 0.008549236692488194\n",
      "epoch:  359, loss: 0.008547995239496231\n",
      "epoch:  360, loss: 0.008547748439013958\n",
      "epoch:  361, loss: 0.008546518161892891\n",
      "epoch:  362, loss: 0.008546313270926476\n",
      "epoch:  363, loss: 0.00854506529867649\n",
      "epoch:  364, loss: 0.00854501686990261\n",
      "epoch:  365, loss: 0.008543683215975761\n",
      "epoch:  366, loss: 0.00854373350739479\n",
      "epoch:  367, loss: 0.008541693910956383\n",
      "epoch:  368, loss: 0.008541292510926723\n",
      "epoch:  369, loss: 0.008540445007383823\n",
      "epoch:  370, loss: 0.008540095761418343\n",
      "epoch:  371, loss: 0.008539198897778988\n",
      "epoch:  372, loss: 0.008538950234651566\n",
      "epoch:  373, loss: 0.008537978865206242\n",
      "epoch:  374, loss: 0.00853781308978796\n",
      "epoch:  375, loss: 0.008536743931472301\n",
      "epoch:  376, loss: 0.00853660050779581\n",
      "epoch:  377, loss: 0.008535554632544518\n",
      "epoch:  378, loss: 0.008535445667803288\n",
      "epoch:  379, loss: 0.008534406311810017\n",
      "epoch:  380, loss: 0.008534458465874195\n",
      "epoch:  381, loss: 0.00853326078504324\n",
      "epoch:  382, loss: 0.00853335577994585\n",
      "epoch:  383, loss: 0.008531585335731506\n",
      "epoch:  384, loss: 0.008531280793249607\n",
      "epoch:  385, loss: 0.008530497550964355\n",
      "epoch:  386, loss: 0.00853014923632145\n",
      "epoch:  387, loss: 0.008529330603778362\n",
      "epoch:  388, loss: 0.008529004640877247\n",
      "epoch:  389, loss: 0.008528199978172779\n",
      "epoch:  390, loss: 0.008527997881174088\n",
      "epoch:  391, loss: 0.008527110330760479\n",
      "epoch:  392, loss: 0.008527105674147606\n",
      "epoch:  393, loss: 0.00852607749402523\n",
      "epoch:  394, loss: 0.008525530807673931\n",
      "epoch:  395, loss: 0.00852650310844183\n",
      "epoch:  396, loss: 0.008524065837264061\n",
      "epoch:  397, loss: 0.008523885160684586\n",
      "epoch:  398, loss: 0.008523098193109035\n",
      "epoch:  399, loss: 0.008523057214915752\n",
      "epoch:  400, loss: 0.008522157557308674\n",
      "epoch:  401, loss: 0.008522290736436844\n",
      "epoch:  402, loss: 0.008521144278347492\n",
      "epoch:  403, loss: 0.008521444164216518\n",
      "epoch:  404, loss: 0.008519712835550308\n",
      "epoch:  405, loss: 0.008520960807800293\n",
      "epoch:  406, loss: 0.008518300019204617\n",
      "epoch:  407, loss: 0.00851825438439846\n",
      "epoch:  408, loss: 0.008517393842339516\n",
      "epoch:  409, loss: 0.008517502807080746\n",
      "epoch:  410, loss: 0.008516457863152027\n",
      "epoch:  411, loss: 0.008516590110957623\n",
      "epoch:  412, loss: 0.008515078574419022\n",
      "epoch:  413, loss: 0.008515453897416592\n",
      "epoch:  414, loss: 0.00851367972791195\n",
      "epoch:  415, loss: 0.008513285778462887\n",
      "epoch:  416, loss: 0.008512592874467373\n",
      "epoch:  417, loss: 0.008512292057275772\n",
      "epoch:  418, loss: 0.008511519990861416\n",
      "epoch:  419, loss: 0.008511347696185112\n",
      "epoch:  420, loss: 0.008510485291481018\n",
      "epoch:  421, loss: 0.008510500192642212\n",
      "epoch:  422, loss: 0.0085094990208745\n",
      "epoch:  423, loss: 0.008509533479809761\n",
      "epoch:  424, loss: 0.008508410304784775\n",
      "epoch:  425, loss: 0.008507834747433662\n",
      "epoch:  426, loss: 0.008508477360010147\n",
      "epoch:  427, loss: 0.00850624404847622\n",
      "epoch:  428, loss: 0.008505929261446\n",
      "epoch:  429, loss: 0.008505194447934628\n",
      "epoch:  430, loss: 0.008504915982484818\n",
      "epoch:  431, loss: 0.008504088968038559\n",
      "epoch:  432, loss: 0.008504005149006844\n",
      "epoch:  433, loss: 0.008503023535013199\n",
      "epoch:  434, loss: 0.008503053337335587\n",
      "epoch:  435, loss: 0.008501846343278885\n",
      "epoch:  436, loss: 0.00850149430334568\n",
      "epoch:  437, loss: 0.008500121533870697\n",
      "epoch:  438, loss: 0.008500562980771065\n",
      "epoch:  439, loss: 0.008498537354171276\n",
      "epoch:  440, loss: 0.00849937554448843\n",
      "epoch:  441, loss: 0.008497324772179127\n",
      "epoch:  442, loss: 0.008497179485857487\n",
      "epoch:  443, loss: 0.008496565744280815\n",
      "epoch:  444, loss: 0.008496571332216263\n",
      "epoch:  445, loss: 0.008495815098285675\n",
      "epoch:  446, loss: 0.008496059104800224\n",
      "epoch:  447, loss: 0.008495025336742401\n",
      "epoch:  448, loss: 0.008497834205627441\n",
      "epoch:  449, loss: 0.008493197150528431\n",
      "epoch:  450, loss: 0.008492778055369854\n",
      "epoch:  451, loss: 0.008494177833199501\n",
      "epoch:  452, loss: 0.008491742424666882\n",
      "epoch:  453, loss: 0.008491754531860352\n",
      "epoch:  454, loss: 0.008491099812090397\n",
      "epoch:  455, loss: 0.00849131029099226\n",
      "epoch:  456, loss: 0.008490370586514473\n",
      "epoch:  457, loss: 0.008490674197673798\n",
      "epoch:  458, loss: 0.00848937127739191\n",
      "epoch:  459, loss: 0.008490337058901787\n",
      "epoch:  460, loss: 0.008488394320011139\n",
      "epoch:  461, loss: 0.00848830584436655\n",
      "epoch:  462, loss: 0.008487781509757042\n",
      "epoch:  463, loss: 0.00848787184804678\n",
      "epoch:  464, loss: 0.008487187325954437\n",
      "epoch:  465, loss: 0.00848749652504921\n",
      "epoch:  466, loss: 0.00848648976534605\n",
      "epoch:  467, loss: 0.00848698616027832\n",
      "epoch:  468, loss: 0.00848553515970707\n",
      "epoch:  469, loss: 0.008486676961183548\n",
      "epoch:  470, loss: 0.00848462525755167\n",
      "epoch:  471, loss: 0.00848462339490652\n",
      "epoch:  472, loss: 0.00848404597491026\n",
      "epoch:  473, loss: 0.0084842499345541\n",
      "epoch:  474, loss: 0.008483505807816982\n",
      "epoch:  475, loss: 0.008483110927045345\n",
      "epoch:  476, loss: 0.008483035489916801\n",
      "epoch:  477, loss: 0.008482525125145912\n",
      "epoch:  478, loss: 0.008482602424919605\n",
      "epoch:  479, loss: 0.008481956087052822\n",
      "epoch:  480, loss: 0.008482251316308975\n",
      "epoch:  481, loss: 0.008481300435960293\n",
      "epoch:  482, loss: 0.00848184060305357\n",
      "epoch:  483, loss: 0.008480382151901722\n",
      "epoch:  484, loss: 0.008481438271701336\n",
      "epoch:  485, loss: 0.008479488082230091\n",
      "epoch:  486, loss: 0.008479489013552666\n",
      "epoch:  487, loss: 0.008478923700749874\n",
      "epoch:  488, loss: 0.008479131385684013\n",
      "epoch:  489, loss: 0.008478378877043724\n",
      "epoch:  490, loss: 0.008477984927594662\n",
      "epoch:  491, loss: 0.008479388430714607\n",
      "epoch:  492, loss: 0.00847705453634262\n",
      "epoch:  493, loss: 0.008477105759084225\n",
      "epoch:  494, loss: 0.008476492948830128\n",
      "epoch:  495, loss: 0.008476841263473034\n",
      "epoch:  496, loss: 0.008475800976157188\n",
      "epoch:  497, loss: 0.008476216346025467\n",
      "epoch:  498, loss: 0.008474862203001976\n",
      "epoch:  499, loss: 0.00847611203789711\n",
      "epoch:  500, loss: 0.008473951369524002\n",
      "epoch:  501, loss: 0.008473880589008331\n",
      "epoch:  502, loss: 0.00847336184233427\n",
      "epoch:  503, loss: 0.008473476395010948\n",
      "epoch:  504, loss: 0.008472802117466927\n",
      "epoch:  505, loss: 0.008472433313727379\n",
      "epoch:  506, loss: 0.008473308756947517\n",
      "epoch:  507, loss: 0.008471514098346233\n",
      "epoch:  508, loss: 0.008472923189401627\n",
      "epoch:  509, loss: 0.008470623753964901\n",
      "epoch:  510, loss: 0.008470732718706131\n",
      "epoch:  511, loss: 0.008470145054161549\n",
      "epoch:  512, loss: 0.008469819091260433\n",
      "epoch:  513, loss: 0.008469758555293083\n",
      "epoch:  514, loss: 0.008469292894005775\n",
      "epoch:  515, loss: 0.008469442836940289\n",
      "epoch:  516, loss: 0.008468791842460632\n",
      "epoch:  517, loss: 0.00846844632178545\n",
      "epoch:  518, loss: 0.008469447493553162\n",
      "epoch:  519, loss: 0.008467601612210274\n",
      "epoch:  520, loss: 0.008468903601169586\n",
      "epoch:  521, loss: 0.008466780185699463\n",
      "epoch:  522, loss: 0.008466861210763454\n",
      "epoch:  523, loss: 0.008466312661767006\n",
      "epoch:  524, loss: 0.008466002531349659\n",
      "epoch:  525, loss: 0.008467230945825577\n",
      "epoch:  526, loss: 0.008465196937322617\n",
      "epoch:  527, loss: 0.008465285412967205\n",
      "epoch:  528, loss: 0.00846473965793848\n",
      "epoch:  529, loss: 0.008465159684419632\n",
      "epoch:  530, loss: 0.008464126847684383\n",
      "epoch:  531, loss: 0.008464481681585312\n",
      "epoch:  532, loss: 0.008463260717689991\n",
      "epoch:  533, loss: 0.008463868871331215\n",
      "epoch:  534, loss: 0.008462388068437576\n",
      "epoch:  535, loss: 0.00846332311630249\n",
      "epoch:  536, loss: 0.008461518213152885\n",
      "epoch:  537, loss: 0.00846282858401537\n",
      "epoch:  538, loss: 0.008460650220513344\n",
      "epoch:  539, loss: 0.008460687473416328\n",
      "epoch:  540, loss: 0.008460151962935925\n",
      "epoch:  541, loss: 0.008460445329546928\n",
      "epoch:  542, loss: 0.008459535427391529\n",
      "epoch:  543, loss: 0.008462846279144287\n",
      "epoch:  544, loss: 0.008458049967885017\n",
      "epoch:  545, loss: 0.008457707241177559\n",
      "epoch:  546, loss: 0.008457682095468044\n",
      "epoch:  547, loss: 0.008457187563180923\n",
      "epoch:  548, loss: 0.008457465097308159\n",
      "epoch:  549, loss: 0.008456559851765633\n",
      "epoch:  550, loss: 0.008459418080747128\n",
      "epoch:  551, loss: 0.008455193601548672\n",
      "epoch:  552, loss: 0.008454869501292706\n",
      "epoch:  553, loss: 0.008456415496766567\n",
      "epoch:  554, loss: 0.008454118855297565\n",
      "epoch:  555, loss: 0.008454292081296444\n",
      "epoch:  556, loss: 0.008453755639493465\n",
      "epoch:  557, loss: 0.008453421294689178\n",
      "epoch:  558, loss: 0.00845341943204403\n",
      "epoch:  559, loss: 0.008452965877950191\n",
      "epoch:  560, loss: 0.008453155867755413\n",
      "epoch:  561, loss: 0.008452538400888443\n",
      "epoch:  562, loss: 0.008452236652374268\n",
      "epoch:  563, loss: 0.008452322334051132\n",
      "epoch:  564, loss: 0.008451870642602444\n",
      "epoch:  565, loss: 0.008451559580862522\n",
      "epoch:  566, loss: 0.008452974259853363\n",
      "epoch:  567, loss: 0.008450787514448166\n",
      "epoch:  568, loss: 0.008450925350189209\n",
      "epoch:  569, loss: 0.008450391702353954\n",
      "epoch:  570, loss: 0.00845007598400116\n",
      "epoch:  571, loss: 0.00845162570476532\n",
      "epoch:  572, loss: 0.008449341170489788\n",
      "epoch:  573, loss: 0.008449525572359562\n",
      "epoch:  574, loss: 0.008448979817330837\n",
      "epoch:  575, loss: 0.008448662236332893\n",
      "epoch:  576, loss: 0.008448637090623379\n",
      "epoch:  577, loss: 0.008448217064142227\n",
      "epoch:  578, loss: 0.008448450826108456\n",
      "epoch:  579, loss: 0.008447813801467419\n",
      "epoch:  580, loss: 0.00844749715179205\n",
      "epoch:  581, loss: 0.00844891369342804\n",
      "epoch:  582, loss: 0.008446757681667805\n",
      "epoch:  583, loss: 0.008448331616818905\n",
      "epoch:  584, loss: 0.008446049876511097\n",
      "epoch:  585, loss: 0.008446224965155125\n",
      "epoch:  586, loss: 0.008445686660706997\n",
      "epoch:  587, loss: 0.00844535417854786\n",
      "epoch:  588, loss: 0.00844534020870924\n",
      "epoch:  589, loss: 0.008444905281066895\n",
      "epoch:  590, loss: 0.008445043116807938\n",
      "epoch:  591, loss: 0.008444472216069698\n",
      "epoch:  592, loss: 0.008444185368716717\n",
      "epoch:  593, loss: 0.008444199338555336\n",
      "epoch:  594, loss: 0.008443793281912804\n",
      "epoch:  595, loss: 0.008444179780781269\n",
      "epoch:  596, loss: 0.008443244732916355\n",
      "epoch:  597, loss: 0.008446235209703445\n",
      "epoch:  598, loss: 0.008442065678536892\n",
      "epoch:  599, loss: 0.008441758342087269\n",
      "epoch:  600, loss: 0.00844182912260294\n",
      "epoch:  601, loss: 0.008441351354122162\n",
      "epoch:  602, loss: 0.00844159908592701\n",
      "epoch:  603, loss: 0.008440952748060226\n",
      "epoch:  604, loss: 0.008440659381449223\n",
      "epoch:  605, loss: 0.008440772071480751\n",
      "epoch:  606, loss: 0.008440302684903145\n",
      "epoch:  607, loss: 0.008440002799034119\n",
      "epoch:  608, loss: 0.008441571146249771\n",
      "epoch:  609, loss: 0.008439316414296627\n",
      "epoch:  610, loss: 0.008439499884843826\n",
      "epoch:  611, loss: 0.00843896809965372\n",
      "epoch:  612, loss: 0.0084386533126235\n",
      "epoch:  613, loss: 0.008438658900558949\n",
      "epoch:  614, loss: 0.008438225835561752\n",
      "epoch:  615, loss: 0.008438428863883018\n",
      "epoch:  616, loss: 0.008437825366854668\n",
      "epoch:  617, loss: 0.008437516167759895\n",
      "epoch:  618, loss: 0.008437592536211014\n",
      "epoch:  619, loss: 0.008437149226665497\n",
      "epoch:  620, loss: 0.00843687541782856\n",
      "epoch:  621, loss: 0.008438315242528915\n",
      "epoch:  622, loss: 0.008436174131929874\n",
      "epoch:  623, loss: 0.008436335250735283\n",
      "epoch:  624, loss: 0.008435831405222416\n",
      "epoch:  625, loss: 0.008435552939772606\n",
      "epoch:  626, loss: 0.008435570634901524\n",
      "epoch:  627, loss: 0.008435158990323544\n",
      "epoch:  628, loss: 0.008435548283159733\n",
      "epoch:  629, loss: 0.00843462347984314\n",
      "epoch:  630, loss: 0.00843731127679348\n",
      "epoch:  631, loss: 0.008433437906205654\n",
      "epoch:  632, loss: 0.00843376200646162\n",
      "epoch:  633, loss: 0.008432909846305847\n",
      "epoch:  634, loss: 0.008433560840785503\n",
      "epoch:  635, loss: 0.00843222625553608\n",
      "epoch:  636, loss: 0.008433053269982338\n",
      "epoch:  637, loss: 0.008431532420217991\n",
      "epoch:  638, loss: 0.008433172479271889\n",
      "epoch:  639, loss: 0.008430861867964268\n",
      "epoch:  640, loss: 0.008431078866124153\n",
      "epoch:  641, loss: 0.008430524729192257\n",
      "epoch:  642, loss: 0.00843024905771017\n",
      "epoch:  643, loss: 0.008430445566773415\n",
      "epoch:  644, loss: 0.008429928682744503\n",
      "epoch:  645, loss: 0.008429622277617455\n",
      "epoch:  646, loss: 0.008429734967648983\n",
      "epoch:  647, loss: 0.008429242298007011\n",
      "epoch:  648, loss: 0.00842897780239582\n",
      "epoch:  649, loss: 0.008429070934653282\n",
      "epoch:  650, loss: 0.008428618311882019\n",
      "epoch:  651, loss: 0.00842835009098053\n",
      "epoch:  652, loss: 0.00842839851975441\n",
      "epoch:  653, loss: 0.008427959866821766\n",
      "epoch:  654, loss: 0.00842837430536747\n",
      "epoch:  655, loss: 0.008427435532212257\n",
      "epoch:  656, loss: 0.008428064174950123\n",
      "epoch:  657, loss: 0.008426746353507042\n",
      "epoch:  658, loss: 0.00842758547514677\n",
      "epoch:  659, loss: 0.008426054380834103\n",
      "epoch:  660, loss: 0.008427523076534271\n",
      "epoch:  661, loss: 0.008425375446677208\n",
      "epoch:  662, loss: 0.008425473235547543\n",
      "epoch:  663, loss: 0.008425003848969936\n",
      "epoch:  664, loss: 0.008424735628068447\n",
      "epoch:  665, loss: 0.00842486321926117\n",
      "epoch:  666, loss: 0.008424405008554459\n",
      "epoch:  667, loss: 0.0084241246804595\n",
      "epoch:  668, loss: 0.008424192667007446\n",
      "epoch:  669, loss: 0.008423758670687675\n",
      "epoch:  670, loss: 0.00842347927391529\n",
      "epoch:  671, loss: 0.00842494610697031\n",
      "epoch:  672, loss: 0.008422767743468285\n",
      "epoch:  673, loss: 0.008422785438597202\n",
      "epoch:  674, loss: 0.008422387763857841\n",
      "epoch:  675, loss: 0.008422140032052994\n",
      "epoch:  676, loss: 0.008422242477536201\n",
      "epoch:  677, loss: 0.008421815931797028\n",
      "epoch:  678, loss: 0.008421527221798897\n",
      "epoch:  679, loss: 0.008421704173088074\n",
      "epoch:  680, loss: 0.008421187289059162\n",
      "epoch:  681, loss: 0.008420898579061031\n",
      "epoch:  682, loss: 0.008421213366091251\n",
      "epoch:  683, loss: 0.008420409634709358\n",
      "epoch:  684, loss: 0.008422217331826687\n",
      "epoch:  685, loss: 0.008419173769652843\n",
      "epoch:  686, loss: 0.008419475518167019\n",
      "epoch:  687, loss: 0.008418671786785126\n",
      "epoch:  688, loss: 0.008421343751251698\n",
      "epoch:  689, loss: 0.008417523466050625\n",
      "epoch:  690, loss: 0.00841794814914465\n",
      "epoch:  691, loss: 0.008416949771344662\n",
      "epoch:  692, loss: 0.008419820107519627\n",
      "epoch:  693, loss: 0.008415796793997288\n",
      "epoch:  694, loss: 0.00841615628451109\n",
      "epoch:  695, loss: 0.008415243588387966\n",
      "epoch:  696, loss: 0.008417210541665554\n",
      "epoch:  697, loss: 0.008414058946073055\n",
      "epoch:  698, loss: 0.008414393290877342\n",
      "epoch:  699, loss: 0.008413532748818398\n",
      "epoch:  700, loss: 0.00841465499252081\n",
      "epoch:  701, loss: 0.00841231644153595\n",
      "epoch:  702, loss: 0.008412432856857777\n",
      "epoch:  703, loss: 0.008411969989538193\n",
      "epoch:  704, loss: 0.008411714807152748\n",
      "epoch:  705, loss: 0.008412010967731476\n",
      "epoch:  706, loss: 0.008411221206188202\n",
      "epoch:  707, loss: 0.008415311574935913\n",
      "epoch:  708, loss: 0.0084101976826787\n",
      "epoch:  709, loss: 0.008409863337874413\n",
      "epoch:  710, loss: 0.008410017006099224\n",
      "epoch:  711, loss: 0.008409497328102589\n",
      "epoch:  712, loss: 0.008409252390265465\n",
      "epoch:  713, loss: 0.008409425616264343\n",
      "epoch:  714, loss: 0.008408929221332073\n",
      "epoch:  715, loss: 0.008408628404140472\n",
      "epoch:  716, loss: 0.00840896274894476\n",
      "epoch:  717, loss: 0.008408157154917717\n",
      "epoch:  718, loss: 0.00841087568551302\n",
      "epoch:  719, loss: 0.008407019078731537\n",
      "epoch:  720, loss: 0.008406760171055794\n",
      "epoch:  721, loss: 0.00840690452605486\n",
      "epoch:  722, loss: 0.008406420238316059\n",
      "epoch:  723, loss: 0.00840616226196289\n",
      "epoch:  724, loss: 0.008406445384025574\n",
      "epoch:  725, loss: 0.008405671454966068\n",
      "epoch:  726, loss: 0.008408309891819954\n",
      "epoch:  727, loss: 0.008404538035392761\n",
      "epoch:  728, loss: 0.008405039086937904\n",
      "epoch:  729, loss: 0.008403991349041462\n",
      "epoch:  730, loss: 0.008404660038650036\n",
      "epoch:  731, loss: 0.008403323590755463\n",
      "epoch:  732, loss: 0.008404463529586792\n",
      "epoch:  733, loss: 0.008402622304856777\n",
      "epoch:  734, loss: 0.008404157124459743\n",
      "epoch:  735, loss: 0.008401960134506226\n",
      "epoch:  736, loss: 0.008402126841247082\n",
      "epoch:  737, loss: 0.008401649072766304\n",
      "epoch:  738, loss: 0.008401365019381046\n",
      "epoch:  739, loss: 0.008401738479733467\n",
      "epoch:  740, loss: 0.008400882594287395\n",
      "epoch:  741, loss: 0.008404280059039593\n",
      "epoch:  742, loss: 0.008399806916713715\n",
      "epoch:  743, loss: 0.008399512618780136\n",
      "epoch:  744, loss: 0.008399632759392262\n",
      "epoch:  745, loss: 0.008399157784879208\n",
      "epoch:  746, loss: 0.008398874662816525\n",
      "epoch:  747, loss: 0.008399102836847305\n",
      "epoch:  748, loss: 0.008398402482271194\n",
      "epoch:  749, loss: 0.008400661870837212\n",
      "epoch:  750, loss: 0.008397266268730164\n",
      "epoch:  751, loss: 0.008397651836276054\n",
      "epoch:  752, loss: 0.008396711200475693\n",
      "epoch:  753, loss: 0.008397399447858334\n",
      "epoch:  754, loss: 0.00839548371732235\n",
      "epoch:  755, loss: 0.008396596647799015\n",
      "epoch:  756, loss: 0.008394775912165642\n",
      "epoch:  757, loss: 0.008396296761929989\n",
      "epoch:  758, loss: 0.008394100703299046\n",
      "epoch:  759, loss: 0.008394280448555946\n",
      "epoch:  760, loss: 0.008393790572881699\n",
      "epoch:  761, loss: 0.008393527008593082\n",
      "epoch:  762, loss: 0.008393783122301102\n",
      "epoch:  763, loss: 0.008393039926886559\n",
      "epoch:  764, loss: 0.008395702578127384\n",
      "epoch:  765, loss: 0.008391937240958214\n",
      "epoch:  766, loss: 0.008392292074859142\n",
      "epoch:  767, loss: 0.008391398936510086\n",
      "epoch:  768, loss: 0.008393649943172932\n",
      "epoch:  769, loss: 0.008390236645936966\n",
      "epoch:  770, loss: 0.008390543051064014\n",
      "epoch:  771, loss: 0.008389720693230629\n",
      "epoch:  772, loss: 0.008393391035497189\n",
      "epoch:  773, loss: 0.008388650603592396\n",
      "epoch:  774, loss: 0.00838908925652504\n",
      "epoch:  775, loss: 0.008388069458305836\n",
      "epoch:  776, loss: 0.008391281589865685\n",
      "epoch:  777, loss: 0.00838697049766779\n",
      "epoch:  778, loss: 0.008386695757508278\n",
      "epoch:  779, loss: 0.008386760018765926\n",
      "epoch:  780, loss: 0.008386320434510708\n",
      "epoch:  781, loss: 0.00838676281273365\n",
      "epoch:  782, loss: 0.008385809138417244\n",
      "epoch:  783, loss: 0.008386611938476562\n",
      "epoch:  784, loss: 0.008385144174098969\n",
      "epoch:  785, loss: 0.008386323228478432\n",
      "epoch:  786, loss: 0.008384495042264462\n",
      "epoch:  787, loss: 0.008384555578231812\n",
      "epoch:  788, loss: 0.00838414765894413\n",
      "epoch:  789, loss: 0.008383884094655514\n",
      "epoch:  790, loss: 0.008383989334106445\n",
      "epoch:  791, loss: 0.008383544161915779\n",
      "epoch:  792, loss: 0.008383288979530334\n",
      "epoch:  793, loss: 0.008383535780012608\n",
      "epoch:  794, loss: 0.00838302168995142\n",
      "epoch:  795, loss: 0.008382733911275864\n",
      "epoch:  796, loss: 0.008383034728467464\n",
      "epoch:  797, loss: 0.008382265456020832\n",
      "epoch:  798, loss: 0.0083850072696805\n",
      "epoch:  799, loss: 0.008381185121834278\n",
      "epoch:  800, loss: 0.008381682448089123\n",
      "epoch:  801, loss: 0.008380674757063389\n",
      "epoch:  802, loss: 0.008381415158510208\n",
      "epoch:  803, loss: 0.008380034938454628\n",
      "epoch:  804, loss: 0.008380860090255737\n",
      "epoch:  805, loss: 0.008379383012652397\n",
      "epoch:  806, loss: 0.008380785584449768\n",
      "epoch:  807, loss: 0.008378759026527405\n",
      "epoch:  808, loss: 0.00837885681539774\n",
      "epoch:  809, loss: 0.008378430269658566\n",
      "epoch:  810, loss: 0.008378187194466591\n",
      "epoch:  811, loss: 0.008378282189369202\n",
      "epoch:  812, loss: 0.008377893827855587\n",
      "epoch:  813, loss: 0.0083776181563735\n",
      "epoch:  814, loss: 0.008377821184694767\n",
      "epoch:  815, loss: 0.008377308957278728\n",
      "epoch:  816, loss: 0.00837703701108694\n",
      "epoch:  817, loss: 0.008377209305763245\n",
      "epoch:  818, loss: 0.008376756682991982\n",
      "epoch:  819, loss: 0.008376476354897022\n",
      "epoch:  820, loss: 0.00837663933634758\n",
      "epoch:  821, loss: 0.008376151323318481\n",
      "epoch:  822, loss: 0.008375892415642738\n",
      "epoch:  823, loss: 0.008376003243029118\n",
      "epoch:  824, loss: 0.008375597186386585\n",
      "epoch:  825, loss: 0.008375316858291626\n",
      "epoch:  826, loss: 0.00837542675435543\n",
      "epoch:  827, loss: 0.008374983444809914\n",
      "epoch:  828, loss: 0.008374733850359917\n",
      "epoch:  829, loss: 0.008374891243875027\n",
      "epoch:  830, loss: 0.008374447003006935\n",
      "epoch:  831, loss: 0.008374175056815147\n",
      "epoch:  832, loss: 0.008374332450330257\n",
      "epoch:  833, loss: 0.00837385281920433\n",
      "epoch:  834, loss: 0.008373593911528587\n",
      "epoch:  835, loss: 0.00837368331849575\n",
      "epoch:  836, loss: 0.008373263292014599\n",
      "epoch:  837, loss: 0.008373011834919453\n",
      "epoch:  838, loss: 0.008373094722628593\n",
      "epoch:  839, loss: 0.008372677490115166\n",
      "epoch:  840, loss: 0.008373204618692398\n",
      "epoch:  841, loss: 0.008372189477086067\n",
      "epoch:  842, loss: 0.008376105688512325\n",
      "epoch:  843, loss: 0.008371173404157162\n",
      "epoch:  844, loss: 0.0083709005266428\n",
      "epoch:  845, loss: 0.008371120318770409\n",
      "epoch:  846, loss: 0.008370633237063885\n",
      "epoch:  847, loss: 0.008370338939130306\n",
      "epoch:  848, loss: 0.008370527066290379\n",
      "epoch:  849, loss: 0.008370035327970982\n",
      "epoch:  850, loss: 0.00836975872516632\n",
      "epoch:  851, loss: 0.008369973860681057\n",
      "epoch:  852, loss: 0.008369470946490765\n",
      "epoch:  853, loss: 0.008369172923266888\n",
      "epoch:  854, loss: 0.00836937315762043\n",
      "epoch:  855, loss: 0.008368898183107376\n",
      "epoch:  856, loss: 0.008368599228560925\n",
      "epoch:  857, loss: 0.00836886279284954\n",
      "epoch:  858, loss: 0.00836813636124134\n",
      "epoch:  859, loss: 0.008370335213840008\n",
      "epoch:  860, loss: 0.008367029018700123\n",
      "epoch:  861, loss: 0.008367408066987991\n",
      "epoch:  862, loss: 0.008366546593606472\n",
      "epoch:  863, loss: 0.0083686001598835\n",
      "epoch:  864, loss: 0.008365423418581486\n",
      "epoch:  865, loss: 0.008365695364773273\n",
      "epoch:  866, loss: 0.008364950306713581\n",
      "epoch:  867, loss: 0.008367495611310005\n",
      "epoch:  868, loss: 0.008363853208720684\n",
      "epoch:  869, loss: 0.008364268578588963\n",
      "epoch:  870, loss: 0.008363332599401474\n",
      "epoch:  871, loss: 0.008367211557924747\n",
      "epoch:  872, loss: 0.008362304419279099\n",
      "epoch:  873, loss: 0.008361990563571453\n",
      "epoch:  874, loss: 0.008362110704183578\n",
      "epoch:  875, loss: 0.008361647836863995\n",
      "epoch:  876, loss: 0.008361405692994595\n",
      "epoch:  877, loss: 0.00836167111992836\n",
      "epoch:  878, loss: 0.008360927924513817\n",
      "epoch:  879, loss: 0.008361735381186008\n",
      "epoch:  880, loss: 0.008359748870134354\n",
      "epoch:  881, loss: 0.008361364714801311\n",
      "epoch:  882, loss: 0.008359089493751526\n",
      "epoch:  883, loss: 0.008359266445040703\n",
      "epoch:  884, loss: 0.008358796127140522\n",
      "epoch:  885, loss: 0.008358527906239033\n",
      "epoch:  886, loss: 0.008358931168913841\n",
      "epoch:  887, loss: 0.008358054794371128\n",
      "epoch:  888, loss: 0.008361835032701492\n",
      "epoch:  889, loss: 0.008357010781764984\n",
      "epoch:  890, loss: 0.008356723934412003\n",
      "epoch:  891, loss: 0.008356972597539425\n",
      "epoch:  892, loss: 0.008356263861060143\n",
      "epoch:  893, loss: 0.008358168415725231\n",
      "epoch:  894, loss: 0.00835515558719635\n",
      "epoch:  895, loss: 0.00835538562387228\n",
      "epoch:  896, loss: 0.00835465732961893\n",
      "epoch:  897, loss: 0.008358263410627842\n",
      "epoch:  898, loss: 0.008353667333722115\n",
      "epoch:  899, loss: 0.008353350684046745\n",
      "epoch:  900, loss: 0.00835485477000475\n",
      "epoch:  901, loss: 0.008352666161954403\n",
      "epoch:  902, loss: 0.00835292600095272\n",
      "epoch:  903, loss: 0.008352207951247692\n",
      "epoch:  904, loss: 0.008354454301297665\n",
      "epoch:  905, loss: 0.008351137861609459\n",
      "epoch:  906, loss: 0.00835163239389658\n",
      "epoch:  907, loss: 0.00835060141980648\n",
      "epoch:  908, loss: 0.008352207951247692\n",
      "epoch:  909, loss: 0.008349435403943062\n",
      "epoch:  910, loss: 0.008349810726940632\n",
      "epoch:  911, loss: 0.00834899302572012\n",
      "epoch:  912, loss: 0.008352788165211678\n",
      "epoch:  913, loss: 0.008347994647920132\n",
      "epoch:  914, loss: 0.00834768544882536\n",
      "epoch:  915, loss: 0.00834781862795353\n",
      "epoch:  916, loss: 0.008347321301698685\n",
      "epoch:  917, loss: 0.008347074501216412\n",
      "epoch:  918, loss: 0.008347325958311558\n",
      "epoch:  919, loss: 0.008346593007445335\n",
      "epoch:  920, loss: 0.008348268456757069\n",
      "epoch:  921, loss: 0.008345460519194603\n",
      "epoch:  922, loss: 0.008345672860741615\n",
      "epoch:  923, loss: 0.00834497157484293\n",
      "epoch:  924, loss: 0.008345522917807102\n",
      "epoch:  925, loss: 0.008343768306076527\n",
      "epoch:  926, loss: 0.008345121517777443\n",
      "epoch:  927, loss: 0.008343106135725975\n",
      "epoch:  928, loss: 0.008343216963112354\n",
      "epoch:  929, loss: 0.008342809043824673\n",
      "epoch:  930, loss: 0.008342569693922997\n",
      "epoch:  931, loss: 0.008343025110661983\n",
      "epoch:  932, loss: 0.008342096582055092\n",
      "epoch:  933, loss: 0.008345583453774452\n",
      "epoch:  934, loss: 0.008341050706803799\n",
      "epoch:  935, loss: 0.00834075827151537\n",
      "epoch:  936, loss: 0.008341003209352493\n",
      "epoch:  937, loss: 0.008340318687260151\n",
      "epoch:  938, loss: 0.008342970162630081\n",
      "epoch:  939, loss: 0.008339270949363708\n",
      "epoch:  940, loss: 0.008338992483913898\n",
      "epoch:  941, loss: 0.008339093998074532\n",
      "epoch:  942, loss: 0.008338642306625843\n",
      "epoch:  943, loss: 0.00833841785788536\n",
      "epoch:  944, loss: 0.008338678628206253\n",
      "epoch:  945, loss: 0.008337927050888538\n",
      "epoch:  946, loss: 0.008338923566043377\n",
      "epoch:  947, loss: 0.008336752653121948\n",
      "epoch:  948, loss: 0.008336951024830341\n",
      "epoch:  949, loss: 0.008336484432220459\n",
      "epoch:  950, loss: 0.0083362041041255\n",
      "epoch:  951, loss: 0.00833673682063818\n",
      "epoch:  952, loss: 0.0083357198163867\n",
      "epoch:  953, loss: 0.008336378261446953\n",
      "epoch:  954, loss: 0.008335070684552193\n",
      "epoch:  955, loss: 0.008336024358868599\n",
      "epoch:  956, loss: 0.008334402926266193\n",
      "epoch:  957, loss: 0.00833606906235218\n",
      "epoch:  958, loss: 0.008333785459399223\n",
      "epoch:  959, loss: 0.00833400059491396\n",
      "epoch:  960, loss: 0.008333305828273296\n",
      "epoch:  961, loss: 0.008334638550877571\n",
      "epoch:  962, loss: 0.008332153782248497\n",
      "epoch:  963, loss: 0.00833236426115036\n",
      "epoch:  964, loss: 0.008331673219799995\n",
      "epoch:  965, loss: 0.008332572877407074\n",
      "epoch:  966, loss: 0.008330496028065681\n",
      "epoch:  967, loss: 0.008330618031322956\n",
      "epoch:  968, loss: 0.00833017285913229\n",
      "epoch:  969, loss: 0.008329891599714756\n",
      "epoch:  970, loss: 0.008330031298100948\n",
      "epoch:  971, loss: 0.008329586125910282\n",
      "epoch:  972, loss: 0.008329307660460472\n",
      "epoch:  973, loss: 0.008329714648425579\n",
      "epoch:  974, loss: 0.00832882896065712\n",
      "epoch:  975, loss: 0.008331883698701859\n",
      "epoch:  976, loss: 0.00832771509885788\n",
      "epoch:  977, loss: 0.008327453397214413\n",
      "epoch:  978, loss: 0.008327535353600979\n",
      "epoch:  979, loss: 0.008327114395797253\n",
      "epoch:  980, loss: 0.008326854556798935\n",
      "epoch:  981, loss: 0.008327084593474865\n",
      "epoch:  982, loss: 0.008326380513608456\n",
      "epoch:  983, loss: 0.008328473195433617\n",
      "epoch:  984, loss: 0.008325250819325447\n",
      "epoch:  985, loss: 0.008325621485710144\n",
      "epoch:  986, loss: 0.008324711583554745\n",
      "epoch:  987, loss: 0.00832798145711422\n",
      "epoch:  988, loss: 0.00832362286746502\n",
      "epoch:  989, loss: 0.008323339745402336\n",
      "epoch:  990, loss: 0.008323578163981438\n",
      "epoch:  991, loss: 0.008323035202920437\n",
      "epoch:  992, loss: 0.008322789333760738\n",
      "epoch:  993, loss: 0.008323078975081444\n",
      "epoch:  994, loss: 0.008322260342538357\n",
      "epoch:  995, loss: 0.008323644287884235\n",
      "epoch:  996, loss: 0.008321059867739677\n",
      "epoch:  997, loss: 0.008321261033415794\n",
      "epoch:  998, loss: 0.008320567198097706\n",
      "epoch:  999, loss: 0.008322235196828842\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size=X.shape[1], device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "opt = torch_numopt.GradientDescentLS(model=model, lr=1, line_search_method=\"bisect\")\n",
    "\n",
    "all_loss = {}\n",
    "for epoch in range(1000):\n",
    "    print(\"epoch: \", epoch, end=\"\")\n",
    "    all_loss[epoch + 1] = 0\n",
    "    for batch_idx, (b_x, b_y) in enumerate(data_loader):\n",
    "        pre = model(b_x)\n",
    "        loss = loss_fn(pre, b_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # parameter update step based on optimizer\n",
    "        opt.step(b_x, b_y, loss_fn)\n",
    "\n",
    "        all_loss[epoch + 1] += loss\n",
    "    all_loss[epoch + 1] /= len(data_loader)\n",
    "    print(\", loss: {}\".format(all_loss[epoch + 1].cpu().detach().numpy().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics: R2 = 0.6895205934928381\n",
      "Test metrics:  R2 = 0.6834542865074851\n"
     ]
    }
   ],
   "source": [
    "pred_train = model.forward(X_train).cpu().detach()\n",
    "pred_test = model.forward(X_test).cpu().detach()\n",
    "print(f\"Train metrics: R2 = {r2_score(pred_train, y_train.cpu())}\")\n",
    "print(f\"Test metrics:  R2 = {r2_score(pred_test, y_test.cpu())}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_numopt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
